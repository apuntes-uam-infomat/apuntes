\section{Sistema lineales de ecuaciones diferenciales}
Los sistemas generales de ecuaciones diferenciales son de la forma

\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     x_1^\prime = F_1(t, x_1, \hdots, x_N)\\
     x_2^\prime = F_2(t, x_1, \hdots, x_N)\\
     \vdots\\
     x_n^\prime = F_n(t, x_1, \hdots, x_N)\\
  \end{array}
  \right.
\end{equation*}

Otra forma de escribir el sistema es en forma vectorial, es decir, si tomamos
\begin{center}
$\vec{X} = \begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{pmatrix}$ $\vec{X^\prime} = \begin{pmatrix}
x_1^\prime\\
x_2^\prime\\
\vdots\\
x_n^\prime\\
\end{pmatrix}$ $\vec{F} = \begin{pmatrix}
F_1\\
F_2\\
\vdots\\
F_N\\
\end{pmatrix}$
\end{center}
tenemos que el sistema lineal es $$\vec{X}^\prime = \vec{F}(t, \vec{x})$$

En esta sección trabajaremos con sistemas lineales, es decir, aquellos en los que las funciones son lineales. Por tanto, estos sistemas tendrán la forma
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     x_1^\prime = a_{11}(t)x_1+a_{12}(t)x_2+\hdots+a_{1N}(t)x_N+b_1(t)\\
     x_2^\prime = a_{21}(t)x_1+a_{22}(t)x_2+\hdots+a_{2N}(t)x_N+b_2(t)\\
     \vdots\\
     x_N^\prime = a_{N1}(t)x_1+a_{N2}(t)x_2+\hdots+a_{NN}(t)x_N+b_N(t)\\
  \end{array}
  \right.
\end{equation*}

En forma matricial
$$\vec{X}^\prime = A(t)\vec{X}+B(t)$$ donde $A$ es una matriz y $B$ es un vector.

Para los sistemas lineales de ecuaciones diferenciales disponemos de un teorema de existencia y unicidad que se presentará a continuación pero no se va a demostrar.

\begin{theorem}[de existencia y unicidad global]
Dados
\begin{itemize}
\item $a_{ij}(t)$ continuos en $(a,b)$
\item $b_j(t)$ continuos en $(a,b)$

El problema $
  \left\lbrace
  \begin{array}{l}
     X^\prime = A(t)X+B(t)\\
     X(t_0) = X_0\ (t_0\in(a,b))\\
  \end{array}
  \right.
$
tiene una \textbf{única} solución definida en todo el intervalo $(a,b)$. Por tanto tenemos \textbf{existencia} y \textbf{unicidad} global.
\end{itemize}
\end{theorem}

\subsection{Sistemas homogéneos}
\begin{definition}\name{Sistema lineal homogéneo}
Un sistema lineal homogéneo de ecuaciones diferenciales es aquel de la forma $$X^\prime(t) = A(t)X(t)$$ Es decir, $B(t) = 0$.
\end{definition}
\noindent\textbf{Propiedades}
\begin{itemize}
\item Si $x(t)$ es solución $\implies \lambda x(t)$ es solución $\forall \lambda \in \R$
\item Si $x(t), y(t)$ son soluciones $\implies x(t)+y(t)$ es solución
\end{itemize}

\obs

Dado un sistema lineal homogéneo de la forma $$X^\prime(t) = A(t)X(t)$$ si tenemos que $X_i(t)$ es solución, con el dato $$X_i(t_0) = \vec{e}_i = \begin{pmatrix}
0\\
\vdots\\
1\\
\vdots\\
0\\
\end{pmatrix} \leftarrow \textbf{posición } i$$

\noindent entonces, dado un dato general de la forma $$X(t_0) = x_0 = x_1^0\vec{e}_1 + x_2^0\vec{e}_2+\hdots+x_N^0\vec{e}_N = \begin{pmatrix}
x_1^0\\
x_2^0\\
\vdots\\
x_N^0\\
\end{pmatrix}$$
si tomamos $$x(t) = x_1^0X_1(t)+x_2^0X_2(t)+\hdots +x_N^0X_N(t)$$ $x(t)$ es solución, ya que es combinación lineal de soluciones:
$$x(t_0) = x_1^0X_1(t_0)+x_2^0X_2(t_0)+\hdots+x_N^0X_N(t_0) = x_1^0e_1+x_2^0e_2\hdots+x_N^0e_N = x_0$$
tenemos entonces que $\set{X_1(t),\hdots,X_N(t)}$ genera todas las soluciones, es decir, forma un sistema generador.

\subsubsection{Independencia lineal}
Vamos a ver cómo en la observación anterior, el conjunto de las soluciones no sólo forma un sistema generador sino una base, es decir, que el espacio de soluciones forma un \textbf{espacio vectorial} de dimensión $n$.

Sabemos que
$Y_1(t), \dotsc, Y_n(t) $ son linealmente independientes $ \iff \alpha_1 Y_1(t)+\hdots+\alpha_n Y_n(t) = 0\ \forall t \in (a, b) \iff \alpha_i = 0\; ∀ i ∈ [1,N]$

\begin{definition}\name{Wronskiano}
Dadas $n$ funciones $X_j(t) \in ℝ^n, j=1,\dotsc,n$ el determinante \textbf{Wronskiano} se define como
$$W(X_1, \hdots, X_n) = \det
\begin{pmatrix}
X_1 & X_2 & \hdots & X_n\\
\downarrow & \downarrow & & \downarrow \\
\end{pmatrix}$$
Si $W(X_1, \hdots, X_n) = 0$, entonces $X_1, \hdots, X_n$ son linealmente dependientes, en caso contrario, tenemos que son linealmente independientes.
\end{definition}

\begin{theorem}
Sean $Y_1(t), Y_2(t), \hdots, Y_n(t)$ soluciones de un sistema homogéneo con coeficientes continuos en un intervalo $(a,b)$, entonces existen dos posibilidades
\begin{itemize}
\item $W(Y_1, \hdots, Y_n)(t) = 0\ \forall t \in (a, b)$
\item $W(Y_1, \hdots, Y_n)(t) \neq 0\ \forall t \in (a, b)$
\end{itemize}
\end{theorem}

\begin{proof}
Supongamos que $W(Y_1(t_0), \hdots, Y_n(t_0)) = 0$ para un cierto $t_0 \in (a,b)$. Tenemos entonces que $Y_1(t_0), \hdots, Y_n(t_0)$ son linealmente dependientes, por tanto $\exists \alpha_1,\hdots,\alpha_n$ no todos nulos tal que $\alpha_1Y_1(t_0)+\hdots+\alpha_nY_n(t_0) = 0$.

Definimos $y(t) = \alpha_1Y_1(t)+\hdots+\alpha_nY_n(t)$, que es solución de $$Y^\prime(t) = A(t)Y(t)$$ y tenemos que $y(t_0) = 0$.

Por tanto tenemos que $y(t)$ es solución del problema
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     Y^\prime(t) = A(t)Y(t)\\
     y(t_0) = 0
  \end{array}
  \right.
\end{equation*}
si tomamos el vector $z(t) = \begin{pmatrix}
0\\\vdots\\0
\end{pmatrix}$, como el sistema es homogéneo, tenemos que $z(t)$ también es solución del \textbf{mismo problema}. Por el teorema anteriormente enunciado, tenemos existencia y unicidad y $z(t) = y(t) = \vec{0}\ \forall t \in (a,b)$

Como $Y(t) = \vec{0}$, tenemos que $W(Y_1,\hdots,Y_N)(t) = 0\ \forall t\in (a,b)$
\end{proof}

Como conclusión de la demostración anterior, podemos decir que el espacio de soluciones de un sistema homogéneo forma un espacio vectorial de dimensión $n$

\vspace{5mm}
\obs

Si disponemos de un sistema no homogéneo $$X^\prime(t) = A(t)X+B(t)$$
con solución general $X_G(t)$, tenemos que $$X^\prime(t) = A(t)X$$ es el sistema homogéneo asociado con solución general $$X_H(t) = \alpha_1X_1(t)+\hdots+\alpha_nX_n(t)$$

\begin{itemize}
\item Sea $X_P$ una solución particular del sistema completo (no homogéneo).

\item Sea $Y(t) = X_G(t)-X_P(t)$.
\end{itemize}
\noindent Derivando $Y^\prime = X_G^\prime -X_P^\prime = (AX_G+B)-(AX_P+B) = A(X_G-X_P) = AY$.
Vemos que $Y(t)$ es solución del sistema homogéneo asociado al sistema completo, y por tanto $Y(t) = \alpha_1X_1(t)+\hdots+\alpha_nX_n(t)$ para alguna familia $\alpha_1,\hdots,\alpha_n$.

De aquí deducimos que $$X_G = X_P+X_H$$
donde
\begin{itemize}
\item $X_G$ es la solución general del sistema completo.

\item $X_P$ es una solución particular del sistema completo.

\item $X_H$ es la solución general del sistema homogéneo asociado.
\end{itemize}

\subsubsection{Sistemas lineales homogéneos con coeficientes constantes}
Vamos a estudiar los sistemas homogéneos de la forma $$X^\prime(t) = AX(t)$$
donde ahora la matriz $A$ es una matriz de constantes, no de funciones.

Para resolver este tipo de sistemas vamos a presentar dos formas de resolución, pero antes definiremos lo que se conoce como \textbf{exponencial de una matriz}:

\begin{definition}\name{Exponencial de una matriz}

Dada una matriz $A_n$, la exponencial de la matriz $A_n$ se define como sigue
$$\exp(A) = I_n + A + \frac{A^2}{2!} + \hdots + \frac{A^n}{n!} + \hdots$$

\end{definition}

\vspace{5mm}
\noindent\textbf{Método 1}

Las soluciones de los sistemas lineales homogéneos con coeficientes constantes son de la forma $$x(t) = e^{\alpha t}\vec{v}$$
derivando obtenemos $$x^\prime(t) = \alpha e^{\alpha t}\vec{v}$$ por lo que a partir del sistema $X^\prime(t) = AX(t)$, como $x(t)$ es solución, obtenemos que $$x^\prime(t) = e^{\alpha t}\alpha\vec{v} = e^{\alpha t}A\vec{v} = AX$$

Necesitamos entonces que $A\vec{v} = \alpha\vec{v}$, es decir, que $\alpha$ sea autovalor y $\vec{v}$ autovector de $A$.

\vspace{5mm}
\noindent\textbf{Método 2}

Dado un sistema lineal homogéneo con coeficientes constantes, las soluciones son las columnas de la matriz $\exp(At)$. Estas columnas son independientes porque para $t=0$ tenemos que $exp(A\cdot0) = I$, por lo que el Wronskiano es distinto de cero, y por tanto también lo es para todo $t$.

Dependiendo de cómo sea la matriz $A$, podemos distinguir varios casos. Para simplificar, supongamos que la matriz $A$ es de dos filas y dos columnas:

\begin{enumerate}
\item \textbf{\textit{A} es una matriz diagonal}

En este caso tenemos que $$A = \begin{pmatrix}
d_1 & 0\\0 & d_2\\
\end{pmatrix}$$
entonces $$exp(At) = \begin{pmatrix}
1 & 0\\ 0 & 1\\
\end{pmatrix} + \sum_{j=1}^\infty \frac{1}{j!} \begin{pmatrix}
d_1t & 0\\ 0 & d_2t\\
\end{pmatrix}^j = \begin{pmatrix}
1 & 0\\ 0 & 1\\
\end{pmatrix} + \sum_{j=1}^\infty \frac{1}{j!} \begin{pmatrix}
(d_1t)^j & 0\\ 0 & (d_2t)^j\\
\end{pmatrix} = $$
$$\begin{pmatrix}
1+d_1t+\frac{d_1^2t^2}{2}+\hdots & 0\\
0 & 1+d_2t+\frac{d_2^2t^2}{2}+\hdots\\
\end{pmatrix} = \begin{pmatrix}
e^{d_1t} & 0\\ 0 & e^{d_2t}\\
\end{pmatrix}$$

\item \textbf{\textit{A} es una matriz diagonalizable}

Tenemos que $A$ es de la forma $$A = PDP^{-1}$$ con $D$ diagonal. Esto implica que, como se ve para el caso particular de $$A^2 = (PDP^{-1})(PDP^{-1}) = PD(P^{-1}P)DP^{-1} = PD^2P^{-1}$$ en general tenemos $$A^n = PD^nP^{-1}$$ donde $P$ es una matriz cuyas columnas son los autovectores de la matriz $A$ y los elementos de la diagonal de la matriz $D$ son los autovalores asociados a dichos autovectores.

En este caso
\begin{center}
$exp(At) = I + At + \frac{A^2t^2}{2} + \hdots +\frac{A^nt^n}{n!} + \hdots =PP^{-1} + PDtP^{-1} + P\frac{D^2t^2}{2}P^{-1} + \hdots = P(I+Dt+\frac{D^2t^2}{2}+\hdots)P^{-1} = Pexp(Dt)P^{-1}$
\end{center}
es decir
$$exp(At) = Pexp(Dt)P^{-1}$$

\item \textbf{\textit{A} no es diagonalizable}

En este caso podemos obtener la forma de \textbf{Jordan} de $A$:
$$A = P\begin{pmatrix}
d & 1\\0 & d\\
\end{pmatrix}P^{-1}$$
La exponencial de $At$ será
\begin{center}
$exp(At) = Pexp({\begin{pmatrix}
d & 1\\0 & d\\
\end{pmatrix}t})P^{-1} = Pexp({\begin{pmatrix}
d & 0\\0 & d\\
\end{pmatrix}t + \begin{pmatrix}
0 & 1\\ 0 & 0\\
\end{pmatrix}}t)P^{-1} = Pexp(\begin{pmatrix}
d & 0\\ 0 & d\\
\end{pmatrix}t)exp(\begin{pmatrix}
0 & 1\\ 0 & 0\\
\end{pmatrix})P^{-1}$
\end{center}
Como la matriz $\begin{pmatrix}
0 & 1\\ 0 & 0\\
\end{pmatrix}$ es \textbf{nilpotente} (lo que quiere decir que para un cierto $n$ la potencia \textit{n-ésima} de la matriz será la matriz nula), tenemos que la exponencial de dicha matriz será una suma \textbf{finita}.
\end{enumerate}

A partir del último método descrito vemos la definición siguiente

\begin{definition}\name{Matriz fundamental}
Dado un sistema lineal homogéneo de la forma $$X^\prime = AX$$
y una matriz $\Phi(t)$, decimos que $\Phi$(t) es una matriz fundamental si sus columnas son soluciones independientes del sistema.
\end{definition}

Veamos ejemplos de ambos métodos.

\begin{example} [(Método 1)]
Dado el sistema homogéneo$$X^\prime = \begin{pmatrix}
x^\prime(t)\\y^\prime(t)
\end{pmatrix}
= \underbrace{\begin{pmatrix}
1 & 3\\3 & 1
\end{pmatrix}}_{A}\begin{pmatrix}
x(t)\\y(t)
\end{pmatrix} = AX$$
Los autovalores de la matriz $A$ son los resultados de la ecuación $$det\begin{pmatrix}
1-\lambda & 3\\3 &1-\lambda\\
\end{pmatrix}=0$$
que son $\lambda = 4$ y $\lambda = -2$
\begin{itemize}
\item $\lambda = 4$

$\begin{pmatrix}
1 & 3\\ 3 & 1\\
\end{pmatrix}\begin{pmatrix}
v_1\\v_2\\
\end{pmatrix} = 4\begin{pmatrix}
v_1\\v_2\\
\end{pmatrix} \iff v_1 = v_2$

\item $\lambda = -2$

$\begin{pmatrix}
1 & 3\\ 3 & 1\\
\end{pmatrix}\begin{pmatrix}
w_1\\w_2\\
\end{pmatrix} = -2\begin{pmatrix}
w_1\\w_2\\
\end{pmatrix} \iff w_1 = -w_2$
\end{itemize}

Tenemos por tanto dos soluciones independientes:
$$x(t) = e^{4t}\begin{pmatrix}
1\\1\\
\end{pmatrix}$$
$$y(t) = e^{-2t}\begin{pmatrix}
1\\-1\\
\end{pmatrix} $$

La solución general es, por tanto
$$X(t) = c_1e^{4t}\begin{pmatrix}
1\\1\\
\end{pmatrix} + c_2e^{-2t}\begin{pmatrix}
1\\-1\\
\end{pmatrix}$$

Dado un dato inicial $X(t_0) = \begin{pmatrix}
x_0\\y_0\\
\end{pmatrix}$ no tenemos más que resolver el sistema $$\begin{pmatrix}
x_0\\y_0\\
\end{pmatrix} =  c_1e^{4t_0}\begin{pmatrix}
1\\1\\
\end{pmatrix} + c_2e^{-2t_0}\begin{pmatrix}
1\\-1\\
\end{pmatrix}$$ para hallar $c_1$ y $c_2$ y por tanto, la solución particular para dicho dato.

\end{example}


\begin{example}[(Método II)]
Dado el sistema homogéneo $$X^\prime = \begin{pmatrix}
x^\prime(t)\\y^\prime(t)
\end{pmatrix}
= \underbrace{\begin{pmatrix}
1 & 3\\3 & 1
\end{pmatrix}}_{A}\begin{pmatrix}
x(t)\\y(t)
\end{pmatrix} = AX$$

Tenemos que $A$ es diagonalizable (tenemos sus autovalores y autovectores del ejemplo anterior).

Dado que $$A = \begin{pmatrix}
1& 1\\1 & -1\\
\end{pmatrix}\begin{pmatrix}
4& 0\\0& -2\\
\end{pmatrix}\begin{pmatrix}
1& 1\\1 & -1\\
\end{pmatrix}^{-1}$$
tenemos que $$\Phi(t) = exp(At) = \begin{pmatrix}
1& 1\\1 & -1\\
\end{pmatrix}\begin{pmatrix}
e^{4t}& 0\\0& e^{-2t}\\
\end{pmatrix}\begin{pmatrix}
1& 1\\1 & -1\\
\end{pmatrix}^{-1} = \begin{pmatrix}

Sol_1 & Sol_2\\ \downarrow & \downarrow\\
\end{pmatrix}$$

Dado un dato de la forma $X(0) = \begin{pmatrix}
a\\b\\
\end{pmatrix}$ la solución particular para el problema con dicho dato es de la forma $$x(t) = \alpha\begin{pmatrix}
Sol_1\\\downarrow
\end{pmatrix} + \beta\begin{pmatrix}
Sol_2\\\downarrow
\end{pmatrix}$$

Queremos saber qué valor han de tener $\alpha$ y $\beta$ para obtener la solución particular de nuestro problema. Sabemos que las columnas de la matriz fundamental son soluciones independientes y tenemos que como $$\Phi(0) = exp(A\cdot 0) = exp(0) = I$$ entonces

$$\alpha\begin{pmatrix}
Sol_1\\\downarrow
\end{pmatrix}_{t=0} + \beta\begin{pmatrix}
Sol_2\\\downarrow
\end{pmatrix}_{t=0} = \alpha\begin{pmatrix}
1\\0
\end{pmatrix}+\beta\begin{pmatrix}
0\\1
\end{pmatrix} = \begin{pmatrix}
\alpha\\\beta
\end{pmatrix}$$ y también tenemos que $$\alpha\begin{pmatrix}
Sol_1\\\downarrow
\end{pmatrix}_{t=0} + \beta\begin{pmatrix}
Sol_2\\\downarrow
\end{pmatrix}_{t=0} = \begin{pmatrix}
x(0)\\y(0)
\end{pmatrix} = \begin{pmatrix}
a\\b
\end{pmatrix}$$ por lo que $\alpha = a$ y $\beta = b$

De aquí obtenemos que la solución particular para el problema con dicho dato será de la forma

$$x(t) = a\begin{pmatrix}
Sol_1\\\downarrow
\end{pmatrix} + b\begin{pmatrix}
Sol_2\\\downarrow
\end{pmatrix} = \Phi(t) \cdot \begin{pmatrix}
a\\b
\end{pmatrix}$$
\end{example}

\obs
Dado que $\Phi(0) = I$, para un problema con dato $X(0) = \begin{pmatrix}
a\\b
\end{pmatrix}$ la solución particular para el problema con dicho dato es $$\Phi(t)\begin{pmatrix}
a\\b
\end{pmatrix}$$

En caso de proporcionarse un dato en un punto distinto de $0$, por ejemplo $\Phi(2) = \begin{pmatrix}
c\\d
\end{pmatrix}$, la solución particular para el problema con dicho dato es
$$\Phi(t-2)\begin{pmatrix}
c\\d
\end{pmatrix}$$

\obs
Si $\Phi(t)$ es una matriz fundamental $\implies \tilde{\Phi(t)} = \Phi(t)C$, donde $C$ es una matriz cuadrada con determinante distinto de $0$, también lo es.

Esto se debe a que como las columnas de $\Phi(t)$ son soluciones independientes, cualquier combinación lineal también es una solución.

\begin{example}
Dado el sistema $$X^\prime(t) = \underbrace{\begin{pmatrix}
3 & -4\\1 &-1
\end{pmatrix}}_A X(t)$$

El autovalor de la matriz es $\lambda = 1$ (doble). El autovector asociado a dicho autovalor es $\vec{u}=\begin{pmatrix}
2w\\w
\end{pmatrix}$.

Vemos que la matriz no es diagonalizable, hallamos por tanto su forma de Jordan: $$J = \begin{pmatrix}
1 & 1\\0 & 1
\end{pmatrix}$$ que está en la base $B = \set{\vec{u}, \vec{v}}$.

Como $J$ está en la base $B$, $$A\vec{u} = \vec{u}$$
y  $$A\vec{v}= \vec{u}+\vec{v}$$.

Dado que ya tenemos $\vec{u}$ podemos hallar $\vec{v}$ y la otra solución del problema. De lo anterior obtenemos
$$(A-I)\vec{u} = \vec{0}$$
$$(A-I)\vec{v} = \vec{u}$$

Multiplicando a ambos lados por $(A-I)$ en la segunda ecuación tenemos $$(A-I)^2\vec{v} = (A-I)\vec{u}$$ y a partir de la primera ecuación vemos que el segundo término de lo anterior es igual a $\vec{0}$.

Por tanto $$(A-I)^2\vec{v} = 0$$
Sólo hay que elegir $\vec{v}$ de forma que se cumpla que
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
	(A-I)^2\vec{v} = 0\\
	(A-I)\vec{v} \neq \vec{0}
  \end{array}
  \right.
\end{equation*}
pues $(A-I)\vec{v} = \vec{u}$

Basta tomar $$\vec{v} = \begin{pmatrix}
1 \\ 0
\end{pmatrix}$$
obteniendo así $$\vec{u} = (A-I)\vec{v}  = \begin{pmatrix}
2\\1
\end{pmatrix}$$

De esta manera, ya podemos hallar la matriz fundamental
$$\Phi(t) = \begin{pmatrix}
2 & 1\\1 & 0
\end{pmatrix}exp(\begin{pmatrix}
1 & 1\\0 & 1\\
\end{pmatrix})\begin{pmatrix}
2 & 1\\1 & 0
\end{pmatrix}^{-1}$$

Vamos a hallar $exp(Jt)$
\begin{center}
$exp(\begin{pmatrix}
1 & 1\\0 & 1\\
\end{pmatrix}t) = exp(\begin{pmatrix}
1 & 0\\ 0 & 1\\
\end{pmatrix}t+\begin{pmatrix}
0 & 1\\ 0 & 0
\end{pmatrix}t) = exp(\begin{pmatrix}
1 & 0\\ 0 & 1\\
\end{pmatrix}t)exp(\begin{pmatrix}
0 & 1\\ 0 & 0
\end{pmatrix}t)$
\end{center}

Nos hace falta saber cuánto vale $exp(\begin{pmatrix}
0 & 1\\ 0 & 0
\end{pmatrix}t)$

$$exp(\begin{pmatrix}
0 & 1\\ 0 & 0
\end{pmatrix}t) = \begin{pmatrix}
1 & 0 \\ 0 & 1\\
\end{pmatrix} + \begin{pmatrix}
0 & t \\ 0 & 0
\end{pmatrix} + \underbrace{\frac{1}{2}\begin{pmatrix}
0 & t \\ 0 & 0
\end{pmatrix}^2 + \hdots}_0 = \begin{pmatrix}
1 & t\\ 0 & 1
\end{pmatrix}$$

Por tanto, la matriz fundamental es
$$\Phi(t) = \begin{pmatrix}
2 & 1\\1  & 0
\end{pmatrix}\begin{pmatrix}
e^t & 0\\ 0 & e^t
\end{pmatrix}\begin{pmatrix}
1 & t\\ 0 & 1
\end{pmatrix}\begin{pmatrix}
0 & 1\\1 & -2
\end{pmatrix} = \begin{pmatrix}
2te^t+e^t & -4te^t\\te^t& e^t-2te^t
\end{pmatrix}$$
\end{example}

En los ejemplos anteriores hemos utilizado que $e^{(A+B)t} = e^{At}e^{Bt}$. Esto puede parecer falso, ya que aunque que la suma de matrices es conmutativa, el producto no lo es. Sin embargo, a continuación se probará la validez de esta operación:

\begin{theorem}
$e^{(A+B)t} = e^{At}e^{Bt}$
\end{theorem}
\begin{proof}
Sea $\Phi(t) = \Phi_1(t)\Phi_2(t)$ donde
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
  	\Phi(t) = e^{(A+B)t}\\
	\Phi_1(t) = e^{At}\\
	\Phi_2(t) = e^{Bt}
  \end{array}
  \right.
\end{equation*}

Sabemos que $\Phi^\prime(t) = (A+B)\Phi(t)$, queremos ver que $$(\Phi_1(t)\Phi_2(t))^\prime = (A+B)\Phi(t)$$

Hallamos $(\Phi_1(t)\Phi_2(t))^\prime$:
$$(\Phi_1(t)\Phi_2(t))^\prime = \Phi^\prime_1(t)\Phi_2(t)+\Phi_1(t)\Phi^\prime_2(t)$$

dado que \begin{center}
$\Phi^\prime_1(t) = A\Phi_1(t)$\\
$\Phi^\prime_2(t) = B\Phi_2(t)$
\end{center}

tenemos

$$\Phi^\prime(t) = A\Phi_1(t)\Phi_2(t)+\Phi_1(t)B\Phi_2(t)$$

Si $\Phi_1(t)$ y $B$ conmutan $\implies$ $\Phi^\prime(t) = (A+B)\Phi_1(t)\Phi_2(t)$

(INCOMPLETO, SI ALGUIEN SABE POR QUÉ CONMUTAN SIEMPRE QUE ME LO DIGA POR FAVOR)
\end{proof}

Veamos un ejemplo de resolución de un sistema lineal homogéneo con coeficientes constantes en la que los autovalores de la matriz son números complejos.

\begin{example}[(Método 1)]
Dado el sistema $$X^\prime = \underbrace{\begin{pmatrix}
1 & -2\\ 4 & 6
\end{pmatrix}}_AX$$

Los autovalores de la matriz son $\lambda_\pm = \frac{7}{2} \pm \frac{\sqrt{7}}{2}i$.

Los autovectores asociados son $v_\pm = \begin{pmatrix}
1& \frac{-5\mp\sqrt{7}i}{4}
\end{pmatrix}$

A partir de aquí tenemos dos soluciones complejas:
\begin{center}
$z_\pm = e^{\lambda_\pm}v_\pm$
\end{center}

y por tanto la solución general compleja$$\alpha z_+ + \beta z_-$$

Para obtener soluciones reales, basta con elegir $\alpha$ y $\beta$ de modo que $$\alpha z_+ + \beta z_- \in \R$$

Basta con tomar \begin{equation*}
  \left\lbrace
  \begin{array}{l}
	x_1 = \frac{z_++z_-}{2} = Re\{ z_+ \}\\
	x_2 = \frac{x_+-z_-}{2i} = Im \{ z_+ \}
  \end{array}
  \right.
\end{equation*}
\end{example}

\begin{example}[(Método II)]
Dado el sistema $$X^\prime = \underbrace{\begin{pmatrix}
1 & -2\\ 4 & 6
\end{pmatrix}}_AX$$

Los autovalores de la matriz son $\lambda_\pm = \frac{7}{2} \pm \frac{\sqrt{7}}{2}i$.

Los autovectores asociados son $v_\pm = \begin{pmatrix}
1& \frac{-5\mp\sqrt{7}}{4}
\end{pmatrix}$

La matriz fundamental es
$$\Phi(t) = \begin{pmatrix}
1 & 1\\ \frac{-5-\sqrt{7}i}{4} & \frac{-5+\sqrt{7}i}{4}
\end{pmatrix}exp(\begin{pmatrix}
\frac{7+\sqrt{7}i}{2} & 0\\ 0 & \frac{7-\sqrt{7}i}{2}
\end{pmatrix})\begin{pmatrix}
1 & 1\\ \frac{-5-\sqrt{7}i}{4} & \frac{-5+\sqrt{7}i}{4}
\end{pmatrix}^{-1}$$

Una vez calculada, tendremos que el resultado de matriz fundamental tiene números complejos y será de la forma $$\Phi(t) = \begin{pmatrix}
Sol_1(t) & Sol_2(t) \\ \downarrow & \downarrow
\end{pmatrix}$$ Para obtener las soluciones reales basta con construir una nueva matriz fundamental de la forma $$\tilde{\Phi}(t) = \begin{pmatrix}
Re(Sol_1(t)) & Im(Sol_1(t))\\ \downarrow & \downarrow
\end{pmatrix}$$
\end{example}

Veamos un último ejemplo de resolución en el que la matriz no es diagonalizable.

\begin{example}
Dado el sistema $$X^\prime = \underbrace{\begin{pmatrix}
3 & 1 & 0\\-1 & 0 & 1\\1 & 2 & 3
\end{pmatrix}}_AX$$

El autovector de la matriz $A$ es $\lambda = 2$ (triple).

El autovector asociado es $\vec{u} = \begin{pmatrix}
z\\-z\\z
\end{pmatrix}$

Tenemos por tanto la solución $x_1(t) = e^{2t}\begin{pmatrix}
1\\ -1\\ 1
\end{pmatrix}$, falta hallar dos restantes.

La forma de Jordan de la matriz $A$ es $$J = \begin{pmatrix}
2& 1& 0\\ 0& 2& 1\\ 0& 0& 2
\end{pmatrix}$$

en la base $B = \set{\vec{u}, \vec{v}, \vec{w}}$, de aquí obtenemos que $\left\lbrace
  \begin{array}{l}
     A\vec{u} = 2\vec{u}\\
     A\vec{v} = \vec{u} + 2\vec{v}\\
     A\vec{w} = \vec{v} + 2\vec{w}\\
  \end{array}
  \right.
$

o lo que es lo mismo $$\left\lbrace
  \begin{array}{l}
     (A-2I)\vec{u} = 0\\
     (A-2I)^2\vec{v} = (A-2I)\vec{u} = 0\\
     (A-2I)^3\vec{w} = (A-2I)^2\vec{v} = 0\\
  \end{array}
  \right.$$

Tomando $\vec{w} = \begin{pmatrix}
0\\0\\1
\end{pmatrix}$ tenemos que $\vec{v} = \begin{pmatrix}
0\\1\\1
\end{pmatrix}$ y $\vec{v} = \begin{pmatrix}
-1\\1\\-1
\end{pmatrix}$

A partir de aquí construimos la matriz fundamental:
$$\Phi(t) = \begin{pmatrix}
-1 & 0 & 0\\ 1 & 1 & 0\\-1& 1& 1
\end{pmatrix}\exp\left(\begin{pmatrix}
2 & 0 & 0\\ 0 & 2 & 0\\0& 0& 2
\end{pmatrix}t+\begin{pmatrix}
0 & 1 & 0\\ 0 & 0 & 1\\0& 0& 0
\end{pmatrix}t\right)\begin{pmatrix}
-1 & 0 & 0\\ 1 & 1 & 0\\-1& 1& 1
\end{pmatrix}^{-1}$$

$$\Phi(t) = \begin{pmatrix}
-1 & 0 & 0\\ 1 & 1 & 0\\-1& 1& 1
\end{pmatrix}\begin{pmatrix}
e^{2t} & 0 & 0\\ 0 & e^{2t} & 0\\0& 0& e^{2t}
\end{pmatrix}\begin{pmatrix}
1 & t & \frac{t^2}{2}\\ 0 & 1 & t\\ 0 & 0 & 1
\end{pmatrix}\begin{pmatrix}
-1 & 0 & 0\\ 1 & 1 & 0\\-1& 1& 1
\end{pmatrix}^{-1}$$

\end{example}

\subsection{Sistemas lineales no homogéneos}
Ya hemos visto que dado un sistema lineal no homogéneo $$X^\prime(t) = A(t)X(t)+B(t)$$ la solución general del mismo es de la forma $$X_G = X_P + X_H$$ donde \begin{itemize}
\item $X_G$ es la solución general del sistema completo.

\item $X_P$ es una solución particular del sistema completo.

\item $X_H$ es la solución general del sistema homogéneo asociado.
\end{itemize}

Vamos a ver el método de variación de las constantes para la resolución de este tipo de sistemas:

\subsubsection{Método de variación de las constantes}
Dado un sistema lineal no homogéneo de la forma $$X^\prime(t) = A(t)X(t)+B(t)$$

Sea $\Phi(t)$ la matriz fundamental del sistema homogéneo asociado $$X^\prime(t) = A(t)X(t)$$

Tenemos que $X_H = \Phi(t)\vec{C}$ donde $\vec{C}$ es una matriz columna. Es decir, $X_H$ son combinaciones lineales de las columnas de $\Phi(t)$.

Buscamos $X_P(t) = \Phi(t)\vec{C}(t)$. Derivando obtenemos: $$X^\prime_P(t) = (\Phi(t)\vec{C}(t))^\prime = \Phi^\prime(t)\vec{C}(t)+\Phi(t)\vec{C}^\prime(t) = A\Phi(t)\vec{C}(t)+\Phi(t)C^\prime(t)$$

Para que se cumpla que $$X_P^\prime(t) = AX_P+B = A\Phi(t)\vec{C}(t)+B$$ necesitamos que $B = \Phi(t)\vec{C}^\prime(t)$

Es decir
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     X_P(t) = \Phi(t)\vec{C}(t)\\
     \vec{C}^\prime(t) = \Phi^{-1}(t)B(t)
  \end{array}
  \right.
\end{equation*}

\obs
La inversa de $\Phi(t)$ siempre existe porque al ser matriz fundamental, sus columnas son linealmente independientes, y por tanto su determinante es siempre distinto de cero.

\begin{example}
Dado el sistema $$X^\prime(t) = \underbrace{\begin{pmatrix}
1 & 2 \\ 3 & 2
\end{pmatrix}}_AX + \underbrace{\begin{pmatrix}
t-1\\-5t+2
\end{pmatrix}}_B$$

La matriz fundamental del sistema homogéneo asociado es $$\Phi(t) = \begin{pmatrix}
2e^{4t} & -3e^{-t}\\ 3e^{4t} & e^{-t}
\end{pmatrix}$$

Tenemos que $\vec{C}^\prime(t) = \Phi^{-1}(t)B(t)$

$$\Phi(t)\begin{pmatrix}
C_1^\prime\\C_2^\prime
\end{pmatrix} = \begin{pmatrix}
t-1\\-5t+2
\end{pmatrix}$$

A partir de aquí tenemos dos opciones: podemos resolver lo anterior, o darnos cuenta de que podemos buscar una solución particular de la forma
$$X_P = \begin{pmatrix}
at+b\\ct+d
\end{pmatrix}$$ ya que en la ecuación sólo aparecen polinomios de grado 1.
\end{example}

\subsection{Matrices con coeficientes T-periódicos}
Dado el sistema $$X^\prime(t) = A(t)X(t)$$ en el que $A(t)$ es una matriz con coeficientes $T$-periódicos, si definimos $$Y(t) = X(t+T)$$ derivando vemos que
$$Y^\prime(t) = X^\prime(t+T) = A(t+T) X(t+T) = A(t+T)Y(t) = A(t)Y(t) $$ por lo que $Y(t)$ también es solución del sistema.

\begin{example}
Dado el sistema
$$X^\prime(t) = \begin{pmatrix}
1 & 0 \\ sin(t) & -1
\end{pmatrix}X(t)$$ denotamos $X(t) = \begin{pmatrix}
x(t)\\y(t)
\end{pmatrix}$ y escribimos el sistema de la siguiente forma

\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     x^\prime(t) = x(t)\\
     y^\prime(t) = sin(t)x(t)-y(t)
  \end{array}
  \right.
\end{equation*}

De la primera ecuación tenemos que $$x(t) = Ae^t$$
obteniendo junto con la segunda ecuación que $$y^\prime(t) = Ae^tsin(t) -y(t)$$
$$y^\prime(t) +y(t) = Ae^tsin(t)$$

Multiplicando por el factor integrante $e^t$
$$(e^ty)^\prime = Ae^{2t}sin(t)$$
$$e^ty = A\int e^{2t}sin(t)dt \underbrace{=}_{\text{por partes}} A\{ \frac{e^{2t}}{5}(2sin(t)-cos(t)) \}$$

Tenemos entonces que
$$X(t) = A\begin{pmatrix}
e^t\\\frac{e^t}{5}(2sin(t)-cos(t))
\end{pmatrix} + AB\begin{pmatrix}
0\\e^{-t}
\end{pmatrix}$$

Las soluciones no son periódicas, sino que trasladadas por un periodo siguen siendo solución.
\end{example}

\obs
Si $\Phi(t)$ es matriz fundamental $\implies \bar{\Phi}(t) = \Phi(t+T)$ también es matriz fundamental. Es decir, $Y(t) = X(t+T)$ es solución $\iff Y(t) = \Phi(t)\vec{C}$

\begin{theorem}
Si $\Phi(t)$ es una matriz fundamental, entonces $$\Phi(t) = B(t) e^{Lt}$$ donde $B(t)$ es una matriz periódica y $L$ es una matriz con coeficientes constantes.
\end{theorem}

\begin{proof}
Sabemos que $$\Phi(t+T) = \Phi(t)\vec{C}$$ siendo el determinante de $\vec{C}$ distinto de cero. En particular,

$$\vec{C} = \{ \Phi(t)\} ^{-1} \Phi(t+T)$$

Vamos a utilizar el siguiente lema:
$$det(\vec{C}) \neq 0 \implies \exists M \st \vec{C} = e^M$$

Entonces $$\Phi(t+T) = \Phi(t) e^M$$
Buscamos $\Phi(t) = B(t) e^{Lt}$, es decir, $$\Phi(t+T) = B(t+T) e^{L(t+T)} = B(t)e^{Lt}e^{LT}$$
$$\Phi(t+T) = \Phi(t)\vec{C} = \Phi(t)e^M=B(t)e^{Lt}e^M$$

Como conclusión se tiene que si existe la descomposición $\Phi(t) = B(t)e^{Lt}$, debe ser $L = \frac{1}{M}$. Además, tenemos que como $B(t)$ es periódica de periodo T, $B(t) = B(t+T)$.

\end{proof}

\begin{example}
Dado el sistema $$X^\prime(t) = \begin{pmatrix}
1 & 0\\sin(t) & -1
\end{pmatrix}X(t)$$
tenemos la matriz fundamental $$\Phi(t) = \begin{pmatrix}
e^t & 0\\\frac{e^5}{5}(2sin(t)-cos(t)) & e^{-t}
\end{pmatrix}$$
Calculamos
$$C = \{ \Phi(t) \}^{-1}\Phi(t+2\pi) = \begin{pmatrix}
e^{2\pi} & 0\\0 & e^{-2\pi}
\end{pmatrix}  = exp(\begin{pmatrix}
2\pi & 0\\0 & e^{-2\pi}
\end{pmatrix} = M) $$

Tenemos que $$L = \frac{1}{2\pi}M = \begin{pmatrix}
1& 0\\ 0 &-1\\
\end{pmatrix}$$

Por tanto
$$e^{Lt} = \begin{pmatrix}
e^t & 0\\0 & e^{-t}
\end{pmatrix}$$

Buscamos $B(t)$ tal que $\Phi(t) = B(t)e^{Lt}$

Entonces
$$B(t) = \Phi(t)e^{-Lt} = \Phi(t) \begin{pmatrix}
e^{-t} & 0\\0 & e^t
\end{pmatrix} = \begin{pmatrix}
1 & 0\\
\frac{1}{5}(2sin(t)-cos(t)) & 1
\end{pmatrix}$$

\end{example}

