\documentclass[palatino, bibnumbers]{apuntes}

\title{Geometría y Topología}
\author{Guillermo Julián Moreno}
\date{15/16 C2}

% Paquetes adicionales
\usepackage{enumitem}
\usepackage{tikztools}
\usepackage{fancysprefs}
\usepackage{tikz-3dplot}
\usepackage{xfrac}
\usepackage{wrapfig}
\usepackage{fastbuild}

\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\usetikzlibrary{intersections}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}

\setlist{itemsep=1pt, topsep=5pt}
\bibliographystyle{alpha}
% --------------------

\precompileTikz

\newcommand{\Id}{\mop{Id}}
\newcommand{\cln}{\colon\!}

\setcounter{tocdepth}{3}

\begin{document}
\pagestyle{plain}

% http://tex.stackexchange.com/a/14243
\relpenalty=9999
\binoppenalty=9999

\begin{abstract}
Estos son los apuntes del curso de Geometría y Topología, del profesor Gabino González.
\end{abstract}

\maketitle


\tableofcontents
\newpage
% Contenido.

\chapter{Conceptos básicos - Variedades}

En Geometría, los objetos que estudiamos se llaman ``variedades''. Veremos de distintos tipos (por ejemplo, en Geometría Diferencial \citep{ApuntesGeoDif} veíamos variedades diferenciables), aunque nosotros empezaremos con las topológicas.

\begin{defn}[Variedad\IS topológica] Una variedad topológica es un espacio topológico $M$ con las siguientes propiedades:
\begin{enumerate}
\item $M$ es $T_2$ (esto es, \concept{Hausdorff}: dos puntos distintos tienen entornos disjuntos).
\item $∀p ∈ M$ admite un entorno $U$ y un homeomorfismo $\appl{φ_u}{U}{ℝ^N}$ (o $\bola^N$).

Al par $(U,φ_u)$ se le llama \concept{Carta} para $p$. Si $φ_u(p) = 0$, se dice que la carta está centrada en $p$. A la colección de cartas se le llamará \concept{Atlas}.
\item Si para cualquier par de cartas $(U,φ_u)$ y $(V,φ_v)$ la aplicación $$\appl{φ_v ○ \inv{φ_u}}{φ_u(U∩V)}{φ_v(U∩V)}$$ es difeomorfismo, estamos entonces ante una \concept{Variedad\IS diferenciable}.
\end{enumerate}
\end{defn}

\begin{figure}[thbp]
\centering
\inputtikz{Cartas}
\caption{Un esquema de las cartas de una variedad $M$ y cómo se comportan en la intersección.}
\label{fig:Cartas}
\end{figure}

La dimensión de la variedad está dada por la dimensión de $ℝ$ a la que son homeomorfas las cartas. La cuestión es que no tenemos claro si eso está bien definido. En el caso diferenciable, la condición de difeomorfismo para la intersección de cartas implica que la dimensión de ambas cartas ha de ser la misma. En el caso topológico también está bien definido, aunque es más difícil de demostrar ya que dependemos de que no exista un homeomorfismo entre $ℝ^n$ y $ℝ^m$ con $n ≠ m$, que no es trivial.

Un ejemplo sencillo de variedad es $M = \bola^N$, con $φ$ la identidad. Otro ejemplo es $\crc$ (la circunferencia), que es una variedad de dimensión 1, que no se puede dar con sólo una carta (la circunferencia no es homeomorfa a $ℝ$)\footnote{$\crc$ es compacta y $\real$ no, y como compacidad es propiedad topológica y los homeomorfismos las preservan, no pueden ser homeomorfas.}. Podríamos darla tomando las dos mitades superior e inferior usando senos y cosenos, y también podríamos hacer la proyección estereográfica (\ref{fig:ProyEstereo}) desde los polos norte y sur $(0,1), (0,-1)$ respectivamente sobre la recta real. En este caso, tendríamos las siguientes cartas: \[
\begin{matrix}
	\appl{α_1}{V_1 = \crc\setminus\set{(0,1)}&}{&ℝ} \\
	p=(s,t) &\longmapsto& \frac{s}{1-t}
\end{matrix}
\qquad
\begin{matrix}
	\appl{α_1}{V_2 = \crc\setminus\set{(0,-1)}&}{&ℝ} \\
	p=(s,t) &\longmapsto& \frac{s}{1+t}
\end{matrix}\]

\begin{figure}[hbtp]
\inputtikz{ProyeccionCirc}
\caption{Proyección estereográfica de la circunferencia.}
\label{fig:ProyEstereo}
\end{figure}

Para comprobar si este atlas es diferenciable, tendríamos que mirar qué ocurre con $α_2 ○ \inv{α_1}$. Después de un montón de cuentas\footnote{Ver \fref{sec:proyeccion_estereografica_crc}.}, nos sale que efectivamente lo es ($α_2 ○ \inv{α_1} = \frac{1}{x}$) en el dominio en el que está definido (el cero no es un problema porque no está dentro del dominio).

Trivialmente, podemos definir cuándo dos atlas son compatibles.

\begin{defn}[Atlas\IS compatibles] Se dice que dos atlas $A_1, A_2$ son compatibles si $A_1 ∪ A_2$ es un atlas. Esto es, si y sólo si las cartas de $A_1$ son compatibles con las de $A_2$.
\end{defn}

\label{def::atlas_compatibles}

Por ejemplo, podemos estudiar si los dos atlas que hemos visto para la circunferencia \crc son compatibles (recordamos que uno era el trigonométrico y otro la proyección estereográfica). Esto es equivalente a preguntarnos si $α_j ○ \inv{φ_i}$ son diferenciables. Se puede ver fácilmente que \[ α_1 ○ \inv{φ_1} (θ) = α_1(\cos θ, \sin θ) = \frac{\cos θ}{1 - \sin θ}\] que es diferenciable. Así podríamos hacerlo con el resto de combinaciones, y por lo tanto tenemos que ambos atlas dan la misma variedad.

Como ejercicio, podríamos dar un atlas $A_3$ en \crc con cartas en forma de semicircunferencia y después comprobar que es compatible con los dos atlas de antes. Otro ejercicio más largo sería hacer lo análogo para $\crc[2]$.

Una vez que tenemos ya definido qué es una variedad, el siguiente paso es saber si podemos hacer análisis ahí: si podemos definir aplicaciones diferenciables en ella o si podemos integrar una función. La segunda parte la veremos más adelante con las formas diferenciales, pero la primera la podemos estudiar ahora.

\begin{figure}[hbtp]
\centering
\inputtikz{ApplDiferenciable}
\caption{Esquema de la definición de la aplicación diferenciable entre dos variedades en base a las cartas.}
\label{fig:ApplDiferenciable}
\end{figure}

\begin{defn}[Aplicación\IS diferenciable] Una aplicación continua $\appl{f}{M}{N}$ entre dos variedades $M$ y $N$ es diferenciable si $∀p ∈ M$ con $f(p) = q ∈ N$ existe una carta $(U,φ_U)$ alrededor de $p$ y una carta $(V, φ_V)$ alrededor de $q$ tal que $f(U) ⊂ V$ y $\appl{φ_V○f ○ \inv{φ_U}}{\bola^m}{\bola^n}$ es diferenciable.
\end{defn}

Es importante ver que este concepto de diferenciabilidad no depende de las cartas elegidas para cada punto. Suponiendo que tenemos otras dos cartas $α_U, α_V$ alrededor de $p$ y $q$ tendríamos que \[ α_V ○ f ○ \inv{α_U} = (α_V ○\inv{φ_V}) ○ (φ_V ○ f ○ \inv{φ_U}) ○ (φ_U ○ \inv{α_U})\]

Por compatibilidad de las cartas, $α_V○\inv{φ_V}$ y $φ_U ○ \inv{α_U}$ son diferenciables. Además, ya que hemos dicho que $f$ es diferenciable con las cartas $φ_U, φ_V$ luego $φ_V ○ f ○ \inv{φ_U}$ es diferenciable igualmente. Así, la composición de esas tres funciones es diferenciable.

\begin{example} Vamos a definir una función entre variedades y ver si es diferenciable. No nos complicaremos mucho: \begin{align*}
\appl{f}{\crc&}{\crc ⊂ ℂ} \\
z &\longmapsto \conj{z}
\end{align*}

Haciendo los cálculos, vemos que \[ φ_1 ○ f ○ \inv{φ_1} (θ) = φ_1 ○ f(e^{iθ}) = φ_1(e^{-iθ}) = \begin{cases} -θ \\ -θ + 2\pi \end{cases} \] que efectivamente es diferenciable.
\end{example}


\begin{example} Definimos ahora una función a priori menos interesante, la identidad: \begin{align*}
\appl{f}{M&}{N} \\
p &\longmapsto p
\end{align*}

$M$ será $(ℝ,φ)$ y $N = (ℝ,α)$, con $φ$ la identidad y $α(t) = t^3$. Es obvio ver que $f$ es diferenciable (si hacemos la composición nos sale directamente). Ahora bien, si tomamos $\appl{f}{N}{M}$, ¿sigue siendo diferenciable? En este caso, si calculamos vemos que $φ ○ f ○ \inv{α}(t) = \sqrt[3]{t}$ pero esta aplicación no es diferenciable en $0$. Esto es lo mismo que decir que estos dos atlas en $ℝ$ no son compatibles: no definen la misma estructura. Sí serían compatibles si estuviésemos hablando sólo de variedades topológicas, porque sí que estamos ante un homomorfismo.
\end{example}

Una vez que hemos visto qué es una variedad, podemos ver cómo construir variedades combinándolas de distintas maneras.

\section{Variedades producto}

La primera opción para construir una variedad es el producto cartesiano, que ya conocemos de otro tipo de conjuntos.

\begin{defn}[Variedad\IS producto] Sean $M^m$, $N^n$ variedades con sus respectivos atlas $A_1 = \set{(U_i, φ_i)}_{i ∈ I}$ y $A_2 = \set{(V_j, α_j)}_{j ∈ J}$. Entonces, la variedad producto es $M×N^{m+n}$ con atlas \[ A_1 × A_2 = \set{(U_i×V_j, φ_i × α_j)}_{(i,j) ∈ I × J} \]
\end{defn}

Un ejemplo sencillo es $\crc × ℝ$, una variedad con cartas de la forma $(φ_1 × α_1) = ((x,y), t)$, donde $(x,y)$ vienen de la carta de la circunferencia que hayamos escogido. En este caso, la variedad es $M = \set{(x,y,z) \tq x^2 + y^2 = 1}$, el cilindro, ya que podemos tomar $\appl{f}{M}{\crc × ℝ}$ es un difeomorfismo (se pueden hacer las cuentas pero son triviales).

Otro es $\crc × \crc$, que nos podemos preguntar si es difeomorfa a $\crc[2]$, más que nada porque uno es un toro y otro una esfera. Sin embargo, un argumento más formal implicaría usar los grupos fundamentales\footnote{Ver \citep[Cap. III]{ApuntesTopologia}.}, ya que $π(\crc[2]) = \set{0}$, $π(\crc) = ℤ$ y por lo tanto $π(\crc × \crc) \cong ℤ × ℤ$. Los grupos fundamentales no son isomorfos por lo que no puede haber difeomorfismo. Lo malo es que este argumento no nos vale para decir, por ejemplo, si $\crc[2] × \crc[3] \cong \crc[5]$. En realidad, no son siquiera homeomorfas, pero para saberlo tendremos que usar las cohomologías de De Rham (\fref{chap:CohomologiaDeRham}).

Durante el curso, veremos más herramientas para saber si existen o no difeomorfismos entre variedades.

\section{Variedad cociente}

En las variedades también se puede aplicar el análogo del conjunto cociente. Eso sí, necesitaremos ciertas condiciones ``extra'' sobre la variedad.

\begin{defn}[Variedad\IS cociente] Sea $M$ una variedad y sea $G < \mop{Diff}(M)$ un subgrupo de los difeomorfismos sobre $M$, tal que $\abs{G} < ∞$ y además $G$ actúa libremente en $M$ (esto es, que $∀g ∈ G$ distinta de la identidad y $∀x ∈ M$ se tiene que $g(x) ≠ x$).

Entonces, definimos el espacio cociente $\quot{M}{G}$ a través de la relación de equivalencia \[ x \sim y \iff ∃g ∈ G \tq  y = g(x) \]

La estructura de variedad se la damos considerando la proyección canónica \begin{align*}
\appl{π}{M&}{\quot{M}{G}} \\
x &\longmapsto [x]
\end{align*} que a cada elemento le asigna su clase.
\end{defn}

Por ejemplo, si consideramos la variedad $M = \crc[2]$ y el grupo $G = \gen{J} = \set{\Id, J}$ con $J(x,y,z) = (-x,-y,-z)$. En este caso, $\projp^2 = \quot{\crc[2]}{\gen{J}}$.

\begin{figure}[hbtp]
\centering
\inputtikz{EsferaPlanoProj}
\caption{Paso de la esfera al plano proyectivo, con la proyección canónica π.}
\label{fig:EsferaPlanoProj}
\end{figure}

Para darle la estructura de atlas, tomamos una carta $(U,φ_U)$ en $M$ tal que $U ∩ g(U) = ∅,\;∀g ∈ G$, por lo que $\appl{\restr{π}{U}}{U}{\quot{M}{G}}$ es un homeomorfismo sobre su imagen $π(U)$, luego biyectivo. Así, las cartas en $\quot{M}{G}$ son de la forma $\left(π(U), φ_U ○ \inv{\restr{π}{U}}\right)$.

En este caso, nos podemos preguntar qué ocurre si $π(U) ∩ π(V) ≠ ∅$ para dos cartas $U,V$: ¿es $\left(φ_V ○ \inv{\restr{π}{V}}\right) ○ \left(\restr{π}{U} ○ \inv{φ_U} \right)$ diferenciable?

La cuestión es que $\inv{\restr{π}{V}} ○ \restr{π}{U} = g ∈ G$ por la construcción de $g$\footnote{Si $π(x) = π(y)$, son de la misma clase y por lo tanto hay un difeomorfismo que lleva de $x$ a $y$.}, y por lo tanto nos queda que $φ_V ○  g ○ \inv{φ_U}$ es diferenciable así que la estructura de atlas que nos queda es compatible.

Un ejemplo muy común es $\projp^n = \quot{\crc[n]}{\gen{J}}$ para $n ∈ ℕ$, que son los espacios proyectivos reales de dimensión $n$.

Esta teoría también funciona siempre que el grupo $G$ cumpla la segunda propiedad y además $∀x ∈ M$ exista un $U$ tal que $g(U) ∩ U = ∅$, $∀g ≠ \Id$.

Otros ejemplos son $\quot{ℝ}{ℤ} \cong \crc$ o $\quot{ℝ^2}{ℤ^2} \cong \crc × \crc$. Ambos son análogos: el primero lleva a la circunferencia y el segundo al toro. Es fácil de ver aunque hay que echarle un poco de imaginación: lo vamos a desarrollar con $\quot{ℝ^2}{ℤ^2}$. En este caso, consideramos $ℤ^2$ como el conjunto de los difeomorfismos en $ℝ^2$ de la forma $f(x,y) = (x+m, y+n)$ con $(m,n) ∈ ℤ^2$, que es obviamente un grupo que no fija puntos. En este caso, el hecho de que sea infinito no nos causa demasiados problemas.

\begin{figure}[hbtp]
\centering
\inputtikz{ToroEspacioCociente}
\caption{Un gráfico para mostrar cómo $\protect\quot{ℝ^2}{ℤ^2}$ es homeomorfo a un toro: la proyección canónica $π$ nos lleva al cuadrado $[0,1]×[0,1]$ con los bordes conectados (los del mismo color). Si pudiésemos ``moldear'' el cuadrado y pegar los bordes, estaríamos ante el toro.}
\label{fig:ToroEspacioCociente}
\end{figure}

La relación de equivalencia que usamos para construir el espacio cociente nos dice que dos puntos $x,y ∈ ℝ^2$ pertenecen a la misma clase si existe un difeomorfismo $g ∈ ℤ^2$ tal que $g(x) = y$. En otras palabras, si tenemos un par $(m,n) ∈ ℤ^2$ tal que $(x_1 + m, x_2 + n) = (y_1, y_2)$, entonces $x = (x_1, x_2)$ y $y=(y_1, y_2)$ están relacionados, por lo que nuestro conjunto de clases de equivalencia son los puntos en $[0,1] × [0,1]$. Eso sí, con una peculiaridad: los bordes están ``unidos'', el de arriba con el de abajo y el de la izquierda con el de la derecha. Es decir, que topológicamente estamos ante un toro (ver \fref{fig:ToroEspacioCociente}).

Si por otra parte trabajásemos con $\quot{ℝ}{ℤ}$, tendríamos algo similar: estaríamos identificando los dos bordes del intervalo $(0,1)$ y tendríamos una circunferencia. Para demostrarlo formalmente, tendríamos que definir una aplicación \begin{align*}
\appl{f}{\quot{ℝ}{ℤ}&}{\crc} \\
[x] &\longmapsto e^{2πix} = (\cos 2πx, \sin 2πx)
\end{align*} y comprobar que es diferenciable, esto es, tenemos que comprobar que $\inv{φ_U} ○ f ○ α$ es diferenciable, teniendo que $φ_U$ es una carta de $\quot{ℝ}{ℤ}$, con $\inv{φ_U} = \restr{π}{U}$ la proyección canónica restringida a $U$; y $α(e^{2πiθ}) = 2πθ$ la otra carta. La composición sería obviamente $π ○ f ○ α = 2πx$ que es diferenciable.

Otro ejemplo posible es ver si podemos definir un difeomorfismo entre $\projp^1$ y $\crc$ como $ρ(z) = z^2$, con $z ∈ ℂ$. Eso sí, sólo pasa con $n=1$: en general, $\projp^n \nsim \crc[n]$.

\subsection{Espacios proyectivos}
\label{sec:EspaciosProyectivos}

A través de la variedad cociente hemos hablado de espacios proyectivos. Vamos a definirlos bien:

\begin{defn}[Espacio\IS proyectivo] Sea \kbb un cuerpo\footnote{En nuestro caso, $ℝ$ o $ℂ$}. Entonces definimos el plano proyectivo como \[ \mathbb{P}^n(\kbb) = \quot{\kbb^{n+1} \setminus \set{0}}{\sim}\] con $\sim$ la relación de equivalencia dada por $x \sim y \iff y = λx$ para algún $λ ∈ \kbb^*$.
\end{defn}

Cuando \kbb es $ℝ$ o $ℂ$, $\mathbb{P}^n(\kbb)$ es una variedad de dimensión $n$ o $2n$ respectivamente.

Las cartas en el plano proyectivo son de la forma\footnote{Como notación, los dos puntos denotan clases de equivalencia, esto es, $[x_1\cln x_2] \equiv \set{\gor{x_1}, \gor{x_2}}$.} \[ U_k = \set{[x_1\cln x_2\cln \dotsb\cln x_k\cln \dotsb\cln x_n] ∈ \mathbb{P}^n(\kbb) \tq x_k ≠ 0} \]

Este conjunto está bien definido. No lo estaría si estuviésemos diciendo que $x_k = 1$, por ejemplo, porque tendríamos que $[1\cln 1\cln 1] = [3\cln 3\cln 3]$ y no quedaría claro si está o no en el conjunto.

Además, se ve fácilmente que $\bigcup_{k=0}^n U_k = \mathbb{P}^n(\kbb)$. Sólo nos queda definir una biyección \begin{align*}
\appl{φ_k}{U_k&}{\kbb^n} \\
[x_1\cln \dotsb\cln x_k\cln \dotsb\cln x_n] &\longmapsto \left(\frac{x_0}{x_k}, \dotsc, \frac{x_n}{x_k}\right)
\end{align*}

La inversa de $φ_k$ sería \[ \inv{φ_k}(y_1, \dotsc,y_n) = [y_1\cln \dotsb \cln y_{k-1} \cln 1 \cln y_{k+1}\cln \dotsb \cln y_n]\]

Sólo nos queda comprobar que las funciones transición $\appl{φ_j ○ \inv{φ_k}}{\kbb^n \setminus \set{y_j = 0}}{\kbb^n \setminus \set{y_k = 0}}$ son diferenciables. Vemos que \[ (φ_j ○ \inv{φ_k})(y_1, \dotsc, y_n) = φ_j\left([y_1\cln \dotsb \cln y_{k-1} \cln 1 \cln y_{k+1}\cln \dotsb \cln y_n]\right) = \left(\frac{y_1}{y_j}, \dotsc, \frac{1}{y_j}, \dotsc, \frac{y_n}{y_j} \right)\], que efectivamente es diferenciable porque en el espacio de partida $y_j ≠ 1$.

Así, tenemos que tanto $\mathbb{P}^n(ℝ) = \projp^n$ y $\mathbb{P}^n(ℂ) = \projcp^n$ son variedades diferenciables.

El plano proyectivo complejo es interesante. Por ejemplo, podemos ver que $\projcp^1 \cong ℂ ∪ \set{∞} \cong \crc[2]$. Otro aspecto interesante de $\projcp^n$ es que no sólo es diferenciable como comentábamos antes: también es holomorfa. Y lo relevante de las variedades holomorfas es que son todas orientables. Esto nos sirve como excusa para introducir otro tipo de variedades.

\section{Variedades orientables}

\begin{defn}[Variedad\IS orientable] \label{def:VariedadOrientable} Una variedad $M$ es orientable si existe un atlas $A = \set{(U_i,φ_i)}_{i∈I}$ tal que $\abs{\Dif (φ_j○\inv{φ_i})} > 0$ para cualesquiera $j,i ∈ I$.
\end{defn}

Un atlas holomorfo siempre verificará la condición de orientabilidad por las ecuaciones de Cauchy-Riemann \citep[Sección III.2]{ApuntesVarCompI}. Definiendo $\appl{f = (φ_j○\inv{φ_i})}{ℂ}{ℂ}$ holomorfa, tenemos que $f = u + iv$ y entonces \[ \Dif f = \det \begin{pmatrix}
u_x & u_y \\ v_x & v_y
\end{pmatrix} \eqreasonup{Eq. CR} \det
\begin{pmatrix}
u_x & -v_x \\ v_x & u_x \end{pmatrix} = u_x^2 + v_x^2 > 0 \]

Otro ejemplo interesante es $\quot{ℝ×(-a,a)}{ℤ}$, de tal forma que $n·(x,λ) = (x+n,λ)$. Es un cilindro (identificamos los dos lados de un rectángulo de anchura infinita y altura $2a$) y orientable.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\begin{tikzpicture}
\fill[blue!20!white] (0, -0.8) rectangle (2, 0.8);
\draw (-1,0) -- (3,0);
\draw (0,-1) -- (0,1);

\node[hnlin, label = {left:$a$}] at (0, 0.8) {};
\node[hnlin, label = {left:$-a$}] at (0, -0.8) {};

\draw[blue, thick, directed, |-|]  (2, 0.8) --  (2, -0.8);
\draw[blue, thick, directed] (0, -0.8) -- (0,0.8);

\end{tikzpicture}
\caption{Representación de $\protect\quot{ℝ×(-a,a)}{ℤ}$ con los lados identificados en sentido contrario.}
\label{fig:CilindroNoOrientable}
\end{wrapfigure}

Ahora bien, si la relación de equivalencia la tomamos como $n·(x,λ) = (x+n, (-1)^nλ)$, esto es, que $(x,λ) \sim (x',λ')$ si y sólo si $x' = x + n$ y $λ' = (-1)^nλ$. Lo que nos queda es algo como lo de la \fref{fig:CilindroNoOrientable}, con los dos lados identificados pero en sentido contrario. En otras palabras, es una banda de Möbius.
\label{Mobius}
Con esto, podemos comprobar que efectivamente la banda de Möbius no es orientable. Sea $I = (-a, a)$, de tal forma que nuestra proyección sea \begin{align*} \appl{π}{ℝ×I&}{M} \\ (x,λ) &\longmapsto [(x,λ)]
\end{align*}

Consideramos ahora dos cartas $U_1 = (0,1) × I$ y $U_2 = (\sfrac{1}{2}, \sfrac{3}{2}) × I$ con respectivas funciones $φ_i = \inv{(\restr{π}{U_1})}$. Vemos ahora qué pasa con $φ_2 ○ \inv{φ_1}$, que tiene dominio $φ_1(π(U_1) ∩ π(U_2)) = \left( (0, \sfrac{1}{2}) × I\right) ∪ \left((\sfrac{1}{2}, 1) × I\right)$; e imagen $φ_2(π(U_1) ∪ π(U_2)) = \left( (\sfrac{1}{2}, 1) × I\right) ∪ \left((1, \sfrac{3}{2}) × I\right)$.

Tomamos ahora un $(x,λ) ∈ (\sfrac{1}{2}, 1) × I$, y tenemos que coger un representante de su clase en la imagen: nos vale con el mismo $(x,λ)$. Sin embargo, si tomamos un $(x,λ) ∈ (0, \sfrac{1}{2}) × I$, su clase es $[x,λ]$. Ahora bien, ese mismo número no está en la imagen, así que tenemos que buscar otro representante, que será $(x+1, -λ)$ por cómo hemos definido la relación de equivalencia. El determinante de esta función será negativo, y por lo tanto tendremos un atlas no orientable.

\begin{prop} Sea $M$ una variedad con un atlas con sólo dos cartas $(U_1, φ_1)$ y $(U_2, φ_2)$ tal que la intersección es un conexo. Entonces $M$ es orientable.
\end{prop}

\begin{proof} (Esbozo) Dado que los cambios de carta son difeomorfismos, el determinante de sus diferenciales no puede ser $0$ nunca. Dado que la intersección es conexa, no puede haber cambios de signo porque no se puede pasar por el $0$.
\end{proof}

\begin{figure}[hbtp]
\centering
\inputtikz{OrientRP2}
\caption{Demostración gráfica de la no-orientabilidad de $\projp^2$: el subconjunto $U$ es una banda de Möbius no orientable.}
\label{fig:OrientRP2}
\end{figure}

Otra variedad de la cual podemos estudiar su orientabilidad es $\projp^2$, que topológicamente es equivalente a un disco con los puntos antipodales identificados (\fref{fig:OrientRP2}). En ese caso, podemos tomar un subconjunto $U$ que es equivalente a la banda de Möbius, que no es orientable y por lo tanto $\projp^2$ no puede serlo.

\section{Grupos de Lie}

Otra forma interesante de definir variedades es a través de los grupos de Lie, que nos permiten mezclar la geometría y el álgebra.

\begin{defn}[Grupo\IS de Lie] \label{def:GrupoLie} Un grupo de Lie es un grupo $(G, \cdot)$ que admite una estructura de variedad diferenciable tal que las aplicaciones producto \begin{align*}
\appl{\cdot}{G × G&}{G} \\
(x,y) &\longmapsto x·y
\end{align*}  e inversa \begin{align*}
\appl{\inv{(·)}}{G&}{G} \\
x &\longmapsto \inv{x}
\end{align*} son diferenciables.
\end{defn}

Un ejemplo muy sencillo es $(ℝ^n, +)$, o también $(\quot{ℝ^n}{ℤ^n}, +)$. También podemos considerar a la circunferencia \crc como un grupo de Lie si lo tomamos como $\crc ⊂ (ℂ^*, ·)$, subgrupo de los complejos. En este caso, para el producto $f(e^{iθ},e^{iα}) = e^{iθ} · e^{iα} = e^{i(θ+α)}$ tendríamos que \[ φ_2(f(\inv{φ_1}(θ,α))) = φ_2(f(e^{iθ}, e^{iα})) = φ_2(e^{i(θ+α)}) = θ + α \] es efectivamente diferenciable. $\crc[3]$ también es un grupo de Lie, a través de los cuaterniones.

Esto es interesante, principalmente porque la condición de poder dotar de estructura de grupo a una variedad (o viceversa) es bastante restrictiva. De hecho, sólo las esferas $\crc$ y $\crc[3]$ tienen estructura de grupo de Lie.

En el caso de $\crc[3]$, el grupo es el de los cuaterniones, las raíces cuartas de la unidad. Sus elementos serán de la forma $u = a + bi + cj + dk$, con $i, j, k$ raíces cuartas de la unidad. La norma será $\norm{u} = a^2 + b^2 + c^2 + d^2$; y las cuentas que querremos hacer será ver que si $u,v ∈ \crc[3]$ entonces $uv ∈ \crc[3]$ o, en otras palabras, que si $\norm{u} = \norm{v} = 1$ entonces $\norm{uv} = 1$. Las cuentas infernales para demostrar eso o la existencia del inverso las dejamos fuera porque son horribles.

\section{Sumas conexas}

\begin{figure}[hbtp]
\centering
\inputtikz{SumaConexa}
\caption{Esquema de cómo realizar una suma conexa: se ``quitan'' dos entornos homeomorfos a una bola en ambas variedades y se ``pegan'' dos partes, uniendo los círculos oscuros y los claros.}
\label{fig:SumaConexa}
\end{figure}

Las variedades también se pueden construir como suma de otras dos variedades.

\begin{defn}[Suma\IS conexa] Sean $M_1^n, M_2^n$ dos variedades de dimensión $n$. Entonces, para dos cartas $φ_1$, $φ_2$ de $M_1$ y $M_2$ escogemos $p ∈ M_1,\,q∈M_2$ (los puntos de unión) y definimos \begin{align*}
M_1^* = M_1 \setminus \inv{φ_1} (\adh{\bola_{\sfrac{1}{2}}(0)}) \\
M_2^* = M_2 \setminus \inv{φ_2} (\adh{\bola_{\sfrac{1}{2}}(0)})
\end{align*} con $φ_1, φ_2$ cartas respectivas centradas en $p$ y $q$. En la \fref{fig:SumaConexa}, estamos quitando la parte punteada de las variedades. Podemos establecer un homomorfismo entre esas dos bolas que hemos quitado dado por \[ α(ρ,θ) = \left(\frac{1}{ρ}, τ(θ)\right) \], usando coordenadas polares y $τ$ una transformación arbitraria (es decir, que podemos girar las variedades al pegarlas).

Entonces podemos definir la relación que nos ``pega'' las variedades como \[ q_1 = \inv{φ_1}(ρ,θ) ∈ \inv{φ_1}(\bola_2 \setminus \adh{\bola_{\sfrac{1}{2}}}) \sim q_2 = \inv{φ_2}(α(ρ, θ)) ∈ \inv{φ_2}(\bola_2 \setminus \adh{\bola_{\sfrac{1}{2}}}) \] cuando estamos en la parte que identificamos, y trivial en el resto de puntos. Entonces podemos definir la suma conexa como
\[ M_1 \hash M_2 = \quot{M_1^* \cup M_2^*}{\sim} \] y sólo nos queda dar el atlas.

Para los puntos que no estén en la parte que pegamos, usamos el atlas que ya venía de la variedad. Para los puntos comunes, pues damos otro atlas que no sé cuál es.
\end{defn}

\begin{figure}[hbtp]
\centering
\inputtikz{SumaConexaResultado}
\caption{Resultado de la suma conexa de dos variedades}
\label{fig:SumaConexaResultado}
\end{figure}

Lo interesante de la suma conexa es que si ambas variedades son orientables, entonces la suma conexa es orientable.

\section{Característica de Euler}

Las sumas conexas también nos permiten definir un teorema de clasificación de superficies compactas.

\begin{theorem}[Teorema\IS de clasificación de superficies compactas] \label{thm:ClasificacionSuperficies} Toda superficie orientable y compacta $S ≠ \crc[2]$ es una suma conexa de toros, y toda superficie compacta no orientable $M$ es una suma conexa de planos proyectivos.
\end{theorem}

El teorema se basa en la característica de Euler, que definimos a continuación:

\begin{defn}[Característica\IS de Euler] \label{def:CaracteristicaEuler} Dada una variedad $M$, se define su característica de Euler como \[ χ(M) = V - A + C\] con $V, A, C$ los vértices, aristas y caras de una triangulación de $M$. \end{defn}

Esta característica es una invariante topológica, y además no depende de la triangulación. Algo que se puede dejar como ejercicio es el hallar la característica de Euler de la suma conexa, que sale \[ χ(M_1 \hash M_2) = χ(M_1) + χ(M_2) - 2 \]

\begin{proof}

Para sacarlo, habría que tener en cuenta que cuando se unen dos variedades, lo hacemos quitando un triángulo. Es decir, que cogemos la suma de vértices, aristas y caras de ambas triangulaciones, pero quitamos tres aristas (un triángulo pasa a estar compartido), tres vértices que también pasan a estar compartidos y las dos caras desaparecen.

"Formalizar" esta demostración está propuesto como ejercicio en la hoja 2.
\end{proof}


\begin{figure}[hbtp]
\begin{subfigure}{0.5\textwidth}
\centering
\inputtikz{TriangulacionToro}
\caption{Esquema para hacer la triangulación del toro a través de una variedad homeomorfa, en este caso un cubo sin un prisma central.}
\label{fig:TriangulacionToroPrisma}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\centering
\inputtikz{TriangToroPlano}
\caption{Dado que un toro es un cuadrado con los lados identificados, también podemos realizar la triangulación sobre ese cuadrado.}
\label{fig:TriangulacionToroPlano}
\end{subfigure}
\caption{Posibles triangulaciones para un toro, sin necesidad de mirar directamente el toro sino espacios homeomorfos a él.}
\label{fig:Triangulacion}
\end{figure}

Podemos ver, por ejemplo, la triangulación del toro para calcular su característica. Dado que es una invariante topológica, se puede hacer una triangulación sobre una variedad homeomorfa a él, como en la \fref{fig:TriangulacionToroPrisma}. Igualmente, también podemos realizar la triangulación sobre el cuadrado con los lados identificados, como en la \fref{fig:TriangulacionToroPlano} (ver \fref{fig:ToroEspacioCociente} si no nos acordamos de cómo se define el toro con los lados identificados). Eso sí, en este caso hay que tener en cuenta que la cuadrícula ha de ser de $3×3$ para que funcione bien con la identificación de lados. Si fuese de $2×2$, habría triángulos que intersecarían en dos puntos: en la mitad del borde y en la esquina.

\chapter{Espacios tangente y cotangente}
\label{chap:EspaciosTangenteyCotangente}

\section{Introducción y definición}

\begin{figure}[hbtp]
\centering
\inputtikz{EspacioTangenteIntroduccion}
\caption{Pongamos que queremos definir el cambio de temperatura (la variación de color azul a rojo) en cada punto $p$ de la curva $α$. Para eso necesitamos definir la dirección en la que nos movemos y también algún tipo de ``derivada'' que nos dé la variación en función de esa dirección. Estas necesidades dan lugar a las definiciones de espacio tangente y cotangente.}
\end{figure}

Una vez que ya hemos definido lo que es una variedad, lo más probable es que nos interese hacer algún tipo de cálculos de derivadas sobre ella. Por ejemplo, podemos querer ver cómo varía una función a lo largo de una curva definida en la variedad.

El problema es cómo definir la derivada. Si tenemos una variedad de dimensión 2 dentro de $ℝ^3$, una curva $α(t) = (x(t), y(t), z(t))$ tendrá tres derivadas parciales, pero eso no tiene sentido porque nosotros estamos trabajando en 2 dimensiones: no puede haber tres derivadas.

Necesitamos cambiar algo, crear algún tipo de estructura que nos permita hacer derivadas y poder trabajar cómodamente sin preocuparnos en exceso de en qué espacio ambiente estamos o cómo hemos ``incrustado'' nuestra variedad.

Para entenderlo, podemos fijarnos en qué es lo que hacemos cuando derivamos una función definida en, por ejemplo, $ℝ^2$. Lo primero que hacemos es sacar el gradiente como $\grad F = \left(\pd{F}{x}, \pd{F}{y}\right)$, pero no nos vale con eso. Lo que queremos es saber la variación de $F$ en una dirección en concreto, así que multiplicamos por un vector.

Para las variedades, haremos algo parecido. Iremos primero a por lo fácil: ¿en qué direcciones nos podemos mover cuando estamos metidos en una variedad? Está claro que no podemos salirnos de esa variedad a través de vectores normales (perpendiculares) a ella, así que sólo nos queda ir por vectores tangentes. Esto nos llevará a definir el espacio tangente, que denotaremos por $\tgs_p M$.

El siguiente paso será definir esos ``vectores gradiente'', que aplicaremos a vectores tangentes a la variedad. Esa ``aplicación'' se puede considerar como una aplicación lineal, y entonces el espacio de vectores gradiente será el espacio cotangente (que denotaremos por $\tgsd_p M$).

\section{Espacio tangente}

\textit{Nota: Las dos secciones siguientes son una mezcla de lo que dio Gabino en clase y de lo que hay en \citep[Capítulo I]{ApuntesGeoDif}.}

\subsection{Definición por medio de curvas}

¿Cómo definimos el espacio tangente a una variedad? Una primera aproximación puede ser tomar un punto $p ∈ M$, y una parametrización $φ$ de un entorno de $p$. Por ejemplo, cogemos la esfera y entonces podemos tomar las derivadas parciales: \( \tgs_p \crc[2] = \gen{\pd{φ}{x}, \pd{φ}{y}} = \gen{(1,0, \sfrac{-a}{c}), (0, 1, \sfrac{-b}{c})} \label{eq:EspTgEsfera} \)

Ahora bien, para hacer esto estamos apoyándonos en el hecho de que la esfera está en $ℝ^3$. ¿Qué hacemos si cogemos otra variedad abstracta, como $\projp^4$, y la metemos en otro espacio? Necesitamos una forma de definirlo independientemente del espacio ambiente. Ahora bien, el ejemplo que hemos puesto nos puede servir. Una forma, más bien poco rigurosa, de expresar el espacio tangente, sería darlo como el conjunto de los posibles vectores tangentes a curvas que pasan por $p$, tal que \[ \tgs_p \crc[2] = \set{α'(0) ∈ ℝ^3 \tq \appl{α}{(-ε, ε)}{\crc[2]}, \, α(0) = p} \] para $ε$ pequeño, donde α son curvas definidas en la esfera.

Un cálculo rápido nos permite ver que estamos cerca de lo que buscamos. Si tomamos $f(x,y,z) = x^2 + y^2 + z^2 - 1$, entonces $\crc[2] = \inv{f}(0)$. Por lo tanto, decir que $α(-ε, ε) ⊆ \crc[2]$ es equivalente a decir que $f(α(t)) = 0$ para $t ∈ (-ε, ε)$. Calculando su derivada, \[ 0 = (f ○ α)'(0) = \eval[2]{\Dif f}_{α(0) = p} · \eval[2]{\Dif α}_{0} = (2a, 2b, 2c) · α'(0) \], luego $α'(0)$ es ortogonal a $(a,b,c)$, y precisamente el espacio de los vectores ortogonales a $(a,b,c)$ es lo que habíamos definido antes como $\tgs_p \crc[2]$.

Por lo tanto, esto sí nos permitirá definir correctamente el espacio tangente a una variedad, aunque tendremos que sortear de alguna forma el hecho de que no sabemos derivar en espacios abstractos. Para ello, nos basaremos en las cartas y en los entornos de cada punto homeomorfos a $ℝ^n$, donde sí sabemos derivar.

\begin{defn}[Espacio\IS tangente] Sea $M$ una variedad diferenciable y sea $p ∈ M$. Definimos el espacio tangente a $M$ en el punto $p$ como el espacio cociente \[ \tgs_p M = \quot{\set{\appl{α}{(-ε, ε)}{M} \tq α(0) = p}}{\sim} \] con $\sim$ la relación de equivalencia dada por \[ α \sim β \iff (φ ○ α)'(0) = (φ ○ β)'(0)\] con $φ$ una carta de $p$. Los vectores de $\tgs_p M$ se escriben como $[α]$ o $α'(0)$, aunque esto es sólo notación ya que la derivada en la variedad no tiene demasiado sentido.
\end{defn}

La primera pregunta que podemos hacernos es si esta definición de $\tgs_p M$ depende de la carta elegida. Es decir, que si la derivada coincide con una carta, coincide también para cualquier otra carta. Bueno, la respuesta es que sí si hacemos unas cuantas operaciones (obviamente).

Con esta definición, se puede probar que $\tgs_p M$ es un espacio vectorial. La cuestión es ver de qué dimensión es, y la definición que hemos dado no nos va a ayudar demasiado.

\subsection{Definición por medio de derivaciones}

Ahora vamos a tomar un enfoque algo distinto. Sabemos qué queremos hacer, que es derivar funciones, así que vamos a tratar de dar una definición lo más minimalista posible de la derivada.

Lo primero que hacemos es definirnos el espacio de funciones sobre el que vamos a trabajar. Simplemente pediremos funciones definidas en un entorno de $p ∈ M$, y entonces definiremos \[ A_{p,M} = \set{\appl{f}{U}{ℝ} \tq p ∈ U,\, f ∈ C^∞} \], cuyos elementos serán los pares $(f,U)$ de las funciones y el entorno en el que están definidas.

Normalmente, los espacios de funciones tienen estructura de espacio vectorial, lo cual nos vendría muy bien en esta definición. Pero tenemos un problema, y es que no sabemos exactamente cómo podemos sumar dos funciones que están definidas en conjuntos distintos. Por suerte, esto se puede salvar definiendo una relación de equivalencia: \[ (f_1, U_1) \sim (f_2, U_2) \iff f_1 = f_2\text{ en }W ⊂ U_1 ∩ U_2 \], que está bien definida por ser los $U$ entornos abiertos que contienen a $p$.

Ahora podemos definir \[ \mathcal{A}_{p,M} = \quot{A_{p,M}}{\sim} \], que es un espacio vectorial y un anillo local (ver \citep{ApuntesAlgebraConmutativa}). Y con esto no sólo nos hemos quitado el problema de no saber sumar funciones, sino que además hemos metido muy, muy sutilmente el concepto de derivada: si dos funciones son equivalentes, entonces son iguales en un entorno de $p$, y por lo tanto tendrán la misma derivada en $p$. Luego podremos definir las derivadas, que aquí llamaremos derivaciones.

\begin{defn}[Derivación] \label{def:Derivacion} Una derivación en $p$ es una aplicación
\begin{align*}
\appl{D}{\mathcal{A}_p&}{ℝ} \\
f &\longmapsto D(f)
\end{align*}

Queremos que esta función conserve de alguna forma la noción de derivada en una dirección, así que buscaremos varias propiedades:

\begin{enumerate}
	\item $D$ es lineal.
	\item $D(λ) = 0$, donde $λ$ es una función constante $λ(x) = λ ∈ ℝ$.\footnote{En realidad, esta propiedad es consecuencia de las otras dos pero viene bien tenerla presente.}
	\item \label{prp:LeibnizVectorTg} Si esto se parece a una derivada, además de definir cómo se derivan las sumas\footnote{Por esto forzamos que $D$ sea lineal.} definiremos cómo se derivan los productos, según la regla de Leibniz: \[ D(f·g) = f · D(g) + D(f) · g\]
\end{enumerate}
\end{defn}

Una vez que hemos emulado las direcciones en las que podemos definir las derivadas, ahora sí podemos definir el espacio tangente:

\begin{defn}[Espacio\IS tangente] Diremos que el espacio tangente a $p$ en $U$ se define como
\[ \tgs_p U ≝ \set{\appl{D}{\mathcal{A}_p}{ℝ}\tq D \text{ es derivación }} \]
\end{defn}

Un ejemplo es ver que, si tomamos una función $\appl{f}{M}{ℝ}$ y una carta $\inv{φ}(t_1, \dotsc, t_n) \equiv (x_1, \dotsc, x_m)$ entonces $\eval[2]{\pd{}{x_i}}_p$ es una derivación que se aplica de la siguiente forma:
\( \eval{\dpd{}{x_i}}_p (f) ≝ \eval{\dpd{f○\inv{φ}}{t_i}}_{φ(p)} \label{eq:DerivacionTangente} \)

Además, las derivaciones son una base de el espacio tangente.

\begin{prop} $\set{\eval[2]{\pd{}{x_1}}_p, \dotsc,\eval[2]{\pd{}{x_n}}_p}$ es una base de $\tgs_p M$. En particular, $\dim \tgs_p M = \dim M$.
\end{prop}

\begin{proof}

Para demostrar que es una base, necesitamos ver que son linealmente independientes y además que generan el espacio. Vamos por partes.

\proofpart{Independencia}

\begin{gather*}
\sum_i = a_i \eval{\dpd{}{x_1}}_p = \vec{0} ∈ \tgs_p M \\
0 = \vec{0}(x_j) = \sum a_i \eval{\dpd{}{x_1}}_p  = a_j \; ∀j
\end{gather*} % OK.

\proofpart{Generadores}

Tenemos que ver que cualquier vector $v ∈ \tgs_p M$ se puede expresar como suma de derivaciones, esto es, que \( v = \sum a_i \eval{\dpd{}{x_i}}_p \label{eq:pruebaGeneradoresPlanoTg} \)

Debe ocurrir que $v(x_j) = a_j$. Sea $\appl{λ}{U_p}{ℝ}$ una función arbitraria, queremos ver qué es $$v(\lambda) = \sum\limits_{i=1}^{n} a_i \eval{\dpd{}{x_i}}_p (\lambda) $$

Empezamos calculando el desarrollo de Taylor de $\lambda$: $$\lambda = \lambda(p) + \left(\sum\limits_{i=1}^{n} \eval{\dpd{\lambda}{x_i}}_p (x_i - p)\right) + \frac{1}{2}\set{\eval{\dpd{\lambda}{x_i \partial x_j}}_p (x_i - p)(x_j - p)}_{i,j=1,\dots, n} + \dots$$

Y sustituimos en \ref{eq:pruebaGeneradoresPlanoTg}
$$ v(\lambda) = \underbrace{v(\underbrace{\lambda(p)}_{\text{constante}})}_{=0} + \underbrace{v(x_i)}_{=a_i} \sum \eval{\dpd{\lambda}{x_i}}_p + \frac{1}{2}\set{\eval{\dpd{\lambda}{x_i \partial x_j}}_p \underbrace{v((x_i - p))}_{=0}\underbrace{v((x_j - p))}_{=0}} + 0 \dots $$

Con lo que obtenemos $$v(\lambda) = \sum a_i \eval{\dpd{\lambda}{x_i}}_p $$

\end{proof}

\obs En la prueba anterior, hemos abusado de notación\footnote{y volverá a pasar}: $$ \eval{\dpd{\lambda}{x_i}}_p := \eval{\dpd{(\lambda \circ \varphi^{-1})}{x_i}}_{\varphi(p)} $$


¿Qué pasa con el cambio de coordenadas? En este caso, podemos hacer cuentas y las cosas funcionan bien:\[ \eval{\dpd{}{y_j}}_p = \sum_{i=1}^n \eval{\dpd{x_i}{y_j}}_p · \eval{\dpd{}{x_i}}_p \]

Se puede demostrar además que este espacio tangente es equivalente al que definimos mediante curvas.

\subsection{Aplicaciones entre variedades}

\begin{figure}[hbtp]
\centering
\inputtikz{AplicacionYDiferencial}
\caption{Ilustración de la diferencial de una aplicación entre variedades: nos lleva vectores tangentes a la variedad origen a vectores tangentes a la variedad de destino.}
\label{fig:AplicacionYDiferencial}
\end{figure}

Una vez que tenemos la definición del espacio tangente, ya podemos empezar a estudiar aplicaciones entre variedades y discutir condiciones sobre su ``derivada'' o, como la llamaremos en geometría, su diferencial. Eso sí, antes tenemos que definir bien qué entendemos por una aplicación diferenciable entre variedades.


\begin{defn}[Función\IS diferenciable] \label{def:FuncionDiferenciableVariedades} Sea $\appl{F}{M}{N}$ una aplicación entre variedades diferenciables. Decimos que es una \textbf{función diferenciable} si para toda carta $(U_i, Φ_i)$  en $X$ y para toda carta $(V_j, Ψ_j)$ en $Y$, la composición $Ψ_j ○ F ○ \inv{Φ_i}$ es diferenciable ($C^∞$) entre abiertos de $ℝ^n$ y $ℝ^m$ en todo punto donde está definida.

Esto es lo mismo que decir que $F$ es compatible con las cartas.
\end{defn}

A partir de esto, ya podemos definir qué es la diferencial de estas aplicaciones, lo que será el análogo del Jacobiano.

\begin{defn}[Diferencial\IS de una aplicación] \label{def:DiferencialAplicacion} Sean $M^m$, $N^n$ dos variedades de dimensión $m$ y $n$ respectivamente, y sea $\appl{F}{M}{N}$ una aplicación entre ellas. Entonces, dado un punto $p ∈ M$, se define la diferencial de $f$ en $p$ como la aplicación lineal dada por \begin{align*}
\appl{\eval[1]{\dif F}_p}{\tgs_p M&}{\tgs_{f(p)} N} \\
D &\longmapsto \eval[1]{\dif F}_{p} (D)
\end{align*}, donde $\eval[1]{\dif F}_{p} (D)$ será una derivación de $\tgs_{F(p)} N$ dada por lo siguiente:
\begin{align}
\appl{\eval[1]{\dif F}_{p} (D)}{\mathcal{A}_{F(p), N} &}{ℝ} \nonumber \\
g &\longmapsto D(g ○ F) \label{eq:DiferencialDerivacion}
\end{align}
\end{defn}

En el fondo, lo único que hacemos es tener una aplicación lineal de un espacio vectorial a otro: la diferencial (que en ocasiones se llama \concept{Pushforward}) nos dará la imagen en $\tgs_{F(p)} N$ de un vector tangente en $\tgs_p M$. Si además consideramos los elementos de $\tgs_p M$ como derivaciones, lo único que estamos haciendo en \eqref{eq:DiferencialDerivacion} es ``adaptar'' $D$, que vive en $\tgs_pM$, para poder hacer las derivaciones de funciones $g ∈ \mathcal{A}_{F(p), N}$.

Aunque esta definición de diferencial la hayamos hecho desde las derivaciones, podemos ver que si lo hubiésemos hecho con curvas nos habría salido lo mismo. Si consideramos una curva $\appl{c}{(-ε, ε)}{M}$ con $c(0) = p$, sabemos que $c'(0) ∈ \tgs_p M$. En ese caso, la diferencial será \[ \eval[1]{\dif F}_p (c'(0)) = (f○c)'(0)\] como uno podría esperar.

Otra de las cosas que uno podría esperar es que una derivada nula implica función constante. Sólo necesitaremos una condición extra, que es la conexión de la variedad.

\begin{prop} \label{prop:DiferencialNulaFuncConstante} Dada una variedad $M$ conexa y una aplicación diferenciable $\appl{F}{M}{N}$, si se cumple que $\difp \equiv 0\; ∀p ∈ M$, entonces $F$ es constante.
\end{prop}

\begin{proof} Queremos ver que si $F(x_0) = y_0$ para algún $x_0 ∈ M$, entonces $F(x) = y_0$ para todo $x ∈ M$. Esto es exactamente lo mismo que decir que $\inv{F}(y_0) = M$.

Ahora bien, como el conjunto $\set{y_0}$ es cerrado, su imagen inversa por $F$ (que por ser diferenciable es continua) ha de ser un cerrado.

Por otra parte, como en $x_0$ la diferencial se anula, entonces $F$ es constante en un entorno de $x_0$, luego \noteby{Guille}{Esto hay que resolverlo bien.}
\end{proof}

También podemos ver si aquí tenemos igualmente que la diferencial se anula en máximos o mínimos. Como es de esperar, no tenemos problemas con eso.

\begin{prop} Sea $\appl{f}{M}{ℝ}$ con $p ∈ M$ un máximo o mínimo local. Entonces $p$ es un \concept{Punto\IS crítico} y $\difp[f] \equiv 0$.
\end{prop}

\begin{proof} Si $f ○ \inv{φ}$ tiene un máximo en $φ(p)$ para una carta φ de $p$, entonces $\Dif (f ○ \inv{φ}) \equiv 0$. En otras palabras, \[ 0 = \eval[2]{\dpd{f ○ \inv{φ}}{t_k}}_{φ(p)} ≝ \eval[2]{\dpd{f}{x_k}}_p ∀k \implies \difp = 0\]
\end{proof}

\subsubsection{Inmersiones y subvariedades}

Un caso especial de aplicaciones entre variedades son las que nos permiten meter variedades en otras de una forma más o menos regular. A su vez, esto nos dará lugar a poder definir qué es una subvariedad, es decir, de qué forma podemos dar estructura de variedad a un subconjunto de otra variedad.

En general, consideraremos ahora una aplicación $\appl{F}{M}{N}$, $F ∈ C^∞$ (infinitamente diferenciable). La primera definición es la de inmersión, que es una forma de meter una variedad dentro de otra.

\begin{defn}[Inmersión] Si $\appl{\difp}{\tgs_p M}{\tgs_{F(p)} N}$ es inyectiva $∀p ∈ M$ entonces es una inmersión. Equivalentemente, $F$ es inmersión si y sólo si \[ \mop{rango} \difp  = \dim M\]
\end{defn}

La condición de que la diferencial sea inyectiva nos permite evitarnos casos en los que el espacio tangente ``colapse''. En el ejemplo de la \fref{fig:FuncionNoInmersion} se puede ver cuando $y = 0$ que el espacio tangente, que en teoría nos tiene que decir todas las direcciones en las que podemos movernos sin salir de la variedad, se convierte en una recta. El trabajar con inmersiones nos permitirá asegurarnos de que el espacio tangente nunca nos va a dejar encerrados en sólo una parte de la variedad.

\begin{figure}[hbtp]
\centering
\inputtikz{FuncionNoInmersion}
\caption{La aplicación $F(x,y) = (x, y^2)$ es una aplicación que nos mete el plano dentro de $ℝ^2$, pero no es una inmersión por los puntos donde $y = 0$.}
\label{fig:FuncionNoInmersion}
\end{figure}

Con sólo una pequeña restricción más, podemos definir lo que es una subvariedad. Simplemente querremos asegurarnos de que cada punto en la variedad destino es imagen de un único punto en la variedad origen. Es decir, buscaremos que la función sea inyectiva.

\begin{defn}[Subvariedad] Dados $M ⊂ N$ y $F$ la identidad, se dice que $(M,F)$ es un subvariedad de $N$ si $F$ es una inmersión inyectiva.
\end{defn}

Por último, queremos tener alguna noción que nos permita decir que una aplicación mete una variedad dentro de otra sin cambiarla esencialmente. Esto es lo que llamaremos un embedding o incrustación:

\begin{defn}[Embedding] Si $F$ tiene diferencial inyectiva, es inyectiva y homeomorfismo sobre su imagen, entonces es un \textit{embedding} o incrustación.
\end{defn}

\begin{example} La aplicación \begin{align*}
\appl{F}{M&}{N} \\
t &\longmapsto (\cos t, \sin t)
\end{align*} es una inmersión con diferencial \[ \dif F \left(\dpd{}{t}\right) = - \sin t · \pd{}{x_1} + \cos t \pd{}{x_2} \] que, aunque no se anula, no es subvariedad.

Sin embargo, si tomásemos $\appl{F}{\crc[1]}{ℝ^2}$ con $f$ la identidad y una carta en polares para $\crc[1]$, entonces sí es inmersión y subvariedad.
\end{example}

También nos interesará estudiar un concepto equivalente al de las proyecciones, que aquí llamaremos submersiones.

\begin{defn}[Submersión] \label{def:Submersion} Si $F$ es suprayectiva y la diferencial también es suprayectiva, entones es una submersión.
\end{defn}

\subsubsection{Fibración de Hopf}
\label{sec:FibracionHopf}

Para acabar con esta sección de aplicaciones, vamos a ver un ejemplo interesante de submersión, que además nos servirá en un futuro para estudiar las cohomologías. Este ejemplo es la \concept{Fibración\IS de Hopf}, que nos da una proyección de $\crc[3]$ en la esfera de $\crc[2]$.

Es interesante porque, tal y como se explica en \citep{liuHopfFibration}, es uno de los primeros ejemplos de fibrados no triviales.

La primera pregunta que debemos hacernos es qué es un \concept{Fibrado}. A grandes rasgos, se trata de la identificación de un espacio $E$ con copias de un espacio $F$ (las fibras) sobre una base $B$. Más formalmente, lo que pedimos es que exista una submersión continua $\appl{π}{E}{B}$ tal que cualquier abierto $U ⊂ E$ es homeomorfo a $π(U) × F$ o, en otras palabras, que la variedad $E$ sea localmente como el cociente $B × F$.

\begin{figure}[hbtp]
\inputtikz{CilindroFibrado}
\caption{El cilindro se puede tomar como un fibrado: a cada altura $h ∈ ℝ$ le asignamos una circunferencia $\crc[1]$. La proyección $π$ nos llevará después cada punto de la circunferencia a su altura correspondiente. Es una forma de ver el cilindro como $ℝ × \crc[1]$.}
\label{fig:CilindroFibrado}
\end{figure}

Por ejemplo, el cilindro se puede tomar como un fibrado $ℝ × \crc[1]$ (ver \fref{fig:CilindroFibrado}): las circunferencias en cada punto de $ℝ$ son las ``fibras''. Este es un fibrado trivial porque no sólo es localmente homeomorfo al producto, sino que toda la variedad es $ℝ × \crc[1]$.

En el caso de la fibración de Hopf, lo que la hace interesante es que es un fibrado no trivial. Aunque en cada punto de la esfera $\crc[2]$ le asignamos una circunferencia $\crc[1]$, al juntarlo todo no vamos a tener que globalmente $\crc[3] \cong \crc[2] × \crc[1]$: el fibrado sólo ``aparece'' cuando lo miramos localmente.

\begin{figure}[hbtp]
\centering
\includegraphics[width = 0.8\textwidth]{img/Hopf_Fibration.png}
\caption{Una representación de la fibración de Hopf, que lleva cada circunferencia de $\crc[3]$ a un punto en $\crc[2]$. Imagen vía \href{http://nilesjohnson.net/hopf.html}{Niles Johnson}, que tiene alguna animación chula sobre el tema.}
\label{fig:FibracionHopf}
\end{figure}

Para definir este fibrado trataremos de hacer algunas cuentas con la esfera $\crc[3]$. Sabemos que está dada por \[ \crc[3] = \set{ x,y,z,t ∈ ℝ \tq x^2 + y^2 + z^2 + t^2 = 1 } \]

La primera idea feliz es que la ecuación de arriba es equivalente a que $\abs{z}^2 + \abs{w}^2 = 1$ para $z,w ∈ ℂ$. Así, podemos meter la esfera dentro del plano completo: $\crc[3] ⊆ ℂ^2$.

La siguiente idea feliz es acordarnos de los espacios proyectivos de la \fref{sec:EspacioProyectivo}. Ahí definíamos $\projcp^1$, y de forma análoga a como teníamos en los reales (ver \fref{fig:EsferaPlanoProj}), en realidad no era más que la esfera con los puntos antipodales identificados. De la esfera podemos pasar a su proyección estereográfica, de tal forma que $\projcp^1 \cong ℂ ∪ \set{∞}$.

Con esto ya podemos definir nuestra proyección de $\crc[3]$ (tomada como subconjunto de $ℂ^2$) al espacio base del fibrado:
\begin{align*}
\appl{π}{\crc[3]&}{\projcp^1} \\
(z, w) &\longmapsto [z\cln w]
\end{align*}

El hecho clave de esta proyección es ver que dos puntos $(z,w), (z', w') ∈ ℂ^2$ tienen la misma imagen si existe un $λ ∈ ℂ$ tal que $z' = λz, w' = λw$, ya que en ese caso $[z\cln w]$ y $[z'\cln w']$ son la misma clase de equivalencia. Y ahora ya para la traca final, en ese caso y sabiendo que $\abs{z}^2 + \abs{w}^2 = \abs{z'}^2 + \abs{w'}^2 = 1$, tenemos que \[ 1 = \abs{z'}^2 + \abs{w'}^2  = \abs{λz}^2 + \abs{λw}^2 = \abs{λ}^2\left(\abs{z}^2 + \abs{w}^2 \right) = \abs{λ}^2 \] luego $\abs{λ} = 1$.

En otras palabras, lo que nos dice esto es que la proyección nos lleva todos los puntos de una misma circunferencia (multiplicar con un complejo de módulo 1 es simplemente rotar en una circunferencia) a un único punto en $\projcp^1$. Así, cada punto $[z\cln w] ∈ \projcp$ tiene asociada la fibra de puntos $e^{iθ}·(z,w)$ con $θ ∈ [0, 2π)$, que como decíamos es una cierta circunferencia. Para el resto de cuentas y una formalización algo mejor, es interesante leer \citep{liuHopfFibration} (que además habla de mecánica cuántica, que siempre mola).

Para nuestro caso de la geometría y topología, lo que nos faltaría ver es que efectivamente $\crc[3] \not\cong \crc[1] × \crc[2]$. Una demostración con cuentas sería totalmente infernal, pero sí que se podrá hacer con las cohomologías de De Rham que veremos en el \fref{chap:CohomologiaDeRham}.

\section{Campos vectoriales}

\begin{defn}[Campo\IS vectorial] Un campo vectorial en $M$ es una aplicación \begin{align*}
\appl{X}{M&}{\tgs M = \bigcup \tgs_p M} \\
p &\longmapsto X(p) ∈ \tgs_p M
\end{align*}, donde $\tgs M$ es lo que se llama el \concept{Fibrado\IS tangente}.
\end{defn}

\begin{defn}[Campo\IS vectorial diferenciable] Un campo vectorial $X$ en $M$ se dirá diferenciable si localmente (esto es, en cada carta $(U, φ \equiv(x_1, \dotsc, x_m))$) es de la forma \[ X(p) = \sum_{i=1}^m λ_i(p) \eval{\dpd{}{x_i}}_p \] donde $λ_i ∈ C^∞(U)$. En realidad, la única cosa peculiar que pedimos es la diferenciabilidad de las $λ_i$, ya que siempre sabemos que un punto del tangente va a depender de esa base.
\end{defn}

Como siempre, el concepto no depende de la carta elegida.

\begin{example} Podemos dar un cambio en $\crc[2]$, a través de las dos cartas. Las definimos primero: \begin{gather*} U = \crc[2] \setminus \set{(0,0,-1)} \\
V = \set{ (x,y,t)  ∈ \crc[2] \tq t < 0}
\end{gather*} de tal forma que el campo puede ser \[ X(p) = \begin{cases}
\eval{\dpd{}{x_1}}_p & p ∈ U \\
\dfrac{1}{2} \left(-x^2 + y^2 + (1+t)^2\right) \eval{\dpd{}{y_1}}_p - xy \eval{\dpd{}{y_2}}_p & p ∈ V
\end{cases}\]

Desde luego, si $X$ es un campo tendrá que ser diferenciable. El problema es que no hemos comprobado si está bien definido, esto es, si ambas definiciones van a dar el mismo valor en los puntos de $U ∩ V$.

No pienso copiar las cuentas. El resultado es que sí, que está bien.
\end{example}

\begin{theorem}[Teorema\IS de Poincaré-Hopf]Sea $X$ un campo definido en $M$. Entonces el número de ceros ``bien contados''\footnote{Es una enunciación del teorema muy vaga y poco formal, pero para enunciarlo bien hay que meterse en líos de índices de campos y no es plan.} coincide con la \nlref{def:CaracteristicaEuler} de la variedad $M$.
% Vaya soberana basura de enunciado.
\end{theorem}



Esto nos permite decir que el campo del ejemplo tiene que tener algún cero. Que tendrá que anularse en el polo sur, cuando $t = -1$, y bueno, que sí, que se anula. Con índice de anulación (concepto que estoy convencido de que se acaba de inventar) igual a 2. Ejemplo de campo en el toro que no se anula luego el toro tiene característica 0. Good news.

\section{Espacio cotangente}

Una vez que tenemos definido el espacio tangente, vamos a querer hacer algo más que simplemente hablar de vectores tangentes. Querremos hablar de aplicaciones lineales sobre ellos, aplicaciones que por ejemplo nos puedan dar la longitud de un vector o, más tirando a nuestro campo de estudio, que nos permitan estudiar la topología a través de las formas diferenciales.

\begin{defn}[Espacio\IS cotangente] Dada una variedad $M$, se define el espacio cotangente a $M$ en $p ∈ M$ como el espacio dual de $\tgs_p M$, esto es, \[ \tgsd_p M = \mop{Hom}_ℝ (\tgs_p M, ℝ) = \set{\appl{A}{\tgs_p M}{ℝ} \tq A\text{ lineal, continua}} \]
\end{defn}

Por ser el dual, el cotangente tiene la misma dimensión que el tangente. Eso sí, el isomorfismo no es canónico. Si $(U, φ \equiv (x_1, \dotsc, x_m))$ es una carta alrededor de $p ∈ M$, entonces $\set{\eval[1]{\pd{}{x_i}}_p}_{i = 1, \dotsc, m}$ es una base de $\tgs_p M$. Nosotros denotaremos como $\set{\difp[x_i]}_{i=1,\dotsc, m}$ la base de $\tgsd_p M$ de tal forma que \[ \difp[x_i] \left(\eval{\dpd{}{x_j}}_p\right) = δ_{ij}\] donde $δ_{ij}$ es la \concept{Delta\IS de Kronecker}\footnote{$δ_{ij} = 1$ si $i = j$, $0$ en otro caso.}.

Como siempre, tendremos que tener cuidado con el cambio de coordenadas.

\textit{N.A.: A partir de ahora voy a pasar bastante de escribir dónde se evalúan los elementos de la base de los espacios tangentes, así que daré simplemente por supuesto que es en el punto $p$ y entonces $\difp[x_i] = \dif x_i $, y $\eval[1]{\pd{}{x_i}}_p \equiv \pd{}{x_i}$. De hecho, por pura vaguería, diré que $\pd{}{x_i} \equiv ∂x_i$ tal que $\pd{f}{x_i} \equiv ∂x_i(f)$ y hala, todo bastante más sencillo.}

Si tenemos $(V, (y_1, \dotsc, y_m))$ otra carta, entonces podremos expresar \[ \dif x_i = \sum_{k=1}^m ∂y_k (x_i) · \dif y_k \]

La igualdad la probaremos aplicando a ambos lados $∂y_j$: \[ \dif x_i (∂y_j) =  \left(\sum_{k=1}^m ∂y_k (x_i) · \dif y_k \right)(∂y_j) = ∂y_j (x_i)\] y por otro lado \[ \dif x_i (∂y_j) = \dif x_i \left(\sum_{k=1}^m ∂y_j(x_k) · ∂x_k\right) = \sum_{k=1}^m ∂y_j(y_k) · \dif x_i (∂x_k) = ∂y_j(x_i) \], que efectivamente es lo mismo.

Con esto ya podemos definir lo que es un campo de covectores ó 1-forma diferencial.

\begin{defn}[Campo\IS de covectores] \label{def:CampoCovectores} Un campo de covectores ó 1-forma diferencial es una aplicación \begin{align*}
\appl{ω}{M&}{\bigcup \tgsd_p M = \tgsd M} \\
p &\longmapsto ω(p) ∈ \tgsd_p M
\end{align*}, donde $\tgsd M$ es lo que llamaremos el \concept{Fibrado\IS cotangente}.
\end{defn}

Un ejemplo sencillo es ver que, dada una aplicación $\appl{f}{M}{ℝ}$ con $f ∈ C^∞$, $\dif f$ es una forma diferencial. ¿Cómo podemos definir exactamente qué es la diferencial de una función? Antes hemos visto que $\dif x_i$ son los elementos de la base de $\tgsd_p M$. Intuitivamente, lo que podemos ver es que $\dif x_i (∂x_j) = δ_{ij}$ nos dice algo que en el fondo es bastante obvio: que la coordenada $x_i$ sólo varía cuando nos movemos en su misma dirección, y que si nos movemos la dirección de cualquier otra coordenada entonces $x_i$ no cambia.

Así, de forma análoga, lo que podemos ver es que $\dif f$ será una aplicación que nos diga cuánto varía $f$ cuando nos movemos en una cierta dirección $(v_1, \dotsc, v_m) = \vv ∈ \tgs_p M$. Si pensamos en el gradiente que vemos en cursos de cálculo, lo que hacemos en el fondo es multiplicar la coordenada $i$-ésima de la dirección en la que nos movemos (la dada por el vector $\vv ∈ \tgs_p M$) por la derivada parcial de $f$ con respecto a la $x_i$. En otras palabras, lo que debería ser $\dif f$ es algo como esto: \[ \dif f = \sum_{i=1}^m \pd{f}{x_i} · \dif x_i \], de tal forma que cuando lo quisiésemos aplicar a un vector $\vv = \sum v_i ·∂x_i$ nos quedaría \[ \dif f (\vv) = \sum_{i=1}^m \pd{f}{x_i} · \dif x_i (\vv) = \sum_{i=1}^m \pd{f}{x_i} · \dif x_i \left(\sum_{j=1}^m v_j · ∂x_j\right) = \sum_{i=1}^m \pd{f}{x_i} · v_i \], que es efectivamente lo que nos sale\footnote{Para el último salto, hemos usado que $\dif x_i$ es una aplicación lineal y que $\dif x_i (∂x_j) = δ_{ij}$.} si hubiésemos pensado en el gradiente de $f$.

\section{Formas diferenciales}

Cuando hemos definido el concepto de \nlref{def:CampoCovectores}, hemos dicho que también se podría llamar 1-forma diferencial. El nombre no es casualidad, así que vamos a ponernos a definir qué son las formas diferenciales y para qué sirven. Eso sí, primero tendremos que ir con las definiciones algebraicas.

\begin{defn}[Forma\IS multilineal alternada de grado $d$] Una forma multilineal alternada de grado $d$ sobre un espacio vectorial $V$ es una aplicación \[ \appl{T}{\underbracket{V×\dotsb ×V}_{d\text{ copias}}}{ℝ} \] lineal en cada variable, y donde alternada significa que si se repite un vector argumento el resultado es cero o, equivalentemente, que si cambiamos de orden dos vectores entonces cambia de signo \[ T(\dotsc, v_i, \dotsc, v_j, \dotsc) = - T(\dotsc, v_j, \dotsc, v_i, \dotsc)\]
\end{defn}

En lo que nos interesa a nosotros, $V$ será el espacio cotangente $\tgsd_p M$. Además, el espacio de todas estas formas lo denotaremos\footnote{Creo que en \citep{ApuntesGeoDif} esto lo llamamos el álgebra pinchorial. Magnífico nombre.} como $\mop{Alt}^d (V)$. Dos ejemplos sencillos es ver que $\mop{Alt}^0 (V) = ℝ$, o que $\mop{Alt}^1(V) =V^*$.

Además, si $\dim V = n$, entonces $\dim \mop{Alt}^n = 1$, y una base es el determinante, esto es, \begin{align*} \appl{\det}{V × \dotsb × V&}{ℝ} \\
(v_1, \dotsc, v_n) &\longmapsto \det \begin{vmatrix} v_1^1 &  & v_n^1 \\ \vdots & \ddots & \vdots \\ v_1^n & & v_n^n \end{vmatrix}
\end{align*}

En general, podremos dar una base para el espacio de formas alternadas: a partir de una base $\set{e_i}_{i=1, \dotsc, n}$ de $V$, la base será la dada por covectores $\set{e_{i}^*}$\begin{align*}
\appl{e_{i_1}^* ∧ \dotsb ∧ e_{i_d}^*}{\overbracket{V × \dotsb × V}^{d\text{ copias}}}{ℝ} \\
(v_1, \dotsc, v_d) = \det \begin{vmatrix}
e_{i_1}^* (v_1) & \cdots & e_{i_d}^* (v_1) \\
\vdots & \ddots & \vdots \\
e_{i_1}^* (v_d) & \cdots & e_{i_d}^* (v_d)
\end{vmatrix} \end{align*} donde $e_i^* (e_j) = δ_{ij}$. En otras palabras, $e_{i}^*(v)$ extrae la coordenada $i$-ésima del vector $v$ o, equivalentemente, si $v = \sum v_i e_i$, entonces $e_i^* (v) = v_i$.

\seprule[Clase mágica pendiente]

Tenemos nuestro espacio $Ω^k(M)$ de formas diferenciales de grado $k$, es decir, formas $ω ∈ Ω^k(M)$ entonces \begin{align*}
\appl{ω}{M&}{\mop{Alt}^k(\tgs_p M)} \\
p & \longmapsto ω(p)
\end{align*}

Sabíamos que $Ω^0(M) = C^∞(M)$ y que además, localmente, podemos expresar las formas diferenciales como \[ \restr{ω}{U} = \sum a_{i_1, \dotsc, i_k} \dif x_1 \y \dotsb \y \dif x_k \] donde $\set{x_i}$ es un sistema de coordenadas del entorno $U$ y con $\set{\dif x_i}$ el dual de $\set{\dpd{}{x_i}}$.

\subsection{Pullback}

Dada una aplicación $\appl{F}{M}{N}$, entonces podemos definir el pullback que nos lleva formas diferenciales de $N$ a $M$, dado por \begin{align*}
	\appl{F^*}{Ω^k(N)&}{Ω^k(M)} \\
	ω &\longmapsto f^* ω
\end{align*} donde
\begin{align*}
	\appl{(F^*ω)(p)}{\tgs_p M × \dotsb × \tgs_p M&}{ℝ} \\
	(v_1, \dotsc, v_k) &\longmapsto ω(F(p)) \left(\difp(v_1), \dotsc, \difp (v_k) \right)
\end{align*}
donde $\difp$ es la \nref{def:DiferencialAplicacion}.

Para ver cómo funciona el pullback, vamos a ver un ejemplo para
\[
	\appl{f}{ℝ^m}{ℝ^n}
\]
, con $f(x) = (f_1(x), \dotsc, f_n(x))$ y coordenadas $(x_1, \dotsc, x_m)$ para $ℝ^m$ e $(y_1, \dotsc, y_n)$ para $ℝ^n$.
Entonces, dada una forma diferencial $\dif x_j ∈ Ω^1(ℝ^m)$ entonces \[ f^*(\dif y_j) = \sum_{k=1}^m \pd{f_j}{x_k} \dif x_k = \dif f_j \]

Para comprobarlo, lo que hacemos es un chorro de cuentas aplicando eso a vectores tangentes $∂y_j$ y sale.

Ahora nos podemos hacer una pregunta para hacer cuentas todavía más infernales: ¿qué ocurre si cogemos una $2$-forma $\dif y_i ∧ \dif y_j ∈ Ω^2(ℝ^n)$? Bueno, pues lo que sale es que
\[
	f^* (\dif y_i ∧ \dif y_j)
	= \sum_{k,l=1}^n \dpd{f_i}{x_k} \dpd{f_j}{x_k} \dif x_k ∧ \dif x_l
\], y de nuevo si lo aplicamos a dos vectores, tenemos que
\[
	\left(\sum_{k,l=1}^n \pd{f_i}{x_k} \pd{f_j}{x_k} \dif x_k ∧ \dif x_l\right)(∂x_r, ∂x_s)
	= \pd{f_i}{x_r} \pd{f_k}{x_s}
\] por la definición de base dual ($\dif x_i (∂x_j) = δ_{ij}$). Por el otro lado,
\begin{align*}
\left(f^*(\dif y_i ∧ \dif y_j)\right) (∂x_r, ∂x_s)
	&= \dif y_i ∧ \dif y_j\left(\dif f (∂x_r), \dif (∂x_s)\right) = \\
	&= \dif y_i ∧ \dif y_j
		\left(
			\sum_{α=1}^m \dpd{f_α}{x_r} ∂x_α,
			\sum_{β=1}^m \dpd{f_β}{x_s} ∂x_β,
		\right) = \\
	&= \dpd{f_i}{x_r} \dpd{f_j}{x_s}
\end{align*}

\noteby{Guille}{Esa mierda de ahí no tiene sentido, no sé dónde cogemos los vectores ni los duales ni nada.}

\subsection{Expresión local de formas diferenciales}

Eso. Y alternaciones. Porque las cartas son localmente $ℝ^n$. Y obviamente coinciden para las cartas.

\begin{prop} $ω(p)$ no depende de la carta elegida para $p$.
\end{prop}


%%%% CLase copiada por dejuan.
\[
	\Omega(M) = \bigoplus_{k≥0} \Omega^K(M)
\]
Es el espacio de todas las formas diferenciales y es un espacio vectorial.


Sea $ω$ una forma diferencial de grado $d$. Entonces,

\[
	ω \equiv \left\{
			ω_u\in\Omega_+^d(φ(u))\\
				+
			ω_v = (φ_uφ_v^{-1})^\ast ω_u
	\right\}_{u\in \text{atlas}}
\]

De esta manera, podemos escribirlo en términos de funciones, tomando $I$ como multiíndice, con lo que $dx_I = dx_{i1}\wedge ... \wedge dx_{in}$

\[ω_u = \sum_{|I| =d}a_I^u dx_I\]


\begin{example}
Vamos a ver un ejemplo con la \concept{forma angular}, $i^*ω = dθ$, la función $\appl{i}{S^1}{\real^2}$ y $ω = -y\dif x + x \dif y$.

Tenemos
\[
	dθ \equiv \left\{ \begin{array}{c} dθ\in Ω^1(0,2π)\\dθ\in Ω^1(-π,π)\end{array}\right.
\]

Las funciones transición $φ_1$ y $φ_2$ son las funciones transición tales que $\appl{φ_1}{e^{iθ}}{θ}$ en $(0,2π)$ y  $\appl{φ_2}{e^{iθ}}{θ}$ en $(-π,π)$.


Ahora nos planteamos si es cierto $i^*ω\left( \dpa{}{θ}\right)_{|p_0} = 1$, tomando $p_0 = e^{iθ_0}$

\begin{align*}
	&i^*ω\left( \dpa{}{θ}\right)_{|p_0} = ω\left(\dif i \left( \dpa{}{θ}\right) \right) = \\
	&ω\left(
			\dpa{x·i}{θ}_{|_{θ_0}} \dpa{}{x}_{|_{p_0}}
	\right)  =
	ω\left(
		-\sin θ_0 \dpa{}{x}_{|_p} + \cos θ_0 \dpa{}{y}_{|_{p_0}}
	\right)\\
	&= -y\dif x + x\dif y \left(
		-\sin θ_0 \dpa{}{x}_{|_p} + \cos θ_0 \dpa{}{y}_{|_{p_0}}
	\right) \\
	&= ... = -\sin(θ_0)(-\sin θ_0)·1 + \cos(θ_0)\cos(θ_0) = 1
\end{align*}

¿Y cuánto valdría $i^*(\dif x ∧ \dif y)$? Necesariamente es $0$, ya que en $S^1$ no caben formas diferenciales mayores de la dimensión. Es decir:

\[
	\Omega(M) = \bigoplus_{k≥0}^{∞} \Omega^K(M) = \bigoplus_{k≥0}^{\dim{M}} \Omega^K(M)
\]

\end{example}


\subsection{Producto exterior}
El producto exterior es una aplicación: $\appl{∧}{\Omega^r(M)×\Omega^s(M)}{\Omega^{r+s}(M)}$

si tenemos dos formas \begin{align*}
ω_u\in\Omega^r(M)\tq ω_u &= \sum a_I \dif x_I \\
τ_u\in\Omega^s(M)\tq τ_u &= \sum b_J \dif x_J
\end{align*}

podemos operarlas con el \concept[Producto\IS exterior]{producto exterior} de la siguiente forma: \[ ω_u \y τ_u ≝ \sum a_I^u b_J^u \dif x_I \y x_J \] teniendo en cuenta que $\dif x_i \y \dif x_i = 0$.

La pregunta es:

\[
\left.
	\begin{array}{c}
		ω_u = (φ_uφ_v^{-1})^*ω_v\\
		τ_u = (φ_uφ_v^{-1})^*τ_v\\
	\end{array}
\right\} \overset{?}{\implies}
ω_u ∧ τ_u = (φ_uφ_v^{-1})^* ω_v∧τ_v
\]


\begin{problem}
Sea $\appl{f}{\real^n}{\real^m}$, entonces $f^*(\dif t_1 ∧ ... ∧ \dif t_n) = f^*\dif t_1 ∧ ... ∧ f^*\dif t_n$.
\solution

\end{problem}

\paragraph{Conclusión:} $(Ω(M),+,∧)$ es un anillo. Además, es graduado y anticonmutativo.

Graduado, aunque no interesa para nada significa que $Ω^k∧Ω^s\subset Ω^{k+s}$

Además, si $\appl{f}{M}{N}$ es diferenciable, la aplicación $\appl{f^*}{Ω(N)}{Ω(M)}$ es un homomorfismo de anillos (graduados).

%% Fin clase de Juan

Un ejemplo bastante asquerosete: \begin{align*}
\appl{f}{ℝ^2&}{ℝ^3} \\
(t_1, t_2)&\longmapsto (\cos t_1, e^{t_2}, t_2 + t_1)
\end{align*} y la forma diferencial \[ ω = \cos t_1 \dif t_1 ∧ \dif t_2 + 5 \dif t_1 ∧ \dif t_3 \] y entonces \begin{align*} f^*ω &= \cos(\cos t_1) \dif \cos t_1 ∧ \dif e^{t_2} + 5 \dif \cos t_1 ∧ \dif (t_2 + t_1) = \\
&= \cos (\cos t_1) · (- \sin t_1 \dif t_1) ∧ (e^{t_2} \dif t_2) + 5 (- \sin t_1 \dif t_1) ∧ (\dif t_2 + \dif t_1) = \\
&= - e^{t_2} \cos (\cos t_1) \sin t_1 \dif t_1 ∧ \dif t_2 - 5 \sin t_1 \dif t_1 ∧ \dif t_2 = \\
&= -(e^{t_2} \cos (\cos t_1) \sin t_1 + 5 \sin t_1) \dif t_1 ∧ \dif t_2
\end{align*}

\subsection{Diferencial exterior}

La diferencial es una aplicación lineal \begin{align*}
\appl{\dif}{Ω^k(\bola^n)&}{Ω^{k+1}(\bola^n)} \\
ω = h \dfl{t_{i_1}}{t_{i_n}} &\longmapsto \dif ω = \dif h ∧ \dfl{x_{i_1}}{x_{i_n}}
\end{align*}

Sólo damos la definición para un término porque es lineal. Por ser un poco más concretos, \[ \dif ω = \sum_{j=1}^k \left(\dpd{h}{x_j} \dif x_j \right) \dfl{x_{i_1}}{x_{i_n}} \]

La diferencial tiene una serie de propiedades:

\begin{prop} Propiedades de la diferencial exterior:
\begin{enumerate}
\item $\dif (ω ∧ η) = \dif ω ∧ η + (-1)^{\deg ω} ω ∧ \dif η $.
\item \label{prp:DifDifCero} $\dif (\dif ω) = 0$.
\item \label{prp:CompatDifPullback} Dada $\appl{f}{ℝ^m}{ℝ^n}$, entonces $f^* (\dif ω) = \dif(f^*ω)$.
\end{enumerate}
\end{prop}

\begin{proof}

\proofpart{$\dif (ω ∧ η) = \dif ω ∧ η + (-1)^{\deg ω} ω ∧ \dif η $}

Tomamos $ω = \sum f_I \dif x_I$, $η = \sum g_J \dif x_J$, con $I,J$ multiíndices. Entonces operamos: \begin{align*}
\dif (ω ∧ η) &= \dif \left(\sum_{I,J} (f_I · g_J) \dif x_I ∧ \dif x_J \right)
	= \sum_{I,J} \dif(f_I · g_J) \dif x_I ∧ \dif x_J = \\
	&= \sum_{I,J} \left(\sum_{k = 1}^N \dpd{(f_I · g_J)}{x_k} \right) \dif x_k ∧ \dif x_I ∧ \dif x_J = \\
	&= \sum_{I,J} \left(\sum_{k = 1}^N g_J \dpd{f_I}{x_k} + f_I \dpd{g_J}{x_k}\right) \dif x_k ∧ \dif x_I ∧ \dif x_J = \\
	&= \sum_{I,J} \sum_{k=1}^N \left(\dpd{f_I}{x_k} \dif x_k ∧ \dif x_I ∧ (g_J \dif x_J) + \dpd{g_J}{x_k} \dif x_k ∧ (f_I \dif x_I) ∧ \dif x_J \right) = \\
	&= \sum_{I,J} \left(\sum_{k=1}^N \dpd{f_I}{x_k} \dif x_k ∧ \dif x_I ∧ (g_J \dif x_J)\right) + \\ & \qquad + \sum_{I,J} \left(\sum_{k=1}^N \dpd{g_I}{x_k} \dif x_k ∧ (f_I \dif x_I) ∧ \dif x_J \right) = \\
	&= \sum_{I,J} \left(\sum_{k=1}^N \dpd{f_I}{x_k} \dif x_k ∧ \dif x_I ∧ (g_J \dif x_J)\right) + \\ & \qquad + \sum_{I,J} \left(\sum_{k=1}^N (-1)^{\deg ω} \dpd{g_I}{x_k} \dif x_k ∧ \dif x_J ∧ (f_I \dif x_I) \right) = \\
	&= \dif ω ∧ η + (-1)^{\deg ω} ω ∧ \dif η
\end{align*}, donde el $(-1)^{\deg ω}$ aparece al mover $\deg ω$ diferenciales (las del multiíndice $\dif x_I$) para ordenarlo todo bien.

\proofpart{$\dif(\dif ω) = 0$}

\end{proof}

Con esto podemos definir bien la diferencial en una variedad abstracta.

\begin{defn}[Diferencial\IS exterior] Dada una forma diferencial $ω ∈ Ω^k(M)$, entonces definimos la diferencial como \begin{align*}
\appl{\dif}{Ω^k(M)&}{Ω^{k+1} (M)} \\
ω &\longmapsto \dif ω = \sum_{j=1}^k \left(\dpd{h}{x_j} \dif x_j \right) \dfl{x_{i_1}}{x_{i_n}}
\end{align*}

Y declaramos que esto es perfectamente compatible con las cartas, es decir, que si $(φ_U ○ \inv{φ_V})^* ω_U = ω_V$, entonces $(φ_U ○ \inv{φ_V})^* \dif ω_U = \dif ω_V$. Sólo hay que usar la \fref{prp:CompatDifPullback}.
\end{defn}

\chapter{Cohomología de De Rham}
\label{chap:CohomologiaDeRham}

\section{Complejo de De Rham}

La diferencial nos permite definir una cadena dada una variedad $M$ de dimensión $k$: \[ Ω^0(M) \xrightarrow{\dif^0} Ω^1(M) \xrightarrow{\dif^2} \dotsb \xrightarrow{\dif^{k-1}} Ω^{k}(M) \xrightarrow{\dif^k} Ω^{k+1}(M) \]

Nos podemos fijar que, por la \fref{prp:DifDifCero}, tenemos que $\img \dif^k ⊂ \ker \dif^{k+1}$. Entonces podemos definir la cohomología de De Rham

\begin{defn}[Grupo\IS de cohomología] Se define el grupo $k$-ésimo de cohomología de De Rham como \[ H^k (M) = \quot{\ker \dif ^k}{\img \dif^{k-1}} \]
\end{defn}

Dos notaciones:

\begin{defn}[Forma\IS exacta] Se dice que $ω ∈ Ω^k(M)$ es una forma diferencial exacta si y sólo si $∃ α ∈ Ω^{k-1}(M)$ tal que $\dif α = ω$.
\end{defn}

\begin{defn}[Forma\IS cerrada] Se dice que $ω ∈ Ω^k(M)$ es una forma diferencia cerrada si y sólo si $\dif ω = 0$.
\end{defn}

Está claro que $\ker \dif^k$ son formas cerradas e $\img \dif^{k-1}$ son las exactas.

Un ejemplo rápido es ver que \[ H^0(M) = \quot{\ker \dif^k}{\set{0}} = \set{ω ∈ Ω^0(M) = C^∞(M) \tq \dif ω = 0} \], luego $H^0(M)$ es un espacio vectorial isomorfo a $ℝ^C$, donde $C$ es el número de componentes conexas de $M$. Para ver esto, sólo tenemos que darnos cuenta de que las funciones cuya diferencial es $0$ son las constantes $c ∈ ℝ$. Por otra parte, si tenemos $C$ componentes conexas tenemos $C$ posibles elecciones de ``constantes'', luego eso nos da $ℝ^C$. Esto, que parece un hecho algo estúpido, en realidad nos da una pista sobre la topología: la conexión de la variedad.

Otro ejemplo interesante es calcular \[ H^k(ℝ) \cong \begin{cases} ℝ & k = 0 \\ \set{0} & k = 1 \\ \set{0} & k > 1 \end{cases}\]

En cuanto nos pasamos de la dimensión de $ℝ$, el grupo de cohomología es $\set{0}$ (no hay formas diferenciales de grado mayor que la dimensión de la variedad). El único punto a verificar sería el de \[ H^1(ℝ) = \quot{\ker \dif^1}{\img \dif^0} = \quot{\set{ω = f(t) \dif t}}{\set{η = \dif φ, φ ∈ C^∞(ℝ)}} = \set{0} \], ya que ambos espacios son iguales: $ω = f(t) \dif t = \dif φ$ con $φ = \int_0^t f(s) \dif s$.

\subsection{Anillo de cohomología}

Definimos el anillo de cohomología como \[ H^*(M) = \bigoplus_{k ≥ 0} H^k (M) \], que es un anillo con las operaciones $(+, ∧)$.

Habría que comprobar para ver que es un anillo que si $ω, η$ son cerradas entonces $ω ∧ η$ y $ω + η$ son cerradas, y que además esto no depende de la elección del representante\footnote{Como lo que estudiamos son formas cerradas, sabemos que $[ω] = [ω + \dif ω_1]$}. Son un montón de cuentas sin ninguna dificultad que salen directamente.

Un difeomorfismo entre variedades $\appl{f}{M}{N}$ induce isomorfismos de espacios vectoriales y de anillos respectivamente: \begin{align*}
\appl{f^*}{H^k(N)&}{H^k(M)} \\
\appl{f^*}{H^*(N)&}{H^*(M)}
\end{align*}


\subsection{Lema de Poincaré}

\noteby{Guille}{No sé dónde meter esto. }
\noteby{Guille}{Arreglar esto muy fuerte.}

\begin{lemma}[Lema\IS de Poincaré] \label{lem:Poincare} $π^* ○ s^*$ es la identidad a nivel de cohomología de $H^*(ℝ^{n+1})$, donde $π$ es algo y $s$ es otra cosa y $π○s(x) = x$.

\noteby{Guille}{Creo que $s_k(x) = (x,k)$ y $π(x,k) = x$, o algo así.}
\end{lemma}

\begin{corol} $π^*$ y $s^*$ son mutuamente inversas a nivel de cohomología, luego $H^k(ℝ^{n+1}) = H^k(ℝ^n)$.
\end{corol}

Por lo tanto $H^k(ℝ^n) = 0$ cuando $k > 0$. Otra forma de decir esto es que en la bola toda forma diferencial cerrada es exacta.

Podemos ver que, en $ℝ^2$, si tomamos $ω ∈ Ω^1(ℝ^2)$ de la forma $ω = F \dif x + G \dif y$, con $F,G ∈ C^∞$, entonces $ω$ será cerrada si y sólo si \[ 0 = \dif ω = (G_x - F_y) \dif x \y \dif y \iff G_x = F_y\]

Y algo.

\begin{proof}[\nref{lem:Poincare}] Basta probar que existe una aplicación lineal $\appl{K}{Ω(ℝ^{n+1})}{Ω(ℝ^{n+1})}$ que satisface la relación \( π^* ○ s^* = \mathrm{Id}_{Ω(ℝ^{n+1})} \pm (\dif K \pm K ○ \dif) \label{eq:PrfPoincare:1}\), pues en ese caso tendríamos que, dada $ω ∈ H^k(ℝ^{n+1})$, \[ π^*○s^*([ω]) = [\mathrm{Id} ○ ω + \underbracket{\dif K ω }_{\text{exacta, } [\dif K ω] = 0}+ \underbracket{K \dif ω}_{\dif ω = 0} ]  = [ω]\]

Entonces definimos \begin{align*}
\appl{K}{Ω(ℝ^{n+1})&}{Ω(ℝ^{n+1})} \\
f(\vx, t) \dif x_I &\longmapsto 0 \\
g(\vx, t) \dif x_J ∧ \dif t & \longmapsto \left(\int_0^t g(x,s) \dif s\right) \dif x_J
\end{align*}

Veamos que $K$ satisface la condición de \eqref{eq:PrfPoincare:1}. Y si se hacen las cuentas pues sale.
\end{proof}

\begin{prop} Más generalmente, $H^*(M×ℝ) = H^*(M)$ usando una demostración análoga.
\end{prop}

\subsection{Invarianza homotópica}

\begin{theorem} Supongamos que $\appl{f,g}{M}{N}$ son aplicaciones homótopas. Entonces $f^* = g^*$ a nivel de cohomología.
\end{theorem}

\begin{wrapfigure}{R}{0.4\textwidth}
\inputtikz{FuncionMeseta}
\caption{Una función ``meseta'' $α ∈ C^∞$ que varía suavemente de $0$ a $1$ en el intervalo $[0,1]$.}
\label{fig:FuncionMeseta}
\end{wrapfigure}

\begin{proof}
Consideramos la aplicación \begin{align*}
\appl{F}{[0,1] × M&}{N} \\
(t,x) &\longmapsto F_t(x)
\end{align*} donde $F_0 = f$ y $F_1 = g$.

Por otra parte, sea $\appl{α}{ℝ}{[0,1]}$ una función meseta como en la \fref{fig:FuncionMeseta}. Entonces definimos \begin{align*}
\appl{G}{M×ℝ&}{N} \\
(x,t) & \longmapsto F(α(t), x)
\end{align*} y tal que \begin{align*}
(G○s_0) (x) &= G(x,0) = F(0,x) = F_0(x) = f \\
(G○s_1) (x) &= G(x,1) = F(1,x) = F_1(x) = g
\end{align*}

Entonces $f^* = (G○s_0)^* = s_0^* ○ G^* = s_1^* ○ G^* = (G○s_1)^* = g^*$ y listos, así que a nivel cohomológico son iguales.\footnoteby{Guille}{Ok.}
\end{proof}

\begin{corol}
Dos variedades homótopas entonces sus anillos de cohomología son iguales.
\end{corol}

En el caso particular de un retracto por deformación, algo.

Vamos a demostrar que $\crc[1]$ no es un retracto por deformación de $ℝ^2$. Si lo fuera, los grupos de cohomología serían ambos nulos $H^1(ℝ^2) = 0$ pero $-y\dif x + x \dif y ∈ H^1(\crc[1])$ no es exacta.

En general, $H^k(ℝ^{n+1} \setminus \set{0}) \cong H^k(\crc[n])$. ¿Cómo encontrar un generador de $H^1(ℝ^2 \setminus \set{0})$. Bueno, podemos coger la contracción $r(x) = \frac{x}{\norm{x}}$ y entonces la forma generadora es \[ r^*(\dif θ) = r^*\left(\frac{-y \dif x + x \dif y}{x^2 + y^2}\right) \]

Estos resultados sobre homotopía siguen siendo válidos a nivel topológico, porque además podemos aproximar cualquier función continua por funciones diferenciables y listos, tete.

\section{Cálculo de cohomologías: Mayer-Vietoris}

Hasta ahora hemos visto algunos resultados para calcular cohomologías. Sabemos que si $M$ es conexa, entonces $H^0(M) = 0$, que $H^k(\bola^n) = 0$ cuando $k > 0$ por el \nref{lem:Poincare}, y que además si $M_1 \simeq M_2$ (homotópicamente equivalentes) entonces $H^*(M_1) = H^*(M_2)$.

Sin embargo, para otros cálculos de cohomologías no tenemos herramientas. Mayer-Vietoris será lo que usaremos para sacar cohomologías, trabajando con abiertos que cubren la variedad y estudiando su intersección. Para definirlo, primero necesitaremos una cierta base previa.

\subsection{Previo: Base algebraica}

\begin{defn}[Sucesión\IS exacta de espacios vectoriales] Una sucesión exacta de espacios vectoriales es una cadena \[ \dotsb \to V_{i-1} \xrightarrow{h_{i-1}} V_{i} \xrightarrow{h_i} V_{i+1} \to \dotsb \] tales que $\ker h_i = \img h_{i-1}$.
\end{defn}

Por ejemplo, si tenemos una sucesión exacta \[ 0 \xrightarrow{h_0} V_1 \xrightarrow{h_1} V_2 \xrightarrow{
h_2} V_3 \xrightarrow{h_3} 0 \] podemos ver varias cosas, como que  $h_1$ es inyectiva ($\ker h_1 \img h_0 = 0$) y que $h_3$ es suprayectiva ($\ker h_3 = \img h_2 = V_3$). Entrando en temas menos obvios, podemos aplicar el primer teorema de isomorfía para $h_2$, de tal forma que \[ \quot{V_2}{V_1} \cong V_3 \]

En ejemplos más concretos, podemos definir una cadena exacta \[ 0 \to ℝ^2 \overset{h_1}{\hookrightarrow} ℝ^5 \xrightarrow{h_2} ℝ^3 \mapsto 0 \], con \begin{align*}
h_1(x_1, x_2) &= (x_1, x_2, 0,0,0) \\
h_2(y_1, y_2, y_3, y_4, y_5) &= (y_3, y_4, y_5)
\end{align*} donde en la última aplicación tenemos que mandar los elementos de $\img h_1$ a $0$ si queremos que sea exacta.

Volviendo a las formas diferenciales, podemos volver a la cadena \[0 \to Ω^0(\bola^n) \xrightarrow{\dif^0} Ω^1(\bola^n) \xrightarrow{\dif^2} \dotsb \xrightarrow{\dif^{k-1}} Ω^{k}(\bola^n) \xrightarrow{\dif^k} Ω^{k+1}(\bola^n)\], que será una sucesión exacta si y sólo si $\ker \dif^{i+1} = \img \dif^i$, equivalente a decir que $\quot{\ker \dif^{i+1}}{\img \dif^i} = H^{i+1}(\bola^n) = 0$. Es un resultado interesante porque nos permite vincular el resultado algebraico (que la cadena sea exacta o no) a un resultado sobre cohomologías.

\begin{prop} Si la cadena \[ 0 \to V_0 \xrightarrow{h_0} V_1 \to \dotsb \to V_{n-1} \xrightarrow{h_{n-1}} V_{n} \to 0 \] es exacta, entonces la suma alternada de dimensiones de los espacios vectoriales es cero:  \[ \sum_{i=0}^n (-1)^i \dim V_i = 0\]
\end{prop}

\begin{proof} Podemos considerar las dos siguientes cadenas: \begin{align*}
0 \to \img h_1 & \xhookrightarrow{i} V_1 \to \dotsb \to V_{n-1} \xrightarrow{h_{n-1}} V_{n} \to 0 \\
0 \to V_0 &\xrightarrow{h_0} V_1 \xrightarrow{h_1} \img h_1 \to 0
\end{align*}, que son sucesiones exactas, y entonces podemos razonar por inducción sumando las formulitas de la suma de dimensión, aunque personalmente no tengo del todo claro por qué está usando lo que quiere probar para demostrarlo exactamente lo mismo.
\end{proof}

\begin{theorem}[Sucesión\IS exacta de Mayer-Vietoris] Sean $U,V$ abiertos de $M$ tales que $M = U ∪ V$. Entonces la siguiente sucesión es exacta: \( \begin{matrix}
0 \to & Ω^*(M) & \xrightarrow{r} &Ω^*(U) \oplus Ω^*(V) & \xrightarrow{δ} & Ω^*(U∩V) & \to 0 \\
& ω &\xrightarrow{r} & (\restr{ω}{U}, \restr{ω}{V}) & & & \\
&   & 				 & (η_1, η_2) & \xrightarrow{δ} & \restr{η_2}{U∩V} - \restr{η_1}{U∩V} &
\end{matrix} \label{eq:SucesionMayerVietoris}\)
\end{theorem}

\begin{proof} $r$ es inyectiva porque $U ∪ V = M$, y además $\ker δ ⊇ \img r$ obviamente: \[ δ○r (ω) = δ(\restr{ω}{U}, \restr{ω}{V}) = \restr{ω}{U∩V} - \restr{ω}{U∩V} = 0 \]

Por otra parte, si $(η_1, η_2) ∈ \ker δ$, entonces $\restr{η_2}{U∩V} = \restr{η_1}{U∩V}$. Entonces, definimos \[ η = \begin{cases} η_1 & \text{en } U \\ η_2 & \text{en } V \end{cases} \], y entonces está claro que está bien definida (en la intersección ambas coinciden, y $U ∪ V = M$) y $r(η) = (η_1, η_2)$.

Nos falta ver que $δ$ es suprayectiva. Sea $σ ∈ Ω(U∩V)$. Necesitamos encontrar $η_1 ∈ Ω(U)$, $η_2 ∈ Ω(V)$ tal que $σ = \restr{η_2}{U∩V} - \restr{η_1}{U∩V} $. Así, definimos una partición de la unidad de $M$ subordinada al recubrimiento de $M$ formado por $U$ y $V$, esto es, dos aplicaciones $\appl{ρ_{U,V}}{M}{ℝ^+}$, con $\sop ρ_U ⊂ U$, $\sop ρ_V ⊂ V$ y además $ρ_U + ρ_V = 1$.

Con esto en la mano, lo que tenemos que definir (o eso le parece a Gabino al menos) es \begin{align*}
η_1 &= -ρ_V σ ∈ Ω(U)\\
η_2 &= ρ_U σ ∈ Ω(V)
\end{align*} salvo signo, y con las formas valiendo $0$ fuera de $U ∩ V$.

Si suponemos que esta definición es buena, entonces haciendo las restricciones correspondientes a $U∩V$ $η_2 - η_1 = ρ_Uσ + ρ_Vσ = σ$, booooooom shakalaka.
\end{proof}

Un ejemplo de esa partición para $\crc[n]$, con dos funciones concretas. que cumplen efectivamente lo que se pedía, con $\tilde{ρ}_U = ρ ○ φ_U$ en $\crc[n] \setminus{S}$ y $0$ en otro caso donde $φ_U$ es la carta y $ρ$ algo. Entonces se pueden definir \[ ρ_U ≝ \frac{\tilde{ρ}_U}{\tilde{ρ}_U + \tilde{ρ}_V} \qquad ρ_V ≝ \frac{\tilde{ρ}_V}{\tilde{ρ}_U + \tilde{ρ}_V} \], de tal forma que el soporte es el que necesitamos y ya está.

\begin{therorem}[Teorema\IS de Mayer-Vietoris de cohomología] La siguiente sucesión es exacta
\[ \dotsb \xrightarrow{\dif^*} H^{k-1} (U∩V) \xrightarrow{\dif^*} H^k(M) \xrightarrow{r} H^k(U) \oplus H^k(V) \xrightarrow{δ} H^k(U∩V) \xrightarrow{\dif^*} H^{k+1} (M)  \xrightarrow{\dif^*} \dotsb \]

$r$, y $δ$ definidas como en \eqref{eq:SucesionMayerVietoris}, y $\dif^*$ es más difícil de definir (Gabino dixit).
\end{therorem}

Una aplicación de esto es de nuevo $\crc[1]$, con las cartas $U,V$ de toda la vida. La idea es que $H^0(U) \oplus H^0(V)$ tiene dimensión 2, $H^0(U∩V)$ tiene dimensión dos por ser dos componentes conexas, $\dim H^1(U) = \dim H^1(V) = 0$ por ser $U,V$ homeomorfos a $ℝ$ y entonces hay una sucesión metida entre dos ceros. Como la suma alternada de dimensiones es cero, luego \[ 1 - 2 + 2 - \dim H^1(M) + 0 = 0\], luego $\dim H^1(M) = 1$.

Y bueno, al final, salen cosas.

%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Cálculos y otras explicaciones}
\input{tex/GT_Calculos.tex}

\chapter{Ejercicios}
\input{tex/GeometriaTopologia_Ejs.tex}

\bibliography{../Apuntes}{}
\printindex
\end{document}
