\documentclass[palatino, bibnumbers]{apuntes}

\title{Geometría y Topología}
\author{Guillermo Julián Moreno}
\date{15/16 C2}

% Paquetes adicionales
\usepackage{enumitem}
\usepackage{tikztools}
\usepackage{fancysprefs}
\usepackage{tikz-3dplot}
\usepackage{xfrac}
\usepackage{wrapfig}
\usepackage{fastbuild}

\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\usetikzlibrary{intersections}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}

\setlist{itemsep=1pt, topsep=5pt}
\bibliographystyle{alpha}
% --------------------

\precompileTikz

\newcommand{\Id}{\mop{Id}}
\newcommand{\cln}{\colon\!}

\setcounter{tocdepth}{3}

\begin{document}
\pagestyle{plain}

% http://tex.stackexchange.com/a/14243
\relpenalty=9999
\binoppenalty=9999

\begin{abstract}
Estos son los apuntes del curso de Geometría y Topología, del profesor Gabino González.
\end{abstract}

\maketitle

\tableofcontents
\newpage
% Contenido.

\chapter{Conceptos básicos - Variedades}

En Geometría, los objetos que estudiamos se llaman ``variedades''. Veremos de distintos tipos (por ejemplo, en Geometría Diferencial \citep{ApuntesGeoDif} veíamos variedades diferenciables), aunque nosotros empezaremos con las topológicas.

\begin{defn}[Variedad\IS topológica] Una variedad topológica es un espacio topológico $M$ con las siguientes propiedades:
\begin{enumerate}
\item $M$ es $T_2$ (esto es, \concept{Hausdorff}: dos puntos distintos tienen entornos disjuntos).
\item $∀p ∈ M$ admite un entorno $U$ y un homeomorfismo $\appl{φ_u}{U}{ℝ^N}$ (o $\bola^N$).

Al par $(U,φ_u)$ se le llama \concept{Carta} para $p$. Si $φ_u(p) = 0$, se dice que la carta está centrada en $p$. A la colección de cartas se le llamará \concept{Atlas}.
\item Si para cualquier par de cartas $(U,φ_u)$ y $(V,φ_v)$ la aplicación $$\appl{φ_v ○ \inv{φ_u}}{φ_u(U∩V)}{φ_v(U∩V)}$$ es difeomorfismo, estamos entonces ante una \concept{Variedad\IS diferenciable}.
\end{enumerate}
\end{defn}

\begin{figure}[thbp]
\centering
\inputtikz{Cartas}
\caption{Un esquema de las cartas de una variedad $M$ y cómo se comportan en la intersección.}
\label{fig:Cartas}
\end{figure}

La dimensión de la variedad está dada por la dimensión de $ℝ$ a la que son homeomorfas las cartas. La cuestión es que no tenemos claro si eso está bien definido. En el caso diferenciable, la condición de difeomorfismo para la intersección de cartas implica que la dimensión de ambas cartas ha de ser la misma. En el caso topológico también está bien definido, aunque es más difícil de demostrar ya que dependemos de que no exista un homeomorfismo entre $ℝ^n$ y $ℝ^m$ con $n ≠ m$, que no es trivial.

Un ejemplo sencillo de variedad es $M = \bola^N$, con $φ$ la identidad. Otro ejemplo es $\crc$ (la circunferencia), que es una variedad de dimensión 1, que no se puede dar con sólo una carta (la circunferencia no es homeomorfa a $ℝ$)\footnote{$\crc$ es compacta y $\real$ no, y como compacidad es propiedad topológica y los homeomorfismos las preservan, no pueden ser homeomorfas.}. Podríamos darla tomando las dos mitades superior e inferior usando senos y cosenos, y también podríamos hacer la proyección estereográfica (\ref{fig:ProyEstereo}) desde los polos norte y sur $(0,1), (0,-1)$ respectivamente sobre la recta real. En este caso, tendríamos las siguientes cartas: \[
\begin{matrix}
	\appl{α_1}{V_1 = \crc\setminus\set{(0,1)}&}{&ℝ} \\
	p=(s,t) &\longmapsto& \frac{s}{1-t}
\end{matrix}
\qquad
\begin{matrix}
	\appl{α_1}{V_2 = \crc\setminus\set{(0,-1)}&}{&ℝ} \\
	p=(s,t) &\longmapsto& \frac{s}{1+t}
\end{matrix}\]

\begin{figure}[hbtp]
\inputtikz{ProyeccionCirc}
\caption{Proyección estereográfica de la circunferencia.}
\label{fig:ProyEstereo}
\end{figure}

Para comprobar si este atlas es diferenciable, tendríamos que mirar qué ocurre con $α_2 ○ \inv{α_1}$. Después de un montón de cuentas\footnote{Ver \fref{sec:proyeccion_estereografica_crc}.}, nos sale que efectivamente lo es ($α_2 ○ \inv{α_1} = \frac{1}{x}$) en el dominio en el que está definido (el cero no es un problema porque no está dentro del dominio).

Trivialmente, podemos definir cuándo dos atlas son compatibles.

\begin{defn}[Atlas\IS compatibles] Se dice que dos atlas $A_1, A_2$ son compatibles si $A_1 ∪ A_2$ es un atlas. Esto es, si y sólo si las cartas de $A_1$ son compatibles con las de $A_2$.
\end{defn}

\label{def::atlas_compatibles}

Por ejemplo, podemos estudiar si los dos atlas que hemos visto para la circunferencia \crc son compatibles (recordamos que uno era el trigonométrico y otro la proyección estereográfica). Esto es equivalente a preguntarnos si $α_j ○ \inv{φ_i}$ son diferenciables. Se puede ver fácilmente que \[ α_1 ○ \inv{φ_1} (θ) = α_1(\cos θ, \sin θ) = \frac{\cos θ}{1 - \sin θ}\] que es diferenciable. Así podríamos hacerlo con el resto de combinaciones, y por lo tanto tenemos que ambos atlas dan la misma variedad.

Como ejercicio, podríamos dar un atlas $A_3$ en \crc con cartas en forma de semicircunferencia y después comprobar que es compatible con los dos atlas de antes. Otro ejercicio más largo sería hacer lo análogo para $\crc[2]$.

Una vez que tenemos ya definido qué es una variedad, el siguiente paso es saber si podemos hacer análisis ahí: si podemos definir aplicaciones diferenciables en ella o si podemos integrar una función. La segunda parte la veremos más adelante con las formas diferenciales, pero la primera la podemos estudiar ahora.

\begin{figure}[hbtp]
\centering
\inputtikz{ApplDiferenciable}
\caption{Esquema de la definición de la aplicación diferenciable entre dos variedades en base a las cartas.}
\label{fig:ApplDiferenciable}
\end{figure}

\begin{defn}[Aplicación\IS diferenciable] Una aplicación continua $\appl{f}{M}{N}$ entre dos variedades $M$ y $N$ es diferenciable si $∀p ∈ M$ con $f(p) = q ∈ N$ existe una carta $(U,φ_U)$ alrededor de $p$ y una carta $(V, φ_V)$ alrededor de $q$ tal que $f(U) ⊂ V$ y $\appl{φ_V○f ○ \inv{φ_U}}{\bola^m}{\bola^n}$ es diferenciable.
\end{defn}

Es importante ver que este concepto de diferenciabilidad no depende de las cartas elegidas para cada punto. Suponiendo que tenemos otras dos cartas $α_U, α_V$ alrededor de $p$ y $q$ tendríamos que \[ α_V ○ f ○ \inv{α_U} = (α_V ○\inv{φ_V}) ○ (φ_V ○ f ○ \inv{φ_U}) ○ (φ_U ○ \inv{α_U})\]

Por compatibilidad de las cartas, $α_V○\inv{φ_V}$ y $φ_U ○ \inv{α_U}$ son diferenciables. Además, ya que hemos dicho que $f$ es diferenciable con las cartas $φ_U, φ_V$ luego $φ_V ○ f ○ \inv{φ_U}$ es diferenciable igualmente. Así, la composición de esas tres funciones es diferenciable.

\begin{example} Vamos a definir una función entre variedades y ver si es diferenciable. No nos complicaremos mucho: \begin{align*}
\appl{f}{\crc&}{\crc ⊂ ℂ} \\
z &\longmapsto \conj{z}
\end{align*}

Haciendo los cálculos, vemos que \[ φ_1 ○ f ○ \inv{φ_1} (θ) = φ_1 ○ f(e^{iθ}) = φ_1(e^{-iθ}) = \begin{cases} -θ \\ -θ + 2\pi \end{cases} \] que efectivamente es diferenciable.
\end{example}


\begin{example} Definimos ahora una función a priori menos interesante, la identidad: \begin{align*}
\appl{f}{M&}{N} \\
p &\longmapsto p
\end{align*}

$M$ será $(ℝ,φ)$ y $N = (ℝ,α)$, con $φ$ la identidad y $α(t) = t^3$. Es obvio ver que $f$ es diferenciable (si hacemos la composición nos sale directamente). Ahora bien, si tomamos $\appl{f}{N}{M}$, ¿sigue siendo diferenciable? En este caso, si calculamos vemos que $φ ○ f ○ \inv{α}(t) = \sqrt[3]{t}$ pero esta aplicación no es diferenciable en $0$. Esto es lo mismo que decir que estos dos atlas en $ℝ$ no son compatibles: no definen la misma estructura. Sí serían compatibles si estuviésemos hablando sólo de variedades topológicas, porque sí que estamos ante un homomorfismo.
\end{example}

Una vez que hemos visto qué es una variedad, podemos ver cómo construir variedades combinándolas de distintas maneras.

\section{Variedades producto}

La primera opción para construir una variedad es el producto cartesiano, que ya conocemos de otro tipo de conjuntos.

\begin{defn}[Variedad\IS producto] Sean $M^m$, $N^n$ variedades con sus respectivos atlas $A_1 = \set{(U_i, φ_i)}_{i ∈ I}$ y $A_2 = \set{(V_j, α_j)}_{j ∈ J}$. Entonces, la variedad producto es $M×N^{m+n}$ con atlas \[ A_1 × A_2 = \set{(U_i×V_j, φ_i × α_j)}_{(i,j) ∈ I × J} \]
\end{defn}

Un ejemplo sencillo es $\crc × ℝ$, una variedad con cartas de la forma $(φ_1 × α_1) = ((x,y), t)$, donde $(x,y)$ vienen de la carta de la circunferencia que hayamos escogido. En este caso, la variedad es $M = \set{(x,y,z) \tq x^2 + y^2 = 1}$, el cilindro, ya que podemos tomar $\appl{f}{M}{\crc × ℝ}$ es un difeomorfismo (se pueden hacer las cuentas pero son triviales).

Otro es $\crc × \crc$, que nos podemos preguntar si es difeomorfa a $\crc[2]$, más que nada porque uno es un toro y otro una esfera. Sin embargo, un argumento más formal implicaría usar los grupos fundamentales\footnote{Ver \citep[Cap. III]{ApuntesTopologia}.}, ya que $π(\crc[2]) = \set{0}$, $π(\crc) = ℤ$ y por lo tanto $π(\crc × \crc) \cong ℤ × ℤ$. Los grupos fundamentales no son isomorfos por lo que no puede haber difeomorfismo. Lo malo es que este argumento no nos vale para decir, por ejemplo, si $\crc[2] × \crc[3] \cong \crc[5]$. En realidad, no son siquiera homeomorfas, pero para saberlo tendremos que usar las cohomologías de De Rham (\fref{chap:CohomologiaDeRham}).

Durante el curso, veremos más herramientas para saber si existen o no difeomorfismos entre variedades.

\section{Variedad cociente}

En las variedades también se puede aplicar el análogo del conjunto cociente. Eso sí, necesitaremos ciertas condiciones ``extra'' sobre la variedad.

\begin{defn}[Variedad\IS cociente] Sea $M$ una variedad y sea $G < \mop{Diff}(M)$ un subgrupo de los difeomorfismos sobre $M$, tal que $\abs{G} < ∞$ y además $G$ actúa libremente en $M$ (esto es, que $∀g ∈ G$ distinta de la identidad y $∀x ∈ M$ se tiene que $g(x) ≠ x$).

Entonces, definimos el espacio cociente $\quot{M}{G}$ a través de la relación de equivalencia \[ x \sim y \iff ∃g ∈ G \tq  y = g(x) \]

La estructura de variedad se la damos considerando la proyección canónica \begin{align*}
\appl{π}{M&}{\quot{M}{G}} \\
x &\longmapsto [x]
\end{align*} que a cada elemento le asigna su clase.
\end{defn}

Por ejemplo, si consideramos la variedad $M = \crc[2]$ y el grupo $G = \gen{J} = \set{\Id, J}$ con $J(x,y,z) = (-x,-y,-z)$. En este caso, $\projp^2 = \quot{\crc[2]}{\gen{J}}$.

\begin{figure}[hbtp]
\centering
\inputtikz{EsferaPlanoProj}
\caption{Paso de la esfera al plano proyectivo, con la proyección canónica π.}
\label{fig:EsferaPlanoProj}
\end{figure}

Para darle la estructura de atlas, tomamos una carta $(U,φ_U)$ en $M$ tal que $U ∩ g(U) = ∅,\;∀g ∈ G$, por lo que $\appl{\restr{π}{U}}{U}{\quot{M}{G}}$ es un homeomorfismo sobre su imagen $π(U)$, luego biyectivo. Así, las cartas en $\quot{M}{G}$ son de la forma $\left(π(U), φ_U ○ \inv{\restr{π}{U}}\right)$.

En este caso, nos podemos preguntar qué ocurre si $π(U) ∩ π(V) ≠ ∅$ para dos cartas $U,V$: ¿es $\left(φ_V ○ \inv{\restr{π}{V}}\right) ○ \left(\restr{π}{U} ○ \inv{φ_U} \right)$ diferenciable?

La cuestión es que $\inv{\restr{π}{V}} ○ \restr{π}{U} = g ∈ G$ por la construcción de $g$\footnote{Si $π(x) = π(y)$, son de la misma clase y por lo tanto hay un difeomorfismo que lleva de $x$ a $y$.}, y por lo tanto nos queda que $φ_V ○  g ○ \inv{φ_U}$ es diferenciable así que la estructura de atlas que nos queda es compatible.

Un ejemplo muy común es $\projp^n = \quot{\crc[n]}{\gen{J}}$ para $n ∈ ℕ$, que son los espacios proyectivos reales de dimensión $n$.

Esta teoría también funciona siempre que el grupo $G$ cumpla la segunda propiedad y además $∀x ∈ M$ exista un $U$ tal que $g(U) ∩ U = ∅$, $∀g ≠ \Id$.

Otros ejemplos son $\quot{ℝ}{ℤ} \cong \crc$ o $\quot{ℝ^2}{ℤ^2} \cong \crc × \crc$. Ambos son análogos: el primero lleva a la circunferencia y el segundo al toro. Es fácil de ver aunque hay que echarle un poco de imaginación: lo vamos a desarrollar con $\quot{ℝ^2}{ℤ^2}$. En este caso, consideramos $ℤ^2$ como el conjunto de los difeomorfismos en $ℝ^2$ de la forma $f(x,y) = (x+m, y+n)$ con $(m,n) ∈ ℤ^2$, que es obviamente un grupo que no fija puntos. En este caso, el hecho de que sea infinito no nos causa demasiados problemas.

\begin{figure}[hbtp]
\centering
\inputtikz{ToroEspacioCociente}
\caption{Un gráfico para mostrar cómo $\protect\quot{ℝ^2}{ℤ^2}$ es homeomorfo a un toro: la proyección canónica $π$ nos lleva al cuadrado $[0,1]×[0,1]$ con los bordes conectados (los del mismo color). Si pudiésemos ``moldear'' el cuadrado y pegar los bordes, estaríamos ante el toro.}
\label{fig:ToroEspacioCociente}
\end{figure}

La relación de equivalencia que usamos para construir el espacio cociente nos dice que dos puntos $x,y ∈ ℝ^2$ pertenecen a la misma clase si existe un difeomorfismo $g ∈ ℤ^2$ tal que $g(x) = y$. En otras palabras, si tenemos un par $(m,n) ∈ ℤ^2$ tal que $(x_1 + m, x_2 + n) = (y_1, y_2)$, entonces $x = (x_1, x_2)$ y $y=(y_1, y_2)$ están relacionados, por lo que nuestro conjunto de clases de equivalencia son los puntos en $[0,1] × [0,1]$. Eso sí, con una peculiaridad: los bordes están ``unidos'', el de arriba con el de abajo y el de la izquierda con el de la derecha. Es decir, que topológicamente estamos ante un toro (ver \fref{fig:ToroEspacioCociente}).

Si por otra parte trabajásemos con $\quot{ℝ}{ℤ}$, tendríamos algo similar: estaríamos identificando los dos bordes del intervalo $(0,1)$ y tendríamos una circunferencia. Para demostrarlo formalmente, tendríamos que definir una aplicación \begin{align*}
\appl{f}{\quot{ℝ}{ℤ}&}{\crc} \\
[x] &\longmapsto e^{2πix} = (\cos 2πx, \sin 2πx)
\end{align*} y comprobar que es diferenciable, esto es, tenemos que comprobar que $\inv{φ_U} ○ f ○ α$ es diferenciable, teniendo que $φ_U$ es una carta de $\quot{ℝ}{ℤ}$, con $\inv{φ_U} = \restr{π}{U}$ la proyección canónica restringida a $U$; y $α(e^{2πiθ}) = 2πθ$ la otra carta. La composición sería obviamente $π ○ f ○ α = 2πx$ que es diferenciable.

Otro ejemplo posible es ver si podemos definir un difeomorfismo entre $\projp^1$ y $\crc$ como $ρ(z) = z^2$, con $z ∈ ℂ$. Eso sí, sólo pasa con $n=1$: en general, $\projp^n \nsim \crc[n]$.

\subsection{Espacios proyectivos}
\label{sec:EspaciosProyectivos}

A través de la variedad cociente hemos hablado de espacios proyectivos. Vamos a definirlos bien:

\begin{defn}[Espacio\IS proyectivo] Sea \kbb un cuerpo\footnote{En nuestro caso, $ℝ$ o $ℂ$}. Entonces definimos el plano proyectivo como \[ \mathbb{P}^n(\kbb) = \quot{\kbb^{n+1} \setminus \set{0}}{\sim}\] con $\sim$ la relación de equivalencia dada por $x \sim y \iff y = λx$ para algún $λ ∈ \kbb^*$.
\end{defn}

Cuando \kbb es $ℝ$ o $ℂ$, $\mathbb{P}^n(\kbb)$ es una variedad de dimensión $n$ o $2n$ respectivamente.

Las cartas en el plano proyectivo son de la forma\footnote{Como notación, los dos puntos denotan clases de equivalencia, esto es, $[x_1\cln x_2] \equiv \set{\gor{x_1}, \gor{x_2}}$.} \[ U_k = \set{[x_1\cln x_2\cln \dotsb\cln x_k\cln \dotsb\cln x_n] ∈ \mathbb{P}^n(\kbb) \tq x_k ≠ 0} \]

Este conjunto está bien definido. No lo estaría si estuviésemos diciendo que $x_k = 1$, por ejemplo, porque tendríamos que $[1\cln 1\cln 1] = [3\cln 3\cln 3]$ y no quedaría claro si está o no en el conjunto.

Además, se ve fácilmente que $\bigcup_{k=0}^n U_k = \mathbb{P}^n(\kbb)$. Sólo nos queda definir una biyección \begin{align*}
\appl{φ_k}{U_k&}{\kbb^n} \\
[x_1\cln \dotsb\cln x_k\cln \dotsb\cln x_n] &\longmapsto \left(\frac{x_0}{x_k}, \dotsc, \frac{x_n}{x_k}\right)
\end{align*}

La inversa de $φ_k$ sería \[ \inv{φ_k}(y_1, \dotsc,y_n) = [y_1\cln \dotsb \cln y_{k-1} \cln 1 \cln y_{k+1}\cln \dotsb \cln y_n]\]

Sólo nos queda comprobar que las funciones transición $\appl{φ_j ○ \inv{φ_k}}{\kbb^n \setminus \set{y_j = 0}}{\kbb^n \setminus \set{y_k = 0}}$ son diferenciables. Vemos que \[ (φ_j ○ \inv{φ_k})(y_1, \dotsc, y_n) = φ_j\left([y_1\cln \dotsb \cln y_{k-1} \cln 1 \cln y_{k+1}\cln \dotsb \cln y_n]\right) = \left(\frac{y_1}{y_j}, \dotsc, \frac{1}{y_j}, \dotsc, \frac{y_n}{y_j} \right)\], que efectivamente es diferenciable porque en el espacio de partida $y_j ≠ 1$.

Así, tenemos que tanto $\mathbb{P}^n(ℝ) = \projp^n$ y $\mathbb{P}^n(ℂ) = \projcp^n$ son variedades diferenciables.

El plano proyectivo complejo es interesante. Por ejemplo, podemos ver que $\projcp^1 \cong ℂ ∪ \set{∞} \cong \crc[2]$. Otro aspecto interesante de $\projcp^n$ es que no sólo es diferenciable como comentábamos antes: también es holomorfa. Y lo relevante de las variedades holomorfas es que son todas orientables. Esto nos sirve como excusa para introducir otro tipo de variedades.

\section{Variedades orientables}

\begin{defn}[Variedad\IS orientable] \label{def:VariedadOrientable} Una variedad $M$ es orientable si existe un atlas $A = \set{(U_i,φ_i)}_{i∈I}$ tal que $\abs{\Dif (φ_j○\inv{φ_i})} > 0$ para cualesquiera $j,i ∈ I$.
\end{defn}

Un atlas holomorfo siempre verificará la condición de orientabilidad por las ecuaciones de Cauchy-Riemann \citep[Sección III.2]{ApuntesVarCompI}. Definiendo $\appl{f = (φ_j○\inv{φ_i})}{ℂ}{ℂ}$ holomorfa, tenemos que $f = u + iv$ y entonces \[ \Dif f = \det \begin{pmatrix}
u_x & u_y \\ v_x & v_y
\end{pmatrix} \eqreasonup{Eq. CR} \det
\begin{pmatrix}
u_x & -v_x \\ v_x & u_x \end{pmatrix} = u_x^2 + v_x^2 > 0 \]

Otro ejemplo interesante es $\quot{ℝ×(-a,a)}{ℤ}$, de tal forma que $n·(x,λ) = (x+n,λ)$. Es un cilindro (identificamos los dos lados de un rectángulo de anchura infinita y altura $2a$) y orientable.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\begin{tikzpicture}
\fill[blue!20!white] (0, -0.8) rectangle (2, 0.8);
\draw (-1,0) -- (3,0);
\draw (0,-1) -- (0,1);

\node[hnlin, label = {left:$a$}] at (0, 0.8) {};
\node[hnlin, label = {left:$-a$}] at (0, -0.8) {};

\draw[blue, thick, directed, |-|]  (2, 0.8) --  (2, -0.8);
\draw[blue, thick, directed] (0, -0.8) -- (0,0.8);

\end{tikzpicture}
\caption{Representación de $\protect\quot{ℝ×(-a,a)}{ℤ}$ con los lados identificados en sentido contrario.}
\label{fig:CilindroNoOrientable}
\end{wrapfigure}

Ahora bien, si la relación de equivalencia la tomamos como $n·(x,λ) = (x+n, (-1)^nλ)$, esto es, que $(x,λ) \sim (x',λ')$ si y sólo si $x' = x + n$ y $λ' = (-1)^nλ$. Lo que nos queda es algo como lo de la \fref{fig:CilindroNoOrientable}, con los dos lados identificados pero en sentido contrario. En otras palabras, es una banda de Möbius.
\label{Mobius}
Con esto, podemos comprobar que efectivamente la banda de Möbius no es orientable. Sea $I = (-a, a)$, de tal forma que nuestra proyección sea \begin{align*} \appl{π}{ℝ×I&}{M} \\ (x,λ) &\longmapsto [(x,λ)]
\end{align*}

Consideramos ahora dos cartas $U_1 = (0,1) × I$ y $U_2 = (\sfrac{1}{2}, \sfrac{3}{2}) × I$ con respectivas funciones $φ_i = \inv{(\restr{π}{U_1})}$. Vemos ahora qué pasa con $φ_2 ○ \inv{φ_1}$, que tiene dominio $φ_1(π(U_1) ∩ π(U_2)) = \left( (0, \sfrac{1}{2}) × I\right) ∪ \left((\sfrac{1}{2}, 1) × I\right)$; e imagen $φ_2(π(U_1) ∪ π(U_2)) = \left( (\sfrac{1}{2}, 1) × I\right) ∪ \left((1, \sfrac{3}{2}) × I\right)$.

Tomamos ahora un $(x,λ) ∈ (\sfrac{1}{2}, 1) × I$, y tenemos que coger un representante de su clase en la imagen: nos vale con el mismo $(x,λ)$. Sin embargo, si tomamos un $(x,λ) ∈ (0, \sfrac{1}{2}) × I$, su clase es $[x,λ]$. Ahora bien, ese mismo número no está en la imagen, así que tenemos que buscar otro representante, que será $(x+1, -λ)$ por cómo hemos definido la relación de equivalencia. El determinante de esta función será negativo, y por lo tanto tendremos un atlas no orientable.

\begin{prop} Sea $M$ una variedad con un atlas con sólo dos cartas $(U_1, φ_1)$ y $(U_2, φ_2)$ tal que la intersección es un conexo. Entonces $M$ es orientable.
\end{prop}

\begin{proof} (Esbozo) Dado que los cambios de carta son difeomorfismos, el determinante de sus diferenciales no puede ser $0$ nunca. Dado que la intersección es conexa, no puede haber cambios de signo porque no se puede pasar por el $0$.
\end{proof}

\begin{figure}[hbtp]
\centering
\inputtikz{OrientRP2}
\caption{Demostración gráfica de la no-orientabilidad de $\projp^2$: el subconjunto $U$ es una banda de Möbius no orientable.}
\label{fig:OrientRP2}
\end{figure}

Otra variedad de la cual podemos estudiar su orientabilidad es $\projp^2$, que topológicamente es equivalente a un disco con los puntos antipodales identificados (\fref{fig:OrientRP2}). En ese caso, podemos tomar un subconjunto $U$ que es equivalente a la banda de Möbius, que no es orientable y por lo tanto $\projp^2$ no puede serlo.

\section{Grupos de Lie}

Otra forma interesante de definir variedades es a través de los grupos de Lie, que nos permiten mezclar la geometría y el álgebra.

\begin{defn}[Grupo\IS de Lie] \label{def:GrupoLie} Un grupo de Lie es un grupo $(G, \cdot)$ que admite una estructura de variedad diferenciable tal que las aplicaciones producto \begin{align*}
\appl{\cdot}{G × G&}{G} \\
(x,y) &\longmapsto x·y
\end{align*}  e inversa \begin{align*}
\appl{\inv{(·)}}{G&}{G} \\
x &\longmapsto \inv{x}
\end{align*} son diferenciables.
\end{defn}

Un ejemplo muy sencillo es $(ℝ^n, +)$, o también $(\quot{ℝ^n}{ℤ^n}, +)$. También podemos considerar a la circunferencia \crc como un grupo de Lie si lo tomamos como $\crc ⊂ (ℂ^*, ·)$, subgrupo de los complejos. En este caso, para el producto $f(e^{iθ},e^{iα}) = e^{iθ} · e^{iα} = e^{i(θ+α)}$ tendríamos que \[ φ_2(f(\inv{φ_1}(θ,α))) = φ_2(f(e^{iθ}, e^{iα})) = φ_2(e^{i(θ+α)}) = θ + α \] es efectivamente diferenciable. $\crc[3]$ también es un grupo de Lie, a través de los cuaterniones.

Esto es interesante, principalmente porque la condición de poder dotar de estructura de grupo a una variedad (o viceversa) es bastante restrictiva. De hecho, sólo las esferas $\crc$ y $\crc[3]$ tienen estructura de grupo de Lie.

En el caso de $\crc[3]$, el grupo es el de los cuaterniones, las raíces cuartas de la unidad. Sus elementos serán de la forma $u = a + bi + cj + dk$, con $i, j, k$ raíces cuartas de la unidad. La norma será $\norm{u} = a^2 + b^2 + c^2 + d^2$; y las cuentas que querremos hacer será ver que si $u,v ∈ \crc[3]$ entonces $uv ∈ \crc[3]$ o, en otras palabras, que si $\norm{u} = \norm{v} = 1$ entonces $\norm{uv} = 1$. Las cuentas infernales para demostrar eso o la existencia del inverso las dejamos fuera porque son horribles.

\section{Sumas conexas}

\begin{figure}[hbtp]
\centering
\inputtikz{SumaConexa}
\caption{Esquema de cómo realizar una suma conexa: se ``quitan'' dos entornos homeomorfos a una bola en ambas variedades y se ``pegan'' dos partes, uniendo los círculos oscuros y los claros.}
\label{fig:SumaConexa}
\end{figure}

Las variedades también se pueden construir como suma de otras dos variedades.

\begin{defn}[Suma\IS conexa] \label{def:SumaConexa} Sean $M_1^n, M_2^n$ dos variedades de dimensión $n$. Entonces, para dos cartas $φ_1$, $φ_2$ de $M_1$ y $M_2$ escogemos $p ∈ M_1,\,q∈M_2$ (los puntos de unión) y definimos \begin{align*}
M_1^* = M_1 \setminus \inv{φ_1} (\adh{\bola_{\sfrac{1}{2}}(0)}) \\
M_2^* = M_2 \setminus \inv{φ_2} (\adh{\bola_{\sfrac{1}{2}}(0)})
\end{align*} con $φ_1, φ_2$ cartas respectivas centradas en $p$ y $q$. En la \fref{fig:SumaConexa}, estamos quitando la parte punteada de las variedades. Podemos establecer un homomorfismo entre esas dos bolas que hemos quitado dado por \[ α(ρ,θ) = \left(\frac{1}{ρ}, τ(θ)\right) \], usando coordenadas polares y $τ$ una transformación arbitraria (es decir, que podemos girar las variedades al pegarlas).

Entonces podemos definir la relación que nos ``pega'' las variedades como \[ q_1 = \inv{φ_1}(ρ,θ) ∈ \inv{φ_1}(\bola_2 \setminus \adh{\bola_{\sfrac{1}{2}}}) \sim q_2 = \inv{φ_2}(α(ρ, θ)) ∈ \inv{φ_2}(\bola_2 \setminus \adh{\bola_{\sfrac{1}{2}}}) \] cuando estamos en la parte que identificamos, y trivial en el resto de puntos. Entonces podemos definir la suma conexa como
\[ M_1 \hash M_2 = \quot{M_1^* \cup M_2^*}{\sim} \] y sólo nos queda dar el atlas.

Para los puntos que no estén en la parte que pegamos, usamos el atlas que ya venía de la variedad. Para los puntos comunes, pues damos otro atlas que no sé cuál es.
\end{defn}

\begin{figure}[hbtp]
\centering
\inputtikz{SumaConexaResultado}
\caption{Resultado de la suma conexa de dos variedades}
\label{fig:SumaConexaResultado}
\end{figure}

Lo interesante de la suma conexa es que si ambas variedades son orientables, entonces la suma conexa es orientable.

\section{Característica de Euler}

Las sumas conexas también nos permiten definir un teorema de clasificación de superficies compactas.

\begin{theorem}[Teorema\IS de clasificación de superficies compactas] \label{thm:ClasificacionSuperficies} Toda superficie orientable y compacta $S ≠ \crc[2]$ es una suma conexa de toros, y toda superficie compacta no orientable $M$ es una suma conexa de planos proyectivos.
\end{theorem}

El teorema se basa en la característica de Euler, que definimos a continuación:

\begin{defn}[Característica\IS de Euler] \label{def:CaracteristicaEuler} Dada una variedad $M$, se define su característica de Euler como \[ χ(M) = V - A + C\] con $V, A, C$ los vértices, aristas y caras de una triangulación de $M$. \end{defn}

Esta característica es una invariante topológica, y además no depende de la triangulación. Algo que se puede dejar como ejercicio es el hallar la característica de Euler de la suma conexa, que sale \[ χ(M_1 \hash M_2) = χ(M_1) + χ(M_2) - 2 \]

\begin{proof}

Para sacarlo, habría que tener en cuenta que cuando se unen dos variedades, lo hacemos quitando un triángulo. Es decir, que cogemos la suma de vértices, aristas y caras de ambas triangulaciones, pero quitamos tres aristas (un triángulo pasa a estar compartido), tres vértices que también pasan a estar compartidos y las dos caras desaparecen.

"Formalizar" esta demostración está propuesto como ejercicio en la hoja 2.
\end{proof}


\begin{figure}[hbtp]
\begin{subfigure}{0.5\textwidth}
\centering
\inputtikz{TriangulacionToro}
\caption{Esquema para hacer la triangulación del toro a través de una variedad homeomorfa, en este caso un cubo sin un prisma central.}
\label{fig:TriangulacionToroPrisma}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\centering
\inputtikz{TriangToroPlano}
\caption{Dado que un toro es un cuadrado con los lados identificados, también podemos realizar la triangulación sobre ese cuadrado.}
\label{fig:TriangulacionToroPlano}
\end{subfigure}
\caption{Posibles triangulaciones para un toro, sin necesidad de mirar directamente el toro sino espacios homeomorfos a él.}
\label{fig:Triangulacion}
\end{figure}

Podemos ver, por ejemplo, la triangulación del toro para calcular su característica. Dado que es una invariante topológica, se puede hacer una triangulación sobre una variedad homeomorfa a él, como en la \fref{fig:TriangulacionToroPrisma}. Igualmente, también podemos realizar la triangulación sobre el cuadrado con los lados identificados, como en la \fref{fig:TriangulacionToroPlano} (ver \fref{fig:ToroEspacioCociente} si no nos acordamos de cómo se define el toro con los lados identificados). Eso sí, en este caso hay que tener en cuenta que la cuadrícula ha de ser de $3×3$ para que funcione bien con la identificación de lados. Si fuese de $2×2$, habría triángulos que intersecarían en dos puntos: en la mitad del borde y en la esquina.

\chapter{Espacios tangente y cotangente}
\label{chap:EspaciosTangenteyCotangente}

\section{Introducción y definición}

\begin{figure}[hbtp]
\centering
\inputtikz{EspacioTangenteIntroduccion}
\caption{Pongamos que queremos definir el cambio de temperatura (la variación de color azul a rojo) en cada punto $p$ de la curva $α$. Para eso necesitamos definir la dirección en la que nos movemos y también algún tipo de ``derivada'' que nos dé la variación en función de esa dirección. Estas necesidades dan lugar a las definiciones de espacio tangente y cotangente.}
\end{figure}

Una vez que ya hemos definido lo que es una variedad, lo más probable es que nos interese hacer algún tipo de cálculos de derivadas sobre ella. Por ejemplo, podemos querer ver cómo varía una función a lo largo de una curva definida en la variedad.

El problema es cómo definir la derivada. Si tenemos una variedad de dimensión 2 dentro de $ℝ^3$, una curva $α(t) = (x(t), y(t), z(t))$ tendrá tres derivadas parciales, pero eso no tiene sentido porque nosotros estamos trabajando en 2 dimensiones: no puede haber tres derivadas.

Necesitamos cambiar algo, crear algún tipo de estructura que nos permita hacer derivadas y poder trabajar cómodamente sin preocuparnos en exceso de en qué espacio ambiente estamos o cómo hemos ``incrustado'' nuestra variedad.

Para entenderlo, podemos fijarnos en qué es lo que hacemos cuando derivamos una función definida en, por ejemplo, $ℝ^2$. Lo primero que hacemos es sacar el gradiente como $\grad F = \left(\pd{F}{x}, \pd{F}{y}\right)$, pero no nos vale con eso. Lo que queremos es saber la variación de $F$ en una dirección en concreto, así que multiplicamos por un vector.

Para las variedades, haremos algo parecido. Iremos primero a por lo fácil: ¿en qué direcciones nos podemos mover cuando estamos metidos en una variedad? Está claro que no podemos salirnos de esa variedad a través de vectores normales (perpendiculares) a ella, así que sólo nos queda ir por vectores tangentes. Esto nos llevará a definir el espacio tangente, que denotaremos por $\tgs_p M$.

El siguiente paso será definir esos ``vectores gradiente'', que aplicaremos a vectores tangentes a la variedad. Esa ``aplicación'' se puede considerar como una aplicación lineal, y entonces el espacio de vectores gradiente será el espacio cotangente (que denotaremos por $\tgsd_p M$).

\section{Espacio tangente}

\textit{Nota: Las dos secciones siguientes son una mezcla de lo que dio Gabino en clase y de lo que hay en \citep[Capítulo I]{ApuntesGeoDif}.}

\subsection{Definición por medio de curvas}

¿Cómo definimos el espacio tangente a una variedad? Una primera aproximación puede ser tomar un punto $p ∈ M$, y una parametrización $φ$ de un entorno de $p$. Por ejemplo, cogemos la esfera y entonces podemos tomar las derivadas parciales: \( \tgs_p \crc[2] = \gen{\pd{φ}{x}, \pd{φ}{y}} = \gen{(1,0, \sfrac{-a}{c}), (0, 1, \sfrac{-b}{c})} \label{eq:EspTgEsfera} \)

Ahora bien, para hacer esto estamos apoyándonos en el hecho de que la esfera está en $ℝ^3$. ¿Qué hacemos si cogemos otra variedad abstracta, como $\projp^4$, y la metemos en otro espacio? Necesitamos una forma de definirlo independientemente del espacio ambiente. Ahora bien, el ejemplo que hemos puesto nos puede servir. Una forma, más bien poco rigurosa, de expresar el espacio tangente, sería darlo como el conjunto de los posibles vectores tangentes a curvas que pasan por $p$, tal que \[ \tgs_p \crc[2] = \set{α'(0) ∈ ℝ^3 \tq \appl{α}{(-ε, ε)}{\crc[2]}, \, α(0) = p} \] para $ε$ pequeño, donde α son curvas definidas en la esfera.

Un cálculo rápido nos permite ver que estamos cerca de lo que buscamos. Si tomamos $f(x,y,z) = x^2 + y^2 + z^2 - 1$, entonces $\crc[2] = \inv{f}(0)$. Por lo tanto, decir que $α(-ε, ε) ⊆ \crc[2]$ es equivalente a decir que $f(α(t)) = 0$ para $t ∈ (-ε, ε)$. Calculando su derivada, \[ 0 = (f ○ α)'(0) = \eval[2]{\Dif f}_{α(0) = p} · \eval[2]{\Dif α}_{0} = (2a, 2b, 2c) · α'(0) \], luego $α'(0)$ es ortogonal a $(a,b,c)$, y precisamente el espacio de los vectores ortogonales a $(a,b,c)$ es lo que habíamos definido antes como $\tgs_p \crc[2]$.

Por lo tanto, esto sí nos permitirá definir correctamente el espacio tangente a una variedad, aunque tendremos que sortear de alguna forma el hecho de que no sabemos derivar en espacios abstractos. Para ello, nos basaremos en las cartas y en los entornos de cada punto homeomorfos a $ℝ^n$, donde sí sabemos derivar.

\begin{defn}[Espacio\IS tangente] Sea $M$ una variedad diferenciable y sea $p ∈ M$. Definimos el espacio tangente a $M$ en el punto $p$ como el espacio cociente \[ \tgs_p M = \quot{\set{\appl{α}{(-ε, ε)}{M} \tq α(0) = p}}{\sim} \] con $\sim$ la relación de equivalencia dada por \[ α \sim β \iff (φ ○ α)'(0) = (φ ○ β)'(0)\] con $φ$ una carta de $p$. Los vectores de $\tgs_p M$ se escriben como $[α]$ o $α'(0)$, aunque esto es sólo notación ya que la derivada en la variedad no tiene demasiado sentido.
\end{defn}

La primera pregunta que podemos hacernos es si esta definición de $\tgs_p M$ depende de la carta elegida. Es decir, que si la derivada coincide con una carta, coincide también para cualquier otra carta. Bueno, la respuesta es que sí si hacemos unas cuantas operaciones (obviamente).

Con esta definición, se puede probar que $\tgs_p M$ es un espacio vectorial. La cuestión es ver de qué dimensión es, y la definición que hemos dado no nos va a ayudar demasiado.

\subsection{Definición por medio de derivaciones}

Ahora vamos a tomar un enfoque algo distinto. Sabemos qué queremos hacer, que es derivar funciones, así que vamos a tratar de dar una definición lo más minimalista posible de la derivada.

Lo primero que hacemos es definirnos el espacio de funciones sobre el que vamos a trabajar. Simplemente pediremos funciones definidas en un entorno de $p ∈ M$, y entonces definiremos \[ A_{p,M} = \set{\appl{f}{U}{ℝ} \tq p ∈ U,\, f ∈ C^∞} \], cuyos elementos serán los pares $(f,U)$ de las funciones y el entorno en el que están definidas.

Normalmente, los espacios de funciones tienen estructura de espacio vectorial, lo cual nos vendría muy bien en esta definición. Pero tenemos un problema, y es que no sabemos exactamente cómo podemos sumar dos funciones que están definidas en conjuntos distintos. Por suerte, esto se puede salvar definiendo una relación de equivalencia: \[ (f_1, U_1) \sim (f_2, U_2) \iff f_1 = f_2\text{ en }W ⊂ U_1 ∩ U_2 \], que está bien definida por ser los $U$ entornos abiertos que contienen a $p$.

Ahora podemos definir \[ \mathcal{A}_{p,M} = \quot{A_{p,M}}{\sim} \], que es un espacio vectorial y un anillo local (ver \citep{ApuntesAlgebraConmutativa}). Y con esto no sólo nos hemos quitado el problema de no saber sumar funciones, sino que además hemos metido muy, muy sutilmente el concepto de derivada: si dos funciones son equivalentes, entonces son iguales en un entorno de $p$, y por lo tanto tendrán la misma derivada en $p$. Luego podremos definir las derivadas, que aquí llamaremos derivaciones.

\begin{defn}[Derivación] \label{def:Derivacion} Una derivación en $p$ es una aplicación
\begin{align*}
\appl{D}{\mathcal{A}_p&}{ℝ} \\
f &\longmapsto D(f)
\end{align*}

Queremos que esta función conserve de alguna forma la noción de derivada en una dirección, así que buscaremos varias propiedades:

\begin{enumerate}
	\item $D$ es lineal.
	\item $D(λ) = 0$, donde $λ$ es una función constante $λ(x) = λ ∈ ℝ$.\footnote{En realidad, esta propiedad es consecuencia de las otras dos pero viene bien tenerla presente.}
	\item \label{prp:LeibnizVectorTg} Si esto se parece a una derivada, además de definir cómo se derivan las sumas\footnote{Por esto forzamos que $D$ sea lineal.} definiremos cómo se derivan los productos, según la regla de Leibniz: \[ D(f·g) = f · D(g) + D(f) · g\]
\end{enumerate}
\end{defn}

Una vez que hemos emulado las direcciones en las que podemos definir las derivadas, ahora sí podemos definir el espacio tangente:

\begin{defn}[Espacio\IS tangente] Diremos que el espacio tangente a $p$ en $U$ se define como
\[ \tgs_p U ≝ \set{\appl{D}{\mathcal{A}_p}{ℝ}\tq D \text{ es derivación }} \]
\end{defn}

Un ejemplo es ver que, si tomamos una función $\appl{f}{M}{ℝ}$ y una carta $\inv{φ}(t_1, \dotsc, t_n) \equiv (x_1, \dotsc, x_m)$ entonces $\eval[2]{\pd{}{x_i}}_p$ es una derivación que se aplica de la siguiente forma:
\( \eval{\dpd{}{x_i}}_p (f) ≝ \eval{\dpd{f○\inv{φ}}{t_i}}_{φ(p)} \label{eq:DerivacionTangente} \)

Además, las derivaciones son una base de el espacio tangente.

\begin{prop} $\set{\eval[2]{\pd{}{x_1}}_p, \dotsc,\eval[2]{\pd{}{x_n}}_p}$ es una base de $\tgs_p M$. En particular, $\dim \tgs_p M = \dim M$.
\end{prop}

\begin{proof}

Para demostrar que es una base, necesitamos ver que son linealmente independientes y además que generan el espacio. Vamos por partes.

\proofpart{Independencia}

\begin{gather*}
\sum_i = a_i \eval{\dpd{}{x_1}}_p = \vec{0} ∈ \tgs_p M \\
0 = \vec{0}(x_j) = \sum a_i \eval{\dpd{}{x_1}}_p  = a_j \; ∀j
\end{gather*} % OK.

\proofpart{Generadores}

Tenemos que ver que cualquier vector $v ∈ \tgs_p M$ se puede expresar como suma de derivaciones, esto es, que \( v = \sum a_i \eval{\dpd{}{x_i}}_p \label{eq:pruebaGeneradoresPlanoTg} \)

Debe ocurrir que $v(x_j) = a_j$. Sea $\appl{λ}{U_p}{ℝ}$ una función arbitraria, queremos ver qué es $$v(\lambda) = \sum\limits_{i=1}^{n} a_i \eval{\dpd{}{x_i}}_p (\lambda) $$

Empezamos calculando el desarrollo de Taylor de $\lambda$: $$\lambda = \lambda(p) + \left(\sum\limits_{i=1}^{n} \eval{\dpd{\lambda}{x_i}}_p (x_i - p)\right) + \frac{1}{2}\set{\eval{\dpd{\lambda}{x_i \partial x_j}}_p (x_i - p)(x_j - p)}_{i,j=1,\dots, n} + \dots$$

Y sustituimos en \ref{eq:pruebaGeneradoresPlanoTg}
$$ v(\lambda) = \underbrace{v(\underbrace{\lambda(p)}_{\text{constante}})}_{=0} + \underbrace{v(x_i)}_{=a_i} \sum \eval{\dpd{\lambda}{x_i}}_p + \frac{1}{2}\set{\eval{\dpd{\lambda}{x_i \partial x_j}}_p \underbrace{v((x_i - p))}_{=0}\underbrace{v((x_j - p))}_{=0}} + 0 \dots $$

Con lo que obtenemos $$v(\lambda) = \sum a_i \eval{\dpd{\lambda}{x_i}}_p $$

\end{proof}

\obs En la prueba anterior, hemos abusado de notación\footnote{y volverá a pasar}: $$ \eval{\dpd{\lambda}{x_i}}_p := \eval{\dpd{(\lambda \circ \varphi^{-1})}{x_i}}_{\varphi(p)} $$


¿Qué pasa con el cambio de coordenadas? En este caso, podemos hacer cuentas y las cosas funcionan bien:\[ \eval{\dpd{}{y_j}}_p = \sum_{i=1}^n \eval{\dpd{x_i}{y_j}}_p · \eval{\dpd{}{x_i}}_p \]

Se puede demostrar además que este espacio tangente es equivalente al que definimos mediante curvas.

\subsection{Aplicaciones entre variedades}

\begin{figure}[hbtp]
\centering
\inputtikz{AplicacionYDiferencial}
\caption{Ilustración de la diferencial de una aplicación entre variedades: nos lleva vectores tangentes a la variedad origen a vectores tangentes a la variedad de destino.}
\label{fig:AplicacionYDiferencial}
\end{figure}

Una vez que tenemos la definición del espacio tangente, ya podemos empezar a estudiar aplicaciones entre variedades y discutir condiciones sobre su ``derivada'' o, como la llamaremos en geometría, su diferencial. Eso sí, antes tenemos que definir bien qué entendemos por una aplicación diferenciable entre variedades.


\begin{defn}[Función\IS diferenciable] \label{def:FuncionDiferenciableVariedades} Sea $\appl{F}{M}{N}$ una aplicación entre variedades diferenciables. Decimos que es una \textbf{función diferenciable} si para toda carta $(U_i, Φ_i)$  en $X$ y para toda carta $(V_j, Ψ_j)$ en $Y$, la composición $Ψ_j ○ F ○ \inv{Φ_i}$ es diferenciable ($C^∞$) entre abiertos de $ℝ^n$ y $ℝ^m$ en todo punto donde está definida.

Esto es lo mismo que decir que $F$ es compatible con las cartas.
\end{defn}

A partir de esto, ya podemos definir qué es la diferencial de estas aplicaciones, lo que será el análogo del Jacobiano.

\begin{defn}[Diferencial\IS de una aplicación] \label{def:DiferencialAplicacion} Sean $M^m$, $N^n$ dos variedades de dimensión $m$ y $n$ respectivamente, y sea $\appl{F}{M}{N}$ una aplicación entre ellas. Entonces, dado un punto $p ∈ M$, se define la diferencial de $f$ en $p$ como la aplicación lineal dada por \begin{align*}
\appl{\eval[1]{\Dif F}_p}{\tgs_p M&}{\tgs_{f(p)} N} \\
v &\longmapsto \difp (v)
\end{align*}, donde $\difp (v)$ será una derivación de $\tgs_{F(p)} N$ dada por lo siguiente:
\begin{align}
\appl{\difp (v)}{\mathcal{A}_{F(p), N} &}{ℝ} \nonumber \\
g &\longmapsto v(g ○ F) \label{eq:DiferencialDerivacion}
\end{align}
\end{defn}

En el fondo, lo único que hacemos es tener una aplicación lineal de un espacio vectorial a otro: la diferencial (que en ocasiones se llama \concept{Pushforward} y se denota como $F_*$) nos dará la imagen en $\tgs_{F(p)} N$ de un vector tangente en $\tgs_p M$. Si además consideramos los elementos de $\tgs_p M$ como derivaciones, lo único que estamos haciendo en \eqref{eq:DiferencialDerivacion} es ``adaptar'' $v$, que vive en $\tgs_pM$, para poder hacer las derivaciones de funciones $g ∈ \mathcal{A}_{F(p), N}$.

Aunque esta definición de diferencial la hayamos hecho desde las derivaciones, podemos ver que si lo hubiésemos hecho con curvas nos habría salido lo mismo. Si consideramos una curva $\appl{c}{(-ε, ε)}{M}$ con $c(0) = p$, sabemos que $c'(0) ∈ \tgs_p M$. En ese caso, la diferencial será \[ \eval[1]{\Dif F}_p (c'(0)) = (f○c)'(0)\] como uno podría esperar.

Otra de las cosas que uno podría esperar es que una derivada nula implica función constante. Sólo necesitaremos una condición extra, que es la conexión de la variedad.

\begin{prop} \label{prop:DiferencialNulaFuncConstante} Dada una variedad $M$ conexa y una aplicación diferenciable $\appl{F}{M}{N}$, si se cumple que $\difp \equiv 0\; ∀p ∈ M$, entonces $F$ es constante.
\end{prop}

\begin{proof} Queremos ver que si $F(x_0) = y_0$ para algún $x_0 ∈ M$, entonces $F(x) = y_0$ para todo $x ∈ M$. Esto es exactamente lo mismo que decir que $\inv{F}(y_0) = M$.

Ahora bien, como el conjunto $\set{y_0}$ es cerrado, su imagen inversa por $F$ (que por ser diferenciable es continua) ha de ser un cerrado.

Por otra parte, como en $x_0$ la diferencial se anula, entonces $F$ es constante en un entorno de $x_0$, luego \noteby{Guille}{Esto hay que resolverlo bien.}
\end{proof}

También podemos ver si aquí tenemos igualmente que la diferencial se anula en máximos o mínimos. Como es de esperar, no tenemos problemas con eso.

\begin{prop} Sea $\appl{f}{M}{ℝ}$ con $p ∈ M$ un máximo o mínimo local. Entonces $p$ es un \concept{Punto\IS crítico} y $\difp[f] \equiv 0$.
\end{prop}

\begin{proof} Si $f ○ \inv{φ}$ tiene un máximo en $φ(p)$ para una carta φ de $p$, entonces $\Dif (f ○ \inv{φ}) \equiv 0$. En otras palabras, \[ 0 = \eval[2]{\dpd{f ○ \inv{φ}}{t_k}}_{φ(p)} ≝ \eval[2]{\dpd{f}{x_k}}_p ∀k \implies \difp = 0\]
\end{proof}

\subsubsection{Inmersiones y subvariedades}

Un caso especial de aplicaciones entre variedades son las que nos permiten meter variedades en otras de una forma más o menos regular. A su vez, esto nos dará lugar a poder definir qué es una subvariedad, es decir, de qué forma podemos dar estructura de variedad a un subconjunto de otra variedad.

En general, consideraremos ahora una aplicación $\appl{F}{M}{N}$, $F ∈ C^∞$ (infinitamente diferenciable). La primera definición es la de inmersión, que es una forma de meter una variedad dentro de otra.

\begin{defn}[Inmersión] Si $\appl{\difp}{\tgs_p M}{\tgs_{F(p)} N}$ es inyectiva $∀p ∈ M$ entonces es una inmersión. Equivalentemente, $F$ es inmersión si y sólo si \[ \mop{rango} \difp  = \dim M\]
\end{defn}

La condición de que la diferencial sea inyectiva nos permite evitarnos casos en los que el espacio tangente ``colapse''. En el ejemplo de la \fref{fig:FuncionNoInmersion} se puede ver cuando $y = 0$ que el espacio tangente, que en teoría nos tiene que decir todas las direcciones en las que podemos movernos sin salir de la variedad, se convierte en una recta. El trabajar con inmersiones nos permitirá asegurarnos de que el espacio tangente nunca nos va a dejar encerrados en sólo una parte de la variedad.

\begin{figure}[hbtp]
\centering
\inputtikz{FuncionNoInmersion}
\caption{La aplicación $F(x,y) = (x, y^2)$ es una aplicación que nos mete el plano dentro de $ℝ^2$, pero no es una inmersión por los puntos donde $y = 0$.}
\label{fig:FuncionNoInmersion}
\end{figure}

Con sólo una pequeña restricción más, podemos definir lo que es una subvariedad. Simplemente querremos asegurarnos de que cada punto en la variedad destino es imagen de un único punto en la variedad origen. Es decir, buscaremos que la función sea inyectiva.

\begin{defn}[Subvariedad] Dados $M ⊂ N$ y $F$ la identidad, se dice que $(M,F)$ es un subvariedad de $N$ si $F$ es una inmersión inyectiva.
\end{defn}

Por último, queremos tener alguna noción que nos permita decir que una aplicación mete una variedad dentro de otra sin cambiarla esencialmente. Esto es lo que llamaremos un embedding o incrustación:

\begin{defn}[Embedding] Si $F$ tiene diferencial inyectiva, es inyectiva y homeomorfismo sobre su imagen, entonces es un \textit{embedding} o incrustación.
\end{defn}

\begin{example} La aplicación \begin{align*}
\appl{F}{M&}{N} \\
t &\longmapsto (\cos t, \sin t)
\end{align*} es una inmersión con diferencial \[ \Dif F \left(\dpd{}{t}\right) = - \sin t · \pd{}{x_1} + \cos t \pd{}{x_2} \] que, aunque no se anula, no es subvariedad.

Sin embargo, si tomásemos $\appl{F}{\crc[1]}{ℝ^2}$ con $f$ la identidad y una carta en polares para $\crc[1]$, entonces sí es inmersión y subvariedad.
\end{example}

También nos interesará estudiar un concepto equivalente al de las proyecciones, que aquí llamaremos submersiones.

\begin{defn}[Submersión] \label{def:Submersion} Si $F$ es suprayectiva y la diferencial también es suprayectiva, entones es una submersión.
\end{defn}

\subsubsection{Fibración de Hopf}
\label{sec:FibracionHopf}

Para acabar con esta sección de aplicaciones, vamos a ver un ejemplo interesante de submersión, que además nos servirá en un futuro para estudiar las cohomologías. Este ejemplo es la \concept{Fibración\IS de Hopf}, que nos da una proyección de $\crc[3]$ en la esfera de $\crc[2]$.

Es interesante porque, tal y como se explica en \citep{liuHopfFibration}, es uno de los primeros ejemplos de fibrados no triviales.

La primera pregunta que debemos hacernos es qué es un \concept{Fibrado}. A grandes rasgos, se trata de la identificación de un espacio $E$ con copias de un espacio $F$ (las fibras) sobre una base $B$. Más formalmente, lo que pedimos es que exista una submersión continua $\appl{π}{E}{B}$ tal que cualquier abierto $U ⊂ E$ es homeomorfo a $π(U) × F$ o, en otras palabras, que la variedad $E$ sea localmente como el cociente $B × F$.

\begin{figure}[hbtp]
\inputtikz{CilindroFibrado}
\caption{El cilindro se puede tomar como un fibrado: a cada altura $h ∈ ℝ$ le asignamos una circunferencia $\crc[1]$. La proyección $π$ nos llevará después cada punto de la circunferencia a su altura correspondiente. Es una forma de ver el cilindro como $ℝ × \crc[1]$.}
\label{fig:CilindroFibrado}
\end{figure}

Por ejemplo, el cilindro se puede tomar como un fibrado $ℝ × \crc[1]$ (ver \fref{fig:CilindroFibrado}): las circunferencias en cada punto de $ℝ$ son las ``fibras''. Este es un fibrado trivial porque no sólo es localmente homeomorfo al producto, sino que toda la variedad es $ℝ × \crc[1]$.

En el caso de la fibración de Hopf, lo que la hace interesante es que es un fibrado no trivial. Aunque en cada punto de la esfera $\crc[2]$ le asignamos una circunferencia $\crc[1]$, al juntarlo todo no vamos a tener que globalmente $\crc[3] \cong \crc[2] × \crc[1]$: el fibrado sólo ``aparece'' cuando lo miramos localmente.

\begin{figure}[hbtp]
\centering
\includegraphics[width = 0.8\textwidth]{img/Hopf_Fibration.png}
\caption{Una representación de la fibración de Hopf, que lleva cada circunferencia de $\crc[3]$ a un punto en $\crc[2]$. Imagen vía \href{http://nilesjohnson.net/hopf.html}{Niles Johnson}, que tiene alguna animación chula sobre el tema.}
\label{fig:FibracionHopf}
\end{figure}

Para definir este fibrado trataremos de hacer algunas cuentas con la esfera $\crc[3]$. Sabemos que está dada por \[ \crc[3] = \set{ x,y,z,t ∈ ℝ \tq x^2 + y^2 + z^2 + t^2 = 1 } \]

La primera idea feliz es que la ecuación de arriba es equivalente a que $\abs{z}^2 + \abs{w}^2 = 1$ para $z,w ∈ ℂ$. Así, podemos meter la esfera dentro del plano completo: $\crc[3] ⊆ ℂ^2$.

La siguiente idea feliz es acordarnos de los espacios proyectivos de la \fref{sec:EspacioProyectivo}. Ahí definíamos $\projcp^1$, y de forma análoga a como teníamos en los reales (ver \fref{fig:EsferaPlanoProj}), en realidad no era más que la esfera con los puntos antipodales identificados. De la esfera podemos pasar a su proyección estereográfica, de tal forma que $\projcp^1 \cong ℂ ∪ \set{∞}$.

Con esto ya podemos definir nuestra proyección de $\crc[3]$ (tomada como subconjunto de $ℂ^2$) al espacio base del fibrado:
\begin{align*}
\appl{π}{\crc[3]&}{\projcp^1} \\
(z, w) &\longmapsto [z\cln w]
\end{align*}

El hecho clave de esta proyección es ver que dos puntos $(z,w), (z', w') ∈ ℂ^2$ tienen la misma imagen si existe un $λ ∈ ℂ$ tal que $z' = λz, w' = λw$, ya que en ese caso $[z\cln w]$ y $[z'\cln w']$ son la misma clase de equivalencia. Y ahora ya para la traca final, en ese caso y sabiendo que $\abs{z}^2 + \abs{w}^2 = \abs{z'}^2 + \abs{w'}^2 = 1$, tenemos que \[ 1 = \abs{z'}^2 + \abs{w'}^2  = \abs{λz}^2 + \abs{λw}^2 = \abs{λ}^2\left(\abs{z}^2 + \abs{w}^2 \right) = \abs{λ}^2 \] luego $\abs{λ} = 1$.

En otras palabras, lo que nos dice esto es que la proyección nos lleva todos los puntos de una misma circunferencia (multiplicar con un complejo de módulo 1 es simplemente rotar en una circunferencia) a un único punto en $\projcp^1$. Así, cada punto $[z\cln w] ∈ \projcp$ tiene asociada la fibra de puntos $e^{iθ}·(z,w)$ con $θ ∈ [0, 2π)$, que como decíamos es una cierta circunferencia. Para el resto de cuentas y una formalización algo mejor, es interesante leer \citep{liuHopfFibration} (que además habla de mecánica cuántica, que siempre mola).

Para nuestro caso de la geometría y topología, lo que nos faltaría ver es que efectivamente $\crc[3] \not\cong \crc[1] × \crc[2]$. Una demostración con cuentas sería totalmente infernal, pero sí que se podrá hacer con las cohomologías de De Rham que veremos en el \fref{chap:CohomologiaDeRham}.

\section{Campos vectoriales}

La definición de espacio tangente nos permite ampliar algo que ya se ve en cursos anteriores de cálculo, que son los campos vectoriales. Así, podremos definir campos en variedades arbitrarias que modelen cosas como el viento en una superficie o el campo magnético en el espacio.

\begin{defn}[Campo\IS vectorial] Un campo vectorial en $M$ es una aplicación \begin{align*}
\appl{X}{M&}{\tgs M = \bigcup \tgs_p M} \\
p &\longmapsto X(p) ∈ \tgs_p M
\end{align*}, donde $\tgs M$ es lo que se llama el \concept{Fibrado\IS tangente}.
\end{defn}

Esta es la definición general, aunque nosotros iremos directamente a por los que son diferenciables.

\begin{defn}[Campo\IS vectorial diferenciable] Un campo vectorial $X$ en $M$ se dirá diferenciable si localmente (esto es, en cada carta $(U, φ \equiv(x_1, \dotsc, x_m))$) es de la forma \[ X(p) = \sum_{i=1}^m λ_i(p) \eval{\dpd{}{x_i}}_p \] donde $λ_i ∈ C^∞(U)$. En realidad, la única cosa peculiar que pedimos es la diferenciabilidad de las $λ_i$, ya que siempre sabemos que un punto del tangente va a depender de esa base.
\end{defn}

Como siempre, el concepto no depende de la carta elegida.

\begin{example} Podemos dar un cambio en $\crc[2]$, a través de las dos cartas. Las definimos primero: \begin{gather*} U = \crc[2] \setminus \set{(0,0,-1)} \\
V = \set{ (x,y,t)  ∈ \crc[2] \tq t < 0}
\end{gather*} de tal forma que el campo puede ser \[ X(p) = \begin{cases}
\eval{\dpd{}{x_1}}_p & p ∈ U \\
\dfrac{1}{2} \left(-x^2 + y^2 + (1+t)^2\right) \eval{\dpd{}{y_1}}_p - xy \eval{\dpd{}{y_2}}_p & p ∈ V
\end{cases}\]

Desde luego, si $X$ es un campo tendrá que ser diferenciable. El problema es que no hemos comprobado si está bien definido, esto es, si ambas definiciones van a dar el mismo valor en los puntos de $U ∩ V$.

No pienso copiar las cuentas. El resultado es que sí, que está bien.
\end{example}

Sin entrar mucho en detalles (ver \citep[Capítulo VI]{ApuntesGeoDif} para algo mejor puesto), vamos a ver un ejemplo de la conexión de los campos vectoriales y la topología de una variedad.

\begin{theorem}[Teorema\IS de Poincaré-Hopf]Sea $X$ un campo definido en $M$. Entonces el número de ceros ``bien contados''\footnote{Es una enunciación del teorema muy vaga y poco formal, pero para enunciarlo bien hay que meterse en líos de índices de campos y no es plan.} coincide con la \nlref{def:CaracteristicaEuler} de la variedad $M$.\end{theorem}

Este es el teorema que permite demostrar otros como el ``teorema de la bola peluda'', que dice que, si le ponemos ``pelos'' a una pelota, nunca podremos peinarlos todos y siempre nos quedarán dos o más coronillas. Esa coronilla no es más que un cero del campo vectorial de pelos, y como la característica de Euler de la bola es 2, tiene que haber dos ceros ``bien contados''.

Igualmente, nos permite decir que el campo del ejemplo tiene que tener algún cero. De hecho, tendrá que anularse en el polo sur, cuando $t = -1$, y el índice de ese cero tendrá que ser 2.

\section{Espacio cotangente}

Una vez que tenemos definido el espacio tangente, vamos a querer hacer algo más que simplemente hablar de vectores tangentes. Querremos hablar de aplicaciones lineales sobre ellos, aplicaciones que por ejemplo nos puedan dar la longitud de un vector o, más tirando a nuestro campo de estudio, que nos permitan estudiar la topología a través de las formas diferenciales.

\begin{defn}[Espacio\IS cotangente] Dada una variedad $M$, se define el espacio cotangente a $M$ en $p ∈ M$ como el espacio dual de $\tgs_p M$, esto es, \[ \tgsd_p M = \mop{Hom}_ℝ (\tgs_p M, ℝ) = \set{\appl{A}{\tgs_p M}{ℝ} \tq A\text{ lineal, continua}} \]
\end{defn}

Por ser el dual, el cotangente tiene la misma dimensión que el tangente. Eso sí, el isomorfismo no es canónico\footnote{Esto es, que no hay una identificación canónica entre vectores del tangente y formas diferenciales como si pueda haber entre vectores de $ℝ^3$ y las aplicaciones lineales correspondientes.}. Si $(U, φ \equiv (x_1, \dotsc, x_m))$ es una carta alrededor de $p ∈ M$, entonces $\set{\eval[1]{\pd{}{x_i}}_p}_{i = 1, \dotsc, m}$ es una base de $\tgs_p M$. Nosotros denotaremos como $\set{\eval[1]{\dif x_i}_p}_{i=1,\dotsc, m}$ la base de $\tgsd_p M$ de tal forma que \[ \difp[x_i] \left(\eval{\dpd{}{x_j}}_p\right) = δ_{ij}\] donde $δ_{ij}$ es la \concept{Delta\IS de Kronecker}\footnote{$δ_{ij} = 1$ si $i = j$, $0$ en otro caso.}.

Como siempre, tendremos que tener cuidado con el cambio de coordenadas.

\textit{N.A.: A partir de ahora voy a pasar bastante de escribir dónde se evalúan los elementos de la base de los espacios tangentes, así que daré simplemente por supuesto que es en el punto $p$ y entonces $\eval{\dif x_i}_p = \dif x_i $, y $\eval[1]{\pd{}{x_i}}_p \equiv \pd{}{x_i}$. De hecho, por pura vaguería, diré que $\pd{}{x_i} \equiv ∂x_i$ tal que $\pd{f}{x_i} \equiv ∂x_i(f)$ y hala, todo bastante más sencillo.}

Si tenemos $(V, (y_1, \dotsc, y_m))$ otra carta, entonces podremos expresar \[ \dif x_i = \sum_{k=1}^m ∂y_k (x_i) · \dif y_k \]

La igualdad la probaremos aplicando a ambos lados $∂y_j$: \[ \dif x_i (∂y_j) =  \left(\sum_{k=1}^m ∂y_k (x_i) · \dif y_k \right)(∂y_j) = ∂y_j (x_i)\] y por otro lado \[ \dif x_i (∂y_j) = \dif x_i \left(\sum_{k=1}^m ∂y_j(x_k) · ∂x_k\right) = \sum_{k=1}^m ∂y_j(y_k) · \dif x_i (∂x_k) = ∂y_j(x_i) \], que efectivamente es lo mismo.

Con esto ya podemos definir lo que es un campo de covectores ó 1-forma diferencial.

\begin{defn}[Campo\IS de covectores] \label{def:CampoCovectores} Un campo de covectores ó 1-forma diferencial es una aplicación \begin{align*}
\appl{ω}{M&}{\bigcup \tgsd_p M = \tgsd M} \\
p &\longmapsto ω(p) ∈ \tgsd_p M
\end{align*}, donde $\tgsd M$ es lo que llamaremos el \concept{Fibrado\IS cotangente}.
\end{defn}

Un ejemplo sencillo es ver que, dada una aplicación $\appl{f}{M}{ℝ}$ con $f ∈ C^∞$, $\dif f$ es una forma diferencial. ¿Cómo podemos definir exactamente qué es la diferencial de una función? Antes hemos visto que $\dif x_i$ son los elementos de la base de $\tgsd_p M$. Intuitivamente, lo que podemos ver es que $\dif x_i (∂x_j) = δ_{ij}$ nos dice algo que en el fondo es bastante obvio: que la coordenada $x_i$ sólo varía cuando nos movemos en su misma dirección, y que si nos movemos la dirección de cualquier otra coordenada entonces $x_i$ no cambia.

Así, de forma análoga, lo que podemos ver es que $\dif f$ será una aplicación que nos diga cuánto varía $f$ cuando nos movemos en una cierta dirección $(v_1, \dotsc, v_m) = \vv ∈ \tgs_p M$. Si pensamos en el gradiente que vemos en cursos de cálculo, lo que hacemos en el fondo es multiplicar la coordenada $i$-ésima de la dirección en la que nos movemos (la dada por el vector $\vv ∈ \tgs_p M$) por la derivada parcial de $f$ con respecto a la $x_i$. En otras palabras, lo que debería ser $\dif f$ es algo como esto: \[ \dif f = \sum_{i=1}^m \pd{f}{x_i} · \dif x_i \], de tal forma que cuando lo quisiésemos aplicar a un vector $\vv = \sum v_i ·∂x_i$ nos quedaría \[ \dif f (\vv) = \sum_{i=1}^m \pd{f}{x_i} · \dif x_i (\vv) = \sum_{i=1}^m \pd{f}{x_i} · \dif x_i \left(\sum_{j=1}^m v_j · ∂x_j\right) = \sum_{i=1}^m \pd{f}{x_i} · v_i \], que es efectivamente lo que nos sale\footnote{Para el último salto, hemos usado que $\dif x_i$ es una aplicación lineal y que $\dif x_i (∂x_j) = δ_{ij}$.} si hubiésemos pensado en el gradiente de $f$.

\section{Formas diferenciales}

Cuando hemos definido el concepto de \nlref{def:CampoCovectores}, hemos dicho que también se podría llamar 1-forma diferencial. El nombre no es casualidad, así que vamos a ponernos a definir qué son las formas diferenciales y para qué sirven.

\begin{defn}[Forma\IS multilineal alternada de grado $d$] Una forma multilineal alternada de grado $d$ sobre un espacio vectorial $V$ es una aplicación \[ \appl{T}{\underbracket{V×\dotsb ×V}_{d\text{ copias}}}{ℝ} \] lineal en cada variable, y donde alternada significa que si se repite un vector argumento el resultado es cero o, equivalentemente, que si cambiamos de orden dos vectores entonces cambia de signo \[ T(\dotsc, v_i, \dotsc, v_j, \dotsc) = - T(\dotsc, v_j, \dotsc, v_i, \dotsc)\]
\end{defn}

En lo que nos interesa a nosotros, $V$ será el espacio tangente $\tgs_p M$. Además, el espacio de todas estas formas lo denotaremos\footnote{Creo que en \citep{ApuntesGeoDif} esto lo llamamos el álgebra pinchorial. Magnífico nombre.} como $\mop{Alt}^d (V)$. Dos ejemplos sencillos es ver que $\mop{Alt}^0 (V) = ℝ$, o que $\mop{Alt}^1(V) =V^*$.

Además, si $\dim V = n$, entonces $\dim \mop{Alt}^n = 1$, y una base es el determinante, esto es, \begin{align*} \appl{\det}{V × \dotsb × V&}{ℝ} \\
(v_1, \dotsc, v_n) &\longmapsto \det \begin{vmatrix} v_1^1 &  & v_n^1 \\ \vdots & \ddots & \vdots \\ v_1^n & & v_n^n \end{vmatrix}
\end{align*}

En general, podremos dar una base para el espacio de formas alternadas: a partir de una base $\set{e_i}_{i=1, \dotsc, n}$ de $V$, la base será la dada por covectores $\set{e_{i}^*}$\begin{align}
\appl{e_{i_1}^* ∧ \dotsb ∧ e_{i_d}^*}{\overbracket{V × \dotsb × V}^{d\text{ copias}}&}{ℝ} \nonumber \\
e_{i_1}^* ∧ \dotsb ∧ e_{i_d}^*(v_1, \dotsc, v_d) &= \det \begin{vmatrix}
e_{i_1}^* (v_1) & \cdots & e_{i_d}^* (v_1) \\
\vdots & \ddots & \vdots \\
e_{i_1}^* (v_d) & \cdots & e_{i_d}^* (v_d)
\end{vmatrix} \label{eq:ApplFormaMultAlt} \end{align} donde $e_i^* (e_j) = δ_{ij}$. En otras palabras, $e_{i}^*(v)$ extrae la coordenada $i$-ésima del vector $v$ o, equivalentemente, si $v = \sum v_i e_i$, entonces $e_i^* (v) = v_i$.

Vista la base algebraica, podemos ir a definir lo que es una forma diferencial.

\begin{defn}[Forma\IS diferencial] Dada $M$ una variedad diferenciable de dimensión $n$, una $k$-forma diferencial es una aplicación que a cada punto $p ∈ M$ le asigna una forma multilineal alternada de grado $k$ definida sobre el espacio $\tgs_pM$:
\begin{align*}
\appl{ω}{M&}{\mop{Alt}^k(\tgs_p M)} \\
p & \longmapsto ω(p)
\end{align*}

La forma lineal tiene como expresión local \[ ω = \sum_{i_1, \dotsc, i_k = 1}^n a_{i_1, \dotsc, i_k} (x_1, \dotsc, x_n) \dif x_{i_1} ∧ \dotsb ∧ \dif x_{i_k} \], donde $\dif x_i$ son los elementos de la base del dual $\tgsd_p M$. Tomando multíindices $I \equiv i_1, \dotsc i_k$, la definición se puede abreviar como $ω = \sum_{I} a_I \dif x_I$.
\end{defn}

De momento, lo de ``expresión local'' lo ignoraremos hasta que veamos el pullback. De momento, una 2-forma diferencial en $ℝ^3$ puede ser algo como $ω = (x + y + z) \dif x ∧ \dif z$. Esta forma la evaluaremos en un punto $p = (3,4,5)$, por ejemplo, y después la aplicaremos a dos vectores $\vv_1 = (-1,0,-2)$, $\vv_2 = (1,1,1)$ (con cuidado de hacerlo con el determinante como en \eqref{eq:ApplFormaMultAlt}). Si hacemos el cálculo, tenemos lo siguiente: \[
ω_p(\vv_1, \vv_2) = (3 + 4 + 5) · \det \begin{vmatrix} -1 & 1 \\ -2 & 1 \end{vmatrix} = 12 · 1 = 12 \]

\subsection{Motivación}
\label{sec:MotivFormas}

¿Qué es una forma diferencial? Aquí la explicación de \citep{tao2007differential} es muy interesante, y haremos un pequeño resumen. Si recordamos las clases de física de Bachillerato, existían funciones de energía $F$ de tal forma que el trabajo para subir una masa unitaria de una altura $a$ a una altura $b$ era $\int_a^b F(x) \dif x$.

En los cursos de cálculo siempre se dice que $\dif x$ no es nada más que notación, y que nos indica con respecto a qué variable integrar. Pero podemos mirar todo ese integrando de forma conjunta, suponiendo, como hacían los matemáticos clásicos, que $\dif x$ es un ``incremento infinitesimal''. Así, $F(x) \dif x$ no es más que lo que nos cuesta desplazar la masa una distancia infinitesimal $\dif x$.

¿Cómo podemos generalizar esto a dimensiones superiores? ¿Qué pasa si en lugar de subir la altura del objeto lo movemos a lo largo de una curva arbitraria $γ ∈ ℝ^3$? Tendremos lo mismo, salvo que contando con más dimensiones: ya no nos bastará con decir sólo el potencial cuando nos movemos en la dirección $x$, también en las direcciones $y$ y $z$. Así, la función a integrar sería algo como $ω = F_x(x,y,z) \dif x + F_y(x,y,z) \dif y + F_z(x,y,z) \dif z$, que en cada punto nos dirá cuánto nos cuesta mover la masa en una dirección $\vv ∈ ℝ^3$. No es casualidad que eso sea también una forma diferencial, que de hecho podemos integrar y sacar que el trabajo total sería $\int_γ ω$.

El siguiente paso es ver qué ocurre si no queremos tratar con curvas, sino con superficies. Por ejemplo, queremos ver cuánta luz pasa a través de una superficie transparente $M$, donde la luminosidad en cada punto vendrá dada por una función $f(x,y)$, con $x,y$ coordenadas de $M$. Para calcularlo, haremos la misma aproximación de siempre con las integrales: dividiremos la superficie en pequeños rectángulos de lado $Δx$ y altura $Δy$ y multiplicaremos por el valor de luminosidad en ese rectángulo: $\int_M f \approx \sum f(x,y) Δx Δy$. La cuestión es que no tenemos claro que siempre vayamos a poder dividir en rectángulos: igual $x$ e $y$ no son coordenadas perpendiculares y $Δx·Δy$ no nos da el área. Por suerte, este problema es salvable: simplemente usamos los paralelogramos definidos por los vectores $(x_1, x_2), (y_1, y_2)$, que tienen área $\det \left|\begin{smallmatrix} x_1 & y_1 \\ x_2 & y_2 \end{smallmatrix}\right|$. Así, estaríamos integrando la forma diferencial $f \dif x ∧ \dif y$, aplicándola a vectores tangentes a $M$ que son los que definen los pequeños rectángulos sobre los que integramos.

Que la definición de ese área sea la misma (aunque menos genérica) que la que dábamos en \eqref{eq:ApplFormaMultAlt} no es casualidad, igual que tampoco lo es que el volumen de un paralelepípedo esté dado por el determinante de los tres vectores que lo definen.

En resumidas cuentas, lo que hace la forma diferencial es darnos una longitud, un área o un volumen en una variedad. Nos permitirán hacer cálculos independientemente de las coordenadas de la variedad y, sobre todo, nos revelarán ciertos aspectos sobre la topología de las variedades, que es lo que estudiaremos en esta asignatura.

\subsection{Operaciones con formas diferenciales}

Por abreviar, $Ω^k(M)$ será nuestro espacio de trabajo, el espacio vectorial de formas diferenciales de grado $k$ definidas sobre $M$. Si $ω ∈ Ω^k(M)$ entonces

Sabíamos que $Ω^0(M) = C^∞(M)$ y que además, localmente, podemos expresar las formas diferenciales como \[ \restr{ω}{U} = \sum a_{i_1, \dotsc, i_k} \dif x_1 \y \dotsb \y \dif x_k \] donde $\set{x_i}$ es un sistema de coordenadas del entorno $U$ y con $\set{\dif x_i}$ el dual de $\set{∂x_i}$. Con esta base, podemos empezar a definir las operaciones sobre formas diferenciales.

\subsubsection{Producto exterior}

La primera operación es el producto exterior, una aplicación $\appl{∧}{\Omega^r(M)×\Omega^s(M)}{\Omega^{r+s}(M)}$ compatible con las cartas, que definiremos de la siguiente forma.

\begin{defn}[Producto\IS exterior] Dadas una $m$-forma $ω = \sum a_I \dif x_I$ y otra $n$-forma $τ = \sum b_J \dif x_J$ con $I,J$ multiíndices, su producto exterior es una $(m+n)$-forma que se define como \[ ω ∧ τ = \sum_{I,J} a_I b_J \dif x_I ∧ \dif x_J \], teniendo en cuenta que $\dif x_i ∧ \dif x_i = 0$.
\end{defn}


\subsubsection{Diferencial exterior}

La diferencial es una aplicación lineal \begin{align*}
\appl{\dif}{Ω^k(\bola^n)&}{Ω^{k+1}(\bola^n)} \\
ω = h \dfl{t_{i_1}}{t_{i_n}} &\longmapsto \dif ω = \dif h ∧ \dfl{x_{i_1}}{x_{i_n}}
\end{align*}

Sólo damos la definición para un término porque es lineal. Por ser un poco más concretos, \[ \dif ω = \sum_{j=1}^k \left(\dpd{h}{x_j} \dif x_j \right) \dfl{x_{i_1}}{x_{i_n}} \]

La diferencial tiene una serie de propiedades:

\begin{prop} Propiedades de la diferencial exterior:
\begin{enumerate}
\item $\dif (ω ∧ η) = \dif ω ∧ η + (-1)^{\deg ω} ω ∧ \dif η $.
\item \label{prp:DifDifCero} $\dif (\dif ω) = 0$.
\item \label{prp:CompatDifPullback} Dada $\appl{f}{ℝ^m}{ℝ^n}$, entonces $f^* (\dif ω) = \dif(f^*ω)$.
\end{enumerate}
\end{prop}

\begin{proof}

\proofpart{$\dif (ω ∧ η) = \dif ω ∧ η + (-1)^{\deg ω} ω ∧ \dif η $}

Tomamos $ω = \sum f_I \dif x_I$, $η = \sum g_J \dif x_J$, con $I,J$ multiíndices. Entonces operamos: \begin{align*}
\dif (ω ∧ η) &= \dif \left(\sum_{I,J} (f_I · g_J) \dif x_I ∧ \dif x_J \right)
	= \sum_{I,J} \dif(f_I · g_J) \dif x_I ∧ \dif x_J = \\
	&= \sum_{I,J} \left(\sum_{k = 1}^N \dpd{(f_I · g_J)}{x_k} \right) \dif x_k ∧ \dif x_I ∧ \dif x_J = \\
	&= \sum_{I,J} \left(\sum_{k = 1}^N g_J \dpd{f_I}{x_k} + f_I \dpd{g_J}{x_k}\right) \dif x_k ∧ \dif x_I ∧ \dif x_J = \\
	&= \sum_{I,J} \sum_{k=1}^N \left(\dpd{f_I}{x_k} \dif x_k ∧ \dif x_I ∧ (g_J \dif x_J) + \dpd{g_J}{x_k} \dif x_k ∧ (f_I \dif x_I) ∧ \dif x_J \right) = \\
	&= \sum_{I,J} \left(\sum_{k=1}^N \dpd{f_I}{x_k} \dif x_k ∧ \dif x_I ∧ (g_J \dif x_J)\right) + \\ & \qquad + \sum_{I,J} \left(\sum_{k=1}^N \dpd{g_I}{x_k} \dif x_k ∧ (f_I \dif x_I) ∧ \dif x_J \right) = \\
	&= \sum_{I,J} \left(\sum_{k=1}^N \dpd{f_I}{x_k} \dif x_k ∧ \dif x_I ∧ (g_J \dif x_J)\right) + \\ & \qquad + \sum_{I,J} \left(\sum_{k=1}^N (-1)^{\deg ω} \dpd{g_I}{x_k} \dif x_k ∧ \dif x_J ∧ (f_I \dif x_I) \right) = \\
	&= \dif ω ∧ η + (-1)^{\deg ω} ω ∧ \dif η
\end{align*}, donde el $(-1)^{\deg ω}$ aparece al mover $\deg ω$ diferenciales (las del multiíndice $\dif x_I$) para ordenarlo todo bien.

\proofpart{$\dif(\dif ω) = 0$}

\end{proof}

Con esto podemos definir bien la diferencial en una variedad abstracta.

\begin{defn}[Diferencial\IS exterior] Dada una forma diferencial $ω ∈ Ω^k(M)$, entonces definimos la diferencial como \begin{align*}
\appl{\dif}{Ω^k(M)&}{Ω^{k+1} (M)} \\
ω &\longmapsto \dif ω = \sum_{j=1}^k \left(\dpd{h}{x_j} \dif x_j \right) \dfl{x_{i_1}}{x_{i_n}}
\end{align*}

Y declaramos que esto es perfectamente compatible con las cartas, es decir, que si $(φ_U ○ \inv{φ_V})^* ω_U = ω_V$, entonces $(φ_U ○ \inv{φ_V})^* \dif ω_U = \dif ω_V$. Sólo hay que usar la \fref{prp:CompatDifPullback}.
\end{defn}

\subsubsection{Pullback}

Dada una aplicación $\appl{F}{M}{N}$, queremos ver cómo podemos llevar formas diferenciales de $N$ a formas diferenciales en $M$.

\begin{defn}[Pullback] Dada una aplicación $\appl{f}{M}{N}$ entre variedades diferenciables, el pullback nos lleva formas diferenciales de $N$ a $M$ y está dado por \begin{align*}
	\appl{F^*}{Ω^k(N)&}{Ω^k(M)} \\
	ω &\longmapsto f^* ω
\end{align*} donde
\begin{align*}
	\appl{(F^*ω)(p)}{\tgs_p M × \dotsb × \tgs_p M&}{ℝ} \\
	(v_1, \dotsc, v_k) &\longmapsto ω(F(p)) \left(\difp(v_1), \dotsc, \difp (v_k) \right)
\end{align*}
donde $\difp$ es la \nlref{def:DiferencialAplicacion}. \end{defn}

Para ver cómo funciona el pullback, vamos a ver un ejemplo para
\[
	\appl{f}{ℝ^m}{ℝ^n}
\]
, con $f(x) = (f_1(x), \dotsc, f_n(x))$ y coordenadas $(x_1, \dotsc, x_m)$ para $ℝ^m$ e $(y_1, \dotsc, y_n)$ para $ℝ^n$.
Entonces, dada una forma diferencial $\dif x_j ∈ Ω^1(ℝ^m)$ entonces \[ f^*(\dif y_j) = \sum_{k=1}^m \pd{f_j}{x_k} \dif x_k = \dif f_j \]

Para comprobarlo, lo que hacemos es un chorro de cuentas aplicando eso a vectores tangentes $∂y_j$ y sale.

Ahora nos podemos hacer una pregunta para hacer cuentas todavía más infernales: ¿qué ocurre si cogemos una $2$-forma $\dif y_i ∧ \dif y_j ∈ Ω^2(ℝ^n)$? Bueno, pues lo que sale es que
\[
	f^* (\dif y_i ∧ \dif y_j)
	= \sum_{k,l=1}^n \dpd{f_i}{x_k} \dpd{f_j}{x_k} \dif x_k ∧ \dif x_l
\], y de nuevo si lo aplicamos a dos vectores, tenemos que
\[
	\left(\sum_{k,l=1}^n \dpd{f_i}{x_k} \dpd{f_j}{x_k} \dif x_k ∧ \dif x_l\right)(∂x_r, ∂x_s)
	= \pd{f_i}{x_r} \pd{f_k}{x_s}
\] por la definición de base dual ($\dif x_i (∂x_j) = δ_{ij}$). Por el otro lado,
\begin{align*}
\left(f^*(\dif y_i ∧ \dif y_j)\right) (∂x_r, ∂x_s)
	&= \dif y_i ∧ \dif y_j\left(\Dif f (∂x_r), \Dif (∂x_s)\right) = \\
	&= \dif y_i ∧ \dif y_j
		\left(
			\sum_{α=1}^m \dpd{f_α}{x_r} ∂y_α,
			\sum_{β=1}^m \dpd{f_β}{x_s} ∂y_β,
		\right) = \\
	&= \dpd{f_i}{x_r} \dpd{f_j}{x_s}
\end{align*}

Efectivamente, tenemos lo mismo y por lo tanto el pullback es compatible con el producto exterior.

\subsubsection{Resumen}

Dado el caos que son las secciones anteriores, ponemos un pequeño resumen de las operaciones y sus propiedades. En general, tenemos $ω = \sum a_I \dif x_I$ de grado $m$ y $τ = \sum b_J \dif x_J$ de grado $n$.

\paragraph{Producto exterior}
\begin{align*}
ω ∧ τ &= \sum a_I b_J · \dif x_I ∧ \dif x_J \\
\dif x_i ∧ \dif x_i &= 0
\end{align*}

\paragraph{Diferencial exterior} \begin{align*}
\dif ω &= \sum_{I} \left(\sum_{i} \dpd{a_I}{x_i} \dif x_i \right) \dif x_I \\
\dif(\dif ω) &= 0 \\
\dif(ω+τ) &= \dif ω + \dif τ \\
\dif(ω ∧ τ) &= \dif ω - τ + (-1)^k ω ∧ \dif τ
\end{align*}

\paragraph{Pullback} Dada $\appl{f}{M}{N}$, con $f(x_1, \dotsc, x_m) = (f_1, f_2, \dotsc, f_n)$ entonces \begin{align*}
f^*ω &= \sum_{I} f ○ a_I · \dif f_I = \sum_{I} a_I(f(x_1, \dotsc, x_m)) \bigwedge_{i ∈ I} \left(\sum_{j=1}^n \dpd{f_i}{x_j} \dif x_j \right) \\
f^*(\dif ω) &= \dif (f^*ω) \\
f^*(ω∧τ) &= f^*ω ∧ f^*τ \\
f^*(ω+τ) &= f^*ω + f^*τ
\end{align*}

\subsection{Expresión local de formas diferenciales}

Hasta ahora hemos dejado de lado el tema de las cartas, y no hemos comprobado si todas las operaciones y las formas diferenciales son compatibles con las cartas. Pero eso tenemos que comprobarlo: queremos ver si, dado un punto $p$ y dos cartas $U, V$ para un entorno de $p$, las formas diferenciales definidas en esas cartas coinciden.

\begin{figure}[tbhp]
\inputtikz{CartasFormasDiferenciales}
\caption{Aquí pondría el dibujo para ver qué es exactamente lo que tenemos que demostrar, pero como no tengo ni idea de qué es lo que tengo que hacer pues lo dejo así.}
\label{fig:CartasFormas}
\end{figure}


\begin{prop} $ω(p)$ no depende de la carta elegida para $p$.
\end{prop}


%%%% CLase copiada por dejuan.
\[
	\Omega(M) = \bigoplus_{k≥0} \Omega^K(M)
\]
Es el espacio de todas las formas diferenciales y es un espacio vectorial.


Sea $ω$ una forma diferencial de grado $d$. Entonces,

\[
	ω \equiv \left\{
			ω_u\in\Omega_+^d(φ(u))\\
				+
			ω_v = (φ_uφ_v^{-1})^\ast ω_u
	\right\}_{u\in \text{atlas}}
\]

De esta manera, podemos escribirlo en términos de funciones, tomando $I$ como multiíndice, con lo que $dx_I = dx_{i1}\wedge ... \wedge dx_{in}$

\[ω_u = \sum_{|I| =d}a_I^u dx_I\]


\begin{example}
Vamos a ver un ejemplo con la \concept{forma angular}, $i^*ω = dθ$, la función $\appl{i}{S^1}{\real^2}$ y $ω = -y\dif x + x \dif y$.

Tenemos
\[
	dθ \equiv \left\{ \begin{array}{c} dθ\in Ω^1(0,2π)\\dθ\in Ω^1(-π,π)\end{array}\right.
\]

Las funciones transición $φ_1$ y $φ_2$ son las funciones transición tales que $\appl{φ_1}{e^{iθ}}{θ}$ en $(0,2π)$ y  $\appl{φ_2}{e^{iθ}}{θ}$ en $(-π,π)$.


Ahora nos planteamos si es cierto $i^*ω\left( \dpa{}{θ}\right)_{|p_0} = 1$, tomando $p_0 = e^{iθ_0}$

\begin{align*}
	&i^*ω\left( \dpa{}{θ}\right)_{|p_0} = ω\left(\dif i \left( \dpa{}{θ}\right) \right) = \\
	&ω\left(
			\dpa{x·i}{θ}_{|_{θ_0}} \dpa{}{x}_{|_{p_0}}
	\right)  =
	ω\left(
		-\sin θ_0 \dpa{}{x}_{|_p} + \cos θ_0 \dpa{}{y}_{|_{p_0}}
	\right)\\
	&= -y\dif x + x\dif y \left(
		-\sin θ_0 \dpa{}{x}_{|_p} + \cos θ_0 \dpa{}{y}_{|_{p_0}}
	\right) \\
	&= ... = -\sin(θ_0)(-\sin θ_0)·1 + \cos(θ_0)\cos(θ_0) = 1
\end{align*}

¿Y cuánto valdría $i^*(\dif x ∧ \dif y)$? Necesariamente es $0$, ya que en $S^1$ no caben formas diferenciales mayores de la dimensión. Es decir:

\[
	\Omega(M) = \bigoplus_{k≥0}^{∞} \Omega^K(M) = \bigoplus_{k≥0}^{\dim{M}} \Omega^K(M)
\]

\end{example}

\chapter{Cohomología de De Rham}
\label{chap:CohomologiaDeRham}

\section{Complejo de De Rham}

La diferencial nos permite definir una cadena dada una variedad $M$ de dimensión $k$: \[ Ω^0(M) \xrightarrow{\dif^0} Ω^1(M) \xrightarrow{\dif^2} \dotsb \xrightarrow{\dif^{k-1}} Ω^{k}(M) \xrightarrow{\dif^k} Ω^{k+1}(M) \]

Nos podemos fijar que, por la \fref{prp:DifDifCero}, tenemos que $\img \dif^k ⊂ \ker \dif^{k+1}$. Entonces podemos definir la cohomología de De Rham

\begin{defn}[Grupo\IS de cohomología] Se define el grupo $k$-ésimo de cohomología de De Rham como \[ H^k (M) = \quot{\ker \dif ^k}{\img \dif^{k-1}} \]
\end{defn}

Dos notaciones:

\begin{defn}[Forma\IS exacta] Se dice que $ω ∈ Ω^k(M)$ es una forma diferencial exacta si y sólo si $∃ α ∈ Ω^{k-1}(M)$ tal que $\dif α = ω$.
\end{defn}

\begin{defn}[Forma\IS cerrada] Se dice que $ω ∈ Ω^k(M)$ es una forma diferencia cerrada si y sólo si $\dif ω = 0$.
\end{defn}

Está claro que $\ker \dif^k$ son formas cerradas e $\img \dif^{k-1}$ son las exactas.

Un ejemplo rápido es ver que \[ H^0(M) = \quot{\ker \dif^k}{\set{0}} = \set{ω ∈ Ω^0(M) = C^∞(M) \tq \dif ω = 0} \], luego $H^0(M)$ es un espacio vectorial isomorfo a $ℝ^C$, donde $C$ es el número de componentes conexas de $M$. Para ver esto, sólo tenemos que darnos cuenta de que las funciones cuya diferencial es $0$ son las constantes $c ∈ ℝ$. Por otra parte, si tenemos $C$ componentes conexas tenemos $C$ posibles elecciones de ``constantes'', luego eso nos da $ℝ^C$. Esto, que parece un hecho algo estúpido, en realidad nos da una pista sobre la topología: la conexión de la variedad.

Otro ejemplo interesante es calcular \[ H^k(ℝ) \cong \begin{cases} ℝ & k = 0 \\ \set{0} & k = 1 \\ \set{0} & k > 1 \end{cases}\]

En cuanto nos pasamos de la dimensión de $ℝ$, el grupo de cohomología es $\set{0}$ (no hay formas diferenciales de grado mayor que la dimensión de la variedad). El único punto a verificar sería el de \[ H^1(ℝ) = \quot{\ker \dif^1}{\img \dif^0} = \quot{\set{ω = f(t) \dif t}}{\set{η = \dif φ, φ ∈ C^∞(ℝ)}} = \set{0} \], ya que ambos espacios son iguales: $ω = f(t) \dif t = \dif φ$ con $φ = \int_0^t f(s) \dif s$.

\subsection{Anillo de cohomología}

Definimos el anillo de cohomología como \[ H^*(M) = \bigoplus_{k ≥ 0} H^k (M) \], que es un anillo con las operaciones $(+, ∧)$.

Habría que comprobar para ver que es un anillo que si $ω, η$ son cerradas entonces $ω ∧ η$ y $ω + η$ son cerradas, y que además esto no depende de la elección del representante\footnote{Como lo que estudiamos son formas cerradas, sabemos que $[ω] = [ω + \dif ω_1]$}. Son un montón de cuentas sin ninguna dificultad que salen directamente.

Un difeomorfismo entre variedades $\appl{f}{M}{N}$ induce isomorfismos de espacios vectoriales y de anillos respectivamente: \begin{align*}
\appl{f^*}{H^k(N)&}{H^k(M)} \\
\appl{f^*}{H^*(N)&}{H^*(M)}
\end{align*}


\subsection{Lema de Poincaré}

\noteby{Guille}{No sé dónde meter esto. }
\noteby{Guille}{Arreglar esto muy fuerte.}

\begin{lemma}[Lema\IS de Poincaré] \label{lem:Poincare} $π^* ○ s^*$ es la identidad a nivel de cohomología de $H^*(ℝ^{n+1})$, donde $π$ es algo y $s$ es otra cosa y $π○s(x) = x$.

\noteby{Guille}{Creo que $s_k(x) = (x,k)$ y $π(x,k) = x$, o algo así.}
\end{lemma}

\begin{corol} $π^*$ y $s^*$ son mutuamente inversas a nivel de cohomología, luego $H^k(ℝ^{n+1}) = H^k(ℝ^n)$.
\end{corol}

Por lo tanto $H^k(ℝ^n) = 0$ cuando $k > 0$. Otra forma de decir esto es que en la bola toda forma diferencial cerrada es exacta.

Podemos ver que, en $ℝ^2$, si tomamos $ω ∈ Ω^1(ℝ^2)$ de la forma $ω = F \dif x + G \dif y$, con $F,G ∈ C^∞$, entonces $ω$ será cerrada si y sólo si \[ 0 = \dif ω = (G_x - F_y) \dif x \y \dif y \iff G_x = F_y\]

Y algo.

\begin{proof}[\nref{lem:Poincare}] Basta probar que existe una aplicación lineal $\appl{K}{Ω(ℝ^{n+1})}{Ω(ℝ^{n+1})}$ que satisface la relación \( π^* ○ s^* = \mathrm{Id}_{Ω(ℝ^{n+1})} \pm (\dif K \pm K ○ \dif) \label{eq:PrfPoincare:1}\), pues en ese caso tendríamos que, dada $ω ∈ H^k(ℝ^{n+1})$, \[ π^*○s^*([ω]) = [\mathrm{Id} ○ ω + \underbracket{\dif K ω }_{\text{exacta, } [\dif K ω] = 0}+ \underbracket{K \dif ω}_{\dif ω = 0} ]  = [ω]\]

Entonces definimos \begin{align*}
\appl{K}{Ω(ℝ^{n+1})&}{Ω(ℝ^{n+1})} \\
f(\vx, t) \dif x_I &\longmapsto 0 \\
g(\vx, t) \dif x_J ∧ \dif t & \longmapsto \left(\int_0^t g(x,s) \dif s\right) \dif x_J
\end{align*}

Veamos que $K$ satisface la condición de \eqref{eq:PrfPoincare:1}. Y si se hacen las cuentas pues sale.
\end{proof}

\begin{prop} Más generalmente, $H^*(M×ℝ) = H^*(M)$ usando una demostración análoga.
\end{prop}

\subsection{Invarianza homotópica}

\begin{theorem} Supongamos que $\appl{f,g}{M}{N}$ son aplicaciones homótopas. Entonces $f^* = g^*$ a nivel de cohomología.
\end{theorem}

\begin{wrapfigure}{R}{0.4\textwidth}
\inputtikz{FuncionMeseta}
\caption{Una función ``meseta'' $α ∈ C^∞$ que varía suavemente de $0$ a $1$ en el intervalo $[0,1]$.}
\label{fig:FuncionMeseta}
\end{wrapfigure}

\begin{proof}
Consideramos la aplicación \begin{align*}
\appl{F}{[0,1] × M&}{N} \\
(t,x) &\longmapsto F_t(x)
\end{align*} donde $F_0 = f$ y $F_1 = g$.

Por otra parte, sea $\appl{α}{ℝ}{[0,1]}$ una función meseta como en la \fref{fig:FuncionMeseta}. Entonces definimos \begin{align*}
\appl{G}{M×ℝ&}{N} \\
(x,t) & \longmapsto F(α(t), x)
\end{align*} y tal que \begin{align*}
(G○s_0) (x) &= G(x,0) = F(0,x) = F_0(x) = f \\
(G○s_1) (x) &= G(x,1) = F(1,x) = F_1(x) = g
\end{align*}

Entonces $f^* = (G○s_0)^* = s_0^* ○ G^* = s_1^* ○ G^* = (G○s_1)^* = g^*$ y listos, así que a nivel cohomológico son iguales.\footnoteby{Guille}{Ok.}
\end{proof}

\begin{corol}
Dos variedades homótopas entonces sus anillos de cohomología son iguales.
\end{corol}

En el caso particular de un retracto por deformación, algo.

Vamos a demostrar que $\crc[1]$ no es un retracto por deformación de $ℝ^2$. Si lo fuera, los grupos de cohomología serían ambos nulos $H^1(ℝ^2) = 0$ pero $-y\dif x + x \dif y ∈ H^1(\crc[1])$ no es exacta.

En general, $H^k(ℝ^{n+1} \setminus \set{0}) \cong H^k(\crc[n])$. ¿Cómo encontrar un generador de $H^1(ℝ^2 \setminus \set{0})$. Bueno, podemos coger la contracción $r(x) = \frac{x}{\norm{x}}$ y entonces la forma generadora es \[ r^*(\dif θ) = r^*\left(\frac{-y \dif x + x \dif y}{x^2 + y^2}\right) \]

Estos resultados sobre homotopía siguen siendo válidos a nivel topológico, porque además podemos aproximar cualquier función continua por funciones diferenciables y listos, tete.

\section{Cálculo de cohomologías: Mayer-Vietoris}

Hasta ahora hemos visto algunos resultados para calcular cohomologías. Sabemos que si $M$ es conexa, entonces $H^0(M) = 0$, que $H^k(\bola^n) = 0$ cuando $k > 0$ por el \nref{lem:Poincare}, y que además si $M_1 \simeq M_2$ (homotópicamente equivalentes) entonces $H^*(M_1) = H^*(M_2)$.

Sin embargo, para otros cálculos de cohomologías no tenemos herramientas. Mayer-Vietoris será lo que usaremos para sacar cohomologías, trabajando con abiertos que cubren la variedad y estudiando su intersección. Para definirlo, primero necesitaremos una cierta base previa.

\subsection{Previo: Base algebraica}

\begin{defn}[Sucesión\IS exacta de espacios vectoriales] Una sucesión exacta de espacios vectoriales es una cadena \[ \dotsb \to V_{i-1} \xrightarrow{h_{i-1}} V_{i} \xrightarrow{h_i} V_{i+1} \to \dotsb \] tales que $\ker h_i = \img h_{i-1}$.
\end{defn}

Por ejemplo, si tenemos una sucesión exacta \[ 0 \xrightarrow{h_0} V_1 \xrightarrow{h_1} V_2 \xrightarrow{
h_2} V_3 \xrightarrow{h_3} 0 \] podemos ver varias cosas, como que  $h_1$ es inyectiva ($\ker h_1 \img h_0 = 0$) y que $h_3$ es suprayectiva ($\ker h_3 = \img h_2 = V_3$). Entrando en temas menos obvios, podemos aplicar el primer teorema de isomorfía para $h_2$, de tal forma que \[ \quot{V_2}{V_1} \cong V_3 \]

En ejemplos más concretos, podemos definir una cadena exacta \[ 0 \to ℝ^2 \overset{h_1}{\hookrightarrow} ℝ^5 \xrightarrow{h_2} ℝ^3 \mapsto 0 \], con \begin{align*}
h_1(x_1, x_2) &= (x_1, x_2, 0,0,0) \\
h_2(y_1, y_2, y_3, y_4, y_5) &= (y_3, y_4, y_5)
\end{align*} donde en la última aplicación tenemos que mandar los elementos de $\img h_1$ a $0$ si queremos que sea exacta.

Volviendo a las formas diferenciales, podemos volver a la cadena \[0 \to Ω^0(\bola^n) \xrightarrow{\dif^0} Ω^1(\bola^n) \xrightarrow{\dif^2} \dotsb \xrightarrow{\dif^{k-1}} Ω^{k}(\bola^n) \xrightarrow{\dif^k} Ω^{k+1}(\bola^n)\], que será una sucesión exacta si y sólo si $\ker \dif^{i+1} = \img \dif^i$, equivalente a decir que $\quot{\ker \dif^{i+1}}{\img \dif^i} = H^{i+1}(\bola^n) = 0$. Es un resultado interesante porque nos permite vincular el resultado algebraico (que la cadena sea exacta o no) a un resultado sobre cohomologías.

\begin{prop} Si la cadena \[ 0 \to V_0 \xrightarrow{h_0} V_1 \to \dotsb \to V_{n-1} \xrightarrow{h_{n-1}} V_{n} \to 0 \] es exacta, entonces la suma alternada de dimensiones de los espacios vectoriales es cero:  \[ \sum_{i=0}^n (-1)^i \dim V_i = 0\]
\end{prop}

\begin{proof} Podemos considerar las dos siguientes cadenas: \begin{align*}
0 \to \img h_1 & \xhookrightarrow{i} V_1 \to \dotsb \to V_{n-1} \xrightarrow{h_{n-1}} V_{n} \to 0 \\
0 \to V_0 &\xrightarrow{h_0} V_1 \xrightarrow{h_1} \img h_1 \to 0
\end{align*}, que son sucesiones exactas, y entonces podemos razonar por inducción sumando las formulitas de la suma de dimensión, aunque personalmente no tengo del todo claro por qué está usando lo que quiere probar para demostrarlo exactamente lo mismo.
\end{proof}

\begin{theorem}[Sucesión\IS exacta de Mayer-Vietoris] Sean $U,V$ abiertos de $M$ tales que $M = U ∪ V$. Entonces la siguiente sucesión es exacta: \( \begin{matrix}
0 \to & Ω^*(M) & \xrightarrow{r} &Ω^*(U) \oplus Ω^*(V) & \xrightarrow{δ} & Ω^*(U∩V) & \to 0 \\
& ω &\xrightarrow{r} & (\restr{ω}{U}, \restr{ω}{V}) & & & \\
&   & 				 & (η_1, η_2) & \xrightarrow{δ} & \restr{η_2}{U∩V} - \restr{η_1}{U∩V} &
\end{matrix} \label{eq:SucesionMayerVietoris}\)
\end{theorem}

\begin{proof} $r$ es inyectiva porque $U ∪ V = M$, y además $\ker δ ⊇ \img r$ obviamente: \[ δ○r (ω) = δ(\restr{ω}{U}, \restr{ω}{V}) = \restr{ω}{U∩V} - \restr{ω}{U∩V} = 0 \]

Por otra parte, si $(η_1, η_2) ∈ \ker δ$, entonces $\restr{η_2}{U∩V} = \restr{η_1}{U∩V}$. Entonces, definimos \[ η = \begin{cases} η_1 & \text{en } U \\ η_2 & \text{en } V \end{cases} \], y entonces está claro que está bien definida (en la intersección ambas coinciden, y $U ∪ V = M$) y $r(η) = (η_1, η_2)$.

Nos falta ver que $δ$ es suprayectiva. Sea $σ ∈ Ω(U∩V)$. Necesitamos encontrar $η_1 ∈ Ω(U)$, $η_2 ∈ Ω(V)$ tal que $σ = \restr{η_2}{U∩V} - \restr{η_1}{U∩V} $. Así, definimos una partición de la unidad de $M$ subordinada al recubrimiento de $M$ formado por $U$ y $V$, esto es, dos aplicaciones $\appl{ρ_{U,V}}{M}{ℝ^+}$, con $\sop ρ_U ⊂ U$, $\sop ρ_V ⊂ V$ y además $ρ_U + ρ_V = 1$.

Con esto en la mano, lo que tenemos que definir (o eso le parece a Gabino al menos) es \begin{align*}
η_1 &= -ρ_V σ ∈ Ω(U)\\
η_2 &= ρ_U σ ∈ Ω(V)
\end{align*} salvo signo, y con las formas valiendo $0$ fuera de $U ∩ V$.

Si suponemos que esta definición es buena, entonces haciendo las restricciones correspondientes a $U∩V$ $η_2 - η_1 = ρ_Uσ + ρ_Vσ = σ$, booooooom shakalaka.
\end{proof}

Un ejemplo de esa partición para $\crc[n]$, con dos funciones concretas. que cumplen efectivamente lo que se pedía, con $\tilde{ρ}_U = ρ ○ φ_U$ en $\crc[n] \setminus{S}$ y $0$ en otro caso donde $φ_U$ es la carta y $ρ$ algo. Entonces se pueden definir \[ ρ_U ≝ \frac{\tilde{ρ}_U}{\tilde{ρ}_U + \tilde{ρ}_V} \qquad ρ_V ≝ \frac{\tilde{ρ}_V}{\tilde{ρ}_U + \tilde{ρ}_V} \], de tal forma que el soporte es el que necesitamos y ya está.

\begin{theorem}[Teorema\IS de Mayer-Vietoris de cohomología] La siguiente sucesión es exacta
\[ \dotsb \xrightarrow{\dif^*} H^{k-1} (U∩V) \xrightarrow{\dif^*} H^k(M) \xrightarrow{r} H^k(U) \oplus H^k(V) \xrightarrow{δ} H^k(U∩V) \xrightarrow{\dif^*} H^{k+1} (M)  \xrightarrow{\dif^*} \dotsb \]

$r$, y $δ$ definidas como en \eqref{eq:SucesionMayerVietoris}, y $\dif^*$ es más difícil de definir (Gabino dixit).
\end{theorem}

Una aplicación de esto es de nuevo $\crc[1]$, con las cartas $U,V$ de toda la vida. La idea es que $H^0(U) \oplus H^0(V)$ tiene dimensión 2, $H^0(U∩V)$ tiene dimensión dos por ser dos componentes conexas, $\dim H^1(U) = \dim H^1(V) = 0$ por ser $U,V$ homeomorfos a $ℝ$ y entonces hay una sucesión metida entre dos ceros. Como la suma alternada de dimensiones es cero, luego \[ 1 - 2 + 2 - \dim H^1(M) + 0 = 0\], luego $\dim H^1(M) = 1$.

Y bueno, al final, salen cosas.

No sé qué estamos haciendo. Cogemos una variedad $M = U ∪ V$, y entonces la cadena que tenemos es \begin{align*}
0 &\to H^0(M) \to H^0(U) \oplus H^0(V) \to H^0(U∩V) \to \\
  &\to H^1(M) \to H^1(U) \oplus H^1(V) \to H^1(U∩V) \to \\
\dotsb  &\to H^k(M) \to H^k(U) \oplus H^k(V) \to H^k(U∩V) \to \\
\dotsb  &\to H^m(M) \to H^m(U) \oplus H^m(V) \to H^m(U∩V) \to  0\\
\end{align*}

Esto es una cadena exacta así que la suma alternada de dimensiones es 0. Usábamos como notación $h^k(M) ≝ \dim H^k(M)$.

Con esto calculábamos la cohomología de la esfera. Cogíamos $\crc[n]$ como $U ∪ V$, donde $U$ y $V$ eran la esfera menos el polo norte y el polo sur respectivamente. Veíamos que, en el caso particular, $U ∩ V = \crc[2] \setminus \set{N, S} = ℝ^2 \setminus \set{S} \simeq \crc[1]$ por homotopía. La cuestión es que como $H^1(U) = 0$, podemos acortar la cadena y tenemos lo siguiente: \[
\begin{matrix}
 0 & \to H^0(\crc[2]) & \to H^0(U) \oplus H^0(V) & \to H^0(U∩V) & \to H^1(\crc[2]) & \to H^1(U) \oplus H^1(V) = 0 \\
 0 & \to 1 & \to 1 \oplus 1 & \to 1 & \to ? & \to 0 \\
\end{matrix}\]

De esta, haciendo la sua alternada de dimensiones nos queda que $h^1(\crc[2]) = 0$.

Por otra parte, tenemos la otra cadena:
\[
\begin{matrix}
0 = H^1(U) \oplus H^1(V) & \to H^1(U ∩V) & \to H^2(\crc[2]) & \to H^2(U) \oplus H^2(V) = 0 \\
0 & \to  ? & \to ? & \to  \\
\end{matrix}\], de la que podemos sacar que $h^2(\crc[2]) = h^1(U ∩ V)$. Ahora bien, $U ∩V \simeq \crc[1]$, y ya habíamos visto\footnote{Haciendo de nuevo la cadena exacta.} que $h^1(\crc[1]) = 1$, luego $h^2(\crc[2])  = 1$.

En general, para esferas tendremos lo siguiente: \[
h^k(\crc[n]) = \begin{cases}
1 & k = 0 \\
0 & 0 < k < n \\
1 & k = n\end{cases}\]

\subsection{Característica de Euler-Poincaré}

\begin{defn}[Característica\IS de Euler-Poincaré] Dada una variedad diferenciable $M$ de dimensión $n$, se define su característica de Euler-Poincaré como \[ χ(M) = \sum_{i = 0}^n (-1)^i h^i(M)\]
\end{defn}

La ventaja de esta definición es que no requiere que la variedad sea compacta. Algunos ejemplos:

\begin{itemize}
\item $χ(\bola^n) = 1$.
\item $χ(\crc[1]) = 0$.
\item $χ(\crc[2]) = h^0(\crc[2]) - h^1(\crc[2]) + h^2(\crc[2]) = 2$.
\item $χ(\crc[n]) = h^0(\crc[2]) \pm h^n(\crc[2])$ con el signo según según valga $n$, luego es $2$ para $n$ par y $0$ para $n$ impar.
\end{itemize}

De momento, parece que queda lo mismo que la \fref{def:CaracteristicaEuler} que dábamos antes a través de triangulaciones.

\begin{prop} Sea $M$ una variedad diferenciable, y $U,V$ dos abiertos que la cubren. Entonces \[ χ(M) = χ(U) + χ(V) - χ(U∩V)\]
\end{prop}

\begin{proof}
Simplemente yendo a la cadena de Mayer-Vietoris y la suma alternada de dimensiones sale.
\end{proof}

Con esto ¿podemos? calcular la característica del toro. Y sale lo que tiene que salir. Y para el toro doble también. Esta es una cosa que a Gabino no le sale muy bien pero que nosotros lo tenemos que saber.

Ej\footnote{No sabemos si esto es un ejemplo o un ejercicio. ¿Cómo no se le habría courrido antes a Gabino? Esto cambia toda mi vida. Madre mía. Es un genio.}. Bueno, que la característica del toro doble es -2. Nos falta por entender qué es la caracte´ristica de una variedad menos un punto. Por ej, tenemos nuestra variedad $M$ de dimensión $n$. y un punto $p$. Entonces dividimos en dos abiertos $U = M \setminus \set{p}$ y otra $V= \bola^n(p)$ una bola alrededor del punto. Entonces $U ∩ V = \bola^n \setminus \set{p} \simeq \crc[n-1]$. Entonces la proposición de antes nos dice que \begin{multline*} χ(M) = χ(M \setminus \set{p}) + χ(\bola^n) - χ(\crc[n-1]) = \\ = χ(M \setminus \set{p}) + 1 - (1 + (-1)^{n}) = χ(M \setminus \set{p}) + (-1)^n \end{multline*}, y ya llega un momento en el que en nuestro caso pues cogemos el toro y como es de dimensión $2$, y dos, por si no lo sabáiisi o no os acordábais, es par, así que entonces la caraterícias del toro menos un punto es la caracte´ristiaasnda ashdashdqi del toro menos uno, que es menos uno, y así sale la característica del toro doble que era lo que buscábamos que salía dos, y ahora mismo luego nos propne un ejercicio y es ver la característica de la suma conexa de dos variedades, que tiene que ver con esto y es esta la suma conexa, es $M$ menos un punto o una bola que da lo mismo, y leuego queda algo como un cilindro que es como $\crc[n]$ que nos va a dar y esto va a ser, la característica del uno y la característica del otro y habrá que quitar algo según la dimensión, que algo hay que hacer dependiendo de la paridad y va a variar... ¿de acuerdo?. Y ya llegados a este punto tengo que decir que simplemente estoy copiando literalmente lo que se está diciendo en clase, y este magnífico caos geométrico y algebraico me da muchísima pereza arreglarlo, creo que cuando lea esto me voy a a arrependir. A todo esto, llevo 1888 columnas en esta línea. Creo que es un récord personal. Booom. Y ahora tengo funcional.

\begin{theorem} $χ_E(M) = χ_{E-P}(M)$.
\end{theorem}

\begin{figure}[hbtp]
\centering
\inputtikz{MayerVietorisCover}
\caption{Esquema para la demostración de la igualdad de las características de Poincaré y Euler-Poincaré, desarrollando un recubrimiento por abiertos para aplicar Mayer-Vietoris a partir de la triangulación.}
\label{fig:MayerVietorisCover}
\end{figure}

\begin{proof}
A dibujito. Aplicamos Mayer-Vietoris: en el centro de cada cara. Ahora definimos los bichos donde $U$ es $M$ menos los centros de las caras y $V$ es la unión especial de las bolas centradas en esos discos. Así, $U ∩V$ es una unión rectangular de bolas sin el centro, que son como $\crc[1] \setminus \set{x}$.

Ahora aplicamos Mayer-Vietoris, y tenemos que $χ(M) = χ(U) + C$. Aplicamos $M-V$ a $U$, y entonces definimos $U_1$ como $M$ sin los segmentos que unen centros de caras adyacentes, y luego la unión de los $B_l$ los cuadriláteros con ``eje'' el segmento correspondinete. Entonces aplicamos de nuevo lo de la caracterísrtica para mayer-vietories y entonces $χ(F_x$) son es elnúmero de vértices, lo otro del de lados y bueno que al final sale. $)$
\end{proof}

\begin{prop} $χ(M_1^n \hash M_2^n)$ es $χ(M_1) + χ(M_2) - 2$ si $n$ es par o $χ(M_1)+ χ(M_2)$ si $n$ es impar.
\end{prop}

\begin{proof} Ver definición de variedad conexa \fref{def:SumaConexa} y lo que veíamos antes de la variedad menos un punto.
\end{proof}

Otro Ej: cohomología de $M_n = ℝ^2 \setminus \set{p_1, \dotsc, p_n}$ con $p_i = (a_i, b_i)$. Al final sale 1, $n$, 0, 0 en orden $k = 0$, etc.

Corolario: Una forma $ω$ en $M_n$ cerrada admite una fncion potencial $ω = \dif φ$. Demostración una serie de garabatos horribles.

Otro colorario es que $h^1(ℝ^2 \setminus ℕ) = ∞$.

Por ejemplo, cuando vemos que $M_1 = ℝ^3 \setminus\set{p_1}$, $h_k(M_1) = 1,0,1,0$ para $k = 0,1,2,>2$, lo que nos dice es que no tenemos agujeros de dimensión 1 pero sí de dimensión 2. El significado de esta frase se deja en el aire.

Si a $ℝ^3$ le quitamos la recta $z = 0$ lo que tenemos es $ℝ^2 \setminus \set{0} × ℝ$, que es homótopo a $\crc[1]$. Porque al lado de la cohomología está la homología, que es como integrar las formas diferenciales en subvariedades que representan los lazos. O algo así. Y en $X_1 = ℝ^3 \setminus \set{z = 0}$ lo que no se va a poder contraer es el lazo alrededor del eje. En $M_1$ lo que no se puede contraer es una esfera. Este es en el sentido impreciso en el que decimos que $h_k(X_1) = 1,1,0$. En el caso de $M_n = ℝ^3 \setminus \set{p_1, \dotsc, p_n}$ hacemos lo anállogo de la otra vez. Entonces para calcular la homología usamos la unión de los abiertos. Por inducción. O mejor antes de inducción vamos a verlo para 2.

Entonces a Gabino se le ocurren dos posibilidades para cubrir $M_2$ con dos abiertos: un $U$ sería una parte de $ℝ^3$ menos $p_1$ y $V$ la otra parte más un poquito más menos $p_2$. Yo me entiendo. El dibujo se hace luego. $U ∩V$ sería fácil, simplemente un cacho de $ℝ^3$. $U \simeq \crc[2] \simeq V$, $U ∩ V = ℝ × [a,b] × ℝ$.

Entonces cogemos la cadena asquerosa de Mayer-Vietoris y bueno $H^1$ son lazos que no se pueden contraer y $H^2$ esferas que no se puedan contraer. La conclusión despues de un rato de no sé qué clase de cuentas es que la batería me ha bajado una hora y que $h^1(M_2) = 0$, $h^2(M_2) = 2$ y $h^3(M_2) = 0$.

Si en $ℝ^3$ quitamos dos rectas... pues hacemos lo mismo con $U$ y $V$, de tal forma que $U = ℝ^3 \setminus \text{una rectα}$ y σαβεμθσ λθ ∅υε √αλε. Jeje. Παρα χυβριρλθ νθσ φαλταν δθσ ρεχτασ. Mετερεμθσ υν χιλινδρθ δε ταλ φθρμα ∅υε χαδα $C_1$ σερα υν δισχθ πθρ $ℝ$, menos la recta que pasa por el centro. ∃δυ  διχε ∅υε σθν δισχθσ μαχιζθσ.

Bueno, entonces $U∩ V$ es homótop a $\crc[1] ∪cuadrado \crc[1]$ y ahora entonces hacemos de nuevo la cadena asquerosa. $V = C_1 ∪cuadrado C_2$. Y bueno, la homoogía se escribe con subíndice y no superíndice. Y al final llega desde $0$ hasta $h^3(U∩V)$ que ya sabe que $0$. Pero buen, $H^k(ℝ^3) = 0$ así que tiene cadenas cerradas antes. Al final $h^1(X_2) = 2$, $h^2(X_2) = 0$. Vaya es justo al revés que en el caso anterior. Aquí tenemos agujeros de dimensión $1$ pero ninguno de dimensión $2$.

Ejercicio: lo mismo para para $X_n$.

Pensamos ahora una cosa híbrida: le quitamos un punto y una recta. Lo que Gabino quiere que aceptemos es que no es mala idea que pensemos que el grupo $H^k$ mide cuántas esferas de dimensión $k$ no se pueden contraer.

En ese caso, nos debería salir que $h^1 = 1$ y $h^2 = 1$ correspondientes a los lazos alredeor de la recta que no se pueden contraer y las esferas alrededor del punto.

Tomamos entonces $M = U$ y $V = \disc × ℝ ∪ \bola$. Necesitamos saber quien és la intersección, que será $\disc × ℝ \setminus L ∪ \bola \setminus \set{p} \sim \crc[1] ∪cuadrado \crc[2]$.

Por cierto, el otro día vimos un resultado muy bonito que es que la característica de Euler no depende de la triangulación e hicimos la demostración para dimensión 2 y Gabino ha subido sus notas estas que escribe él para él las ha escaneado y subido a Moodle y ha añadido esas notas cochambrosas que escribe él y ha fotocopiado los dibujos que dice como sería en dimensión 3, y así nos afianzamos de que funciona. Gabino dixit.

De nuevo, hacemos la cadena y al final que $h^3(M) = 0$, $h^1(M) = 1$, $h^2(M) = 2$, que eslo que esperábamos? bueno, que para ejercicio con $ℝ^4 \setminus\set{p_1, \dotsc, p_n}$.

\seprule[Clase mágica]

Una variedad  $M^n$ es orientable si y sólo si admite una forma de volumen $w ∈ Ω^n(M)$ tal que $ω(p) ≠ 0$ para $∀ p ∈ M$.

\begin{theorem}[Teorema\IS de Stokes] $\int_M \dif η = 0$ si $M$ es orientable y compacta y $η ∈ Ω^{n-1} (M)$.
\end{theorem}

\begin{corol} $M$ orientable y copacta entonces $H^n(M) ≠ 0$.
\end{corol}
\begin{proof}
Si $ω$ es la forma de volumen $\dif ω =0$ pero no es exacta porque si lo fuera $\int_M ω = 0$ y es distinto porque no es cero.
\end{proof}

La base de $H^n(M) = ℝ · [ω]$. Que sí, me lo creo.

\begin{lemma}[Dualidad\IS de Poincaré]
 Dada $M^n$ variedad compacta y orientable entonces podemos dar una aplicación bilineal

 \begin{align*}
	Ω^p(M) × Ω^q(M) &\longmapsto ℝ \\
	(v,ω) & \longmapsto \int_M v ∧ ω
\end{align*}
con $p + q = n$.
Es bilineal y no degenerada, esto es, que si lo tomamos como producto escalar es una aplicación bilineal (a mí me convence, ya lo arreglo luego).
\end{lemma}

De hecho hay un isomorfismo canónico entre $H^p(M) \simeq (H^q(M))^*$.

Y en particular $h^0(M) = h^n(M) = 1$ y $χ(M) = 0$ para las variedades de dimensión impar.


Para la demostración de este lema, necesitamos definir las cohomologías de soporte compacto  y algunas cosas más.

\subsubsection{Cohomología con soporte comapcto}

\begin{theorem}[Teorema\IS de Poincaré para $H^\ast_c$]
\[
	h_c^p(ℝ^n) = \left\{\begin{array}{ccc} 0 & si & p≠n \\ 1 & si & p = n\end{array}\right.
\]
\end{theorem}
\begin{proof}
No la vamos a hacer entera, pero vamos a calcular $H^1(ℝ^n) \simeq ℝ$. Vamos a verlo construyendo la sucesión de siempre:


\[
	0 \to Ω_c^0(ℝ) \to Ω_c^1(ℝ) \overset{d}{\to} Ω_c^2(ℝ) = 0
\]

Sabemos que $H^1 = \quot{Ω^1_c(ℝ)}{dΩ_c^0(ℝ)} \simeq ℝ$.

Además, podemos construir una aplicación:$\appl{φ}{Ω^1_c(ℝ)}{ℝ}$ tal que $ω=f(x)dx \to \int_{-∞}^{∞}f(x)dx = \int_a^bf(x)dx$. Los límites de integración cambian porque estamos en un soporte compacto.

Esta aplicación $φ$ es suprayectiva, con lo que: $\quot{Ω^1_c(ℝ)}{\ker  φ} \simeq ℝ$.

El último pasa para dar en la demostración es comprobar que $\ker φ = Ω^0_c(ℝ)$.

Vamos a ver cada una de las inclusiones:

$⊃:$
\[ω = dg = g'(x)dx \implies \int_{-∞}^{∞} ω = \int_{-∞}^{∞}g'(x)dx = \underbrace{g(∞)}_{g(b)} - \underbrace{g(-∞)}_{g(a)} = 0\]


$⊂:$
Supongamos que $ω=f(x)dx \in \ker φ$. Además, $f$ tiene que tener soporte compacto por hipótesis, y queremos ver que ...

Tomando $ω = \int_a^x f(s)ds = g(x)$, tenemos que $ω = dg$. Con esto vemos que toda forma diferencial cerrada es exacta, con lo que $H^1(ℝ) = 0$, cosa que ya sabíamos.
Pero queremos demostrar que $H^1_c(ℝ) = 0$, es decir, que $g$ tiene soporte compacto.

Si $x> b$,
\[g(x) = \int_a^x f(s)ds = \underbrace{\int_a^b f(s)ds}_{\int_a^b ω} + \underbrace{\int_b^x f(s)ds}_{sop(f) \subset [a,b]} = 0 + 0 = 0\]

El razonamiento es análogo para $x<a$.

Sólo falta el caso $a<x<b$, cuya demostración es igual que la demostración de la Cohomología general .

\end{proof}


\paragraph{M-V (Maier - Viértoris) para $H^\ast_c$ en $M=U\cup V$}

Al tomar compactos, debido a que $h^p_c(ℝ)$ se comporta al revés que $h^p(ℝ)$, no debería sorprendernos que la cadena de MV sea en el otro sentido.

\[
	0 \leftarrow Ω_c(M) \overset{δ}{\leftarrow} Ω_c(U) \oplus Ω_c(V) \overset{Ext.}{\leftarrow} Ω_c(U\cap V) \leftarrow 0
\]

En este caso, es más fácil ver que esta sucesión es algo (¿exacta?) porque $δ$ es suprayectiva y $Ext.$ es algo.
Esto da lugar a una sucesión exacta de cohomologías:

\[
	H_c^{p+1} \leftarrow H_c^p(M) \leftarrow H_c^p(U) \oplus H_c^p(V) \leftarrow H_c^p(U\cap V) \leftarrow H_c^{p-1}
\]


El siguiente paso es... Emm... Bueno, vale. Y... que es una herramienta para indicar la demostración de la dualidad de Poincaré. ¡Eso es! La dualidad de Poincaré. ¿Qué es la dualidad de Poincaré?

\begin{lemma}[Dualidad\IS de Poincaré (compactos)]


 Dada $M^n$ variedad no necesariamente compacta y orientable entonces podemos dar una aplicación bilineal

 \begin{align*}
	H^p(M) × H_c^q(M) &\longmapsto ℝ \\
	([v],[ω]) & \longmapsto \int_M v ∧ ω
\end{align*}
con $p + q = n$.
Es bilineal y no degenerada, esto es, que si lo tomamos como producto escalar es una aplicación bilineal.
\end{lemma}

Vamos a definir lo que es un buen recubrimiento, que es el último ingrediente que nos hace falta para poder demostrar la dualidad de Poincaré.

\begin{defn}[Buen\IS recubrimiento]
Se dice que $U_1,U_2,...,U_n$ es un buen recubrimiento para $M$ si

\begin{itemize}
 	\item $ M =\bigcup U_i$
 	\item $\bigcap U_i^k \overset{dif.}{\simeq} B \vee \emptyset ∀k$
 \end{itemize}
\end{defn}

Otro ingrediente necesario para la demostración es el siguiente resultado:

\begin{lemma}[Lema\IS de los cinco]
\label{lemma:delos5}
2 sucesiones de grupos exactas con homomorfismos entre ellos tal que forman un diagrama conmutativo. Supongamos además que todos menos el central son isomorfismos.
\end{lemma}

\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.7]{img/lemadelos5.png}
\caption{Diagrama del lema de los 5}
\label{img:delos5}
\end{figure}

\begin{proof}
\href{https://es.wikipedia.org/wiki/Lema_de_los_cinco}{Para consultar la demostración.}
\end{proof}

\begin{proof}[Dualidad\IS de Poincaré]

La demostración se hace por inducción sobre el número de abiertos de un "buen recubrimiento".

\proofpart{$n = 1$}

Tenemos $M \simeq ℝ^n$ y aquí ya sabemos que:

\[
	\underbrace{H^p(ℝ^n)}_{0, \text{ si }p≠0} \simeq \underbrace{\left(H_c^q(ℝ^n)\right)^{\ast}}_{0,\text{ si } q≠n}
\]

Además, sabemos que $p+1 = n$.

Como se ve clarísimamente, en el primer paso, no hay ningún problema.


\proofpart{$n\to n+1$}

Construimos

\[
	M = \underbrace{U_1 \cup ... \cup U_n}_{U} \cup \underbrace{U_{n+1}}_{V}
\]

De esta manera, $U$ es un buen recubrimiento con $n$ abiertos (para el que se cumple lo que estamos demostrando).

Por otro lado, $V$ es un buen recubrimiento con $1$ abierto (para el que también se cumple lo que estamos demostrando).

Además, $(U\cap V = U_1\cap U_{n+1})\cup ... \cup (U_n\cap U_{n+1})$, por construcción y definición de los buenos recubrimientos, este es un buen recubrimiento.

Ahora ya estamos en condiciones de aplicar MV-ordinario de $p$ y MV-compacto del complementario ($q$).

Al tomar el dual en la sucesión de los compactos, las flechas cambian de dirección. Por la hipótesis de inducción, tenemos que existen isomorfismos entre la cadena de arriba y la de abajo, salvo en $H^p(M) \to H^q_c(M)$

Vamos a construir las siguientes sucesiones:

\begin{equation*}
\label{absurdo:1}
H^{n-1}(U) \oplus H^{n-1}(V) \to H^{n-1}(U\cap V) \to H^n(M) \to H^{n}(U)\oplus H^n(V) \to H^n(U\cap V)
\end{equation*}

\begin{equation}
\label{absurdo:2}
H_c^{q+1}(U) \oplus H_c^{q+1}(V) \to H_c^{q+1}(U\cap V) \to H_c^q(M) \to H_c^{q}(U)\oplus H_c^q(V) \to H_c^q(U\cap V)
\end{equation}

Tomando duales en \ref{absurdo:2}, cambia el sentido de la sucesión:

La aplicación de los duales no se si se hace tan así:

\begin{equation}
\label{absurdo:2}
\left(H_c^{q+1}(U) \oplus H_c^{q+1}(V) \right)^{\ast}\to\left(H_c^{q+1}(U\cap V) \right)^{\ast}\to\left(H_c^q(M) \right)^{\ast}\to\left(H_c^{q}(U)\oplus H_c^q(V) \right)^{\ast}\to\left(H_c^q(U\cap V)\right)
\end{equation}



Y construimos el siguiente diagrama:

\begin{equation}
\label{absurdo:4}
\begin{array}{ccccc}
H^{n-1}(U) \oplus H^{n-1}(V) 	&	\to H^{n-1}(U\cap V) 	&	\to H^n(M) 	&	\to H^{n}(U)\oplus H^n(V) 	&	\to H^n(U\cap V)\\
\textcolor{green}{\downarrow} & \textcolor{green}{\downarrow} & \textcolor{red}{\downarrow} & \textcolor{green}{\downarrow} & \textcolor{green}{\downarrow} \\
\left(H_c^{q+1}(U)\right)^{\ast} \oplus \left(H_c^{q+1}(V) \right)^{\ast}	&	\to\left(H_c^{q+1}(U\cap V) \right)^{\ast}	&	\to\left(H_c^q(M) \right)^{\ast}	&	\to\left(H_c^{q}(U)\right)^{\ast}\oplus \left(H_c^q(V) \right)^{\ast}	&	\to\left(H_c^q(U\cap V)\right)
\end{array}
\end{equation}

Las flechas verdes corresponden a isomorfismos por hipótesis de inducción. $V$ tiene 1 abierto y $U$ tiene $n$ abiertos, con lo que los correspondientes son isomorfos. Hemos construido este diagrama para poder aplicar el lema de los 5 y concluir que la aplicación representada por la flecha roja es un isomorfismo.

Por el lema de los 5,

\[
\appl{i}{H^n(M)}{\left(H_c^q(M)\right)^{\ast}} \text{ es un isomorfismo.}
\]

Y por algo que no se muy bien porqué, hemos terminado la demostración del lema de Poincaré.

\end{proof}

A cuento de no se qué, vamos a ver este teorema (igual es que no hemos terminado la demostración del lema). Creo que es una conclusión del lema recién demostrado.

\begin{theorem}
Una variedad compacta (y más generalmente una variedad que admite un buen recubrimiento finito) tiene grupos de cohomología de dimensión finita.
\end{theorem}

\begin{proof}
Por inducción en el número de abiertos, utilizando un argumento parecido al anterior.

\begin{equation}
\label{juliantolai:1}
H^{p-1}(U) \oplus H^{p-1}(V) \to \underbrace{H^{p-1}(U\cap V)}_{\dim{} < ∞} \to H^p(M) \to \underbrace{\underbrace{H^{p}(U)}_{\dim < ∞} \oplus \underbrace{H^p(V)}_{\dim{} < ∞}}_{\dim < ∞} \to H^p(U\cap V)
\end{equation}

\end{proof}

\obs Sin poder precisar bien, tenemos el siguiente comentario. La idea de utilizar el número de abiertos que necesita una variedad para un buen recubrimiento nos puede dar una idea de la complejidad de su topología. \footnote{Matemagia}.


Vamos a ver un teorema que resuelve la cohomología de las variedades producto:
\begin{theorem}[Fórmula\IS de Künneth]
Sean $π_1,π_2$ las proyecciones obvias del producto de variedades: $M\times N$.

De esta manera, tenemos $\appl{\pi_1^\ast}{H^p(M)}{H^p(M×N)}$ y $\appl{\pi_2^\ast}{H^p(N)}{H^p(M×N)}$ $∀p,q$.

Sean $\{ω_i^p\}$ una base de $H^p(M)$ y $\set{\eta_j^k}$ una base de $H^k(N)$. \textbf{Entonces}:

\[
	\{π_1^\ast ω_i^p ∧ π_2^\ast \eta_j^k\}_{\begin{array}{c}i,j\\p+k=n\end{array}} \text{ es una base de } H^n(M×N)
\]

En particular:

\[h^n(M×N) = \sum_{p+k=n} h^p(M)h^k(N)\]

\end{theorem}

\obs Normalmente este teorema utiliza el producto tensorial, que como no todos hemos visto y no parece que queramos perder tiempo en él, lo hemos expresado de otra manera.

Vamos a ver unos ejemplos antes de la demostración para entenderlo:


\begin{example}
Tomamos $\crc[1]×\crc[1]$, es decir, un toro $T_2$.

...
\end{example}

Vamos a ver otro ejemplo más interesante:


\begin{example}

\begin{equation}
	\begin{array}{l}
		h^0(\crc[1]×\crc[2]) = h^0(\crc[1])·h^0(\crc[2]) = 1·1 = 1\\
		h^1(\crc[1]×\crc[2]) = h^0(\crc[1])h^1(\crc[2]) + h^0(\crc[1])h^1(\crc[2]) = 0 + 1 = 1\\
		h^2(\crc[1]×\crc[2]) = h^0(\crc[1])h^2(\crc[2]) + h^1(\crc[1])h^1(\crc[2]) + h^2(\crc[1])h^0(\crc[2]) =  1+0+0 = 1\\
		h^3(\crc[1]×\crc[2]) = h^0(\crc[1])\underbrace{h^3(\crc[2])}_{0} + h^1(\crc[1])h^2(\crc[2]) + \underbrace{h^2(\crc[1])}_{0}h^1(\crc[2]) + \underbrace{h^3(\crc[1])}_{0}h^0(\crc[2]) = 1
	\end{array}
\end{equation}

Confirmamos la dualidad de Poincaré:

\[
h^0(\crc[1]×\crc[2]) = h^3(\crc[1]×\crc[2])
\]
\[
h^1(\crc[1]×\crc[2]) = h^2(\crc[1]×\crc[2])
\]
\end{example}

Una prueba de que $\crc[4] \not\simeq \crc[2] × \crc[2]$: el grupo de cohomología de grado 2 tiene dimensión 0 en $\crc[4]$ y dimensión 2 en $\crc[2] × \crc[2]$.

Ahora vamos a ver el enunciado real del teorema/fórmula de Künneth en términos del producto tensorial.

\begin{theorem}[Teorema\IS de Künneth] \label{thm:Kunneth} Sean $M$, $N$ dos variedades diferenciables y $\appl{π_1, π_2}{M×N}{M,N}$ las respectivas proyecciones naturales. Entonces hay un isomorfismo \[ \bigoplus_{p+ q = d} H^p(M) \otimes H^q(N) \cong H^d (M × N)\], donde una forma diferencial $(ω^p \otimes η^q)_{p+q = d} \mapsto \sum_{p+q = d} π_1^* ω^p ∧ π_2^* η^q$
\end{theorem}

Recordemos el producto tensorial.

\begin{wrapfigure}[5]{R}[0.1\textwidth]{0.3\textwidth}
\centering
\vspace{-15pt}
\begin{tikzpicture}[yscale = 1.5, xscale = 2]
\node (VxW) at (0,0) {$V×W$};
\node (VoW) at (0, -1) {$V \otimes W$};
\node (E) at (1, 0) {$E$};

\draw[->] (VxW) -- node[midway, above] {$φ$} (E);
\draw[->] (VoW) -- node[midway, right] {$Φ$} (E);
\draw[->] (VxW) -- node[midway, left] {$\otimes$} (VoW);
\end{tikzpicture}
\caption{Diagrama conmutativo para el producto tensorial.}
\label{fig:ProdTensorial}
\end{wrapfigure}

\begin{defn}[Producto\IS tensorial] Sean $V,W$ espacios vectoriales de dimensiones respectivas $n = \dim V$ y $m = \dim W$. Entonces existe un espacio vectorial $T = V \otimes W$ de dimensión $n m$ tal que:

\begin{enumerate}
\item Existe una aplicación bilineal \begin{align*}
	\appl{\otimes}{V×W&}{T} \\
	(v,w) &\longmapsto v \otimes w
\end{align*}
\item Para toda aplicación bilineal $\appl{φ}{V×W}{E}$ con $E$ otro espacio vectorial arbitrario existe una aplicación lineal $\appl{Φ}{T}{E}$ tal que el diagrama \ref{fig:ProdTensorial} es conmutativo (\concept{Propiedad\IS universal del producto tensorial}).
\end{enumerate}

La construcción de los elementos del producto tensorial se hace de la siguiente forma. Dadas $\set{v_i}_{i=1}^n$ y $\set{w_j}_{j=1}^m$ bases respectivas de $V$ y $W$, entonces el producto tensorial lleva vectores de la siguiente forma: \begin{align*}
\appl{\otimes}{V×W&}{T} \\
\left(\sum a_i v_i, \sum b_j w_j\right) &\longmapsto \sum a_i b_j (v_i \otimes w_j)
\end{align*} donde los $(v_i \otimes w_j)$ serán los elementos de la base de $T$, de donde se deduce que la dimensión de $T$ es $n · m$.

Por otra parte, la aplicación Φ que hace conmutar el diagrama de la \fref{fig:ProdTensorial} se define a partir de los elementos de la base, como $Φ(v_i \otimes w_j) = φ(v_i, w_j)$.
\end{defn}

\begin{example} Algunos ejemplos de productos tensoriales:
\begin{itemize}
\item $V \otimes \set{0} = \set{0}$.
\item $V \otimes \kbb \cong \kbb$, donde $\kbb$ es el cuerpo base de $V$.
\item El ejemplo más interesante para nosotros. Dada la aplicación bilineal \begin{align*}
\appl{φ}{H^p(M) × H^q(N)&}{H^{p+q}(V × W)} \\
(ω, η) &\longmapsto π_1^* ω ∧ π_2^* η
\end{align*} existe, por la propiedad universal, una aplicación $Φ$ que hace conmutar el diagrama de la siguiente forma: \begin{align*}
\appl{Φ}{H^p(M) \otimes H^q(N)&}{H^{p+q}(V×W)} \\
ω \otimes η &\longmapsto π_1^* ω ∧ π_2^* η
\end{align*}
\end{itemize}
\end{example}

\begin{proof}[\nref{thm:Kunneth}, idea] Al igual que hemos hecho con otras demostraciones, se hará inducción sobre $k$, el número de abiertos de un buen cubrimiento de $N$.

Si $k$ fuese 1, entonces $N \cong \bola^n$, y por el \nref{lem:Poincare} tendríamos que $H^d(M × N) \simeq H^d(M)$, y en el otro lado $\bigoplus_{p+ q = d} H^p(M) \otimes H^q(N)$ se iría para todos los grupos de $N$ salvo cuando $q = 0$, luego la identificacíon saldría trivial.

% Me estoy quedando MUY dormido.
Para aplicar la hipótesis de inducción coge una sucesión exacta y lo multiplica tensorialmente por $H^p(M)$, de tal forma que sigue dando una sucesión exacta, y luego hace una suma directa en $p+q=d$ y al final pues debe haber un pifostio muy, muy importante que ni siquiera Edu ha copiado, por lo que yo no me siento obligado a hacerlo. Me voy a dejar la marquita de TODO para en algún momento antes del examen repasar esto de alguna fuente más fiable y o bien dejar la demostración bien hecha (aunque la hubiese copiado seguiría siendo una basura) o bien borrarlo.
\end{proof}

Al final lo que concluimos es que Mayer-Vietoris sirve para hacer demostraciones como esta o como la de la dualidad de Poincaré o cosas.

% A partir de aquí,  copiado por VdJ

Ahora nos vamos a dedicar a estudiar $H^\ast(\projcp^n)$

\begin{theorem}
\[
	H^\ast(\projcp^n) = 
		\left\{ 
			\begin{array}{ccc} 	
				1 & \text{ si } & k \text{ par}\\ 
				0 & \text{ si } & k \text{ impar}\\ 
			\end{array}
		\right.
\]
\end{theorem}

\begin{proof}
Con el mismo esquema que siempre, utilizando $MV$ e inducción sobre la dimensión del $\projcp^n$.

\proofpart{Base inducción $n=1$}

\[ n=1 \to \projcp^1 = ℂ\cup \{∞\} \simeq \crc[2]\]

\proofpart{Paso inducción $n-1\to n$}

Suponemos que $H^\ast(\projcp^{n-1}) = 
		\left\{ 
			\begin{array}{ccc} 	
				1 & \text{ si } & k \text{ par}\\ 
				0 & \text{ si } & k \text{ impar}\\ 
			\end{array}
		\right.$ y queremos probarlo para $\projcp^n$

\[\projcp^n = U\cup V\]

Tomando:
\[
	\begin{array}{l}
		U = \projcp^n-\{[0:0:...:0:1]\}\\
		V = B^{2n}\left([0:0:...:0:1]\right)\\
		U \cap V \simeq \crc[n-1]\\
		U\underset{(1)}{\simeq} \projcp^{n-1} = \{[z_0:...:z_{n-1}:0]\}\subset \projcp^n
	\end{array}
\]

Además, tenemos que $(1)$ también indica que $\projcp^{n-1}$ es un retracto de deformación fuerte de U.

\begin{proof}
Necesitamos definir lo que es retracto de deformación para poder demostrarlo.

\todoby{Guille}
\end{proof}

Ahora hacemos la típica sucesión exacta de $H^\ast$ de cosas hasta la saciedad. 

\[
\begin{array}{llll}
 	\overbrace{H^0(\projcp^n)}^{0} 
& \to 
  	\overbrace{H^0(U)\oplus H^0(V)}^{2}
& \to 
  	\overbrace{H^0(U\cap V)}^{1}
\to&\\
 	\overbrace{H^1(\projcp^n)}^{} 
& \to 
  	\overbrace{H^1(U)\oplus H^1(V)}^{}
& \to 
  	\overbrace{H^1(U\cap V)}^{0}
\to&\\
 	\overbrace{H^2(\projcp^n)}^{} 
& \to 
  	\overbrace{H^2(U)\oplus H^2(V)}^{}
& \to 
  	\overbrace{H^2(U\cap V)}^{0}
\to&\\
... & ... & ... \\
 	\overbrace{H^k(\projcp^n)}^{} 
& \to 
  	\overbrace{H^k(U)\oplus H^k(V)}^{}
& \to 
  	\overbrace{H^k(U\cap V)}^{0}
\to&\\
... & ... & ... \\
 	\overbrace{H^{2n-3}(\projcp^n)}^{} 
& \to 
  	\overbrace{H^{2n-3}(U)\oplus H^{2n-3}(V)}^{}
& \to 
  	\overbrace{H^{2n-3}(U\cap V)}^{0}
\to&\\
 	\overbrace{H^{2n-2}(\projcp^n)}^{} 
& \to 
  	\overbrace{H^{2n-2}(U)\oplus H^{2n-2}(V)}^{}
& \to 
  	\overbrace{H^{2n-2}(U\cap V)}^{0}
\to&\\
 	\overbrace{H^{2n-1}(\projcp^n)}^{0} 
& \to 
  	\overbrace{H^{2n-1}(U)\oplus H^{2n-1}(V)}^{0}
& \to 
  	\overbrace{H^{2n-1}(U\cap V)}^{1}
\to&\\
 	\overbrace{H^{2n}(\projcp^n)}^{1} 
& \to 
  	0 &&
\end{array}
\]

Sacamos unas conclusiones interesantes:

\[
	k≤n-2 \to h^k(\projcp^n) = h^k(\projcp^{n-1}) = (\text{hipótesis inductiva}) =	\left\{ 
			\begin{array}{ccc} 	
				1 & \text{ si } & k \text{ par}\\ 
				0 & \text{ si } & k \text{ impar}\\ 
			\end{array}
		\right. 
\]

Entonces aplica el teorema.

Si $k>n-2$, vemos sencillamente que ya está demostrado.

\end{proof}

\begin{example} (Ejercicio)

$\crc[2]×\crc[4]\simeq \projcp^3$

Vamos a verlo:

\[
	\begin{array}{l}
		h^1(\crc[2]×\crc[4]) = ... = 0 = h^1(\projcp^3)\\
		h^2(\crc[2]×\crc[4]) = ... = 1 = h^2(\projcp^3)\\
		h^3(\crc[2]×\crc[4]) = ... = 0 = h^3(\projcp^3)\\
	\end{array}
\]

Esto tiene dimensión 6, y por dualidad ya los tenemos todos. Sabemos (por dualidad) que $h^0(\crc[2]×\crc[4]) = h^6(\crc[2]×\crc[4])$ $h^1(\crc[2]×\crc[4])=h^5(\crc[2]×\crc[4])$...

Asíque completamos:


\[
	\begin{array}{l}
		h^1(\crc[2]×\crc[4]) = ... = 0 = h^1(\projcp^3)\\
		h^2(\crc[2]×\crc[4]) = ... = 1 = h^2(\projcp^3)\\
		h^3(\crc[2]×\crc[4]) = ... = 0 = h^3(\projcp^3)\\
		h^4(\crc[2]×\crc[4]) = 1 = h^2(\projcp^3)\\
		h^5(\crc[2]×\crc[4]) = 1 = h^1(\projcp^3)\\
		h^6(\crc[2]×\crc[4]) = 1 = h^0(\projcp^3)
	\end{array}
\]

Este es el típico caso en el que no podemos decir nada.
%
Qué curioso el ejemplo, en el que este argumento no nos deja diferenciar las variedades. \footnote{Este ejemplo ha salido del azar fortuito de escoger 2 esferas que multiplicar cualesquiera.}
\end{example}

\begin{example}
¿$\crc[2]×\crc[2] \simeq \projcp^2$?


\[
	\begin{array}{l}
		h^1(\crc[2]×\crc[2]) = ... = 0 = h^1(\projcp^2)\\
		h^2(\crc[2]×\crc[2]) = ... = 2 \neq 1 = h^2(\projcp^2)
	\end{array}
\]

Por esto sabemos que no puede ser iguales.
\end{example}

\begin{example}
Se deja como ejercicio calcular:

\[
h^k(\crc[d]×\projcp^2) \simeq \crc[d+2]
\]

...

...

...

...

...

...

...

...

...

...

Oh vaya, no ha sido buen ejemplo poner $d$ en vez de un número fijo.
\end{example}


\obs Retomando ejemplos anteriores, queremos saber si $H^2\left(\crc[2]×\crc[4]\right) \simeq H^2(\projcp^3)$.
%
La única información que tenemos de momento es $h^2\left(\crc[2]×\crc[4]\right) = h^2(\projcp^3)$. 
%
Como $h$ es la dimensión de $H$, si tuvieran dimensiones distintas no podrían ser isomorfos.
%
Al tener la misma dimensión, tenemos que buscar otro camino.
%
Vamos a estudiar los generadores.


Sabemos\footnote{Matemagia} además que $H^2\left(\crc[2]×\crc[4]\right) = \gen{π_1^{\ast}1\wedge π_2^{\ast}\text{vol}_{\crc[2]}}$

Vamos a multiplicar una cosa por ella misma, siendo la multiplicación el producto exterior:

\[
	\left(π_2^{\ast}\text{vol}_{\crc[2]}\right)^2 = π_2^{\ast}\text{vol}_{\crc[2]}\wedge π_2^{\ast}\text{vol}_{\crc[2]} = π_2^{\ast}(\text{vol}_{\crc[2]}\wedge \text{vol}_{\crc[2]}) \overset{(1)}{=} 0
\]

$(1)$ porque nos pasamos de la dimensión.

Queremos ver las generadoras de $H^2(\projcp^3)$. 
%
Si $H^2(\projcp^3) = \gen{\eta}$, querríamos ver si que $\eta^2:= \eta\wedge\eta \neq 0$. 


Recurrimos a la sucesión de M-V. Esta sucesión aplicada a este caso:
%
\[
	H^K(\projcp^3) \simeq H^K(\projcp^3-\{\ast\}) \simeq H^k(\projcp^2)
\]
%
siendo los isomorfismos correspondientes las restricciones.


\textbf{Cuentas que no se de donde salen:}

Tenemos: $H^4(\projcp^3) \overset{τ}{\simeq} H^4(\projcp^2)\;\; H^2(\projcp^3) \simeq H^2(\projcp^2)$.

Además, $H^2(\projcp^3) = \gen{\eta}$ y $H^2(\projcp^2) = \gen{\eta_1}$. De esta manera, el isomorfismo $τ$ cumple $τ(\eta\wedge\eta) = τ(\eta_1\wedge\eta_1)$.

Nos preguntamos: ¿$\eta\wedge\eta = 0$? Y la respuesta es afirmativa por la dualidad de Poincaré.
%
Hemos llegado a que basta ver que
\[
	\eta_1\wedge\eta_1 ≠ 0 \text{ en } H^4(\projcp^2)
\]


El argumento de la dualidad de Poincaré es el siguiente:

$H^2(\projcp^2) \to \left(H^2(\projcp^2)\right)^{\ast}$ siendo la aplicación que lleva $α \to \int_{\projcp^2} α\wedge \_$ (es una aplicación ya que pertenece al espacio dual).

$\eta_{1_{H_0}} \to \int \eta_1\wedge \_ \neq 0 \text{ en } \left(H^4(\projcp^2)\right)$.

\paragraph{Conclusión:} Gabino no es un topólogo porque no tiene intuición sobre monstruos de M-V.

\paragraph{Ejercicio:}
Ponerte ejercicios de  que $\projcp \not\sim \crc[n]$


\subsubsection{Cohomología $\projp$}

\begin{theorem}[Cohomología\IS de $\projp$]

\[
	h^k(\projp^n) = 
		\left\{ 
			\begin{array}{ccl}
				1 &\text{ si }& k=0\\
				0 &\text{ si }& 1≤k≤n-1\\
				1 &\text{ si }& n\%2==0\\
				0 &\text{ si }& n\%2!=0
			\end{array}
		\right.
\]

\end{theorem}

\begin{proof}
Ideas de la demostración (aunque está bastante desarrollada).

Se empieza estudiando la cohomología de variedades cociente $\bar{M} = \quot{M}{G}$ donde $G$ es un grupo finito que actúa sin puntos fijos, como $\projp^n = \quot{\crc[n]}{\gen{J}}$ siendo $J(x_1,...,x_n,x_{n+1}) = - (x_1,...,x_{n+1})$

Para estudiar esta cohomología, tomamos $\appl{π}{M}{\quot{M}{G}}$ y construimos:

\[
	\appl{π^\ast}{Ω(\quot{M}{G})}{Ω(M)^G=\{ ω\inΩ(M) \tq g^\astω = ω\;∀g\in G\}\subset Ω(M)} 
\]
Siendo esta aplicación la que lleva $\eta\to\pi^\ast\eta$

Comprobamos que $g^\ast(π^\ast\eta) = g^\ast\pi^\ast\eta = \underbrace{(π\circ g)^\ast}_{π}\eta = \pi^\ast\eta$.

De hecho, esta aplicación es biyectiva. 
%
Tomando $π^{-1}$, restringido a 2 abiertos $U,V$, $π$ es biyectiva con lo que tiene inversa y está bien definido. 
%
Tendríamos que $\left(π(U),π_U^{-1}\right)$ es una carta. 
%
¿Es compatible? Es decir, ¿qué apsa si $π(U) \cap π(V) ≠ 0$? Vamos a verlo:

\[\left(π_V^{-1}\right)^\ast ω = (g\circ π_U^{-1})^\ast ω = \left(π_U^{-1}\right)^\ast g^\ast ω = \left(π_U^{-1}\right)^\ast ω\]

El siguiente paso sería estudiar que $Ω(\quot{M}{G}) \equiv Ω(M)^G$ 

\begin{prop}
\[
	H^K(\quot{M}{G}) = H^K(M)^G = \{ ω \in H^k(M) \tq g^\ast ω = ω\;\; ∀g\in G\}
\]
\end{prop}
\begin{proof}
\todoby{Guille}
\end{proof}

Utilizando este resultado:

\[
	H^k(\projp^n) = H^K(\crc[n])^{\gen{J}} \subset H^k(\crc[n])
\]

Con este resultado tenemos todas las dimensiones de $H^k(\projp^n)$ cuando $0≤k≤n-1$.

El próximo día (que creo que nunca va a llegar) veremos qué ocurre con $k=n$.

\end{proof}



%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Cálculos y otras explicaciones}
\input{tex/GT_Calculos.tex}

\chapter{Ejercicios}
\input{tex/GeometriaTopologia_Ejs.tex}

\bibliography{../Apuntes}{}
\printindex
\end{document}
