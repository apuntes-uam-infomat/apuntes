\documentclass[palatino, bibnumbers]{apuntes}

\title{Geometría y Topología}
\author{Jose Antonio García del Saz}
\date{16/17 C2}

% Paquetes adicionales
\usepackage{enumitem}
\usepackage{kpfonts}
\usepackage{tikztools}
\usepackage{fancysprefs}
\usepackage{tikz-3dplot}
\usepackage{physics}
\usepackage{xfrac}
\usepackage{wrapfig}
\usepackage{fastbuild}
\usepackage{tikz-cd}

\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\usetikzlibrary{intersections}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}

\tikzset{
	snake/.style={
		rounded corners,
		to path={
			-- ([xshift=1em]\tikztostart.east)
			-- ([xshift=1em]\tikztostart.south east)
			-- ([xshift=-1em]\tikztotarget.north west)
			-- ([xshift=-1em]\tikztotarget.west)
			-- (\tikztotarget)
		}
	},
	snake up/.style={
		rounded corners,
		to path={
			-- ([xshift=-1em]\tikztostart.west)
			-- ([xshift=-1em]\tikztostart.north west)
			-- ([xshift=1em]\tikztotarget.south east)
			-- ([xshift=1em]\tikztotarget.east)
			-- (\tikztotarget)
		}
	}
}

\setlist{itemsep=1pt, topsep=5pt}
\bibliographystyle{alpha}
% --------------------

%\precompileTikz

\newcommand{\Id}{\mop{Id}}
\newcommand{\cln}{\colon\!}

\setcounter{tocdepth}{3}

\begin{document}
\pagestyle{plain}
\newcommand\tab[1][1cm]{\hspace*{#1}}
% http://tex.stackexchange.com/a/14243
\relpenalty=9999
\binoppenalty=9999
%\newcommand\restr[2]{\ensuremath{\left.#1\right|_{#2}}}
\begin{abstract}
Estos son los apuntes del curso de Geometría y Topología, del profesor Fernando Chamizo.
\end{abstract}

\maketitle

\tableofcontents
\newpage
% Contenido.

\chapter{Álgebra Tensorial}

\section{Tensores en  $ℝ^{n}$}
\subsection{Definiciones y ejemplos}
Estudiar los tensores en $ℝ^n$ es en realidad estudiar el Álgebra Lineal pero en varias variables. En primer curso (Álgebra I) estudiamos las aplicaciones, las cuales eran de la forma:
\begin{align*}
	\appl{π}{ℝ^{n}&}{ℝ^{m}} \\
	\overline{x} &\longmapsto[\overline{y}=A\cdot \overline{x}]
\end{align*}

\begin{defn}[Aplicación\IS lineal] Sea $f$ una aplicación entre dos espacios vectoriales $V$, $W$ sobre el mismo cuerpo $K$. Decimos que $f$ es una \textbf{aplicación lineal} si se cumplen las siguientes propiedades ($λ\in K$):
	\begin{enumerate}
		\item $f(λ\overline{x})=λ\cdot f(\overline{x})$ .
		\item $f(\overline{x_1}+\overline{x_2})=f(\overline{x_1})+f(\overline{x_2})$
	\end{enumerate}
\end{defn}

\begin{defn}[Aplicación\IS bilineal] Sea 
	\begin{align*}
	\appl{f}{ℝ^{n}×ℝ^{n}&}{ℝ} \\
	\overline{x},\overline{y} &\longmapsto{f(\overline{x},\overline{y})}
	\end{align*}
una aplicación, decimos que es \textbf{bilineal} si es una aplicación lineal en cada una de las dos variables, es decir:
\begin{enumerate}
	\item $f(λ\overline{x},\overline{y})=λ\cdot f(\overline{x},\overline{y})$; $f(\overline{x_1}+\overline{x_2},\overline{y})=f(\overline{x_1},\overline{y})+f(\overline{x_2},\overline{y})$
	\item $f(\overline{x},λ\overline{y})=λ\cdot f(\overline{x},\overline{y})$; $f(\overline{x},\overline{y_1}+\overline{y_2})=f(\overline{x},\overline{y_1})+f(\overline{x},\overline{y_2})$
\end{enumerate}
\end{defn}
\textbf{Observación:} todas las aplicaciones bilineales entre dos espacios se pueden escribir de la siguiente manera:
$$f(\overline{x},\overline{y})=\overline{x}^{T}A\overline{y}$$ con A una matriz n×n.
\newpage
\begin{defn}[Aplicación\IS multilineal] Decimos que una aplicación es \textbf{multilineal} si es lineal en cada una de sus variables. 
\end{defn}

\begin{defn}[Tensor\IS n veces covariante] Es cualquier aplicación multilineal
	$\appl{T}{\varprod_{i=1}^n V}{ℝ}$, siendo V un espacio vectorial de dimensión finita sobre $ℝ$ (que como sabemos de otros cursos son isomorfos a $ℝ^{n}$).
\end{defn}

\begin{example} Sea
	\begin{align*}
		\appl{T}{ℝ^{3}×ℝ^{3}&}{ℝ} \\
		T\left(\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix},\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix}\right) &\longmapsto{x_1\cdot y_3}
	\end{align*}
es obvio que T es multilineal, luego T es un tensor 2 veces covariante en $ℝ^{3}$.
\end{example}
\begin{example} Sea
	\begin{align*}
		\appl{T}{ℝ^{3}×ℝ^{3}&}{ℝ} \\
		T\left(\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix},\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix}\right) &\longmapsto{x_1\cdot x_3}
	\end{align*}
	se ve rápidamente que \underline{no} es una aplicación lineal respecto de la variable $\overline{x}$.
\end{example}
\begin{example} Sea
	\begin{align*}
		\appl{T}{ℝ^{3}×ℝ^{3}×ℝ^{3}&}{ℝ} \\
		T\left(\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix},\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix},\begin{pmatrix}z_1\\z_2\\z_3\end{pmatrix}\right) &\longmapsto{\begin{vmatrix}
			x_1 & y_1 &  z_1 \\ 
			x_2 & y_2 & z_2 \\ 
			x_3 & y_3 & z_3 \\ 
		\end{vmatrix}}
	\end{align*}
	la propiedad de linealidad del producto por un escalar es obvia por las propiedades de los determinantes. La propiedad de linealidad que conserva la adición se demuestra fácilmante desarrollando el determinante por adjuntos en la primera columna. Luego T es un tensor 3 veces covariante.
\end{example}
\newpage
En Álgebra Lineal estudiamos no sólo las aplicaciones $\appl{T}{ℝ^{n}}{ℝ}$ , sino también las de la forma $\appl{T}{ℝ^{n}}{ℝ^{m}}$. Ahora bien, estás últimas pueden convertirse al primer tipo mediante un elemento del \textbf{espacio dual}.

\begin{defn}[Espacio\IS dual] Sea V un espacio vectorial entonces se define el espacio dual y se denota con $V^{*}$ de la siguiente manera: \[ V^{*} = \{ \appl{f}{V}{ℝ} \tq \text{f es lineal} \}\]
\end{defn}

Con la definición anterior, un truco para pasar de unas aplicaciones a otras es que consideramos un elemento del espacio dual ($\phi\inℝ^{m}$) y uno del espacio vectorial original ($\overline{x}\inℝ^{n}$) y entonces tenemos una aplicación $$\hat{f}(\phi,\overline{x})=\phi(f(\overline{x}))\inℝ$$

De la misma manera, en lugar de considerar "tensores vectoriales" (esta expresión no es correcta, pero se da para ilustrar), los cuales serían aplicaciones multilineales $\appl{T}{\varprod_{1}^s V}{\varprod_{1}^r V}$, es más conveniente pensar en que el tensor va a depender también de los elementos del espacio dual. Luego vamos a considerar tensores de la forma $$\appl{T}{\underbrace{V^{*}×\cdots×V^{*}}_{\text{r veces}}×\underbrace{V×\cdots×V}_{\text{s veces}}}{ℝ}$$ que sean multilineales.

\begin{defn}[Tensor\IS r veces contravariante y s veces covariante] Es cualquier aplicación multilineal de la forma:
	$$\appl{T}{\underbrace{V^{*}×\cdots×V^{*}}_{\text{r veces}}×\underbrace{V×\cdots×V}_{\text{s veces}}}{ℝ}$$ siendo V un espacio vectorial de dimensión finita sobre $ℝ$ (isomorfo a $ℝ^{n}$) y $V^{*}$ su espacio dual. Diremos análogamente que T es un tensor de tipo (r,s).
\end{defn}

Como ya sabemos, si V es un espacio vectorial y tenemos una base de V (llamémosla $\base = \{ \overline{v_1},...,\overline{v_n} \}$), entonces existe una base natural de $V^{*}$
llamada \textbf{base dual} (denotada por $\base^{*}=\{\tilde{\phi}^{1},\cdots ,\tilde{\phi}^{n}\}$) y que está determinada por la propiedad: $$\tilde{\phi}^{i}(\overline{v_j})=\delta_j^{i} = \begin{cases} 0 & i ≠ j \\ 1 & i = j \end{cases}$$
\newpage
\begin{example} Daremos un ejemplo casi absurdo para ilustrar. Dar la base dual es muy fácil cuando tenemos una base ortonormal, por ejemplo la base canónica de $ℝ^{2}$:
$$\base = \{\overline{e_1},\overline{e_2}\}=\left\{\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}\right\}$$En este caso obtenemos la base dual solamente girando los elementos de la base canónica, obteniendo: $$\base^{*} =\{\tilde{\phi^{1}},\tilde{\phi^{2}}\}=\left\{\begin{pmatrix}1&0\end{pmatrix},\begin{pmatrix}0&1\end{pmatrix}\right\}$$
\end{example}
\begin{example} Imaginemos que estamos en $ℝ^{2}$ pero en esta ocasión tenemos una cualquiera de sus bases $\base=\{\overline{v_1},\overline{v_2}\}$. Ahora consideramos la matriz $A=\begin{pmatrix}v_{11}&v_{21}\\v_{12}&v_{22}\\ \end{pmatrix}$, y entonces tenemos que $\begin{cases}\overline{v_1}=A\cdot \overline{e_1} \\\overline{v_2}=A\cdot \overline{e_2}\end{cases}$. Construímos entonces la base dual $\base^{*}=\{ \tilde{\phi^{1}},\tilde{\phi^{2}}\}$ de esta manera con la matriz de cambio de base: $\begin{cases} \tilde{\phi^{1}}(\overline{x})=\begin{pmatrix}0&1\end{pmatrix}\cdot A^{-1}\cdot \overline{x} \\\tilde{\phi^{2}}(\overline{x})=\begin{pmatrix}1&0\end{pmatrix}\cdot A^{-1}\cdot \overline{x}\end{cases}$
\end{example}
\begin{example} Si f es un \textit{endomorfismo}, digamos $\appl{f}{ℝ^{n}}{ℝ^{n}}$, se corresponde de manera unívoca con un tensor de tipo (1,1):
	\begin{align*}
	\appl{T}{(ℝ^{n})^{*}×ℝ^{n}&}{ℝ} \\
	T(\tilde{\phi},\overline{x}) &\longmapsto{\tilde{\phi}(f(\overline{x}))}
	\end{align*}
\end{example}
\begin{example} Cualquier vector de un espacio vectorial ($\overline{v}\in V$) se puede hacer corresponder con un tensor (1,0)  que se alimenta de elementos del dual y devuelve reales definido como:
	\begin{align*}
	\appl{T_{\overline{v}}}{V^{*}&}{ℝ} \\
	T_{\overline{v}}(\tilde{\phi}) &\longmapsto{\tilde{\phi}(\overline{v})}
	\end{align*}
\end{example}

Vamos a introducir notación para las definiciones y resultados venideros, y hay que hacerla nuestra ya que es el infierno de casi todos los estudiantes al abrir un libro de cálculo tensorial. De aquí en adelante (aunque se venga haciendo desde el inicio) se usarán \textbf{subíndices} para numerar vectores ($\overline{v_1},\overline{v_2},...,\overline{v_n}$) y \textbf{superíndices} para numerar elementos del espacio dual, aunque también sean vectores ($\tilde{\phi^{1}},\tilde{\phi^{2}},...,\tilde{\phi^{n}}$) .
\newpage
\begin{defn}[Componentes\IS de un tensor] Sea un tensor $T$ de tipo (r,s) y sean $\base=\{\overline{v_1},...,\overline{v_n}\}$, $\base^{*}=\{\tilde{\phi^{1}},...,\tilde{\phi^{n}}\}$ las bases de $V$ y $V^{*}$ respectivamente, se definen las componentes de $T$ en la base $\base$ como la siguiente colección de números:
	$$T_{j_1\space j_2\space \cdots \space j_s}^{i_1\space i_2\space \cdots \space i_r}=T(\tilde{\phi^{i_1}},\tilde{\phi^{i_2}},...,\tilde{\phi^{i_r}},\overline{v}_{j_1},\overline{v}_{j_2},...,\overline{v}_{j_s})$$
\end{defn}
\begin{example} Calcular las componentes del tensor:
	\begin{align*}
	\appl{D}{ℝ^{2}×ℝ^{2}&}{ℝ} \\
	D\left(\begin{pmatrix}x_1\\x_2\end{pmatrix},\begin{pmatrix}y_1\\y_2\end{pmatrix}\right) &\longmapsto{\begin{vmatrix}
		x_1 & y_1 \\ 
		x_2 & y_2 \\ 
		\end{vmatrix}}
	\end{align*}
	Lo primero que hacemos es coger una base (cogemos la canónica porque será lo más usual este curso). Como es un tensor (0,2), no habrá superíndices en las componentes, sólo subindices. Las componentes son: $$D_{j_1\space j_2}=\begin{cases}
		D_{1\space1}=D(\overline{e_1},\overline{e_1})=\begin{vmatrix}1&1 \\ 0&0 \\ \end{vmatrix}=0\\
		D_{1\space2}=D(\overline{e_1},\overline{e_2})=1\\
		D_{2\space1}=-1\\
		D_{2\space2}=0\\
	\end{cases}$$
\end{example}
\subsection{Convenio de Einstein}
Se usa para ahorrar en escritura cuando se habla del álgebra tensorial. En resumen consiste en que cuando se repite un índice arriba y abajo entonces hay que suponer que sumamos en él.
\begin{example} En primer curso, la combinación lineal se escribía como: $λ_1\cdot \overline{v_1}+\cdots +λ_n\cdot \overline{v_n}$. Si usamos el convenio esto se escribiría simplemente como $λ^{i}\cdot \overline{v}_i$
\end{example}
\begin{example} Sea una aplicación lineal $f(\overline{x})=A\cdot \overline{x}$, se corresponde unívocamente con un tensor (1,1):
	\begin{align*}
		\appl{T}{(ℝ^{n})^{*}×ℝ^{n}&}{ℝ} \\
		T(\tilde{\phi},\overline{v}) &\longmapsto{\tilde{\phi}(\overline{v})}
	\end{align*}
Las componentes (en la base canónica) son las $a_j^{i}$, donde $i$ son las filas y $j$ las columnas.
$$f(\overline{x})=a_j^{i}\cdot x^j;\overline{x}=x^j\cdot \overline{e_j}$$
Las componentes $T_j^{i}\equiv T(\tilde{\phi}^{i},\overline{e_j})=\begin{pmatrix}0&\cdots & 1 & \cdots &0\end{pmatrix}\cdot A\cdot \begin{pmatrix}
0 \\ \cdots \\ 1 \\ \cdots\\0\end{pmatrix}=a_j^{i}$
\end{example}
\newpage
\begin{example} Un tensor (1,3) muy importante es el llamado tensor de Riemann $\appl{T}{R=V^{*}×V×V×V}{ℝ}$. En relatividad $dim V = 4$ y tiene $4\cdot 4\cdot 4\cdot 4=256$ componentes y, para aplicarlo a un elemento del dual, digamos con componentes $(a_1,a_2,a_3,a_4)$, y a tres vectores, con coordenadas$(b_1,b_2,b_3,b_4), (c_1,c_2,c_3,c_4), (d_1,d_2,d_3,d_4)$, debemos escribir:
$$\sum_{i=1}^{4}\sum_{j=1}^{4}\sum_{k=1}^{4}\sum_{l=1}^{4}R_{j\space k\space l}^{i}a_i\space b^{j}\space c^{k}\space d^{l}$$
Esta expresión tiene demasiados sumatorios, si la reescribimos con el criterio de Einstein queda $R_{j\space k\space l}^{i}a_i\space b^{j}\space c^{k}\space d^{l}$\newline
\end{example}
\begin{defn}[Producto\IS tensorial] Sea T un tensor de tipo (r,s) y sea S otro tensor de tipo (u,v), de define su producto tensorial $T\otimes S$ como un nuevo tensor de tipo (r+u,s+v): $$T\otimes S(\tilde{\phi^{1}},\cdots ,\tilde{\phi^{r+u}})=T(\tilde{\phi^{1}},\cdots ,\tilde{\phi^{r}},\overline{v}_1,\cdots,\overline{v}_s)\cdot S(\tilde{\phi^{r+1}},\cdots ,\tilde{\phi^{r+u}},\overline{v}_{s+1},\cdots,\overline{v}_{s+v})$$
\end{defn}
\begin{example}Tomaremos estos dos tensores de tipo (0,1) en $ℝ^{2}$: $$T(\overline{x})=2\cdot x^{1}+x^{2}; S(\overline{x})=5\cdot x^{1}$$ y hallaremos ahora las componentes de $T\otimes S$, que será por definición un nuevo tensor de tipo (0,2), de la forma $\appl{T\otimes S}{ℝ^{2}×ℝ^{2}}{ℝ}$, y le llamaremos P a partir de ahora. Entonces tenemos que: $$\begin{cases}
	P_{1\space1}=P(\overline{e}_1,\overline{e}_1)=10\\
	P_{1\space2}=0\\
	P_{2\space1}=P(\overline{e}_2,\overline{e}_1)=5\\
	P_{2\space2}=0\\
	\end{cases}$$
	
\end{example}
\begin{example}En física e ingeniería se consideran muchos tensores importantes. Por ejemplo, el \textbf{tensor de inercia} es un tensor de tipo (0,2) que mide (entre otras cosas) lo difícil que es girar un sólido rígido respecto a un eje dado (en términos físicos es difícil si necesito un gran \textbf{trabajo} para girarlo).\\
	$$
	\begin{tikzpicture}
	\draw (-1,0) arc (180:360:1cm and 0.5cm);
	\draw[dashed] (-1,0) arc (180:0:1cm and 0.5cm);
	\draw (0,0) circle (1cm);
	\shade[ball color=blue!10!white,opacity=0.20] (0,0) circle (1cm);
	\draw[thick,->] (0,-1.5,0) -- (0,1.5,0) node[above]{$eje$};
	\end{tikzpicture}
\tab
	\begin{tikzpicture}
	\draw (-1,0) arc (180:360:1cm and 0.5cm);
	\draw[dashed] (-1,0) arc (180:0:1cm and 0.5cm);
	
	\draw (0,0) circle (1cm);
	\shade[ball color=blue!10!white,opacity=0.20] (0,0) circle (1cm);
	\draw[thick,->] (-0.9,-1.5,0) -- (-0.9,1.5,0) node[above]{$eje$};
	\end{tikzpicture}
	$$
Por ejemplo, en la figura, suponiendo que es una esfera que pesa toneladas, la intuición nos dice sin hacer cálculos que costará menos rotar la esfera respecto al primer eje que rotarla respecto al segundo (y estamos en lo cierto).
\end{example}
\begin{example}Los tensores también aparecen en el \textbf{entrelazamiento cuántico}, dentro del campo de la mecánica cuántica.
Una partícula se corresponde con un vector, que como sabemos se corresponde con un tensor (1,0). Pues si tenemos dos partículas $\overline{v}$ y $\overline{w}$, entonces el sistema formado por ambas partículas es  $\overline{v}\otimes\overline{w}$.
\end{example}
\section{Repaso (intuitivo) de Geometría Diferencial}
Empezaremos con el objeto matemático al que llamamos \textbf{variedad}. Como idea, una variedad es un objeto geométrico que se puede parametrizar por abiertos de $ℝ^{n}$, pero el gran salto con respecto de la geometría de la asignatura de GCS, es que la variedad no está inmersa en $ℝ^{n}$, sino que está ahí (como el universo), sin preocuparnos lo que sea que la rodea.\newline
\indent La idea de variedad diferenciable n-dimensional es, por tanto, la de un objeto geométrico compuesto por parches que son similares a abiertos de $ℝ^{n}$. Partimos de un espacio topológico M al que exigimos que tenga la propiedad de Hausdorff y una base numerable (segundo axioma de numerabilidad). La primera propiedad es natural si queremos poder tratar separadamente los puntos, y las segunda va también en este sentido, porque permite asegurar la existencia de particiones de la unidad, que son totalmente necesarias para hacer el an análisis local típico de la geometría diferencial.
Una carta nos dice la manera de allanar un parche de M en $ℝ^{n}$ (es la inversa de la parametrización en ese abierto de $ℝ^{n}$).
$$\includegraphics{img/GT17_Variedad_carta}$$
El número de parámetros que la parametrización requiere es llamado en geometría \textbf{dimensión} de la variedad (mientras en mecánica lo llamaríamos \textbf{grados de libertad}). Si llamamos $\phi$ a la inversa de la parametrización (la carta), entonces tendremos (mirar figura superior) algo como $\phi(x^{1},\cdots,x^{n})$, que nos lleva a $ℝ^{n}$ y donde $x^{i}$ son lo que llamamos \textbf{funciones coordenadas}. Vamos a formalizarlo rápidamente para no empezar a divagar.

\begin{defn}[Carta\IS n-dimensional] Una carta n-dimensional de M es un par $(\mathcal{U},\phi)$ donde $\mathcal{U}$ es un abierto
de M y $\phi$ es una función $\appl{\phi}{\mathcal{U}}{ℝ^{n}}$ que es homeomorfismo sobre su imagen.
\end{defn}

Como un punto puede estar tapado por varios parches, diferentes abiertos de cartas, debemos asegurarnos de que el análisis no se estropea bajando por una $\phi$ o por otra. Una de las cosas que se suelen pedir en Geometría Diferencial es que si existe otra parametrización $\psi$, entonces tanto $\psi\circ\phi$ como $\phi\circ\psi$ han de ser de clase $C^{\infty}$ (En el abierto intersección de $ℝ^{n}$), y en ese caso se dice que las cartas son \textbf{compatibles} 

\begin{defn}[Derivada parcial i-ésima en una variedad] Se dice que una función $\appl{f}{M}{ℝ}$ es $C^{\infty}$ si para cada carta $(\mathcal{U},\phi)$ la función $\appl{f\circ \phi^{-1}}{\phi(\mathcal{U})}{ℝ}$ lo es, y se define para cada $p \in \mathcal{U}$ la derivada parcial i-ésima en la variedad como $$\restr{\pdv{f}{x^{i}}}{p}=D_i(f\circ \phi^{-1})(\phi(p))$$ donde $D_i$ denota la derivada parcial usual respecto de la variable i-ésima.
\end{defn}

No daremos la definición rigurosa, pero sí diremos que a las funciones $C^{\infty}$ entre dos variedades $M$ y $N$ que tienen inversa $C^{\infty}$ se llaman \textbf{difeomorfismos}
\newpage
Un problema técnicamente más complejo es la definición del \textbf{espacio tangente}, que en el caso de subvariedades de $ℝ^{n}$ es muy fácil. No es una mera adaptación porque allí los vectores tangentes eran “pelos” orientados que se salían de la subvariedad, mientras que ahora concebimos las variedades como una entidad única, sin referencia a un posible “exterior”. Hay varias maneras de superar este obstáculo. Aquí mencionaremos las definiciones matemáticas que corresponden a ver los vectores tangentes como velocidades de curvas y como derivadas direccionales. La segunda es más abstracta, se hace introduciendo implícitamente el concepto de derivación.

\begin{defn}[Espacio\IS tangente de una variedad] Se llama espacio tangente de una variedad M en un punto p al conjunto cociente $T_p(M)=K_p(M) /\sim$ donde $K_p(M)=\{ $Funciones $\appl{c}{(-\epsilon,\epsilon)}{M}$ con $c(0)=p \}$ y $\sim$ identifica las funciones curvas tales que $(\phi\circ c_1)'(0)=(\phi\circ c_2)'(0)$ con $(\mathcal{U},\phi)$ una carta. Se llama \textbf{vector tangente} de M en p a cualquiera de sus elementos.
\end{defn}

\begin{defn}[Vector\IS tangente en una variedad] Se llama vector tangente de $M$ en $p$ a cualquier operador $ℝ$-lineal $\appl{v}{E_p(M)}{ℝ}$ que satisface $v(fg)=v(f)g(p)+f(p)v(g)$ para todo $f,g \in E_p(M)$, donde $E_p(M)$ es el anillo de funciones $M\longmapsto ℝ$ definidas en un entorno suficientemente pequeño de p. Se llama espacio tangente de $M$ en un punto $p$ al conjunto formado por todos los vectores tangentes.
\end{defn}

A partir de las curvas que corresponden a los ejes coordenados (una vez que bajamos a $ℝ^{n}$ por la carta) se obtienen unos vectores tangentes que denotaremos con el extraño nombre $\restr{\pdv{}{x^{i}}}{p}$. Para ser rigurosos, si $\{\overline{e}_1,\overline{e}_2,\cdots,\overline{e}_n\}$ es la base canónica, fijada una carta $(\mathcal{U},\phi=(x^1,\cdots,x^n))$ con la primera definición se tiene $$\restr{\pdv{}{x^{i}}}{p}=[c_i]\tab con \tab c_i(t)=\phi^{-1}(\phi(p)+t\overline{e}_i),\tab i=1,2,...,n $$

Denominar a estos vectores con el mismo símbolo que el de las derivadas parciales no es casual, pes con la segunda definición no son más que las derivadas parciales i-ésimas en la varidad, es decir 
\begin{equation}
\appl{\restr{\pdv{}{x^{i}}}{p}}{f}{\restr{\pdv{f}{x^{k}}}{p}}
\end{equation}
Por razones obvias se les suele denotar con la notación abreviada $\restr{\partial_i}{p}$, o incluso $\partial_i$ si el punto no se indica.
\begin{prop} El espacio tangente $T_p(M)$ tiene una estructura natural de espacio vectorial cuya dimensión es la de la variedad diferenciable M.
\end{prop}
\begin{prop} Para cada punto p de una variedad diferenciable n-dimensional $M$, el conjunto $\{\restr{\partial_1}{p},\restr{\partial_2}{p},\cdots,\restr{\partial_n}{p}\}$ es una base de $T_p(M)$.
\end{prop}
Hasta aquí , si aún no nos hemos quitado la vida con la notación, podemos proceder.
\newpage
Con $\appl{f}{M}{N}$ podemos pasar curvas en curvas lo cual induce una aplicación $T_p(M)\longmapsto T_{f(p)}(N)$. Aunque ésta es la idea intuitiva, es más sintético proceder tomando en cuenta la segunda definición de espacio tangente.

\begin{defn}[Aplicación\IS tangente] Sea $\appl{f}{M}{N}$. Se llama aplicación tangente de f en p y se denota con $\restr{\dv{f}}{p}$, a la aplicación lineal $T_p(M)\longmapsto T_{f(p)}(N)$ que aplica un elemento de $T_p(M)$ (considerado con la segunda definición), digamos $v(\cdot)$ en $v(\cdot\circ f)$
\end{defn}
\begin{prop} Sea $\appl{f}{M}{N}$ y sean $(\mathcal{U}(p),\phi)$ y $(\mathcal{V}(f(p)),\psi)$ cartas de M y N respectivamente en los puntos indicados. La matriz de la aplicación tangente $\restr{df}{p}$ en las bases $\{\restr{\pdv{}{x^1}}{p},\cdots,\restr{\pdv{}{x^m}}{p}\}$ y $\{\restr{\pdv{}{y^1}}{f(p)},\cdots,\restr{\pdv{}{y^n}}{f(p)}\}$ correspondientes a estas cartas es la matriz jacobiana de $\psi\circ f\circ\phi^{-1}$ en $\phi(p)$.
\end{prop}

Dada una carta $(\mathcal{U},\phi=(x^1,\cdots,x^n))$ de M tiene sentido considerar las aplicaciones tangentes de las funciones coordenadas $\restr{dx^{i}}{p}$ como funciones de $M$ en $ℝ$ con la estructura de variedad obvia. Usando las definiciones de vector tangente se puede probar que $$\restr{dx^{i}}{p}(\restr{\pdv{}{x^j}}{p})=\delta_j^i$$ o dicho de otra forma $$\{\restr{dx^1}{p},\restr{dx^2}{p},\cdots,\restr{dx^n}{p}\}\tab \text{es la base dual de} \tab\{\restr{\pdv{}{x^1}}{p},\restr{\pdv{}{x^2}}{p},\cdots,\restr{\pdv{}{x^n}}{p}\}$$

\begin{defn}[Espacio\IS cotangente] Dada una carta $(\mathcal{U},\phi=(x^1,\cdots,x^n))$ de $M$, al espacio vectorial sobre $ℝ$ generado por $\{\restr{dx^1}{p},\restr{dx^2}{p},\cdots,\restr{dx^n}{p}\}$ se denomina espacio cotangente de M en p y se denota con $T_p^{*}(M)$, por ser el dual de $T_p(M)$.
\end{defn}

\begin{defn}[Uno forma] Los elementos de $T_p^{*}(M)$ se llaman uno formas (o covectores).
\end{defn}

Como cabía esperar, en lo sucesivo descargaremos la notación para las aplicaciones tangentes y las bases introducidas de $T_p(M)$ y $T_p^{*}(M)$ omitiendo el punto cuando no sea relevante. Por ejemplo, escribiremos $dx^1$ en lugar de $\restr{dx^1}{p}$.

Una vez más insistimos en que todos los espacios vectoriales sobre $ℝ$ son lo mismo, y una vez fijadas las bases las operaciones se realizan coordenada a coordenada como nos enseñaron en primero cuando casi todo era con vectores de $ℝ^{n}$. Los elementos del dual no albergan nada nuevo y siguen funcionando como se indicó en la sección anterior (y en el curso de primero) por mucho que pongamos $d$ y $\partial$ por todos los lados. En un ejemplo: \\ $$(2dx^1+3dx^2)(2\pdv{}{x^1}-\pdv{}{x^2})=1 \tab\text{porque}\tab \begin{pmatrix}2&3\end{pmatrix}\begin{pmatrix}2\\-1\end{pmatrix}=1$$
\newpage
\section{Tensores en el espacio tangente}
\chapter{Geometría Riemanniana}
\section{Cálculo de variaciones y mecánica}
%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Ejercicios}
\input{tex/GeoTopo17_Ejs.tex}
\bibliography{../Apuntes}
\printindex
\end{document}
