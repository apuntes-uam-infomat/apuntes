%
% Soluciones a los ejercicios de Probabilidad II.
%
% Curso 2014 - 2015 2º cuatrimestre
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hoja 1}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}

Se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra.

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]De una urna con 10 bolas blancas y 10 bolas negras se extraen simultaneamente 3 bolas. 
Calcular la probabilidad de que exactamente dos de ellas sean blancas. Responder a la misma
pregunta si las bolas se extraen de manera sucesiva.
\solution

\begin{expla}
En este caso no hay reposición de las bolas extraídas. Por tanto no hay diferencia a la hora de calcular la probabilidad entre la extracción simultánea y la sucesiva.

Para resolver esos problemas, la forma más intuitiva es buscar la fracción:
\[
\frac{\text{Numero de casos favorables}}{\text{Numero de casos posibles}}
\]
\end{expla}
A = Extracción simultánea de 3 bolas blancas

b = blanca

n = negra
\[
P(A)=\underbrace{\frac{10}{20}}_{b}\underbrace{\frac{9}{19}}_{b}\underbrace{\frac{10}{18}}_{n}+\underbrace{\frac{10}{20}}_{b}\underbrace{\frac{10}{19}}_{n}\underbrace{\frac{9}{18}}_{b}+\underbrace{\frac{10}{20}}_{n}\underbrace{\frac{10}{19}}_{b}\underbrace{\frac{9}{18}}_{b} = 3\frac{900}{6840}=\frac{2700}{6840}=0.39
\]

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 1.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]Disponemos de dos urnas,
$U_1$, que contiene 6 bolas azules y 8 bolas blancas, y
$U_2$,
 que contiene
3 bolas azules y 9 bolas blancas. Se sortea con un dado equilibrado de 4 caras la elecci\'on de una
urna, escogiendose
$U_1$,
si salen 1,
2 o 3, y
$U_2$,
si sale 4. Posteriormente se extrae al azar una bola de
esa urna.

\ppart ?` Cual es la probabilidad de que la bola extraida sea azul?
Sugerencia: usar la regla de la probabilidad total. Respuesta: $43/112$.

\ppart  Si la bola extraida resulta ser blanca  ?`cual es la probabilidad de que proceda de la
urna
$U_1$?

Sugerencia: usar  Bayes o el apartado anterior. Respuesta:
$16/23 = 1 -  43/112$.
\solution
\begin{expla}

$U_1 \rightarrow$ 6a, 8b  (1,2,3)

$U_2 \rightarrow$ 3a, 9b  (4)
\end{expla}
\spart
A = bola extraída azul

$U_1$ = Extraemos de la urna 1

$U_2$ = Extraemos de la urna 2
\[
P(A) = P(A\cap U_1)+P(A\cap U_2) = P(A|U_1)P(U_1)+P(A|U_2)P(U_2)=
\]
\[
=\frac{3}{4}\cdot\frac{6}{14}+\frac{1}{4}\cdot\frac{3}{12}=\frac{18}{56}+\frac{3}{48}=\frac{18}{56}+\frac{1}{16}=\frac{36}{112}+\frac{7}{112}=\frac{43}{112}
\]

\spart
B = bola extraida blanca.

$U_1$ = Extraemos de la urna 1.

\[
P(U_1|B)= \frac{P(U_1 \cap B)}{P(B)} = \frac{P(B|U_1)P(U_1)}{P(B|U_1)P(U_1)+P(B|U_2)P(U_2)}=
\]
\[
=\frac{\frac{8}{14}\cdot\frac{3}{4}}{\frac{8}{14}\cdot\frac{3}{4}+\frac{9}{12}\cdot\frac{1}{4}}=\frac{\frac{24}{56}}{\frac{24}{56}+\frac{9}{48}}=\frac{\frac{24}{56}}{\frac{48}{112}+\frac{21}{112}}=\frac{\frac{24}{56}}{\frac{69}{112}}=\frac{48}{69}=\frac{16}{23}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]Enfermedades raras. Ning\'un test biol\'ogico es 100 $\%$ preciso. Supongamos que un test para
determinar si cierta infecci\'on se ha producido, da falsos positivos en un 1 $\%$ de los casos, y falsos
negativos en un 2 $\%$  de los casos. Si una de cada 100 000 personas entre la poblaci\'on general est\'a
infectada, determinar la probabilidad de que una persona escogida al azar est\'e infectada, sabiendo
que el test ha dado positivo.
\solution

\begin{expla}

\begin{center}
\includegraphics[scale=0.75]{img/Dvenn5.png}
\end{center}


\begin{itemize}

\item Las personas o están infectadas o no están infectadas. 1=P(A1)+P(A2)

\item Los tests o dan positivo o dan negativo: 1 = P(+)+P(-)

\item $A_1$ = persona infectada $\rightarrow P(A1)=10^{-5}$

\item $A_2$ = persona no infectada $\rightarrow P(A_2)=1-10^{-5}$

\item + = test positivo

\item - = test negativo

\item Falso positivo (probabilidad de que el test de positivo estando la persona sin infectar) = $1\% = P(+|A_2)=0.01$

\item $P(+|A_2)+P(-|A_2)=1 \rightarrow P(-|A_2)=0.99$

\item Falso negativo (probabilidad de que el test de negativo sabiendo que la persona esta infectada) = $2\% = P(-|A_1)=0.02$

\item $P(-|A_1)+P(+|A_1)=1 \rightarrow P(+|A_1)=0.98$
\end{itemize}

\end{expla}



\[
P(A_1|+)=\frac{P(A_1\cap +)}{P(+)}=\frac{P(+|A_1)P(A_1)}{P(+|A_1)P(A_1)+P(+|A_2)P(A_2)}=
\]
\[
=\frac{0.98\cdot10^{-5}}{0.98\cdot10^{-5}+0.01\cdot\frac{99999}{10^5}}=0.000979
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]Angel y Benito tienen sendas barajas espa\~nolas (40 cartas). Cada uno saca de su
baraja una carta al azar (es decir, con iguales probabilidades, e independientemente). Hallar:

\ppart La probabilidad de obtener al menos un as. 

\ppart La probabilidad de obtener dos cartas del mismo palo. 

\ppart La probabilidad de no obtener ning\'un as.

\ppart La probabilidad de no obtener ni una copa ni una espada.
\solution

\begin{expla}

\end{expla}

\spart
A = obtener al menos un AS

B = no obtener ningun AS
\[
P(A)=\frac{4}{40}\cdot\frac{4}{40}+\frac{4}{40}\cdot\frac{36}{40}+\frac{36}{40}\cdot\frac{4}{40} = 0.19
\]

Otra forma
\[
P(A)=1-P(B)=1-\frac{36}{40}\cdot\frac{36}{40} = 1 - 0.81 = 0.19
\]

\spart
C = dos cartas del mismo palo

Da igual de que palo sea la primera carta, la cosa es que la segunda sea del mismo.

Pensando de otra forma tenemos el siguiente espacio muestral:

$\Omega = \{(c, o),(c, e),(c, b),(c, c),(o, o),(o, c),(o, e),(o, b),(e, c),(e, o),(e, b),(e, e),(b, c),(b, e)\\,(b, o),(b, b)\}$

\[
p(C)=\frac{1}{4}
\]

\spart
B = no obtener ningun AS

\[
P(B)=\frac{36}{40}\cdot\frac{36}{40} = 0.81
\]

\spart
D = no obtener ni una copa ni una espada

\[
P(D) = \frac{20}{40}\cdot\frac{20}{40}=\frac{1}{4}
\]


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Ana y Bea eligen cada una un n\'umero al azar, entre 0 y 2. Sean $A, B, C, D,$ los siguientes
eventos: 

\  $A$: La diferencia entre ambos n\'umeros es al menos 1/3.

\ $B$:   Al menos uno de los n\'umeros es mayor que 1/3.

\ $C$: Los dos n\'umeros son iguales.

\ $D$: El n\'umero de Bea es mayor que 1/3.

Hallar $P(B)$, $P(C)$ y $P(A\cup D)$.
\solution

\begin{expla}
Suponemos que el número elegido es natural, es decir, pertenece al conjunto $\{0,1,2\}$.
\end{expla}

\spart
E = Ningún número es mayor que 1/3.

\[
P(B)=1 - P(E)= 1 - \frac{1}{3}\cdot\frac{1}{3} = \frac{8}{9}
\]

\spart

Da igual qué numero escojas el primero, el que importa es el segundo.

Pensando de otra forma tenemos el siguiente espacio muestral:

$\Omega=\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2),(2,0),(2,1),(2,2)\}$

\[
P(C)=\frac{1}{3}
\]

\spart
Dado el espacio muestral $\Omega$ definido en el apartado anterior, vemos que:

$P(A)=\frac{2}{3}$ 

Ya que los elementos que cumplen A son: \{(0,1),(0,2),(1,0),(1,2),(2,0),(2,1)\}

$P(D)=\frac{2}{3}$

Ya que los elementos que cumplen D son: \{(0,1),(0,2),(1,1),(1,2),(2,1),(2,2)\}

Por tanto, los elementos que cumplen $A\cup D$ serán la unión de los elementos que cumplen A y los que cumplen D.

\[
P(A \cup D) = \frac{8}{9}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]Con 12 chicas y 4 chicos se forman al azar 4 grupos de 4 personas.
Calcular la probabididad de que haya un chico en cada grupo. Sugerencia:
usar la regla del producto.
 
\solution

\begin{expla}

$A_n$ = Un chico en el grupo n

B = Un chico en cada grupo
\end{expla}

$P(A_1)=4\cdot\frac{4}{16}\cdot\frac{12}{15}\cdot\frac{11}{14}\cdot\frac{10}{13}=0.4835$

$P(A_2|A_1)=4\cdot\frac{3}{12}\cdot\frac{9}{11}\cdot\frac{8}{10}\cdot\frac{7}{9}=0.509$

$P(A_3|A_1\cap A_2)=4\cdot\frac{2}{8}\cdot\frac{6}{7}\cdot\frac{5}{6}\cdot\frac{4}{5}=0.5714$

$P(A_4|A_1\cap A_2\cap A_3)=4\cdot\frac{1}{4}\cdot\frac{3}{3}\cdot\frac{2}{2}\cdot\frac{1}{1}=1$


\[
P(B)=\bigcap_{n=1}^4P(A_n)=P(A_1)P(A_2|A_1)P(A_3|A_2\cap A_1)P(A_4|A_1\cap A_2\cap A_3)=0.14
\]




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]Benito tiene un dado  trucado, con 6 caras  numeradas del 1 al 6. 
La probabilidad de las
distintas caras es proporcional al n\'umero de puntos inscritos en
ellas. Hallar la probabilidad de que Benito obtenga con ese dado un n\'umero
par.
\solution

\begin{expla}

Teniendo en cuenta que la suma de los números del dado es 21

\end{expla}

\[
P(par)=P(2)+P(4)+P(6)=\frac{2}{21}+\frac{4}{21}+\frac{6}{21}=\frac{12}{21}=0.57
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] En el esquema que aparece a continuaci\'on, el agua fluye  desde $A$ hacia
$B$. Hay, como se indica en el dibujo, ocho compuertas.
Independientemente unas de otras, cada compuerta est\'{a} abierta con
probabilidad $p$, $0 <p <1$. Calcular la probabilidad de que el agua llegue
 de $A$ a $B$. Calcular dicha probabilidad cuando $p = 1/3$.
% Drawing generated by LaTeX-CAD 1.8a - requires latexcad.sty
% (c) 1996 John Leis leis@usq.edu.au
$$\xymatrix{    &   & \circ\ar @{.}[dr]!U||&  &   \circ  \ar @{.}[dr]!U|| &  &  \\
A \ar[r]  & \circ  \ar @{.}[ru]!U||   \ar @{.}[rd]!U||   & &  \circ \ar @{.}[ru]!U||  \ar @{.}[rd]!U||  & & \circ \ar[r]  & B\\  
  &   & \circ \ar @{.}[ru]!U||  &  &   \circ  \ar @{.}[ru]!U||   &  &  }$$ 

Respuestas: $p^8 - 4 p^6 + 4 p^4, 289/6561$.
\solution

\begin{expla}

Se entiende que en las intersecciones el agua va en todas las direcciones.

Llamamos C al punto intermedio (a la intersección entre los dos rombos).

Para llegar hasta el punto C, tenemos que considerar la existencia de 4 puertas: $P_1, P_2, P_3, P_4$, enumeradas de izquierda a derecha y de arriba hacia abajo. Por tanto, para que el agua llegue a C, deben estar abiertas al menos $P_1$ y $P_2$, o $P_3$ y $P_4$.

Si consideramos el siguiente espacio muestral:

$\Omega = \{(0000),(0001),(0010),(0011),(0100),(0101),...,(1110),(1111)\}$

Formado por 16 elementos, en los que un 1 en la posición n indica que la puerta $P_n$ está abierta, tenemos que con esas 16 combinaciones el agua NO llegaría a C en los elementos con 4 0's (1), los elementos con 3 0,s (4) y los elementos 1010, 0101, 1001, 0110 (4). Por tanto solo nos sirven 7 combinaciones. La de todo 1's, las de 3 1's (4), 1100 y 0011.

$P(1100)=P(0011)=p^2(1-p)^2$

$P(1110)=P(1101)=P(1011)=P(0111)=p^3(1-p)$

$P(1111)=p^4$

Por tanto la probabilidad de llegar a C desde A es:

\[
P(AC)=2p^2(1-p)^2+4p^3(1-p)+p^4
\]

La probabilidad de llegar al punto B, desde el punto C es exactamente la misma que la de ir desde A hasta C.
\end{expla}
Por tanto, según la regla del producto quedaría:
\[
P(AB)=P(AC)P(CB|AC)=P(AC)^2=(2p^2(1-p)^2+4p^3(1-p)+p^4)^2=
\]
\[
=(2p^2(p^2+1-2p)+4p^3-4p^4+p^4)^2 = (2p^4 + 2p^2-4p^3-3p^4+4p^3)^2 =
\]
\[
=(-p^4+2p^2)^2=p^8-4p^6+4p^4
\]

Para $p=\frac{1}{3}$, nos queda:

\[
P(AB)=\frac{1}{3^8}-4\cdot\frac{1}{3^6}+4\cdot\frac{1}{3^4}=\frac{289}{6561}
\]



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.9  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9]En una reuni\'on hay 25 personas. Calcular la
probabilidad de que celebren su cumplea\~{n}os el mismo d\'ia del
a\~{n}o al menos dos personas. Observación: con frecuencia es m\'as f\'acil
calcular intersecciones que uniones. Sugerencia: calcular la probabilidad
del evento complementario.
\solution


A = cumpleaños mismo día al menos dos personas.

$A^c$ = cumpleaños distinto día todas las personas.

\[
P(A)=1-P(A^c)=1-\frac{365}{365}\cdot\frac{364}{365}\cdot\frac{363}{365}\cdot...\cdot\frac{341}{365}=1-0.43=0.57
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.10  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]Inclusi\'on-Exclusi\'on: Probar que 
$
P(\cup_{i=1}^n A_i)= \sum_{k=1}^n \sum_{I\subset \{1, \dots, n\}, |I| = k} (-1)^{k-1}
P(\cap_{i\in I} A_i).
$
\solution

\begin{expla}

Tenemos que probar:

\[
P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)
\]

Vamos a ver lo que significa para una colección de 3 subconjuntos $\{A_1,A_2,A_3\}$; de forma que sea fácil de ver. El término que puede llevar a la duda es el segundo sumatorio, sólo dice que escojamos todas las subcolecciones posibles de tamaño k dentro de nuestra colección $\{A_1,A_2,A_3\}$.

\begin{center}
\includegraphics[scale=0.75]{img/Dvenn4.png}
\end{center}

\[
P(A_1\cup A_2 \cup A_3)=\underbrace{(-1)^0P(A_1)+(-1)^0P(A_2)+(-1)^0P(A_3)}_{k=1}+
\]
\[
+\underbrace{(-1)^1P(A_1\cap A_2)+(-1)^1P(A_1\cap A_3)+(-1)^1P(A_2\cap A_3)}_{k=2}+\underbrace{(-1)^2P(A_1\cap A_2\cap A_3)}_{k=3}=
\]
\[
=\underbrace{P(A_1)+P(A_2)+P(A_3)}_{k=1}+\underbrace{(-P(A_1\cap A_2)-P(A_1\cap A_3)-P(A_2\cap A_3))}_{k=2}+\underbrace{P(A_1\cap A_2\cap A_3)}_{k=3}
\]

\end{expla}

\begin{itemize}
        \item Hipótesis:

        Suponemos que \[
        P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset
\{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)
        \]
        \item Base de inducción: n=2

        Para n = 2 se tiene que $$P(A_1\cup A_2)=P(A_1)+P(A_2)-P(A_1\cap
A_2)$$ Vemos que esto es cierto para cualquier conjunto.

        \item Inducción:
        Supongamos la hipótesis cierta para n. Vamos a demostrar que es
válida para n+1.
        $${P\big(\bigcup_{i=1}^{n+1}A_i\big)=P\big((\bigcup_{i=1}^nA_i)\cup A_{n+1}\big)=P\big(\bigcup_{i=1}^nA_i\big)+P(A_{n+1})-P\big((\bigcup_{i=1}^nA_i)\cap
A_{n+1}\big)}$$
        A esto hemos llegado aplicando la fórmula para n = 2.

        Por hipótesis inductiva sabemos lo que vale $P\big(\bigcup_{i=1}^nA_i\big)$.
        Ahora escribimos $P\big((\bigcup_{i=1}^nA_i)\cap
A_{n+1}\big)=P\big(\bigcup_{i=1}^n(A_i\cap A_{n+1})\big)$

        $$P(\bigcup_{i=1}^n(A_i\cap A_{n+1}))=\sum_{1\leq i_1\leq
n}P(A_{i_1}\cap A_{n+1})\quad -\sum_{1\leq i_1<i_2\leq
n}P\big((A_{i_1}\cap A_{n+1})\cap (A_{i_2}\cap A_{n+1})\big)\quad
$$$$+\sum_{1\leq i_1<i_2<i_3\leq n}P\big((A_{i_1}\cap A_{n+1})\cap
(A_{i_2}\cap A_{n+1})\cap (A_{i_3}\cap
A_{n+1})\big)\;+\;\cdots$$$$+\;(-1)^{n-1}\sum_{1\leq
i_1<\cdots<i_n\leq n}P\big((A_{i_1}\cap A_{n+1})\cap \cdots \cap
(A_{i_n}\cap A_{n+1})\big)$$

        Además tenemos que:
        $$ P\big((A_{i_1}\cap A_{n+1})\cap(A_{i_2}\cap A_{n+1})\cap \cdots
\cap (A_{i_n}\cap A_{n+1})\big)=P(A_{i_1}\cap A_{i_2}\cap\cdots\cap
A_{i_n}\cap A_{n+1})$$
        Al final nos queda que :
        $$P(\bigcup_{i=1}^{n+1}A_i) = \sum_{k=1}^{n}\sum_{I\subset
\{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i) + P(A_{n+1})$$$$ -
\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}P(A_{i_1}
\cap A_{i_2} \cap .... \cap A_{i_k} \cap A_{n+1})$$
        Y esto es igual a:
        $$\sum_{k=1}^{n+1}\sum_{I\subset
\{1,...,n+1\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)$$
\end{itemize}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.11  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[11] Emparejamientos al azar: tenemos $n$ cartas, que colocamos al azar en $n$ sobres
(en vez de cuidadosamente poner cada carta en su sobre).
Calcular la probabilidad de que alguna carta est\'a en el sobre correcto
(es decir, al menos una carta). Estimar dicha probabilidad cuando
$n\to\infty$. Sugerencia: usar Inclusi\'on-Exclusi\'on.
Observar que la probabilidad de que todas las cartas de 1 a $k$ esten en el sobre correcto
es $(n-k)!/n!$. Respuesta en el l\'{\i}mite: $1 - e^{-1}$.
\solution

\begin{expla}

Para hacernos una idea del problema consideramos n=3 y el siguiente espacio de probabilidad:

$\Omega_3=\{(123),(132),(213),(231),(312),(321)\}$

Que representa todas las posibles combinaciones (numero de carta-numero sobre, es decir, 3!) que puede haber con 3 cartas y 3 sobres. 

Según esto la probabilidad de que la carta 1 este en el sobre 1 sería: $\frac{2}{6} = \frac{2}{3\cdot 2}=\frac{1}{3}$

Sea: $A_i$ = carta n-esima en el sobre correcto.

Se observa fácilmente que: $P(A_i) = \frac{(n-1)!}{n!}$, siendo n el número de cartas totales.

Sea A = todas las cartas en el sobre correcto. Simplemente hay que observar que si la primera carta esta en el sobre correcto, para poner la segunda en su sobre tenemos una carta y un sobre menos donde elegir.

\[
P(A)=P(\bigcap_{j=1}^nA_j)=P(A_1)P(A_2|A_1)P(A_3|A_2\cap A_1)...P(A_j|\bigcap_{i=1}^{n-1}A_i)=
\]
\[
\frac{(n-1)!}{n!}\cdot\frac{(n-2)!}{(n-1)!}\cdot\frac{(n-3)!}{(n-2)!}\cdot...\cdot\frac{(n-n+1)!}{(n-n+2)!}\cdot\frac{(n-n)!}{(n-n+1)!}=\frac{1}{n!}
\]

Sea $B_k$ = todas las cartas de la 1 a la k en el sobre correcto.

\[
P(B_k)=P(\bigcap_{n=1}^kA_n)=P(A_1)P(A_2|A_1)P(A_3|A_2\cap A_1)...P(A_n|\bigcap_{i=1}^{k-1}A_i)=
\]
\[
\frac{(n-1)!}{n!}\cdot\frac{(n-2)!}{(n-1)!}\cdot\frac{(n-3)!}{(n-2)!}\cdot...\cdot\frac{(n-k+1)!}{(n-k+2)!}\cdot\frac{(n-k)!}{(n-k+1)!}=\frac{(n-k)!}{n!}
\]

Esta probabilidad es la misma cojamos las k primeras cartas o k cartas en diferentes posiciones.
\end{expla}

Vamos a usar Inclusión-Exclusión:
\[
P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)
\]

C = Al menos una carta en el sobre correcto: 

\[
P(C)=P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}\frac{(n-k)!}{n!}=\sum_{k=1}^{n}\left( (-1)^{k-1}\frac{(n-k)!}{n!}\binom{n}{k}\right)=
\]
\[
= \sum_{k=1}^{n}\left( (-1)^{k-1}\frac{(n-k)!}{n!}\cdot \frac{n!}{(n-k)!\cdot k!}\right)= \sum_{k=1}^{n}\left( (-1)^{k-1}\frac{1}{k!}\right) \stackrel{n \rightarrow \infty}{\rightarrow} 1-e^{-1}
\]
Ahora vamos a demostrar que:

\[
\sum_{k=1}^{n}\left( (-1)^{k-1}\frac{1}{k!}\right) \stackrel{n \rightarrow \infty}{\rightarrow} 1-e^{-1}
\]

\begin{proof}
\[
\sum_{k=1}^{n}\left( (-1)^{k-1}\frac{1}{k!}\right) = 1 + \sum_{k=0}^{n}\left((-1)^{k-1}\frac{1}{k!}\right) = 1 - \sum_{k=0}^{n}\left((-1)^{k}\frac{1}{k!}\right)
\]

Por otro lado, aplicando Taylor, sabemos que:

\[
f(x)=e^x=f(0)+\frac{f'(0)}{1!}(x-0)+\frac{f''(0)}{2!}(x-0)^2...
\]

Y por tanto:

\[
f(-1)=e^{-1}=1-\frac{1}{1!}+\frac{1}{2!}-\frac{1}{3!}... = \sum_{k=0}^{n}\left((-1)^{k}\frac{1}{k!}\right)
\]
\end{proof}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.12  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[12] Media o esperanza. Con los datos del problema anterior, calcular el n\'umero esperado de
emparejamientos al azar, es decir, cuantas cartas esperamos que est\'an en el sobre correcto.
Comentario: este problema es muy f\'acil.
\solution

\begin{expla}
\[
\mathbb{E}(X) = \sum_{w \in \Omega} X(w)P(w)
\]

En nuestro caso $\Omega$ esta formado por n! eventos (tal y como vimos en el ejercicio anterior), X es una variable aleatoria que vale "m", siendo m el número de cartas que están bien emparejadas con su sobre.

Así no se debe hacer: 
Llamamos $P(B_k)$ a la probabilidad de k cartas bien emparejadas. 

\[
\mathbb{E}(X) = \sum_{w \in \Omega} X(w)P(w) = \sum_{k=0}^n kP(B_k) = \sum_{k=0}^n k\cdot\frac{(n-k)!}{n!}
\]

Pero esto no vale porque $P(B_k)$ no te específica si el resto de cartas están o no emparejadas.
\end{expla}
SOLUCIÓN DEL PROFESOR:
Llamamos $A$ a la variable aleatoria que dice cuántas cartas están en el sobre correcto.

Consideramos $A_i$ a la variable aleatoria que vale 1 si la carta i está en el sobre i-ésimo y 0 en caso contrario. Y tenemos que $P(A_i=1)=\frac{1}{n}$.

Por tanto:
\[
\mathbb{E}(A)=\mathbb{E}({\sum_{i=1}^{n} A_i}) = \sum_{i=1}^n \mathbb{E}(A_i) = \sum_{i=1}^n 1 \cdot \frac{1}{n} = 1
\]




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.13  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[13] Dado $C$ con $P(C) > 0$, decimos que $A$ y $B$ son condicionalmente independientes
con respecto a $C$ si $P(A\cap B|C) =P(A|C) P(B|C)$. Probar que si  $P(B\cap C) > 0$, 
$P(A\cap B|C) =P(A|C) P(B|C)$ es equivalente a $P(A|C) = P(A|B \cap C)$. 


\solution

\begin{expla}
Partimos de: $P(A\cap B|C)=P(A|C)P(B|C)$  

Despejamos $P(A|C)$ y operamos:
\end{expla}

\[
P(A|C)=\frac{P(A\cap B|C)}{P(B|C)}=\frac{\frac{P(A\cap B\cap C)}{P(C)}}{\frac{P(B \cap C)}{P(C)}}=\frac{P(A \cap B\cap C)}{P(B\cap C)}=P(A|B\cap C)
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.14  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[14] Estudiar para $ \alpha>0 $ la convergencia en
media cuadr\'atica (es decir, en $L^2$) de la sucesi\'on $\{X_n\}_{n=1}^\infty$, sabiendo que

\[  P(X_n=n)=\frac{1}{n^\alpha}, \hspace{5mm} P(X_n=0)=1-\frac{1}{n^\alpha}.
  \]
\solution

\begin{expla}
Dado un espacio de probabilidad $(\Omega, \algb{M}, P)$, y una variable aleatoria $X_n$, tenemos que $X_n$ es también una función medible que cumple:

\[
X_n \stackrel{L_2 (n\rightarrow \infty)}{\rightarrow} X \Leftrightarrow \norm{X_n -X}_2  \rightarrow 0 \Leftrightarrow \int{\abs{X_n -X}^2dP} \rightarrow 0
\]

Tenemos que cuando $n \rightarrow \infty$, se cumple que:
\begin{itemize}
\item $P(X_n = n) \rightarrow 0$
\item $P(X_n = 0) \rightarrow 1$
\end{itemize}

Así definimos X como una variable aleatoria que cumple:

\begin{itemize}
\item $P(X = n) = 0$
\item $P(X = 0) = 1$
\end{itemize}

\end{expla}

\[
\int{\abs{X_n -X}^2dP} = \int_{\Omega}{\abs{X_n(w) -X(w)}^2dP(w)}
\]

Como X toma el valor 0 con probabilidad 1, tenemos:

\[
\int_{\Omega}\abs{X_n(w) -X(w)}^2dP(w)=\int_{\Omega}\abs{X_n(w)}^2dP(w)=\int_{\Omega}(X_n(w))^2dP(w) =
\]
\[
=\mathbb{E}((X_n)^2)=n^2\cdot\frac{1}{n^{\alpha}}+0^2\cdot(1-\frac{1}{n^{\alpha}})=\frac{n^2}{n^{\alpha}} \stackrel{n \rightarrow \infty}{\rightarrow} 0 \text{ si } \alpha>2 
\]

Por tanto la sucesión $\{X_n\}_{n=1}^\infty$ converge cuadráticamente si $\alpha > 2$.



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\newpage
\section{Hoja 2}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}

Se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
y que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra.


%%%%%%%%%%%%%%%%%%  PROBLEMA 2.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]Sea $X$ una v.a. con distribuci\'on $N(0,1)$ (normal con media 0 y varianza 1). 
Calcular el tercer momento $E(X^3)$.
\solution

\begin{expla}
Sea una variable aleatoria X, a esta se le asigna una función de distribución ($F_X(t)$) y otra de densidad ($f_X(t)$) que cumplen:

\[
F_X(t)=\int_{-\infty}^{t}f_x(t)dt
\]

En el caso de distribución N(0,1) tenemos:
\[
f(x)=\frac{e^{-t^2/2}}{\sqrt{2\pi}}
\]

Sabiendo que la esperanza se calcula como:
\[
\mathbb{E}(X)=\int_{-\infty}^{\infty}t\cdot f(t)dt
\]

Escribimos:
\end{expla}

\[
\mathbb{E}(X^3)=\int_{-\infty}^{\infty}t^3\cdot \frac{e^{-t^2/2}}{\sqrt{2\pi}}dt = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}t^3\cdot e^{-t^2/2}dt
\]

Hacemos un cambio de variable y llamamos $w=\frac{t^2}{2}$ por tanto: $\frac{dw}{dt}=t$

\[
\frac{1}{\sqrt{2\pi}}\int t^3\cdot e^{-t^2/2}dt = \frac{1}{\sqrt{2\pi}}\int 2tw\cdot e^{-w}\frac{dw}{t} = \frac{2}{\sqrt{2\pi}}\int w\cdot e^{-w}dw
\]

Ahora vamos a integrar por partes

$u=w$

$du=1$

$v=-e^{-w}$

$dv=e^{-w}$

\[
\frac{2}{\sqrt{2\pi}}\int w\cdot e^{-w}dw = \frac{2}{\sqrt{2\pi}}\left( -we^{-w}+\int e^{-w}dw \right) =\frac{2}{\sqrt{2\pi}}\left( -we^{-w}-e^{-w} \right)=
\]
\[
= \frac{-2e^{-w}}{\sqrt{2\pi}}(w+1)
\]

Y nos queda que:
\[
\mathbb{E}(X^3)=\int_{-\infty}^{\infty}t^3\cdot \frac{e^{-t^2/2}}{\sqrt{2\pi}}dt = \left.\frac{-2e^{-t^2/2}}{\sqrt{2\pi}}(t^2/2+1)\right|_{-\infty}^{\infty}=0
\]




\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 2.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]Sea $\mathcal{B} \subset \mathcal{A}$ la sub-$\sigma$-\'algebra generada por una partici\'on
$\{A_1, \dots,A_n\}$ de $\Omega$, donde todos los conjuntos de la partici\'on tienen probabilidad positiva.
Dada una v.a. $X$, demostrar que si la variable aleatoria $T(X)$ satisface

1) para todo $B\in \mathcal{B}$, $\int_B T(X) dP = \int_B X dP$, y

2) $T(X)$ es  $\mathcal{B}$-medible, 

entonces cuando $w\in A_i$, $T(X)(w)= \frac{1}{P(A_i)}\int_{A_i} X dP$.

Probar que si  $T(X)(w)$ est\'a definida mediante  $T(X)(w):= \frac{1}{P(A_i)}\int_{A_i} X dP$ cuando  
 $w\in A_i$, entonces $T(X)$ satisface las condiciones 1) y 2) enunciadas arriba.

Este ejercicio demuestra que las condiciones 1) y 2) determinan de modo \'unico la esperanza condicional,
cuando la  sub-$\sigma$-\'algebra est\'a generada por una partici\'on (el resultado tambi\'en es cierto
para  sub-$\sigma$-\'algebras arbitrarias, pero entonces es necesario definir la esperanza condicional
mediante un procedimiento distinto).
\solution

\begin{expla}

Tenemos que demostrar la doble implicación, empezamos de izquierda a derecha y luego lo demostraremos de derecha a izquierda.
\end{expla}

\begin{itemize}
\item $\Rightarrow)$ Sabemos que:
\begin{enumerate}
\item $\forall B \in \algb{B}, \int_{B}T(X)dP = \int_{B}XdP$
\item $T(X)$ es $\algb{B}-medible$
\end{enumerate}

Tenemos que llegar a: $T(X)(w)= \frac{1}{P(A_i)}\int_{A_i} X dP$ con $w \in A_i$.

Partimos del punto 1, sabemos que esa propiedad se cumple $\forall B \in \algb{B}$, por tanto, se cumplirá para todos los elementos $A_i$ de la partición:
\[
\int_{A_i}T(X)dP = \int_{A_i}XdP  \Leftrightarrow  \frac{1}{P(A_i)}\cdot\int_{A_i}T(X)dP = \frac{1}{P(A_i)}\cdot\int_{A_i}XdP
\]

la parte derecha ya es idéntica a la parte derecha de la fórmula a la que queremos llegar, por tanto, sólo nos queda demostrar que: $\frac{1}{P(A_i)}\cdot\int_{A_i}T(X)dP = T(X)(w)$

Observamos que esto es cierto si la integral no depende de T(X) y se puede sacar como constante, ya que de darse eso tendríamos:

\[
\frac{1}{P(A_i)}\cdot\int_{A_i}T(X)dP = \frac{1}{P(A_i)}\cdot T(X)\cdot\int_{A_i}1dP = \frac{P(A_i) \cdot T(X)}{P(A_i)} = T(X)
\]

(Ver definición de integral sobre una medida)

Es aquí cuando usamos que T(X) es $\algb{B}-medible$. Eso quiere decir, que la antiimagen de un medible pertenece a $\algb{B}$. Como los elementos de $\algb{B}$ son los conjuntos $A_i$, quiere decir que T(X) (que es lo mismo que T(X(w))) tiene que tener valor constante para todos los elementos $w \in A_i$. 

¿Por qué?, bueno, piensa que en $A_2$ tenemos 2 elementos: $w_1$ y $w_2$, si $T(X(w_1))=2$ y $T(X(w_2))=4$, entonces, $T^{-1}(1,3)=w_1$ que es un elemento que NO pertenece a $\algb{B}$. Si no ha quedado claro con esta explicación, se puede ver algún ejemplo más tras la demostración de 'Ley de X' de estos apuntes.

Por tanto, definimos T(X)(w)=T(X(w)) y concluimos que T(X)(w) es constante para los w pertenecientes a cada partición $A_i$. Por tanto queda demostrada la implicación primera ya que:

\[
\frac{1}{P(A_i)}\cdot\int_{A_i}T(X)(w)dP = \frac{1}{P(A_i)}\cdot T(X)(w)\cdot\int_{A_i}1dP = \frac{P(A_i) \cdot T(X)(w)}{P(A_i)} =
\]
\[
=T(X)(w)  \text{ para todo } w \in A_i
\]

\item $\Leftarrow)$ Sabemos que:

$T(X)(w)= \frac{1}{P(A_i)}\int_{A_i} X dP$ con $w \in A_i$.

Tenemos que llegar a: 
\begin{enumerate}
\item $\forall B \in \algb{B}, \int_{B}T(X)dP = \int_{B}XdP$
\item $T(X)$ es $\algb{B}$-medible
\end{enumerate}

Empezamos demostrando 2: Lo hacemos por reducción al absurdo. Suponemos que no es $\algb{B}$-medible, entonces existe al menos un conjunto $A_j$ con al menos dos elementos $w_1$ y $w_2$ tal que $T(X(w_1))=c_1$ y $T(X(w_2))=c_2$ con $c_1 \neq c_2$. Por tanto:

\[
c_1=T(X)(w_1)=\frac{1}{P(A_j)}\int_{A_j} X dP \neq \frac{1}{P(A_j)}\int_{A_j} X dP=T(X)(w_2)=c_2
\]

Lo cual es contradictorio, porque como podemos ver, T(X)(w) es constante para elementos pertenecientes al mismo $A_i$, es decir, a la misma clase de equivalencia.

Ahora demostramos 1:

\[
T(X)(w)= \frac{1}{P(A_i)}\int_{A_i}X dP \Leftrightarrow T(X)(w)\cdot P(A_i)= \int_{A_i}X dP
\]

Como T(X) es $\algb{B}$-medible, entonces es constante para todos los w pertenecientes a una misma clase de equivalencia $A_i$, por tanto podemos introducir T(X) dentro de la integral quedando:
\[
\int_{A_i}T(X)dP = \int_{A_i}XdP
\]

Sea $B \in \algb{B}$ de modo que $B=A_{i_1} \cup A_{i_2} \cup ... \cup A_{i_k}$, entonces llegamos a la propiedad 1:

\[
\forall B \in \algb{B}, \int_{B}T(X)dP = \int_{A_{i_1}}T(X)dP + \int_{A_{i_2}}T(X)dP + ... + \int_{A_{i_k}}T(X)dP =
\]
\[
= \int_{A_{i_1}}XdP + \int_{A_{i_2}}XdP + ... + \int_{A_{i_k}}XdP = \int_{B}XdP
\]



\end{itemize}




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 2.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]En un examen tipo test se plantean 5 preguntas para responder verdadero o falso. 
 Los puntos asignados a las preguntas V o F son: respuesta correcta,
1 punto, respuesta incorrecta,
- 1 punto, en blanco, 0 puntos. Valor m\'{\i}nimo del  problema: 0 puntos. Es decir, si la puntuaci\'on es
negativa se asigna un cero al problema. 

a)  Calcular la nota esperada de un alumno que responda a las 5 perguntas
de manera aleatoria, por ejemplo lanzando una moneda equilibrada 5 veces.

b) Sabiendo que el  alumno ha respondido correctamente a  la
primera pregunta, calcular la nota esperada.
\solution

\textcolor{green}{ESTA SOLUCIONADO EN EL EXAMEN FINAL DEL CURSO 2013/2014}

\begin{expla}
Se nos podría ocurrir plantear el problema de la siguiente manera:

Sea $X_i$ v.a. independientes que cumplen : $P(X_i(w)=1)=1/2=P(X_i(w)=-1) \forall i$

Por tanto, tenemos que:
\[
\mathbb{E}(X_i)=\frac{1}{2}\cdot(1)+\frac{1}{2}\cdot(-1)=0
\] 

Y si consideramos que i=5:
\[
\mathbb{E}\left(\sum_{i=1}^{5}X_i\right)=\sum_{i=1}^{5}\mathbb{E}(X_i)=0
\]

Pero esto no lo podemos plantear así ya que la nota mínima del problema es de 0. Y eso implica que las variables aleatorias $X_i$ no se pueden sumar ya que son dependientes y no expresarían la solución del problema.

Por tanto lo planteamos de otra manera: Sea el siguiente espacio muestral:

$\Omega = \{(00000), (00001), (00010),..., (11111)\}$, hay un total de $2^5=32$ combinaciones diferentes de posibles respuestas. Teniendo en cuenta que las preguntas no se dejan en blanco. Consideramos la variable aleatoria Z, asociada a ese espacio muestral.

\end{expla}

\spart
\begin{itemize}
\item 5 mal: nota del problema = 0
\item 1 bien y 4 mal: nota del problema = 0
\item 2 bien y 3 mal: nota del problema = 0
\item 3 bien y 2 mal: nota del problema = 1
\item 4 bien y 1 mal: nota del problema = 3
\item 5 bien: nota del problema = 5
\end{itemize}

\[
\mathbb{E}(Z)=\frac{\binom{5}{0}}{32}\cdot0+\frac{\binom{5}{1}}{32}\cdot0+\frac{\binom{5}{2}}{32}\cdot0+\frac{\binom{5}{3}}{32}\cdot1+\frac{\binom{5}{4}}{32}\cdot3+\frac{\binom{5}{5}}{32}\cdot5=
\]
\[
=\frac{\binom{5}{3}}{32}\cdot1+\frac{\binom{5}{4}}{32}\cdot3+\frac{\binom{5}{5}}{32}\cdot5=\frac{\frac{5!}{3!\cdot 2!}}{32}\cdot1+\frac{\frac{5!}{4!\cdot 1!}}{32}\cdot3+\frac{1}{32}\cdot5=\frac{10}{32}\cdot1+\frac{5}{32}\cdot3+\frac{1}{32}\cdot5=\frac{10}{32}+\frac{15}{32}+\frac{5}{32}=\frac{30}{32}=0.9375
\]

\spart
En esta ocasión:

A = posibles respuestas sabiendo que la primera respuesta es correcta =$\{(10000), (10001), (10010),..., (11111)\}$.


Estudiamos las posibles dentro del conjunto A, mirando los 4 digitos de mas a la derecha:
\begin{itemize}
\item 4 mal: nota del problema = 0
\item 1 bien y 3 mal: nota del problema = 0
\item 2 bien y 2 mal: nota del problema = 1
\item 3 bien y 1 mal: nota del problema = 3
\item 4 bien: nota del problema = 5
\end{itemize}

\[
\mathbb{E}(Z|A)=\frac{1}{P(A)}\left( \frac{\binom{4}{0}}{32}\cdot0+\frac{\binom{4}{1}}{32}\cdot0+\frac{\binom{4}{2}}{32}\cdot1+\frac{\binom{4}{3}}{32}\cdot3+\frac{\binom{4}{4}}{32}\cdot5 \right)=
\]

\[
= \frac{1}{\frac{1}{2}}\left( \frac{\binom{4}{2}}{32}\cdot1+\frac{\binom{4}{3}}{32}\cdot3+\frac{\binom{4}{4}}{32}\cdot5\right)=
2\cdot \left( \frac{\frac{4!}{2!\cdot 2!}}{32}\cdot1+\frac{\frac{4!}{3!\cdot 1!}}{32}\cdot3+\frac{1}{32}\cdot5 \right) =
\]
\[
= 2\cdot \left(\frac{6}{32}\cdot1+\frac{4}{32}\cdot3+\frac{1}{32}\cdot5 \right) = 2 \cdot \frac{23}{32}=\frac{46}{32}=1.4375
\]
%TODO se ha hecho por la cuenta la vieja


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 2.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]Lanzamos un dado equilibrado de 4 caras dos veces. Sea $W$ la variable aleatoria que toma como valor
el m\'aximo de los dos lanzamientos. A continuaci\'on lanzamos una moneda equilibrada $W$ veces,  usando
$S$ para denotar el n\'umero de caras obtenido. Todos los lanzamientos son independientes.

a) Hallar $E(S|W)$.
 
b)  Hallar $E(S)$.
\solution

\begin{expla}
Por definición:

\[
E_{P_B}(X)=\frac{1}{P(B)}\int_{B}X(w)dP(w)=E(X|B)
\]

W = máximo de lanzar un dado de 4 caras 2 veces.

Consideramos el siguiente espacio muestral:

$\Omega=\{(1,1),(1,2),(1,3),(1,4),(2,1),(2,2),...,(4,1),(4,2),(4,3),(4,4)\}$

Donde $W(a,b)=max(a,b)$

Sea $P(X_i)=1/2$ probabilidad de que salga cara en el lanzamiento i-ésimo de una moneda. $X(w)=1$ si sale cara y 0 si sale cruz.

Por tanto: definimos $Y_n \sim Binomial(n,\frac{1}{2})$. $Y_n$ corresponde a las caras obtenidas al lanzar una moneda n veces.

\end{expla}


\spart
De manera intuitiva, vemos que el numero de caras que pueden salir, condicionando el numero de tiradas que vamos a hacer (W), es exactamente:

$\mathbb{E}(S|W)=P(cara)\cdot(\text{número de tiradas})=\frac{W}{2}$

Y que responde a una $Binomial(W,\frac{1}{2})$, quedando:

\[
\mathbb{E}(S|W)=\sum_{i=1}^{W}\mathbb{E}(X_i)=\mathbb{E}(Y_W)=\frac{W}{2}
\] 


SOLUCIÓN DEL PROFESOR:
$X_i=1$ si en el lanzamieno i-ésimo sale cara, $X_i=0$ si no. $P(X_i=1)=\frac{1}{2}$

$S_n=X_i + X_2 +...+X_n$

Definimos $S_w$ con $w=max{Y_1,Y_2}$ siendo $Y_1=$ primer lanzamiento e $Y_2=$segundo lanzamiento.

Sabemos que $P(Y_n=m)=\frac{1}{4}$ para $m=1,2,3,4$. Por tanto $P(Y_1=i_1,Y_2=i_2)=\frac{1}{16}$ para $1\leq i_1, i_2 \leq 4$

$P(W=1)=\frac{1}{16}$, $P(W=2)=\frac{2}{16}$, $P(W=3)=\frac{6}{16}$ y $P(W=4)=\frac{7}{16}$

Por tanto:

$$
\mathbb{E}(S|W)(w) =
  \left\lbrace
  \begin{array}{l}
     \mathbb{E}(S|W=1) = \frac{1}{2} \text{ si } w \in W^{-1}(1) \\
     \mathbb{E}(S|W=2) = 1 \text{ si } w \in W^{-1}(2) \\
     \mathbb{E}(S|W=3) = \frac{3}{2} \text{ si } w \in W^{-1}(3) \\
     \mathbb{E}(S|W=4) = 2 \text{ si } w \in W^{-1}(4) \\
  \end{array}
  \right.
$$


%Vamos a intentar aplicar la fórmula que tenemos para la definición de esperanza condicionada, nos quedaría:


%\[
%\mathbb{E}(S|W)=\frac{1}{P(W)}\sum_{w\in W}X(w)P(w)
%\]

%Habría que definir el espacio muestral S, que contemplaría el número de veces que se tira la moneda y las posibilidades de obtener un número de caras para esas veces, es bastante lioso, por lo tanto lo que hacemos es declarar una variable aleatoria $S_{i,w}$ que calcula la probabilidad de obtener cara en el lanzamiento i-ésimo, sabiendo que vamos a tirar w veces.

%\[
%= \frac{1}{P(W)}\sum_{w \in W}(1\cdot P(w=cara)\cdot P(w\in W)+0\cdot P(w=cruz)\cdot P(w\in W)) = 
%\]

%\[
%= \frac{1}{P(W)}\sum_{w \in W}1\cdot P(w=cara)\cdot P(w\in W)= \frac{1}{P(W)}\sum_{w \in W}1\cdot \frac{1}{2}\cdot P(W) =  W\cdot \frac{1}{2}
%\]

%Lo que hemos querido poner es que hay que mirar también la probabilidad de que se realizan W tiradas, ya que el sumatorio se realiza sobre elementos que pertenecen al espacio muestral donde se tiene en cuenta el número de tiradas que se realizan, y que viene determinado por W que es otra variable aleatoria.

\spart
$P(W=1)=\frac{1}{16}$, $P(W=2)=\frac{2}{16}$, $P(W=3)=\frac{6}{16}$ y $P(W=4)=\frac{7}{16}$

%TODO : este ejercicio esta resuelto sin aplicar ninguna fórmula dada en teoría.

\[
\mathbb{E}(S)=\mathbb{E}(\mathbb{E}(S|W))=\sum_{i=1}^{4}P(W=i)\cdot\mathbb{E}(S|W=i)=\sum_{i=1}^{4}\left( P(W=i)\cdot i \cdot \frac{1}{2}\right)= 
\]

\[
=\frac{1}{16}\cdot1\cdot\frac{1}{2}+\frac{2}{16}\cdot2\cdot\frac{1}{2}+\frac{6}{16}\cdot3\cdot\frac{1}{2}+\frac{7}{16}\cdot4\cdot\frac{1}{2}=\frac{1}{32}+\frac{1}{8}+\frac{18}{32}+\frac{28}{32}=\frac{51}{32}
\]
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Hoja 3}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}

\textcolor{red}{Los ejercicios con con cosas en rojo están bien resueltos, eliminaré esos comentarios en cuánto solucione las dudas}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc..

Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede demostrarse que si $p < 1$ esta expresi\'on no define una norma.


%%%%%%%%%%%%%%%%%%  PROBLEMA 3.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] La desigualdad aritm\'etico-geom\'etrica 
dice que la media  aritm\'etica de un conjunto de n\'umeros no negativos es al menos tan grande
como su media  geom\'etrica, es decir, si $a_1, \dots ,a_n > 0$ satisfacen  $a_1 + \cdots  + a_n = 1$,  y  
 $x_1, \dots ,x_n \ge 0$, entonces $\Pi_{i=1}^n x_i^{a_i} \le \sum_{i=1}^n a_i x_i$.
Demostrar dicha desigualdad. Sugerencia: usar la concavidad
de log, o la convexidad de exp.
\solution

\begin{expla}
Una función convexa cumple que: $f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)$, para todo $x\leq y$. 't' y '1-t' se puede cambiar por cualquier numero finito de coeficientes que sumen 1,(la cosa es que evaluemos la función en un punto intermedio entre x e y).
Una función f es cóncava $\Leftrightarrow$ -f es convexa.
\end{expla}
Vemos que la igualdad se cumple también con el logaritmo.

\[
\prod_{i=1}^n x_i^{a_i} \leq \sum_{i=1}^{n}a_i x_i \Leftrightarrow \log\left(\prod_{i=1}^n x_i^{a_i}\right) \leq \log \left(\sum_{i=1}^{n}a_i x_i\right)
\]

Por tanto, definimos la función convexa: $f(x) = -\log(x)$, entonces, por convexidad tenemos que:
\[
-\log \left(\sum_{i=1}^{n}a_i x_i\right) \leq \sum_{i=1}^{n} a_i\left(-\log(x_i)\right) = \sum_{i=1}^{n} -\log\left((x_i)^{a_i}\right) 
\]
 
Multiplicamos por '-1' a ambos lados y nos queda:
\[
\log \left(\sum_{i=1}^{n}a_i x_i\right) \geq \sum_{i=1}^{n} \log\left((x_i)^{a_i}\right) = \log\left(\prod_{i=1}^n x_i^{a_i}\right) 
\]


\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 3.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Demostrar la siguiente desigualdad de Young: para $t, u \ge 0$, y $p,q > 1$ tales que
$1/p + 1/q =1$, tenemos $tu \le t^p/ p + u^q/ q$. Observaci\'on: \'esta es otra forma de
escribir la  desigualdad aritm\'etico-geom\'etrica para $n=2$, mediante un cambio obvio de variables.
\solution

\begin{expla}
Utilizamos $t=\sqrt[p]{w}$ y $u=\sqrt[q]{z}$
\end{expla}
Nos queda:
\[
tu\leq\frac{t^p}{p}+\frac{u^q}{q} \Leftrightarrow \sqrt[p]{w}\sqrt[q]{z} \leq \frac{\sqrt[p]{w}^p}{p} + \frac{\sqrt[q]{z}^q}{q} \Leftrightarrow w^{\frac{1}{p}} z^{\frac{1}{q}}\leq\frac{w}{p}+\frac{z}{q}
\]

Que es equivalente a la situación dele ejercicio anterior con: $a_1=\frac{1}{p}$, $a_2=\frac{1}{q}$. 

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Sean $p,q \ge 1$ exponentes conjugados, es decir, $p$ y $q$ satisfacen
$1/p + 1/q =1$ (si $p=1$, entonces $q = \infty$, y viceversa).
Dado un espacio de medida arbitrario $(\Omega, \mathcal{A}, \mu)$, demostrar la desigualdad de
H\"older: si $f\in L^p$ y $g\in L^q$, entonces $fg\in L^1$, y $\|fg\|_1 \le \|f\|_p\|g\|_q$.
Sugerencias: El caso $p=1, q=\infty$ sale directamente de las definiciones.
Para $p >1$, suponemos que  $\|f\|_p, \|g\|_q\ne 0$, y reemplazamos 
$f$ y $g$ con 
 $f/\|f\|_p$ y $g/\|g\|_q$.  Despu\'es usamos la desigualdad de Young (punto a
punto) e integramos.
\solution

\begin{expla}
Utilizamos la desigualdad de Young, que hemos demostrado en el ejercicio anterior:
\[
tu\leq\frac{t^p}{p}+\frac{u^q}{q}
\]

Cogemos:
\[
t=\frac{\abs{f}}{\norm{f}_p}
\]
\[
u=\frac{\abs{g}}{\norm{g}_q}
\]

Además usamos que:
\[
\norm{f}_p=\left( \int \abs{f}^p  \right)^\frac{1}{p}  \Leftrightarrow  \norm{f}_p^p= \int \abs{f}^p \Leftrightarrow 1 = \frac{\int \abs{f}^p}{\norm{f}_p^p}
\]
\end{expla}

Sustituyendo en la Desigualdad de Young:
\[
\frac{\abs{f}}{\norm{f}_p} \cdot \frac{\abs{g}}{\norm{g}_q} \leq \frac{1}{p}\left( \frac{\abs{f}}{\norm{f}_p} \right)^p + \frac{1}{q}\left( \frac{\abs{g}}{\norm{g}_q} \right)^q = \frac{1}{p} +  \frac{1}{q} = 1 
\]

Ahora integramos:

\[
\int \frac{\abs{f}\abs{g}}{{\norm{f}_p}{\norm{g}_q}} d\mu =
\frac{\int \abs{fg}d\mu}{{\norm{f}_p}{\norm{g}_q}} = 
\frac{{\norm{fg}_1}}{{\norm{f}_p}{\norm{g}_q}} \leq 1
\]

Y nos queda:
\[
\norm{fg}_1 \leq \norm{f}_p \norm{g}_q
\]

Para ver que $fg \in L^1$ es obvio teniendo la desigualdad anterior:
\[
\int \abs{fg} = \norm{fg}_1 \leq \norm{f}_p \norm{g}_q < \infty
\]

Para el caso p=1 y q=$\infty$. Suponemos $f\cdot g \geq 0$.
\[
\int f\cdot g \leq \int f \cdot \norm{g}_{\infty}= \norm{g}_{\infty} \int f = \norm{g}_{\infty} \norm{f}_{1}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4] Dada $f:\Omega\to \mathbb{R}$ en $L^p$,  $1 < p <  \infty$, escoger $g\in L^q$ tal que
$\int fg = \|fg\|_1 = \|f\|_p\|g\|_q$. Concluir que 
$\|f\|_p= \operatorname{sup}_{\{g\in L^q: \|g\|_q=1\}} \int fg $.
\solution
NOTA: AL RESOLVER ESTE EJERCICIO JESUS MUNARRIZ ESTUVO UNOS 40 MINUTOS DANDO COSAS DE "CULTURA GENERAL" (UN RABO CULTURA GENERAL), QUE A DÍA DE HOY, Y POSIBLEMENTE A DÍA DE MAÑANA, NO ESTARÁN EN ESTOS APUNTES.

SOLUCIÓN DEL PROFESOR:
Para buscar la g, empezamos pensando que ocurre cuando p=q, que esto sucede si p=q=2, entonces, es fácil ver que si cojo $g=f$, tenemos que:

\[
\int fg = \int f^2
\]

\[
\norm{fg}_1 = \int \abs{f^2} = \int f^2
\]

\[
\norm{f}_2 \norm{g}_2 = \left( \int \abs{f}^2 \right)^{\frac{1}{2}} \left( \int \abs{f}^2 \right)^{\frac{1}{2}} =  \int \abs{f}^2 = \int f^2
\]

Entonces, la g la buscamos de forma que dependa de f y  que pertenezca a $L^q$, es decir, de forma que al hacer la integral del valor absoluto elevado a q, sea menor que infinito, para ello, sabemos que f pertenece a $L^p$, por tanto, es fácil ver que si definimos $signo(f)=1$ si $f\geq0$ y $signo(f)=-1$ si $f<0$, y consideramos $g=signo(f)\abs{f}^\frac{p}{q}$; entonces esta g es válida:

\begin{itemize}
\item $g \in L^q$, si, ya que: $\int \abs{g}^q = \int\left( \abs{f}^{\frac{p}{q}}\right)^q = \int \abs{f}^p < \infty$
\item $\int fg = \int \abs{f} \abs{f}^{\frac{p}{q}} = \int \abs{f}^{1+\frac{p}{q}} = \int \abs{f}^p$
\item $\norm{fg}_1 = \int \abs{fg} = \int fg = \int \abs{f}^p$
\item $$\norm{f}_p\norm{g}_q = \left( \int \abs{f}^p \right)^\frac{1}{p} \left( \int \abs{g}^q \right)^\frac{1}{q} = \left( \int \abs{f}^p \right)^\frac{1}{p} \left( \int (\abs{f}^\frac{p}{q})^q \right)^\frac{1}{q} = $$
$$= \left( \int \abs{f}^p \right)^\frac{1}{p} \left( \int \abs{f}^p \right)^\frac{1}{q} = \left( \int \abs{f}^{p}\right)^{\frac{1}{p}+\frac{1}{q}} = \int \abs{f}^p$$
\end{itemize}

Por tanto se cumple que $\int fg = \|fg\|_1 = \|f\|_p\|g\|_q$.

Para concluir vemos que:
\[
\int fg = \norm{f}_p \norm{g}_q  \Leftrightarrow \int \frac{fg}{\norm{g}_q} = \norm{f}_p 
\]

Por tanto, si tenemos una h tal que $\norm{h}_q = 1$, por Hölder tendríamos que $\int fh \leq \norm{f}_p \norm{h}_q = \norm{f}_p$. Esto implica que:
\[
\norm{f}_p =  \operatorname{sup}_{\{g\in L^q: \|g\|_q=1\}} \int fg
\]

¿Por qué? Sabiendo que para un h que cumpla $\norm{h}_q = 1$, entonces $\int fh \leq \norm{f}_p$, solo falta ver que lo siguiente es falso:
\[
 \operatorname{sup}_{\{\in L^q: \|h\|_q=1\}} \int fh < \norm{f}_p
\]

Es decir, que existe un h al que eso no es menor, sino que es igual. y efectivamente existe ese h, que es:
\[
h = \frac{signo(f)\abs{f}^\frac{p}{q}}{\norm{signo(f)\abs{f}^\frac{p}{q}}_q}
\]
Tiene modulo 1 y cumple los mismos 4 puntos que cumple la g (la norma sale  de las integrales como una constante).

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Usar $\|f\|_p= \operatorname{sup}_{\{g\in L^q: \|g\|_q=1\}} \int fg $ cuando $1 < p <  \infty$
para obtener la desigualdad triangular o de Minkowski: $\|f + g\|_p \le \|f\|_p + \|g\|_p$.
Probar tambi\'en los casos (bastante obvios) $p=1$  y  $p = \infty$.
\solution

\begin{expla}

\end{expla}
Para $1<p<\infty$:

\[
\norm{f+g}_p = sup_{\{h\in L^q: \norm{h}_q=1\}}\int (f+g)h =  sup_{\{h\in L^q: \norm{h}_q=1\}}\left(\int fh + \int gh\right) \leq 
\]
\[
\leq sup_{\{h\in L^q: \norm{h}_q=1\}}\int fh +  sup_{\{h\in L^q: \norm{h}_q=1\}} \int gh = \norm{f}_p + \norm{g}_p   
\]

Para p=1:

$\norm{f+g}_1 = \int \abs{f+g} \leq \int (\abs{f} + \abs{g})=  \int \abs{f} + \int \abs{g}$

Para $p=\infty$:

$\norm{f+g}_{\infty} = supEsencial\abs{f+g} \leq supEsencial\abs{f} + supEsencial\abs{g} = \norm{f}_{\infty} + \norm{g}_{\infty}$
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6] Probar que $\|\cdot\|_p$ es una norma en $L^p$ (considerando que dos funciones
son iguales
cuando son iguales en casi todo punto).
\solution

\begin{expla}
Para comprobar que es una norma, tenemos que comprobar que cumple las tres propiedades de una norma:
\begin{enumerate}
\item Positividad: $\norm{f}\geq 0$ y $\norm{f} = 0 \Leftrightarrow f=0$.
\item Proporcionalidad: $\norm{\lambda f} = \abs{\lambda}\norm{f}$.
\item Desigualdad triangular: $\norm{u+v} \leq \norm{u}+\norm{v}$     
\end{enumerate}

Los elementos $f \in L^p$ cumplen que $\int \abs{f}^p < \infty$. 

Además: $\norm{f}_p = \left( \int \abs{f}^p\right)^\frac{1}{p}$.
\end{expla}
\begin{enumerate}
\item  $\norm{f}_p = 0 \Leftrightarrow f=0$. 

$\Leftarrow)$ es obvia.

$\Rightarrow)$ Si $f \neq 0$ en un conjunto de medida positiva, entonces $\int \abs{f}^p >0$ para $1<p<\infty$. \textcolor{red}{POR QUE LO DE 1<p<$\infty$?}.
\item $\norm{\lambda f}_p=\abs{\lambda}\norm{f}_p$, lo demostramos: 
\[
\norm{\lambda f}_p =\left( \int \abs{\lambda f}^p \right)^{\frac{1}{p}} = \left( \int \abs{\lambda}^p \abs{f}^p\right)^{\frac{1}{p}} = (\abs{\lambda}^p)^{\frac{1}{p}} \left(\int \abs{f}^p\right)^{\frac{1}{p}} = \abs{\lambda} \norm{f}_p  
\]
\item $\norm{f+g}_p \leq \norm{f}_p+\norm{g}_p$.
Demostrado en el ejercicio anterior.
\end{enumerate}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7] Probar la desigualdad de Jensen: si $X:\Omega\to I\subset \mathbb{R}$, donde
$I$ es un intervalo en $\mathbb{R}$, y $X$ tiene media finita, entonces para toda funci\'on
convexa $g:I\to \mathbb{R}$, $g(EX) \le E(g(X))$. 

Sugerencia: n\'otese que al trabajar en un espacio de probabiildad, si $L(t) := a t + b$ es una recta, entonces conmuta con la integraci\'on,
es decir,
$L(\int X(\omega) dP (\omega)) = \int L (X(\omega)) dP (\omega)$. La desigualdad de Jensen
es consecuencia de esta observaci\'on, junto con el hecho de que las funciones convexas
est\'an por encima de todas sus rectas soporte.
\solution

\begin{expla}

\end{expla}
Dado que g es convexa, podemos encontrar una función lineal $f(x)=ax+b$ tal que $f(\mathbb{E}(X)) = g(\mathbb{E}(X))$ y $f(x)\leq g(x)$ para todo $x \in \mathbb{R}$.

Es decir, existe una recta que siempre queda por debajo de g:
\begin{figure}[h]
\centering
\includegraphics[page=1,scale=0.745]{img/jensen.png}
\end{figure} 

Utilizamos la linealidad y tenemos que:
\[
\mathbb{E}(g(X)) \geq \mathbb{E}(f(X)) = \mathbb{E}(aX+b)=a\mathbb{E}(X)+b=f(\mathbb{E}(X))=g(\mathbb{E}(X))
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] Probar que si $0 < r\le s \le \infty$,
 entonces $\|f\|_{L^r(\Omega, \mathcal{A}, P)}\le \|f\|_{L^s(\Omega, \mathcal{A}, P)}$, luego $L^s(\Omega, \mathcal{A}, P) \subset L^r(\Omega, \mathcal{A}, P)$ (sugerencia, usar Jensen o H\"older). Demostrar que si el espacio tiene medida infinita, esta
inclusi\'on puede fallar.
\solution

\begin{expla}
Tenemos que ver que para $0 <r\leq s \leq \infty$ se cumple que $\norm{f}_r \leq \norm{f}_s$, es decir:

\[
\left( \int \abs{f}^r dP \right)^{\frac{1}{r}} \leq \left( \int \abs{f}^s dP \right)^{\frac{1}{s}}
\]

%Por Hölder tenemos que: $\|fg\|_1 \leq \|f\|_p\|g\|_q$, que en nuestro caso sería: $\|f^2\|_1 \leq \|f\|_p\|f\|_q$


\end{expla}
Dos formas:
\begin{enumerate}
\item Usamos Jensen:

Jensen dice que para X variable aleatoria y $\phi$ función convexa pertenecientes a $L^1$, se tiene que:
\[
\phi(\mathbb{E}(X)) \leq \mathbb{E}(\phi(X))
\]

\textcolor{red}{preguntar: Jensen especifica que las funciones deben pertenecer a $L^1$, hemos visto que si pertenecen a $L^r$ con $r>1$, entonces pertenecen a $L^1$, pero qué pasa si $r<1$??}

Tenemos que:

\[
\norm{f}_{L^r(\Omega, \algb{M}, P)} = \norm{f}_r= \left( \int_{\Omega} \abs{f}^r dP \right)^{\frac{1}{r}}
\]

Para simplificar la notación, omitiremos a partir de ahora $\Omega$ y dP. Usamos como función convexa $g(t)=\abs{t}^{\frac{s}{r}}$.

Por tanto tenemos que:
\[
g(\mathbb{E}(\abs{f}^r)) = \left(\int \abs{f}^r\right)^{\frac{s}{r}} \leq\int \left( \abs{f}^r \right)^{\frac{s}{r}} = \mathbb{E}(g(\abs{f}^r)) 
\]

Elevamos a $\frac{1}{s}$ a izquierda y derecha y obtenemos (desarrollamos desde el centro azul, hacia la izquierda y hacia la derecha):
\[
\norm{f}_r = \left(\int \abs{f}^r\right)^{\frac{1}{r}} = \textcolor{blue}{\left(\int \abs{f}^r\right)^{\frac{s}{rs}} \leq \left(\int \abs{f}^{\frac{rs}{r}}\right)^{\frac{1}{s}}} = \left(\int \abs{f}^s\right)^{\frac{1}{s}} = \norm{f}_s
\]

\textcolor{red}{OTRA VEZ SOLO VALIDO SI S ES MENOR QUE INFINITO?}

\item Usando Hölder:
si $f\in L^p$ y $g\in L^q$, entonces $fg\in L^1$, y $\|fg\|_1 \le \|f\|_p\|g\|_q$

Definimos la función $g=1$, que pertenece a $L^q$. Y cogemos $0<r<s<\infty$. Suponemos que $f \in L^s$ y $f\geq 0$, entonces tenemos que:
\[
\abs{f}=\abs{f}\cdot 1  \Leftrightarrow f^r = f^r \cdot 1
\]

Entonces queremos acotar $\int f^r \cdot 1$. Vemos que $f^r \in L^{\frac{s}{r}}$ y que $1 \in L^{\frac{s}{r}'}$  siendo $\frac{s}{r}'$ el conjugado de $\frac{s}{r}$, es decir cumple que ($\frac{1}{r/s}+\frac{1}{s/r} = 1$). Vamos a demostrar que  $f^r \in L^{\frac{s}{r}}$:
\[
 \int \abs{f^r}^{\frac{s}{r}} =  \int \abs f^s < \infty \text{ porque } f\in L^s
\]

Y ahora sí usamos Hölder:

\[
\int f^r = \int f^r \cdot 1 = \norm{f^r \cdot 1}_1 \leq \norm{f^r}_{\frac{s}{r}}\norm{1}_{\frac{s}{r}'}=\left( \int(f^r)^{\frac{s}{r}} \right)^{\frac{r}{s}} \cdot 1 = \left( \int f^s \right)^{\frac{r}{s}}
\]

Elevamos a izquierda y derecha por $\frac{1}{r}$ y obtenemos que:
\[
\left( \int f^r \right)^{\frac{1}{r}} \leq \left( \int f^s \right)^{\frac{1}{s}}
\]

Que es lo que queríamos demostrar.
\end{enumerate}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section{Hoja 4}

\textcolor{red}{A LA ESPERA DE SER CORREGIDA}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc..

Recordatorio: si $0 < p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. 

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Ejemplos sobre la f\'ormula del cambio de variables.

\vskip .3 cm

a) Sea $X: [0,1)\to \mathbb{R}$ la variable aleatoria $X(w) := 2 w (\operatorname{mod}(1)$. Es decir,
si $w\in [0, 1/2)$, entonces  $X(w) = 2 w$, mientras que si  $w\in [1/2, 1)$, entonces  $X(w) = 2 w - 1$.
En  $[0,1)$ tenemos la probabilidad uniforme con los conjuntos de Borel (o Lebesgue, para este problema
da igual). Hallar la funci\'on de distribuci\'on $F_X$,  y  $P_X$, la probabilidad imagen o ley de $X$.
 Decidir razonadamente si $X$ es una v.a. continua, y en caso de respuesta afirmativa, hallar su funci\'on
de densidad $f_X$.

\vskip .3 cm

b) Sea $Y: [0,1]^2\to \mathbb{R}$ la variable aleatoria $Y(x,y) := y$.
En  $[0,1]^2$ tenemos la probabilidad uniforme con los conjuntos de Borel (o Lebesgue, para este problema
da igual). Hallar la funci\'on de distribuci\'on $F_Y$,  y  $P_Y$, la probabilidad imagen o ley de $Y$.
 Decidir razonadamente si $Y$ es una v.a. continua, y en caso de respuesta afirmativa, hallar su funci\'on
de densidad $f_Y$.

\vskip .3 cm


c) Decidir razonadamente si para toda funci\'on de Borel acotada $g:\mathbb{R}\to \mathbb{R}$,
$$\int_{ [0,1)} g(X(w)) dw = \int_{ [0,1]^2} g(Y(x,y)) dx dy.$$

\solution

\begin{expla}

\end{expla}

\spart 
Vemos que la variable aleatoria X(w) es una función que crece con pendiente 2 en el intervalo $[0,1/2)$ y en $[1/2,1)$:

\begin{figure}[h]
\centering
\includegraphics[page=1,scale=0.745]{img/f.png}
\end{figure} 

Por tanto, se ve que tiene la misma probabilidad de valer cualquier valor entre 0 y 1, la función de distribución será simplemente:

$$
F_x(x) =
  \left\lbrace
  \begin{array}{l}
     0 \text{ si } x < 0\\
     x \text{ si } x \in [0,1) \\
     1 \text{ si } x \geq 1 \\
  \end{array}
  \right.
$$

Por su parte, la ley de x, o $P_x$, recordemos que es:
$P_x(B) = P(X^{-1}(B)) = P(\{w \in \Omega : X(w) \in B\})=P(X \in B)$. En este caso coincide con la medida de Lebesgue de B.

Por ejemplo, si B es el intervalo $(1/2, 3/4)$, vemos que $X(w)$ toma esos valores en un exactamente $3/4-1/2=1/4$ de los valores de w.

Por tanto: $P_x(B)=\mu(B)$ (siendo $\mu$ la medida de Lebesgue).

Ahora queremos ver que X es una variable aleatoria continua. Nos remitimos a la definición de variable aleatoria continua:

X es una variable aleatoria continua $\Leftrightarrow$ $P_x$ es absolutamente continua con respecto a la medida de Lebesgue.

Recordemos lo que significa que una medida $\mu$ sea absolutamente continua con respecto a otra $\nu$:  $\mu << \nu$ si $\mu(A)=0 \Rightarrow \nu(A)=0$

Tenemos que ver que $P_x$ es absolutamente continua con respecto a la medida de Lebesgue, lo que es obvio ya que $P_x(B)=\mu(B)$. La llamaremos $P_{x1}$ para diferenciarla del apartado B.

Por último definimos la función de densidad $f_x(x)$.

$$
f_x(x) =
  \left\lbrace
  \begin{array}{l}
     0 \text{ si } x < 0\\
     1 \text{ si } x \in [0,1) \\
     0 \text{ si } x \geq 1 \\
  \end{array}
  \right.
$$
\spart
Obtenemos el mismo resultado que en el caso anterior para $F_x$ y para $f(x)$.

También para $P_x$, que lo llamaremos $P_{x2}(B)=\mu(B)$.


\spart 

Tenemos que demostrar que:

$$\int_{ [0,1)} g(X(w)) dw = \int_{ [0,1]^2} g(Y(x,y)) dx dy.$$

Vamos a utilizar la fórmula del cambio de variable:
\[
\int_{\Omega}g(X(w))dP(w)=\int_{\mathbb{R}}g(x)dP_X(x)
\]

Para aplicar la fórmula del cambio de variable, lo único que tenemos que hacer es cambiar los límites de integración por los valores en los que se mueven $X(w)$ e $Y(x,y)$, y cambiar la medida sobre la que se integra.

Empezamos por el primer término, y le aplicamos la fórmula del cambio de variable:
$$\int_{ [0,1)} g(X(w)) dw = \int_{ [0,1)} g(2w \mod{1}) dw= \int_{ [0,1)} g(x) dP_{x1}(x)$$

Ahora cogemos el segundo término:
$$\int_{ [0,1]^2} g(Y(x,y)) dx dy = \int_{ [0,1)} g(y) dP_{x2}(y)$$

Que son iguales ya que $P_{x2}(x) = P_{x1}(x)$.

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 4.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Demostrar la desigualdad de Jensen condicional: si $X:\Omega\to I\subset \mathbb{R}$, donde
$I$ es un intervalo en $\mathbb{R}$, $g:I\to \mathbb{R}$ es convexa, y $X, g(X) \in L^1$, entonces
 $g(E(X|\mathcal{B})) \le E(g(X)|\mathcal{B})$. 

\solution

\begin{expla}
Mismo razonamiento que en el ejercicio 7 de la hoja 3.
\end{expla}

Dado que g es convexa, podemos encontrar una función lineal $f(x)=ax+b$ tal que $f(\mathbb{E}(X|\algb{B})) = g(\mathbb{E}(X|\algb{B}))$ y $f(x)\leq g(x)$ para todo $x \in \mathbb{R}$.

Es decir, existe una recta que siempre queda por debajo de g:
\begin{figure}[h]
\centering
\includegraphics[page=1,scale=0.745]{img/jensen.png}
\end{figure} 

Utilizamos la linealidad y tenemos que:
\[
\mathbb{E}(g(X)|\algb{B}) \geq \mathbb{E}(f(X)|\algb{B}) = \mathbb{E}(aX+b|\algb{B})=a\mathbb{E}(X|\algb{B})+b=f(\mathbb{E}(X|\algb{B}))=g(\mathbb{E}(X|\algb{B}))
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Sean $p,q \ge 1$ exponentes conjugados, es decir, $p$ y $q$ satisfacen
$1/p + 1/q =1$ (si $p=1$, entonces $q = \infty$, y viceversa).
Dado un espacio de probabilidad  $(\Omega, \mathcal{A}, P)$, enunciar y probar la desigualdad condicional de 
H\"older. Sugerencia: usar la desigualdad de H\"older.
\solution

\begin{expla}
Tenemos que demostrar que:
\[
\mathbb{E}(\abs{XY}|\algb{B}) \leq \mathbb{E}(\abs{X}^p|\algb{B})^{\frac{1}{p}} \mathbb{E}(\abs{Y}^q|\algb{B})^{\frac{1}{q}}
\]

Por ser esperanzas condicionadas, tenemos que cumplen que son $\algb{B}$-medibles y que:
$\forall B \in \algb{B}$, $\int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$. Además estas dos propiedad definen la esperanza condicionada de modo único.

\end{expla}
Empezamos aplicando dicha propiedad sobre $\mathbb{E}(\abs{XY}|\algb{B})$:
\[
\int_{\Omega}\mathbb{E}(\abs{XY}|\algb{B}) = \int_{\Omega}\abs{XY} = \norm{XY} \leq \norm{X}_p \norm{Y}_q = \left( \int_{\Omega} \abs{X}^p \right)^{\frac{1}{p}} \left( \int_{\Omega} \abs{X}^q \right)^{\frac{1}{q}} =  
\]

\[
= \left( \int_{\Omega} \mathbb{E}(\abs{X}^p|\algb{B}) \right)^{\frac{1}{p}} \left( \int_{\Omega} \mathbb{E}(\abs{X}^q|\algb{B}) \right)^{\frac{1}{q}}
\]

Por tanto, como esta propiedad define la esperanza de modo único, vemos que:

\[
\mathbb{E}(\abs{XY}|\algb{B}) \leq \mathbb{E}(\abs{X}^p|\algb{B})^{\frac{1}{p}} \mathbb{E}(\abs{Y}^q|\algb{B})^{\frac{1}{q}}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4] Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una martingala adaptada a la filtraci\'on
$\{\mathcal{A}_n\}_{n=0}^{\infty}$, entonces para todo $n>0$ tenemos 
$EX_n = EX_0$. 
Enunciar los resultados an\'alogos para sub y supermartingalas.
\solution

\begin{expla}
Nos basamos en dos propiedades:
\begin{enumerate}
\item Por parte de la martingala usamos que:
$$\forall_n, \mathbb{E}(X_{n+1}|\algb{M}_n)=X_n$$
\item Por parte de la probabilidad condicionada usamos que:
$$\forall B \in \algb{B}, \int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$$
\end{enumerate}
\end{expla}
Aplicamos 1) sobre $X_n$ en el primer paso, 2) en el tercero (combinado con que $\Omega$ pertenece a $\algb{A_n}$), y definición de esperanza en el segundo y en el cuarto.
\[
\mathbb{E}(X_n) =\mathbb{E}(\mathbb{E}(X_{n+1}|\algb{A}_n)) = \int_{\Omega} \mathbb{E}(X_{n+1}|\algb{A}_n) dP = \int_{\Omega} X_{n+1}dP = \mathbb{E}(X_{n+1})
\]

Siguiendo este procedimiento n veces de derecha a izquierda llegamos a lo que nos piden: $\mathbb{E}(X_n) = \mathbb{E}(X_0)$.

Para las submartingalas y supermartingalas es lo mismo solo que poniendo $\leq$ y $\geq$ respectivamente.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una martingala adaptada a la filtraci\'on
$\{\mathcal{A}_n\}_{n=0}^{\infty}$, entonces para todo $m, n\ge 0$ tenemos
$E(X_{n + m}|\mathcal{A}_n) = X_n$. Enunciar los resultados an\'alogos para sub y supermartingalas.
\solution

\begin{expla}
Nos basamos en dos propiedades:
\begin{enumerate}
\item Por parte de la martingala usamos que:
$$\forall_n, \mathbb{E}(X_{n+1}|\algb{M}_n)=X_n$$
\item Propiedad de la torre (la 7) de la probabilidad condicionada:
Sean $\algb{C} \subset \algb{B} \subset \algb{M}$ $\salgb$s, si $X \in L^ 1(\algb{M})$ entonces:
\begin{itemize}
\item $\mathbb{E}(\mathbb{E}(X|\algb{C})|\algb{B}) = \mathbb{E}(X|\algb{C})$.
\item $\mathbb{E}(\mathbb{E}(X|\algb{B})|\algb{C}) = \mathbb{E}(X|\algb{C})$.
\end{itemize}
\end{enumerate}
\end{expla}
En el caso de las martingalas tenemos que:
$\algb{A}_n \subset \algb{A}_{n+1} \subset \algb{A}_{n+2}...$

\[
X_n = \mathbb{E}(X_{n+1}|\algb{A}_n) = \mathbb{E}(\mathbb{E}(X_{n+2}|\algb{A}_{n+1})|\algb{A}_n) = \mathbb{E}(X_{n+2}|\algb{A}_n) =
\]

\[
= \mathbb{E}(\mathbb{E}(X_{n+3}|\algb{A}_{n+2})|\algb{A}_n) =
\mathbb{E}(X_{n+3}|\algb{A}_n) = ... = \mathbb{E}(X_{n+m}|\algb{A}_n)
\]

Para las submartingalas y supermartingalas es lo mismo solo que poniendo $\leq$ y $\geq$ respectivamente.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6] C\'omo ganar un euro con probabilidad 1, en un juego justo: la estrat\'egia del doble o nada.
Apostamos un euro a que sale cara. Si sale cara nos plantamos, si sale cruz apostamos
2 euros a que sale cara. Si sale cara nos plantamos, si sale cruz apostamos
4 euros a que sale cara. Etc.. Demostrar que la  estrat\'egia anterior gana un euro
 con probabilidad 1.
\solution

\begin{expla}

\end{expla}
$Y_n = 2^n$ si sale cara en el lanzamiento n

$Y_n=-2^n$ si sale cruz en el lanzamiento n

$P(Y_n=2^n)=P(Y_n=-2^n)=\frac{1}{2}$

Definimos $\Omega = \{1,-1\}\times\{2,-2\}\times\{4,-4\}\times...\times\{2^n,-2^n\}$ $\forall n\in \mathbb{N}$. Así, un $w \in \Omega$ sería por ejemplo $w=(-1,-2,-4,8)$, donde $Y_3(w)=-11$.

Vemos que las $Y_n$ son variables aleatorias independientes. Así, las ganancias hasta la partida n son $S_n=\sum_{i=1}^{n}Y_i$.

Vamos a tomar la $\salgb$ generada por cada $Y_n$, por tanto: $\algb{M}_1 = \sigma(Y_1)=\sigma\left\{\{1\}\times\prod_{i=1}^{\infty}\{2^i,-2^i\},\{-1\}\times\prod_{i=1}^{\infty}\{2^i,-2^i\} \right\}$

Y denotamos $\sigma(Y_1, Y_2,...,Y_n)=\sigma(\{Y_1\}\times\{Y_2\}\times ... \times\{Y_n\}\times \prod_{j=n+1}^{\infty}\{2^{n},-2^{n}\} : Y_n= 1 \text{ o } -1, k=1,...,n )$.

En ese ejemplo (con $w=(-1,-2,-4,8)$) tenemos que $S_3(w)=-7$ y $S_4(w)=1$.

%Tenemos que $S=\{S_n, \sigma(Y_1, Y_2,...,Y_n)\}$ es una martingala.

Ahora añadimos la restricción de dejar de apostar cuando salga cara una vez, y llamamos a esta variable aleatoria $T_n$. Tenemos que $T_n \rightarrow 1$ cuando $n \rightarrow \infty$. 

Esto es así porque $$P(T_n=1) = 1-P(T_n\neq 1) = 1 - P(T_1\neq1 \cap T_2\neq1 \cap ... \cap T_{n-1}\neq1) =$$ $$= 1 - P(Y_1=-1 \cap Y_2=-2 \cap ... \cap Y_{n-1}=-2^{n-2}) = 1-\left(\frac{1}{2}\right)^n$$

Y  $1-\left(\frac{1}{2}\right)^n \rightarrow 0$ cuando $n \rightarrow \infty$.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Hoja 5}

\textcolor{red}{A LA ESPERA DE SER CORREGIDA}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc.


Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede
demostrarse que si $p < 1$ esta expresi\'on no define una norma.

%%%%%%%%%%%%%%%%%%  PROBLEMA 5.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Sea $X: [0,1) \to [0,1)$ la identidad $X(w) = w$, donde a $[0,1)$ se le asigna la
probabilidad uniforme (en este caso, la medida de Lebesgue). Sea  $\mathcal{A}_n := \sigma([0,1/2^n), [1/2^n,2/2^n) , \dots,
[ (2^n - 1)/2^n, 1))$ la $ \sigma$-\'algebra generada por los intervalos di\'adicos $ [j/2^n,(j + 1)/2^n)$, $j = 1\dots, n-1$.
Calcular $E(X|\mathcal{A}_n)$. Decidir razonadamente si la sucesi\'on de v.a. $ \{E(X|\mathcal{A}_n)\}_{n=0}^{\infty}$
converge (y en caso de respuesta afirmativa, determinar a qu\'e) en alguno de los siguientes sentidos:
a) uniformemtente, b) en $L^p$, determinando para que valores de $p$ hay convergencia, c) en casi todo punto,
d) en medida.
\solution

\begin{expla}
Vemos que $\algb{M}_n$ esta generada a partir de una partición, de elementos disjuntos, y por tanto:
\[
\mathbb{E}(X|\algb{M}_n)(w)=\frac{1}{P(A_i)} \int_{A_i}X(w)dP(w)
\]

Definimos $\mathbb{E}(X|\algb{M}_1)(w)$

$$
\mathbb{E}(X|\algb{M}_1)(w) =
  \left\lbrace
  \begin{array}{l}
      \frac{1}{P([0, 1/2))}\int_{[0,1/2)}wdP(w)= \left.2 \frac{w^2}{2}\right|_0^{1/2} = 1/4  \\
     ... 3/4  \\
  \end{array}
  \right.
$$

Y $\mathbb{E}(X|\algb{M}_2)(w)$:
$$
\mathbb{E}(X|\algb{M}_2)(w) =
  \left\lbrace
  \begin{array}{l}
      \frac{1}{8}\\
      \frac{3}{8}\\
      \frac{5}{8}\\
      \frac{7}{8}\\
  \end{array}
  \right.
$$

\end{expla}

Obtenemos que:
$$
\mathbb{E}(X|\algb{M}_n)(w) =
  \left\lbrace
  \begin{array}{l}
      \frac{1}{2^{n+1}}\\
      \frac{3}{2^{n+1}}\\
      ...\\
      \frac{2^{n+1}-1}{2^{n+1}}\\
  \end{array}
  \right.
$$

Por tanto, observamos, que  $\mathbb{E}(X|\algb{M}_n)(w)$ converge a X(w)=w.

\begin{enumerate}
\item Convergencia uniforme: Dado $f_n:X \rightarrow \mathbb{R}$ y $f_n \rightarrow f$.  $\forall \epsilon >0$ $\exists n_0(\epsilon)$ tal que: $\abs{f_n(x)-f(x)}<\epsilon$ $\forall n\geq n_0(\epsilon) y \forall x \in X.$

Lo cual es cierto ya que al aumentar el valor de n, no implica que mas puntos 'w' se aproximan, sino que para cada valor de n que aumentamos, todos los puntos 'w' se aproximan un poco más al límite.

\item Convergencia en $L^p$:
\[
f_n \stackrel{L_p (n\rightarrow \infty)}{\rightarrow} g \Leftrightarrow \norm{f_n -g}_p  \rightarrow 0 \Leftrightarrow \int{\abs{f_n -g}^pd\mu} \rightarrow 0
\]

Por tanto, tenemos que ver que:
\[
\int \abs{\mathbb{E}(X|\algb{M}_n)(w)-X(w)| }^p \rightarrow 0
\]

Como ambos sumandos son números positivos podemos aplicar lo siguiente: si $a>0$ y $b>0$, entonces $\abs{a-b} \leq \abs{a}- \abs{b}$ o bien $\abs{a-b} \leq \abs{b}- \abs{a}$. Por tanto, sin perdida de generalidad, en este caso nos da igual cual de los dos casos se cumplen, por tanto escribimos:

\[
\int \abs{\mathbb{E}(X|\algb{M}_n)(w)-X(w)| }^p \leq \int \abs{\mathbb{E}(X|\algb{M}_n)(w)}^p- \int \abs{X(w)}^p 
\]

\item Convergencia en casi todo punto:

\item Convergencia en medida:

%Pero por definición de esperanza condicionada y por ser X $\algb{M}_n$-medible, tenemos que:
%\[
%\mathbb{E}(X|\algb{M}_n) = X
%\]

%Elevamos a p e integramos y ya tenemos la igualdad que buscamos
\end{enumerate}

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 5.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]  Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una submartingala, y
$0  <  r\le s\le \infty$, entonces $\|X\|_r  \le \|X\|_s$, donde 
$\|X\|_p := \sup_{n\in \mathbb{N}} \|X_n\|_{p}$. 
\solution

\begin{expla}
utilizamos el ejercicio 3.8, es decir, que si $0<r\leq s \leq \infty$, entonces $\norm{f}_r \leq \norm{f}_s$.

La norma de una martingala se define:
\[
\norm{X}_p = \sup_n \norm{X_n}_p
\]
\end{expla}

Tenemos que ver que:
\[
\sup_n \norm{X_n}_r \leq \sup_n \norm{X_n}_s
\]

Entonces, existirá un p tal que:

\[
\sup_n \norm{X_n}_r  = \norm{X_p}_r \leq \norm{X_p}_s \leq \sup_n \norm{X_n}_s
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 5.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una martingala, y $ \|X\|_s <\infty$, donde 
$1\le s < \infty$, entonces  $Y := \{|X_n|^s\}_{n=0}^{\infty}$  es una submartingala
en $L^1$. 
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 5.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]  Sea $\{X_n\}_{n=0}^{\infty}$ una sucesi\'on de v.a.i.i.d., tales que 
$P(X_i = 1)= P(X_i = -1) = 1/2$, y sea $S_n := \sum_{i=0}^n X_i$.
Vimos en clase que $S := \{S_n\}_{n=0}^{\infty}$  es una martingala adaptada a la filtraci\'on
$\{\mathcal{A}_n\}_{n=0}^{\infty}$, donde $\mathcal{A}_n := \sigma(X_0, \dots, X_n)$.
Sea $c > 0$. 
Calcular $\lim_{n\to \infty} P( -c < S_n < c)$. Sugerencia: se puede usar el Teorema del L\'{\i}mite Central,
visto en Probabilidad I.
 Decidir razonadamente si la martingala $S = \{S_n\}_{n=0}^{\infty}$ 
 est\'a en $L^1$, es decir, si $\sup_n \|S_n\|_1 < \infty$. Decidir razonadamente
si $S_n$ converge en casi todo punto a una funci\'on $S_ \infty$ tal que
$P(|S_\infty| = \infty) = 0$. 

\solution

\begin{expla}
El teorema central del límite dice que:
\[
\lim_{n \rightarrow \infty} P(\sqrt{n}(S_n - \mu) \leq \sigma t) = N(0,1)(t)
\]

Que para valores grandes de n, es equivalente a decir:
\[
\lim_{n \rightarrow \infty} P(\sqrt{n}(S_n - \mu) \leq t) = N(0,1)(\frac{t}{\sigma})
\]

Sabemos que $\bar{X} = \frac{1}{n} \sum_{i=1}^{n}X_i$

\end{expla}

En nuestro caso tenemos que:

\[
\bar{S_n} = \frac{1}{n} \sum_{i=1}^{n}S_n
\]

\[
\mu = \mathbb{E}(S_n)= 0
\]

\[
\sigma^2 = V(S_n) = \mathbb{E}(S_n^2)-\mathbb{E}(S_n)^2 = 1 - 0 = \frac{1}{n} 
\]

\[
\sigma = \frac{1}{\sqrt{n}}
\]


Vamos a normalizar la probabilidad, y dejarlo de la forma:
\[
P(\frac{\sqrt{n}(\bar{S_n}-\mu)}{\sigma} \leq z)
\]

En nuestro caso, nos debería quedar:
\[
P(n\bar{S_n}\leq z)
\]


Así, usamos que $S_n = n\bar{S_n}$, y nos queda que:
\[
P(-c<S_n<c)=P(-c<n\bar{S_n}<c)=P(n\bar{S_n}>-c)-P(n\bar{S_n}>c)=
\]

\[
=1-2P(n\bar{S_n}>c) \rightarrow 1-N(0,1)(c)
\]



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Hoja 6}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc.

Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede
demostrarse que si $p < 1$ esta expresi\'on no define una norma.


%%%%%%%%%%%%%%%%%%  PROBLEMA 6.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Sea $\{\mathcal{A}_n\}_{n=0}^{\infty}$ una filtraci\'on de sub-$\sigma$-\'algebras de $\mathcal{A}$,
y sea $Y\in L^1(\Omega, \mathcal{A}, P)$. Demostrar que $X_n:=  E(Y|\mathcal{A}_n)$ define una
martingala en $L^1$. Decidir razonadamente si $X_n$ converge a $Y$ en $L^1$.

\solution

\begin{expla}
Vamos a ver que $X_n:=  E(Y|\mathcal{A}_n)$ cumple las tres propiedades de una martingala en $L^1$:
\begin{enumerate}
\item $\forall n$, $X_n \in L^1(\Omega, \algb{A}. P)$.
\item $\forall n$, $X_n$ es $\algb{A}_n$-medible.
\item $\forall_n$, $\mathbb{E}(X_{n+1}|\algb{A}_n)=X_n$ c.s.

\end{enumerate}

\end{expla}
\begin{enumerate}
\item Tenemos que ver que $\mathbb{E}(Y|\mathcal{A}_n) \in L^1$, es decir, que $\int_{\Omega} \abs{\mathbb{E}(Y|\mathcal{A}_n)} dP < \infty$.

Tenemos una propiedad de las esperanzas condicionadas (la tercera) que dice que: $\abs{\mathbb{E}(X|\algb{B})} \leq \mathbb{E}(\abs{X}|\algb{B})$.

En nuestro caso, aplicando esta propiedad y la segunda propiedad de la definición de esperanza condicionada ($\forall B \in \algb{B}$, $\int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$), nos queda que:

\[
\int_{\Omega} \abs{\mathbb{E}(Y|\mathcal{A}_n)} \leq  \int_{\Omega} \mathbb{E}(\abs{Y}|\mathcal{A}_n) = \int_{\Omega} \abs{Y} < \infty
\]

%\textcolor{red}{si $\int f=\int g$, entonces $\int \abs{f} = \int \abs{g}$ ??? Si pero no al revés}
%Si no queremos suponer que Y es positiva, llamamos f=$\mathbb{E}(Y|\mathcal{A}_n)$ y g=Y

%$$\int f=\int g \Leftrightarrow \abs{\int f} = \abs{\int g} \Leftrightarrow \abs{\int f} \leq \int \abs{f} $$
\item Tenemos que ver que $\mathbb{E}(Y|\mathcal{A}_n)$ es $\algb{A}_n$-medible. Lo cual es cierto por definición de esperanza condicionada.

\item Tenemos que ver que $\mathbb{E}(\mathbb{E}(Y|\mathcal{A}_{n+1})|\mathcal{A}_n) = \mathbb{E}(Y|\mathcal{A}_n)$

Aplicamos la regla de la torre (séptima propiedad de una esperanza condicionada), puesto que $\algb{A}_n \subset \algb{A}_{n+1}$, y sale directamente:

\[
\mathbb{E}(\mathbb{E}(Y|\mathcal{A}_{n+1})|\mathcal{A}_n) = \mathbb{E}(Y|\mathcal{A}_n)
\]

\item Y además, esta martingala debe pertenecer a $L^1$, es decir: $\sup_n \norm{X_n}_1 = \sup_n \norm{\mathbb{E}(Y|\mathcal{A}_n)}_1 < \infty$. Que es obvio ya que $X_n \in L^1$.

\end{enumerate}
Ahora vamos a ver razonadamente si $X_n$ converge a Y en $L^1$. Por definición de convergencia tenemos que ver que:
\[
X_n \stackrel{L_1 (n\rightarrow \infty)}{\rightarrow} Y \Leftrightarrow \norm{X_n -Y}_1  \rightarrow 0 \Leftrightarrow \int{\abs{X_n -Y}dP} \rightarrow 0 \Leftrightarrow \int{\abs{\mathbb{E}(Y|\mathcal{A}_n) -Y}dP} \rightarrow 0
\]


Y tenemos que:
\[
\int \abs{\mathbb{E}(Y|\algb{A}_n) - Y}dP \stackrel{n \rightarrow \infty}{\rightarrow} \int \abs{\mathbb{E}(Y|\algb{A}) - Y}dP = 0
\]

Esto se cumple porque Y es $\algb{A}$-medible y se cumple que $\mathbb{E}(Y|\algb{A}) = Y$. 





\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 6.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Sea $X := \{X_n\}_{n=0}^{\infty}$   una martingala en $L^2$. Probar que los incrementos 
son ortogonales, donde los incrementos se definen como $Y_0 := X_0$, y 
para $n > 0$, $Y_n := X_n - X_{n-1}$. Es decir, demostrar que si $j\ne k$, entonces
$(Y_j, Y_k) = \int Y_j Y_k = 0$. 
\solution

\begin{expla}
Para quien no lo recuerde, ya demostramos en el ejercicio 4 de la hoja 4 que dada una martingala  $X := \{X_n\}_{n=0}^{\infty}$, se cumple que $\mathbb{E}(X_n) = \mathbb{E}(X_{n+1})$. Lo demostramos así:

Nos basamos en dos propiedades:
\begin{enumerate}
\item Por parte de la martingala usamos que:
$$\forall_n, \mathbb{E}(X_{n+1}|\algb{M}_n)=X_n$$
\item Por parte de la probabilidad condicionada usamos que:
$$\forall B \in \algb{B}, \int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$$
\end{enumerate}

Aplicamos 1) sobre $X_n$ en el primer paso, 2) en el tercero (combinado con que $\Omega$ pertenece a $\algb{A_n}$), y definición de esperanza en el segundo y en el cuarto.
\[
\mathbb{E}(X_n) =\mathbb{E}(\mathbb{E}(X_{n+1}|\algb{A}_n)) = \int_{\Omega} \mathbb{E}(X_{n+1}|\algb{A}_n) dP = \int_{\Omega} X_{n+1}dP = \mathbb{E}(X_{n+1})
\]
\end{expla}


Por tanto, dado $X_j$ y $X_k$ con j,k cualesquiera, tenemos que:

$$\mathbb{E}(X_j)=\mathbb{E}(X_k) \Leftrightarrow \mathbb{E}(X_j)- \mathbb{E}(X_k)=0 \Leftrightarrow \int X_j - \int X_k = 0 \Leftrightarrow \int X_j - X_k =0 \Leftrightarrow \mathbb{E}(X_j - X_k) = 0$$

Para k<j, sabemos que por ser X una martingala $\algb{A}_k \subset \algb{A}_j$ y que $X_k$ es $\algb{A}_k$-medible y por tanto también $\algb{A}_j$-medible. Entonces para $j\neq k$, $Y_k = X_k - X_{k-1}$ es también  $\algb{A}_j$-medible por ser resta de funciones  $\algb{A}_j$-medibles.

Dicho esto, podemos aplicar que: $\mathbb{E}(Y_j | \algb{A}_j) = Y_j$ y que $\mathbb{E}(Y_k | \algb{A}_j) = Y_k$ (solo cierto si $Y_k$ es $\algb{A}_j$-medible). 

También aplicamos la propiedad 6 de esperanzas condicionadas:
Si X es $\algb{B}$-medible, entonces $\mathbb{E}(XY|\algb{B}) = X\mathbb{E}(Y|\algb{B})$, suponiendo que $XY,Y \in L^1$.

Nos queda que:

\[
\mathbb{E}(Y_j Y_k) = \mathbb{E}(\mathbb{E}(Y_j|\algb{A}_j) Y_k) = \mathbb{E}(\mathbb{E}(X_j-X_{j-1}|\algb{A}_j) Y_k) = Y_k\mathbb{E}(\mathbb{E}(X_j-X_{j-1}|\algb{A}_j)) = 
\]
\[
= \mathbb{E}(X_k-X_{k-1}|\algb{A}_j) \mathbb{E}(\mathbb{E}(X_j-X_{j-1}|\algb{A}_j)) = 0
\]

Ya que por definición de esperanza condicionada de $\mathbb{E}(X_k-X_{k-1}|\algb{A}_j)$, tenemos que:
\[
\int_{\Omega} \mathbb{E}(X_k-X_{k-1}|\algb{A}_j) = \int_{\Omega} X_k-X_{k-1} = \mathbb{E}(X_k-X_{k-1}) = 0
\]










\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 6.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Probar el Teorema de Pit\'agoras generalizado: Si $u_1, \dots, u_n$ son vectores ortogonales
en un espacio vectorial con un producto interno (escalar o herm\'{\i}tico), entonces
$\|\sum_1^n u_i\|^2 = \sum_1^n \| u_i\|^2$, donde  $\| w\|^2:= (w,w)$.

\solution

\begin{expla}
Lo resolvemos por inducción.
\end{expla}

\begin{itemize}
\item Paso base: n=2, tenemos que $u \bot v \Rightarrow <u,v>=0$.

\[
\norm{u+v}^2=<u+v,u+v>=<u,u>+<u,v>+<v,u>+<v,v> = \norm{u}^2 + \norm{v}^2
\]

\item Paso inductivo: Supongo que $\norm{\sum_{i=1}^n u_i}^2 = \sum_{i=1}^{n} \norm{u_i}^2 $ con $u_i \bot u_j$ para $i\neq j$, por tanto $<u_i,u_j>=0$ para $i \neq j$.

\[
\norm{\sum_{i=1}^{n+1}}^2 = <\sum_{i=1}^{n+1} u_i, \sum_{i=1}^{n+1} u_i> = <u_{n+1}+\sum_{i=1}^n u_i,u_{n+1}+\sum_{i=1}^n u_i> =
\]
\[
= <u_{n+1},u_{n+1}>+<u_{n+1},\sum_{i=1}^n u_i> + <\sum_{i=1}^n u_i, u_{n+1}>+<\sum_{i=1}^n u_i,\sum_{i=1}^n u_i>=
\]
\[
=\norm{u_{n+1}}^2 + \norm{\sum_{i=1}^{n} u_i} \Rightarrow \|\sum_1^n u_i\|^2 = \sum_1^n \| u_i\|^2
\]
\end{itemize}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Hoja 7}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc..


%%%%%%%%%%%%%%%%%%  PROBLEMA 7.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Lanzamos una moneda lastrada, con probabilidad de sacar cara
igual a $3/5$. Si sale cara lanzamos un dado equilibrado con cuatro
caras numeradas del 1 al 4, y si sale cruz lanzamos un dado
equilibrado con seis caras numeradas del 1 al 6. Sea $Y$ el n\'umero
obtenido. Denotando $X=1$ si sale cara, $X=0$ si sale cruz, hallar
a) $E(Y|X)$, y  b) $E(Y)$.
\solution

\begin{expla}

\end{expla}

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 7.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Tenemos un dado equilibrado con 6 caras numeradas del 1 al 6. Lanzamos la  primera vez y 
apuntamos el n\'umero $x$ obtenido. A continuaci\'on, 
lanzamos el dado $x$ veces y sumamos los valores $y_i$ obtenidos: $s_x = y_1 + \cdots  + y_x$.
Hallar $E(X)$, $E(Y_i)$, y $E(S_X)$, donde $X$ es la variable aleatoria ``resultado del primer lanzamiento", $Y_i$ el resultado del lanzamiento
$ 1 + i$, y $S_X = \sum_{k = 1}^X Y_k$. Se asume que todas
las tiradas son independientes. 
Determinar la relaci\'on entre las tres medias.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] a) Enunciar la Ley Fuerte de Los Grandes N\'umeros.

b) Consideramos $(0,1)$ con la medida de Lebesgue, Escribimos $w\in (0,1)$ usando
la expansi\'on decimal habitual. Definimos $X_n(w)$ como el n\'umero de sietes que aparecen
en las $n$ primeras posiciones de la expansi\'on decimal de $w$ (por ejemplo, $X_3(0.777) = 3,
 x_3(0.12345) = 0$.  Decidir razonadamente si $\lim_n n^{-1}X_n$ converge casi seguro, y
 en caso de respuesta afirmativa, hallar el l\'{\i}mite.
 
 c) Decidir razonadamente si 
 $$
 \lim_n \frac{X_n - 10^{-1}n}{n^{2/3}}
 $$ 
 converge casi seguro, y
 en caso de respuesta afirmativa, hallar el l\'{\i}mite.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]  Calcular las funciones generatrices de probabilidad, de momentos y las funciones caracter\'{\i}sticas de $X$ cuando $X$ es
a) Bernoulli$(p)$,
b) Binomial$(n,p)$,
c) Poisson$(\lambda)$. Usar unicidad y la propiedad multiplicativa para conclu\'{\i}r que la suma de v.a. independientes
con distribuci\'on Poisson$(\lambda_i)$, $i = 1, \dots, n$, es Poisson$(\sum_1^n \lambda_i)$.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Calcular la funci\'on generatriz de momentos y la funci\'on caracter\'{\i}stica de $X\sim \operatorname{Uniforme}(0,1)$. 
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]Calcular la funci\'on generatriz de momentos y la funci\'on caracter\'{\i}stica de $Z\sim N(0,1)$. 
 
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]Probar que la funci\'on caracter\'{\i}stica de $X$ con media 0 y varianza $1$
satisface
$$
\phi_X(t) = 1 - \frac{1}{2} t^2   + o(t^2)
$$ 
cuando $t\to 0$. Sugerencia: usar la expansi\'on de Taylor de $e^{ir}$, donde $r$ es real.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] Demostrar el TCL usando los ejercicios anteriores, unicidad y la propiedad multiplicativa de las funciones
caracter\'{\i}sticas, y el Teorema de Continuidad de L\'evy-Cram\'er.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.9  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9]Para $n=1, 2, 3, \dots$, sea  $\{X_{n,m}\}_{m=1}^{n}$  una configuraci\'on triangular de v.a., tales que las variables en cada fila son independientes, sea $S_{n} := \sum_{i=1}^n X_{n,i}$, y sean $0 < a < b < 1$. Supongamos que las
variables en la fila $n$ son Bernoulli($p_n$), es decir, $P(X_n = 1) = p_n$, $P(X_n = 0) = 1 - p_n$.
Usar el Teorema de Berry-Esseen para demostrar que si $a \le p_n \le b$ para todo $n$,
entonces $(S_n - n p_n)/\sqrt{n p_n (1 - p_n)}$ converge en distribuci\'on a $Z\sim N(0,1)$.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.10  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]Probar que si $\{X_n\}_{n=0}^{\infty}$  es una sucesi\'on de v.a.i.i.d. en $L^2$, se cumple
la condici\'on de Lindeberg. Sugerencia, usar el Teorema de la Convergencia Dominada de
Lebesgue.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.11  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[11] Para $n=1, 2, 3, \dots$, sea  $\{X_{n, m}\}_{m=1}^{n}$  una configuraci\'on triangular de v.a., tales que las variables en cada fila son independientes, y sea $S_{n} := \sum_{i=1}^n X_{n, i}$.

a) Enunciar la condici\'on de Lindeberg.

b) Suponiendo que las variables en la fila $n$ son Bernoulli($p_n$), con
$p_n = n^{- 1/2}$,
decidir razonadamente
si 
$(S_n - n p_n)/\sqrt{n p_n (1 - p_n)}$ converge en distribuci\'on a $Z\sim N(0,1)$.

c) Sea $s \in (0,1)$. Decidir razonadamente qu\'e sucede si en el apartado anterior, en
vez de  $p_n = n^{ - 1/2}$ tenemos $p_n = n^{ - s}$.

d) Decidir razonadamente qu\'e sucede si en el apartado b), en
vez de  $p_n = n^{- 1/2}$ tenemos $p_n = n^{ - 1}$.
Sugerencia: recordar la Ley de los N\'umeros Peque\~nos.
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.12  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[12] Probar que la condici\'on de Lyapunov implica la condici\'on de Lindeberg. 
La condici\'on de Lyapunov nos dice que existe un $\delta > 0$ tal que
$\lim_{n\to \infty} Lyap(n, \delta) = 0$, donde 
$$
Lyap(n, \delta) := \frac{1}{s_n^{2 + \delta}} \sum_{k=1}^n  E|X_k - \mu_k|^{2 + \delta} .
$$
\solution

\begin{expla}

\end{expla}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

