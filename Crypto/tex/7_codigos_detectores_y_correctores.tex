\chapter{Códigos detectores y correctores de errores}
\section{Motivación}
\begin{example}
Supongamos que tenemos un robot en Marte (o en cualquier lugar muy lejano) y queremos comunicarnos con él, indicándole en cuál de las 4 direcciones naturales queremos que se mueva:
\[\left\{ \begin{array}{l}
N = 00 \\ E = 01 \\ O = 10 \\ S = 11
\end{array}\right.\]

En cualquier momento puede ocurrir un problema que dificulte la recepción del mensje por parte del robot, ya sea el ruido del propio canal de comunicación, o la aparición de un rayo cósmico que modifique algún bit.

El problema de esto es que, en cuanto el robot reciba un par de órdenes perturbadas, habremos perdido el control del mismo, pues no sabremos cuál es su posición.

La \textbf{clave del problema} es que, puesto que todas las palabras de longitud dos son válidas en nuestro código, el robot no puede darse cuenta de si el mensaje ha sido perturbado o no.

Para solventar este problema, una opción es emplear cadenas más largas para codificar cada movimiento, a fin de poder encontrar errores posibles. Por ejemplo, podemos duplicar el código que representa cada posible movimiento.

\[\left\{ \begin{array}{l}
N = 0000 \\ E = 0101 \\ O = 1010 \\ S = 1111
\end{array}\right.\]

Si enviamos al robot el mensaje $0101$ y el robot recibe $0111$, el robot detecta que ha habido un error, pero no puede corregirlo con exactitud.

Por tanto, lo que hará el robot será corregir el mensaje minimizando la probabilidad de fallo, es decir, trata de maximizar la probabilidad condicionada:
\[\mathbb{P} \left(\text{han enviado m} | \text{he recibido }\tilde{m} \right)\]

Esta probabilidad dependerá de las características del canal. De forma general tenemos que un bit muta con probabilidad pequeña $p$ y, consecuentemente, llega de forma adecuada con probabilidad $p-1$. Como hipótesis consideramos que $p$ es pequeño (desde luego $p<\frac{1}{2}$).

Para simplificar el problema supondremos, además, que los errores en los bits son independientes.

Puesto que $p$ es pequeño, es más probable que un bit llegue de forma correcta a que llegue mutado. Por tanto, a la hora de maximizar la probabilidad descrita anteriormente, necesitamos encontrar el mensaje original al que se puede llegar mediante el menor número de cambios posibles.

Así tenemos que, dado $\tilde{m} = 0111$
\[\left\{ \begin{array}{l}
m = N= 0000 \text{ con 3 cambios}\\ m= E = 0101 \text{ con 1 cambio }\\ m= O = 1010 \text{ con 3 cambios}\\ m =S = 1111 \text{ con 1 cambio}
\end{array}\right.\]

Por tanto, lo más probable es que la orden fuese desplazarse hacia el este o el sur, pero no sabemos cuál de las dos es.

\obs Si se produce un sólo error sabemos detectarlo, aunque no corregirlo. No obstante, si se producen dos errores es posible que no seamos capaces ni de detectarlo.

A fin de poder corregir un único error lo que haremos será repetir el código tres veces, de modo que tenemos

\[\left\{ \begin{array}{l}
N = 000000 \\ E = 010101 \\ O = 101010 \\ S = 111111
\end{array}\right.\]

Con este método, seremos capaces de detectar la presencia de hasta dos bits erróneos.

Supongamos que recibimos el mensaje $\tilde{m}=011101$, mirando, como antes, el código original más probable tenemos:

\[\left\{ \begin{array}{l}
m = N= 000000 \text{ con 4 cambios}\\ m= E = 010101 \text{ con 1 cambio }\\ m= O = 101010 \text{ con 5 cambios}\\ m =S = 111111 \text{ con 2 cambios}
\end{array}\right.\]

En esta ocasión tenemos un \textbf{vecino más próximo} que será el mensaje $m=E$.

Además, si conseguimos garantizar que sólo habrá un error en el mensaje recibido, podemos estar seguros de que hemos encontrado el mensaje original, pues no hay forma de llegar hasta el mensaje que hemos recibido modificando un único bit de alguno de los otros mensajes posibles.

\obs Si se produce un error podemos corregirlo, dos errores podremos detectarlos, a partir de 3 errores es posible que no los detectemos.

Con este mismo esquema, supongamos que recibimos la palabra $\tilde{m} = 11101$. En este caso, como en el anterior, es sencillo detectar que se ha producido un error.

Si empleamos el mismo método que en el caso anterior, tendríamos que el vecino más cercano es $m=S$ pero estamos restingiéndonos a asumir que sólo hay un error. No obstante, el mensaje podría haber sido $m=E$ y encontrarnos con dos errores.

\obs O asumimos que sólo hay un error y lo corregimos o aceptamos que puede haber dos y no podemos corregirlo

\end{example}

Veamos ahora cómo generalizar este tipo de problemas y la solución al mismo.

\begin{defn}[Código q-nario]
Dado un alfabeto con $q$ elementos, $F_q$, un código (de bloques) q-nario de longitud $n$ es $\algb{C}\in F_q^n$.

Los números de teléfono de españa, por ejemplo, constituyen un código 10-ario de longitud 9: $F_{10}^9$
\end{defn}

De manera general, siendo $|\algb{C}| = M$ querremos encontrar un código con $M$ grande, para tener más ordenes posibles para el robot, con un $n$ pequeño, para que no tenga que enviar demasiada información y que permita corregir muchos errores.

\obs Todo a la vez es imposible. \textbf{La teoría de códigos trata de hacerlo lo mejor posible.}

\section{Distancia de Hamming}
\begin{defn}[Distancia de Hamming]
Dados $x=(x_1,...,x_n)$, $y=(y_1,...,y_n) \in F^n$ se define la \textbf{Distancia de Hamming} como
\[d(x,y)=\#\{i: x_i \neq y_i\}\]
\end{defn}

\begin{prop}[$d$ es una distancia]
Puesto que somos matemáticos debemos convencernos de la que la función $d$ definida es realmente una distancia. Para ello comprobamos que se cumplen las propiedades de una distancia:
\begin{enumerate}
\item
\[d(x,y) \geq 0\]

Puesto que estamos mirando el cardinal de un conjunto, trivialmente esta propiedad es cierta

\item
\[d(x,y)=0 \iff x=y\]

Puesto que estamos contando el número de bits diferentes, si el número es 0, evidentemente las dos cadenas son la misma

\item
\[d(x,y)=d(y,x)\]

Esta propiedad es trivial puesto que el operador $\neq$ es conmutativo

\item
\[d(x,y) \leq d(x,z)+d(z,y)\]

Tenemos que comparar el número de cambios necesarios para convertir $x$ en $z$ y luego $z$ en $y$. Si tomamos $d=d_H$, donde $d_H$ es la distancia mínima, la desigualdad es obvia.

\begin{defn}[Distancia mínima]
Dado un código $\algb{C}\in F_q^n$, su \textbf{distancia mínima} se define como:
\[d(\algb{C}) = \min\{d(x,y) \ t.q. \ \ x,y \in \algb{C}, \ x \neq y\}\]
\end{defn}
\end{enumerate}
\end{prop}

Si utilizamos $\algb{C} \subset F_q^n$ con $|G|=M$ para transmitir $M$ mensajes, lo que queremos es que las palabras empleadas esten lo más espaciadas posible, de modo que un pequeño cambio en un elemento de una palabra no nos haga caer en otra palabra válida. Es decir, queremos que la distancia mínima del código, es decir, $d(\algb{C})$ sea grande.

\begin{theorem}
Sea $\algbC \subset F_q^n$ un código con $d(\algb{C})=d$, podemos \textbf{detectar} hasta $d-1$ errores
\end{theorem}
\begin{proof}
El procedimiento estándar para detectar errores es:
\begin{enumerate}
\item Recibo $\tilde{x} \in F_q^n$

\item Si $\tilde{x}  \in \algb{C}$ seguimos trabajando.

\item Si $\tilde{x} \notin  \algb{C}$ \textbf{PITO}.
\end{enumerate}

Si el número de errores es menor o igual que $d-1$, es imposible que desde un $x\in \algb{C}$ llegemos a otro $\tilde{x} \in \algb{C}$, puesto que, por definición, $d(x,\tilde{x})\geq d$.

Es decir, el algoritmo descrito nunca podrá confundirse si tenemos un máximo de $d-1$ errores.
\end{proof}

\begin{theorem}
Sea $\algb{C} \subset F_q^n$ un código con $d(\algb{C})=d$, podemos \textbf{corregir} hasta $\floor{\frac{d-1}{2}}$
\end{theorem}
\begin{proof}
Desde el punto de vista geométrico (o topológico) podemos ver que, dados dos mensajes $x,y \in \algb{C}$
\[B\left(x,\floor{\frac{d-1}{2}}\right) \cap B\left(y,\floor{\frac{d-1}{2}}\right) = \emptyset\]

Apoyándonos en esta idea, veamos la demostración del teorema.

El procedimiento para corregir errores es:
\begin{enumerate}
\item Dado un mensaje $\tilde{x} \notin \algb{C}$, a fin de corregir el error buscamos $x_0$ tal que
\[d(\tilde{x},x_0)=\min\{d(x,\tilde{x}): \ x\in \algb{C}\}\]

\item Si tenemos un empate, tomamos uno al azar.
\end{enumerate}

Este algoritmo podría fallar si y sólo si
\[\exists x_0 \in \algb{C} \tq d(x_0,\tilde{x}) \leq d(x,\tilde{x})\]
Pero en este caso tenemos que
\[d(x,x_0) \leq d(x,\tilde{x})+d(\tilde{x},x_0) \leq 2d(x,\tilde{x}) \leq 2 \floor{\frac{d-1}{2}} \leq d-1 < d\]
con lo que llegamos a una contradicción, pues por definición $d(x,x_0)\geq d$.
\end{proof}

\begin{example}
Sea el lenguaje

\[\left\{ \begin{array}{l}
N = 000000 \\ E = 010101 \\ O = 101010 \\ S = 111111
\end{array}\right.\]

Supongamos que se envía ``S=111111'' y recibimos ``010111'' y empleamos un algoritmo de corrección de errores.

En este caso veremos que el mensaje más cercano es ``E=010101'' que implica un único error en el mensaje.

No obstante, en este caso el resultado es incorrecto pues en realidad se produjeron 2 errores.
\end{example}

Con lo visto hasta ahora, es evidente que la forma de conseguir detectar/corregir más errores pasa por emplear códigos con distancias mínimas grandes.

En general, podemos crear códigos con distancias mínimas tan grandes como queramos.

\begin{defn}[Codigo binario de repetición de longitud n]
Este código consiste en repetir cada bit n veces.
\[R_{2,n}=\{\underbrace{111....111}_{n\text{ veces}}, \underbrace{000....000}_{n\text{ veces}}\}\]

Si $F_q=\{a_1,...,a_q\}$ es un alfabeto, el \textbf{código de repetición de longitud n} es:
\[R_{2,n}=\{\underbrace{a_1a_1a_1....a_1a_1a_1}_{n\text{ veces}}, ... , \underbrace{a_qa_qa_q....a_qa_qa_q}_{n\text{ veces}}\}\]
\end{defn}

Podemos ver que las distancias mínimas para un código de repetición de longitud n será siempre $d(R_q,n)=n$.

El problema de este método es que acaba siendo muy costoso cuando queramos hacer $n$ grande.

Para poder clasificar y estudiar la calidad de los diferentes códigos debemos definir algunos términos:

\begin{defn}[Distancia relativa]
Sea $\algb{C} \subset F_q^n$ con $d(\algb{C})=d$ y $|\algb{C}|=M=q^l$ definimos la \textbf{distancia relativa} como:
\[δ=\frac{d}{n}\]
\end{defn}

\begin{defn}[Tasa de transmisión]
Sea $\algb{C} \subset F_q^n$ con $d(\algb{C})=d$ y $|\algb{C}|=M=q^l$ definimos la \textbf{tasa de transmisión} como:
\[R=\frac{l}{n}\]
\end{defn}

\begin{example}
En un código de repetición siempre tendremos una distancia relativa igual a 1 y una tasa de transmisión inversamente proporcional a $n$. Es decir:
\[δ=1, \ \ R = \frac{1}{n}\]
\end{example}

\obs Dado un código con más de una palabra\footnote{Si sólo tenemos una palabra es un caso trivial}, es decir $|\algb{C}|>1$ tenemos:
\[0 < δ,R \leq 1\]

En general intentaremos que tanto δ cono $R$ sean cercanos a 1. No obstante, es imposible que ambas tiendan a 1 a la vez por lo que se tratará de optimizar, dependiendo de la situación concreta en que vayamos a trabajar.

\begin{defn}[Código lineal]
Diremos que un código $\algb{C}$ es un \textbf{código lineal} si:
\begin{enumerate}
\item $F_q$ es un cuerpo [o un anillo conmutativo si uno sabe lo que es un módulo sobre un anillo]

\item $\algb{C}$ es un espacio vectorial de $F_q^n$ [o es un submódulo si $F_q^n$ es un anillo]
\end{enumerate}
\end{defn}

\begin{example}
Todos los códigos que hemos visto hasta ahora son \textbf{códigos lineales}.

\[\algb{C}_2 = \{\vx\in F_2^4: x_1-x_3 = 0 , \  x_2-x_3 = 0\}\]
\[\algb{C}_3 = \{\vx \in F_3^6: x_1=x_3=x_5, \ x_2=x_4=x_6\}\]
\end{example}

\obs Sea $|\algb{C}|=M=q^l$ tenemos que $l=k=dim_{F_q}(\algb{C})$

\begin{defn}[Peso]
Si $F_q$ es un cuerpo tal que $x\in F_q^n$ llamamos \textbf{peso de x} a:
\[w(x) = \# \{i: \ x_i \neq 0\}\]
Es decir, dada una palabra nos cuenta la cantidad de 1s.
\end{defn}

\begin{defn}[Peso mínimo de $\algb{C}$]
\[w(\algb{C}) = \min \{ w(x) : \ \ x \in \algb{C}, x \neq 0\}\]
\end{defn}

\begin{lemma}
Si $\algb{C}$ es un código lineal tenemos que:
\begin{enumerate}
\item \[\forall x \neq y \in \algb{C} \ \ d(x,y)=w(x-y)\]
\item \[d(\algb{C})=w(\algb{C})\]
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}
\item
\[d(x,y) = \# \{i \tq x_i\neq y_i \} = \# \{i \tq x_i - y_i \neq 0\} = w(x-y)\]
\item Vamos a demostrar el lemma viendo que los siguientes dos conjuntos son iguales. Es decir, vamos a probar que
\[\{d(x,y): \ x,y \in \algb{C}, x\neq y\} = \{w(z): \ z \in \algb{C}, \ z \neq 0\}\]
Esta igualdad es sencilla de ver comprobando los dos contenidos.

Para cada elemento del conjunto de la izquierda, tenemos uno en la derecha.
\[\forall x,y \in \algbC \ \ x \neq y \  d(x,y) = w(x-y)=w(z)\]

y viceversa
\[w(z) = d(z,0), \ 0 \in \algb{C}\]

Sólo nos queda la duda de si $0 \in \algb{C}$ o no, pero es trivial ver que si puesto que estamos trabajando con un código lineal.
\end{enumerate}
\end{proof}

\obs La ventaja de trabajar con el peso mínimo es que si $|\algb{C}|=M$ encontrar $d(\algb{C})$ requiere a priori ${M \choose 2}=\frac{M^2-M}{2}$ comparaciones mientras que calcular $w(\algb{C})$ sólo requiere $M-1$.

A partir de este momento vamos a empezar a mejorar los códigos que hemos empleado hasta ahora, pues sólo hemos visto códigos triviales basados en repetición.

\begin{example}
Consideramos que tenemos al mismo robot de siempre con las mismas 4 órdenes posibles. En esta ocasión vamos a definir:
\[\algb{C}_4 = \{x \in F_2^3 \ x_1+x_2+x_3 = 0 \}=\{000,011,101,110\}\]

La ecuación en las $x_i$ se denomina \concept{comprobador global de paridad}.

Con el código que acabamos de defiir tenemos:
\[d(\algb{C})=w(\algb{C}_4)=2=d(\algb{C})\]

Por tanto, este código es capaz de detectar un error, igual que hacía $\algb{C}_2$ pero a un precio menor puesto que la longitud de $\algb{C}_2$ es 4 mientras que la de $\algb{C}_4$ es 2.

Podemos ver también que ambos códigos tienen dimensión 2 lo que nos da unas tasas de transmisión y distancias relativas de:
\[R(\algb{C}_2)=\frac{2}{4}=\frac{1}{2}, δ(\algb{C}_2)=\frac{1}{2} \ \ \ R(\algb{C}_4)=\frac{2}{3}, δ(\algb{C}_4)=\frac{2}{3}\]
\end{example}

No obstante, el código $\algb{C}_4$ que acabamos de ver no es capaz de corregir errores. A fin de mejorar esta característica veamos un nuevo código.

\begin{example}
Consideramos esta vez el código
\[\algb{C}_3 = \left\{x\in F_2^6 \tq  \begin{array}{l}
x_1+x_3 = 0 \\
x_1+x_5 = 0 \\
x_2+x_4 = 0 \\
x_2+x_6 = 0
\end{array}\right\}\]

Lo que nos da una dimensión $k= 2$ y una distancia mínima $d=3$.
\end{example}

¿Podemos hacerlo mejor y obtener un código aún más corto?

\begin{example}
Sea el código:
\[\algb{C}_5 = \left\{x\in F_2^6 \tq  \begin{array}{l}
x_1+x_2+x_3= 0 \\
x_1+x_4 = 0 \\
x_2+x_5 = 0
\end{array}\right\}=\left\{\begin{array}{l}
00000\\
01101\\
10110\\
11011
\end{array}\right\}\]

Es sencillo ver que este código tiene dimensión $k=2$ y una distancia mínima $d=3$.

Por tanto tenemos característica iguales a las del ejemplo anterior salvo que con mensajes de longitud menor, por lo que claramente hemos mejorado el código del anterior ejemplo.
\end{example}

Hasta ahora hemos visto un algoritmo de detección/corrección de errores capaz de detectar hasta $d-1$ errores y de corregir hasta $\frac{d-1}{2}$, siendo $d$ la distancia mínima del código empleado.

No obstante este algoritmo puede mejorarse como nos garantiza el siguiente teorema.

\begin{theorem}\label{theorem:Codigo_detector_corrector}
Sea $\algb{C}$ un código con $d(\algb{C}) = d=2r+s+1$, entonces \textbf{existe un algoritmo} que corrige hasta $r$ errores \textbf{y simultáneamente} detecta hasta $r+s$.
\end{theorem}
\begin{proof}
La mejor forma de demostrar este teorema es definir el algoritmo que satisface los requisitos esperados.
\begin{enumerate}
\item Recibo $\tilde{x} \in F_q^n$
\item Calculo $d_0 = \min\{d(x,\tilde{x}) \forall x \in \algb{C}\}$
\item Si $d_0 \leq r$ leo $x_0 \in \algb{C}$ tal que $d(x_0,\tilde{x})=d_0$
\item En otro caso \textbf{PITO}
\end{enumerate}

Este algoritmo esta asumiendo que $x_0$ calculado en el paso 3 es único.

Esta asunción es totalmente legítima puesto que, en caso contrario:
\[x_1,x_0 \in \algb{C} \text{ con } d(x_o,\tilde{x})=d_0=d(x_1,\tilde{x}) \implies d(x_0,x_1)\leq 2d_0 \leq 2r < d\]
con lo que llegamos a una contradicción, pues $d$ es la \textbf{distancia mínima}

Por tanto, queda claro que hasta $r$ errores serán corregidos puesto que si se han producido menos de $r$ errores hemos encontrado un candidato que corrige esos errores: $x_0$.

Ahora nos queda comprobar que este algoritmo es capaz de detectar hasta $r+s$ errores.

Supongamos que se ha enviado el mensaje $x$ pero hemos recibido $\tilde{x}$ con $r+1\leq d(x,\tilde{x})\leq r+s$.

Lo que debemos garantizar en este caso es que el algoritmo no va a leer un $x_0 \neq x$, cosa que ocurriría si caemos en el paso 3.

Pero si existe $x_0 \in \algb{C}$ con $d(x_0,\tilde{x})\leq r$ entonces
\[d(x_0,x)\leq d(x_0,\tilde{x})+d(\tilde{x},x) \leq r+r+s < d\]

\end{proof}
%dice que detectamos hasta r errores pero nosotros no sabemos cuantos errores se han producido. Que ocurre si realmente hay r+k errores?
\begin{example}
Conisderamos el código:
\[\algb{C}_6=\algb{C}_4 \times \algb{C}_4 \times \algb{C}_4 \in F_2^4\]

Que nos permite escribir las posibles órdenes como:
\[\left\{ \begin{array}{l}
N = 00000000 \\ E = 011011011 \\ O = 101101101 \\ S = 110110110
\end{array}\right.\]
donde tenemos $n=9,\ \ dim=k=2, \ \ d=6$.

En este caso tendremos:
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Algoritmo & Corrige hasta & Detecta hasta \\
\hline
\hline
Algoritmo de corrección & 0 & -\\
\hline
$6 = 2 \times 2 + 1 + 1$ & 2 & 3 \\
\hline
$6=2\times 1 + 3 + 1 $ & 1 & 4 \\
\hline
Algoritmo de detección-corrección & 0 & 5 \\
\hline
\end{tabular}
\end{center}

Supongamos que recibimos $\tilde{x}=011111111$, según el algoritmo empleado tomaremos una decisión diferente.

En este caso tenemos:

\begin{minipage}{0.3\textwidth}
\[\left\{ \begin{array}{l}
d(\tilde{x},N) = 8 \\
d(\tilde{x},E) = 2 \\
d(\tilde{x},O) = 4 \\
d(\tilde{x},S) = 4
\end{array}\right.\]
\end{minipage}
\begin{minipage}{0.65\textwidth}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Algoritmo & Acción \\
\hline
\hline
$6 = 2 \times 2 + 1 + 1$ & Leo E \\
\hline
$6=2\times 1 + 3 + 1 $ & \textbf{PITO} \\
\hline
Algoritmo de detección-corrección & \textbf{PITO}\\
\hline
\end{tabular}
\end{center}
\end{minipage}
\end{example}


\section{Código de barras}
Vamos a estudiar el código de barras, un código bien conocido por todos y presente en todos los productos que compramos.

En los códigos de barras las barras codifican exactamente la misma información que los números y se emplean puesto que resulta más sencillo para la máquina interpretar las barras que los números.

Aunque la máquina trabaja con las barras, los números siguen presentes a modo de redundancia, para que cualquier persona pueda leerlos y corregir a la máquina si se produce un error.

Existen dos tipos principales de códigos de barras:
\begin{enumerate}
\item \textbf{EAN}: European Article Number
\[\text{EAN} \subset F_{10}^{13}\]
\item \textbf{UPC}: Universal Product Code
\[\text{UPC} \subset F_{10}^{12}\]
\end{enumerate}

Es interesante considerar $F_{10} = \ent_{10}$ puesto que nos resultará más intuitivo a la hora de estudiar cómo funciona este sistema.

Si consideramos el código de barras como la secuencia de números:
\[x_0,x_1,...,x_{12}\]
en el caso EAN y sin el $x_0$ para el caso UPC, la estructura general de un código es:
\[\underbrace{x_0,x_1,...,x_6}_{\text{código del fabricante}}\underbrace{x_7,x_8,...,x_{11}}_{\text{Código de producto}}\underbrace{x_{12}}_{\text{Redundancia}}\]

La redundancia se emplea para comprobar que el mensaje ha sido leído correctamente mediante la comprobación de que se satisface la ecuación:
\[x_{12} = -\left(\sum_{i=1}^5x_{2i} + 3 \sum_{i=0}^5x_{2i+1} \right)\mod 10\]

Con lo que hemos estudiado hasta ahora de códigos podemos ver que, según trabajemos con EAN/UPC tenemos:
\[n=13/12, \ \ k=12/11, \ \ M = 10^{12}/10^{11}\]

A priori podemos ver que la distancia mínima de estos códigos es, como mucho, 2, puesto que cualquier combinación de números es válida siempre que coincida en bit de redundancia.

\subsection{Detección de errores}
De primeras no sabemos si es posible cambiar un único número sin necesidad de cambiar el código de redundancia.

Por tanto este sistema puede detectar, como mucho 1 error. Para comprobar que realmente puede detectar un error necesitamos comprobar que la distancia ínima es exactamente 2.

\begin{proof}
Vamos a comprobar que siempre detecta un error.

Supongamos que tenemos un $x_i$ que, por algún motivo, se cambia por $x_i+e$ donde $e\in \ent_{10}$ no nulo.

En este caso, cuando el código de barras va a comprobar la valided del código leído, calcula:
\[x_{12}+\left(\sum_{i=1}^5x_{2i}+ 3 \sum_{i=0}^5x_{2i+1} \right)\mod 10 =  \left\{\begin{array}{lll}e & si & \text{i es par} \\ 3e & si & \text{i es impar}\end{array}\right. \mod 10\]

El sistema será capaz de detectar el error siempre que $e$ y $3e$ sean distintos de 0 módulo 10.

Es evidente que $e=0 \mod 10$ siendo $e \in \ent_{10}$ sólamente si $e=0$, caso que no nos preocupa ya que entonces no habría error.

Por otro lado, puesto que $(3,10)=1$ podemos ver que $3e = 0 \mod 10 \implies e=0 \mod 10$.
\end{proof}

Con esta demostración tenemos claro que el código de barras siempre va a detectar los casos en que se ha cometido un error. Sin embargo, es posible que se produzcan dos errores sin que el sistema sea capaz de detectarlo.

Por ejemplo, si dos posiciones pares se leen mal y en una se lee $x_i+e$ y en la otra $x_j-e$ el error no sería detectable.

A la hora de que el error sea detectado y un humano tenga que introducir el código de manera manual, el error más probable que implica el cambio de más de una cifra es la permutación de dos cifras.

Siempre que esta permutación se produza entre dos cifras en posición par o dos cifras en posición impar el error puede no ser detectable de manera obvia.

Veámos que ocurre cuando se permutan dos cifras en posiciones par e impar respectivamente.

\begin{proof}

Sean las cifras $x_{2i}$, $x_{2j+1}$, el error cometido en cada caso es $\pm e=x_{2j+1}-x_{2i}$.

Al comprobar la validez del código, nos comeremos el error si tenemos que:
\[e-3e = 0 \mod 10 \implies e=5\]
\end{proof}

No obstante, tenemos un total de 45 posibles intercambios de los cuales hay sólo 5 que no son detectables:
\[\{0-5,1-6,2-7,3-8,4-9\}\]

\obs Si observamos un teclado estándar podemos ver que estas parejas de dígitos nunca están contiguas de forma que se reduce el riesgo de aceptar por válido un mensaje con un error.

De forma general, si cambiamos sólamente el parámetro del código de barras, es decir, si tomamos:
\[x_{12} = -\left(\sum_{i=1}^5x_{2i} + A \sum_{i=0}^5x_{2i+1} \right)\mod 10\]

obtendremos diferentes resultados en función del valor de $A$.
\begin{itemize}
\item \textbf{Si $A$ es par}

Detecta el intercambio de cifras en posicion par con cifras en posición impar, pero puede cambiar un único dígito sin darnos cuenta, es decir, la distancia mínima pasaría a ser 1.

\item \textbf{Si $A=5$ }

Caso penoso, no hay error posible que podamos detectar.

\item \textbf{Si $A$ es impar distinto de 5}

No siempre podremos detectar el intercambio de cifras en posiciones par e impar respectivamente pero mantenemos distancia mínima igual a 2.
\end{itemize}

\subsection{Corrección de errores}
Si sabemos que se ha producido un único error y sabemos qué cifra es la que está mal, es posible corregir el error puesto que podremos calcular el error $e$ o $3e$ según el dígito que haya cambiado.

En el caso de que sea $3e =k$ el error, podremos despejar $e$ puesto que $3 \in U(\ent_{10})$

\begin{example}
Sean los siguientes productos:
\[\begin{array}{ll}
\text{Leche Lauki entera} & 8-414700-01101-3\\
\text{Leche Lauki desnatada} & 8-414700-01102-0
\end{array}\]

En ocasiones hay códigos reducidos empleados para productos donde no es viable escribir todos los dígitos de un código de barras estándar.
\[\begin{array}{ll}
\text{Yogur Danone de fresa} & 8410-5158
\end{array}\]

Tras ver estos ejemplos (no relacionados con lo que haremos en este ejemplo), lo que haremos será comprobar si el siguiente código es válido.
\[6-39844-06292-3\]

Para empezar, puesto que tiene 12 cifras podemos ver que es un código UPC.

Realizando la comprobación estándar vemos que:
\[\left(\sum_{i=1}^6x_{2i} + 3 \sum_{i=0}^5x_{2i+1} \right)\mod 10 = 2 \neq 0\]
lo que nos indica que hay un error de distancia 2.

A partir de esto no tenemos forma de saber en qué cifra se ha producido el error puesto que todas las cifras pueden cambiar dando lugar a este error con independencia de que sea una cifra en posición par o impar.


\end{example}

\section{International Standard Book Number}
\begin{defn}[ISBN]
El International Standard Book Number (en español, Número Estándar Internacional de Libros o Número Internacional Normalizado del Libro), abreviado ISBN, es un identificador único para libros, previsto para uso comercial.

Fue creado en el Reino Unido en 1966 por las librerías y papelerías británicas W. H. Smith y llamado originalmente Standard Book Numbering (en español, ‘numeración estándar de libros’), abreviado SBN. Fue adoptado como estándar internacional ISO 2108 en 1970.
\end{defn}

Este código se compone de un conjunto de números de la forma:
\[\underbrace{x_1,x_2,...,x_9}_{\text{información}}\underbrace{x_{10}}_{\text{control}} \ \ \ x_i \in \{0,...,9\} \text{ para } i=1...9\]

El bit de control se emplea para comprobar que se mantiene la igualdad:
\[\sum_{i=1}^{10}ix_i = 0 \mod 11 \implies -10 x_{10} = \sum_{i=1}^9ix_i \mod 11 \implies x_{10} = \sum_{i=1}^9ix_i \mod 11\]


\begin{example}
Si tenemos el ISBN ``84-239-5922-?'' podemos calcular el bit de control como:
\[x_{10} = \sum_{i=1}^9ix_i \mod 11 = 8\]

Si ahora tomamos el código ISBN ``84-239-5921-?'' tenemos:
\[x_{10} = \sum_{i=1}^9ix_i \mod 11 = 10\]
\end{example}

Pero no podemos representar el 10 con un solo dígito con nuestro sistema decimal.

En el caso del código bancario deciden poner un 1 en estos casos lo que resulta en una idea pésima pues reducimos la distancia mínima del código.

La opción empleada en el ISBN es emplear un ``X'', que representa con un sólo caracter el número 10 en el sistema romano.

Ahora que sabemos cómo está definido el ISBN podemos ver que:
\[ISBN \subset (\ent_{11})^{10}\]
pero no es un código lineal puesto que podemos salirnos del código al sumar dos números válidos, ya que el 10 sólo puede aparecer en el bit de control.

Con lo que hemos estudiado hasta ahora de códigos podemos ver que para el ISBN tenemos:
\[n=10, \ \ k=\emptyset, \ \ M = 10^{9}\neq 11^k \ \forall k \in \nat\]

\subsection{Detección de errores}

A priori sólo podemos garantizar que la distancia mínima es menor o igual que 2.

Si logramos probar que este código siempre detecta un error, tendremos que su distancia mínima es exactamente 2. Vamos a ello.
\begin{proof}
Supongamos que un dígito se ha cambiado y tenemos $\tilde{x}_i= x_i +e$.

Para comprobar si el mensaje es correcto debemos ver si
\[\sum_{i=1}^{10}ix_i = 0 \mod 11\]

En esta ocasión obtendremos:
\[\sum_{i=1}^{10}ix_i = ie \mod 11\]
pero, puesto que estamos en un cuerpo (ya que 11 es primo) todo elemento tiene inverso.

Por tanto, para que nuestro código no detecte un error tiene que ocurrir
\[ie=0 \implies i^{-1}ie=0 \implies e=0\]

Es decir, que no haya error.
\end{proof}

Por otro lado es bien sencillo ver que dos errores no siempre serán detectados puesto que pueden complementarse pero ¿Qué ocurre si dos dígitos se intercambian?.

En esta ocasión tendremos $x_i=x_i+e$, $x_j=x_j-e$ siendo $e=x_j-x_i$. Nuestro código no detectará el error siempre que
\[0 = \sum_{i=1}^{10}ix_i = ie-je = e(i-j) \implies i=j \Or e=0\]

Por tanto sólo \textbf{no detectaremos} el error en caso de que el dígito se intercambie consigo mismo, es decir, si no hay error.

Queda claro por tanto que este código siempre detecta el intercambio de dos dígitos.

\subsection{Corrección de errores}
Supongamos que hay un dígito que se ha perdido y que sabemos qué dígito es.

En este caso podríamos recuperar el valor perdido $X$ resolviendo la ecuación
\[\sum_{i=1, i\neq i_0}^{10}ix_i + i_0X = 0 \mod 11\]

Nuevamente, como estamos en un cuerpo es posible calcular el inverso de $i_0$ con lo que fácilmente podremos despejar y resolver la ecuación.


\section{Número de Identificación Fiscal}
El NIF consiste en una serie de caracteres de la forma:
\[\underbrace{x_7,x_6,x_5,x_4,x_3,x_2,x_1,x_0}_{\text{antiguo DNI}}-\underbrace{L}_{\text{control}}\]

Algunas entidades utilizan mal el NIF pues piden al usuario introducir únicamente las cifras y dejan que la máquina calcule la letra con lo que se pierde el beneficio de la redundancia.

La idea es que el usuario introduzca los 9 caracteres de modo que la máquina puede comprobar si se ha producido algún error.

Veamos cómo se calcula la letra del NIF. Vamos a tratar de averiguarlo a modo de ejercicio.
\begin{example}
Evidentemente, la letra debe calcularse como el módulo de algún valor.

Este módulo será igual a la cantidad de letras que queremos emplear que será, como mucho 26.

Tenemos claro por tanto que calcularemos la letra haciendo una operación del tipo:
\[\text{algo} \mod n \text{ con } 21 \leq n \leq 26\]

Si tomamos dos NIF que tengan la misma letra y calculamos el valor del DNI en los módulos 19,21,22,23,24,25 vemos que sólo coinciden los resultados si tomamos módulo 23.

\end{example}

Por tanto tenemos que la letra de control se utiliza para comprobar la valided del NIF dado atendiendo a la fórmula:
\[\text{DNI}\mod 23 = \sum_{i=0}^710^{i}x_i= n\]
donde $n$ es un valor asociado a una letra según una tabla que relaciona las letras empleadas con los números menores que 23 de forma aleatoria.

Una vez sabemos cuáles son los posibles valores de cada caracter del NIF podemos ver que
\[NIF \subset \{0,...,9\}^8 \times \{A,...,Z\} \subset (\ent_{23})^9\]

Con lo que hemos estudiado hasta ahora de códigos podemos ver que para el código NIF tenemos:
\[n=9, \ \ k=\emptyset, \ \ M = 10^{8}\neq 23^k \ \forall k \in \nat\]

\subsection{Detección de errores}
Nuevamente, como en los casos anteriores, tenemos que la distancia mínima será como mucho 2.

Vamos a demostrar que es exactamente 2.
\begin{proof}
Supongamos que un número es leído de forma incorrecta y tenemos $x_i=x_i+e$.

En esta ocasión tendremos tendremos
\[\sum_{i=0}^710^{i}x_i - n \mod 23 = 10^{i} e \mod 23\]
que sólo podrá ser 0 si $e=0$.

Por tanto el NIF siempre es capaz de detectar un error por lo que la distancia mínima será 2.
\end{proof}

\subsection{Corrección de errores}
De la misma forma, también será posible recuperar un caracter borrado (sabiendo a qué posición pertenecía) resolviendo la ecuación:
\[X=(10^{i_0})^{-1} \left( n- \sum_{i=0,i\neq i_0}^710^{i}x_i \right)\]
en la que podemos garantizar la existencia del inverso puesto que estamos trabajando en un cuerpo.

Por otro lado, si se produce un intercambio de dos caracteres tendremos $x_i=x_i+e$, $x_j=x_j-e$ siendo $e=x_j-x_i$. Al comprobar la validez del código, nos comeremos el error siempre que:
\[0 = \sum_{i=0}^710^{i}x_i - n \mod 23 =(10^i-10^j)e = 10^j(10^{i-j}-1)e\]

Puesto que estamos en un cuerpo sabemos que $10^j\neq 0 \ \forall j$. No obstante aún tenemos el riesgo de que:
\[10^{i-j}-1 = 0 \mod 23\]

Pero por el pequeño teorema de Fermat sabemos que $10^{22}=1 \mod 23$, de modo que para que no se detectara el intercambio necesitaríamos $i-j=22$, pero esto es imposible pues sabemos que en el NIF tan solo figuran 7 dígitos. Por tanto \textbf{el NIF es capaz de detectar el intercambio de 2 dígitos}.

\obs Se puede comprobar que si en vez de haber tomado 23 letras se hubieran escogido 17 o 19, el NIF seguiría siendo capaz de detectar el intercambio de dos dígitos, sin embargo con solo 11 letras no se podría corregir.

\section{Probabilidades}

En general, empleamos los códigos dectectores/correctores de errores para disminuir la probabilidad de leer mal un mensaje.

Si tenemos que la probabilidad de que un bit se modifique durante la transmisión del mensaje es $p$, hasta ahora hemos supuesto $p<\frac{1}{2}$.

Sin embargo, puede ocurrir que tengamos un canal de comunicación tan pésimo que $p>\frac{1}{2}$. En este caso, lo que podemos hacer es invertir el mensaje recibido (cambiar todos los bits) y empezar a emplear toda la teoría vista hasta ahora, puesto que acabamos de convertir nuestro canal en otro con $p<\frac{1}{2}$.

El gran problema lo alcanzamos cuando $p=\frac{1}{2}$ puesto que en este caso llegarían 0s y 1s al azar.

Genralizando este problema a un canal por el que transmitimos $q$ posibles valores con probabilidad de error $p$, consideramos que enviando un símbolo cualquiera, podremos recibir cualquier otro con probabilidad $\frac{p}{q}$.

\begin{example}
Supongamos que tenemos un código $\algbC = \{ \text{SI} = 1, \ \text{NO} = 0\}$ y lo enviamos por un canal con $p=\frac{1}{2}$.

En este caso el mensaje leído será incorrecto con probabilidad $p$ y correcto con probabilidad $1-p$.
\end{example}

Este ejemplo no nos dice nada, pues es extremadamente sencillo. Veamos otro con más interés.
\begin{example}
Supongamos que tenemos un código de tripitición:
\[\algbC = \{ \text{SI} = 111, \ \text{NO} = 000\}\]

En este caso la probabilidad de leer el mensaje correctamente es:
\[\algbP(\text{correcto})=(1-p)^3+3p(1-p)^2\]
puesto que si sólo se produce un error lo detectaremos y corregiremos correctamente empleando la técnica del vecino más próximo.

Por otro lado, es sencillo ver que
\[\algbP(\text{incorrecto}) = 1 - \algbP(\text{correcto})\]
\end{example}

Hay que recalcar que la probabilidad de error en el canal es un dato que nos viene dado y que no podemos modificar desde el punto de vista matemático (eso será cosa de los ingenieros).

Nuestro objetivo como matemáticos será minimizar la probabilidad de error en el código restringiéndonos al canal de comunicación dado.

\begin{defn}[($n,M,d$)-código q-ario]

Sea $F_q$ un alfabeto con $|F_q|=q$, decimos que un $(n,M,d)$-código q-ario es un código $\algbC$ tal que
\[\algbC \subset F_q^n, \ \ d(\algbC)=d \text{ y } |\algbC|=M\]

Es decir, tenemos un código que emplea $q$ símbolos distintos para formar $M$ mensajes posibles de longitud $n$ con distancia mínima $d$.
\end{defn}

\begin{defn}[.$A_q(n,d)$]
\[A_q(n,d) = \max\{M: \ \exists \ (n,M,d)\text{-código q-ario}\}\]
\end{defn}

\begin{prop}[Cota de Singelton]
Para todo código se cumple que
\[M \leq q^{n-d+1}\]
es decir
\[A_q(n,d) \leq q^{n-d+1}\]
\end{prop}
\begin{proof}
Consideramos la aplicación
\[\appl{π}{F_q^n}{F_q^{n-d+1}}\]
definida como una proyección sobre las primeras $n-d+1$ coordenadas.

Puesto que $\algbC \subset F_q^n$ podemos restringir nuestra aplicación al código:
\[\appl{π}{\algbC}{F_q^{n-d+1}}\]

Si la aplicación fuese inyectiva tendríamos que
\[|\algbC| \leq |F_q^{n-d+1}| \implies M \leq q^{n-d+1}\]

Por tanto, sólo tenemos que probar que es inyectiva. Para ello supongamos que hay dos puntos cuya imagen es la misma, es decir:
\[x,y \in F_q^n \ \ F(x)=F(y) \implies d(x,y) \leq d-1\]
Pero la distancia mínima es, por definición $d$ por lo que no es posible que tengamos dos plabras a distancia $d-1$ a no ser que sean iguales, con lo que la distancia entre ellas sería 0.

Es decir, tenemos $x=y$ con lo que la función es inyectiva.
\end{proof}

\obs La cota de Singleton no siempre se alcanza.

\begin{example}
Vamos a estudiar $A_2(5,3)$. Por la cota de Singleton sabemos que
\[|M| \leq q^{n-d+1} = 2^3=8\]

Sin embargo, en este caso sabemos que el valor máximo que puede alcanzar $M$ es $A_2(5,3)=4$, que dista bastante de la cota de Singleton. Para comprobarlo debemos ver que existe un $(5,4,3)$-código binario y que no existe un $(5,5,3)$-código binario.

Vamos a ello.

Ya hemos visto en estos apuntes un $(5,4,3)$-código binario cuando definimos códigos para un robot:
\[\algb{C}_5 = \left\{x\in F_2^6 \tq  \begin{array}{l}
x_1+x_2+x_3= 0 \\
x_1+x_4 = 0 \\
x_2+x_5 = 0
\end{array}\right\}=\left\{\begin{array}{l}
00000\\
01101\\
10110\\
11011
\end{array}\right\}\]

\end{example}

Antes de continuar con el ejemplo vamos a ver algunos conceptos nuevos, necesarios para la finalización del mismo.

\begin{defn}[Códigos equivalentes]
Diremos que dos códigos $\algbC_1$ y $\algbC_2$ (necesariamente de la misma longitud) son equivalentes si uno puede obtenerse a partir del otro combinando los siguientes dos tipos de transformaciones:
\begin{enumerate}
\item Intercambiar dos variables.

Si escribimos todas las palabras posibles una debajo de otra formando una matriz, esta operción es equivalente a permutar dos columnas de la matriz.

\item Permutar los símbolos que aparecen en una posición dada

Si escribimos todas las palabras posibles una debajo de otra formando una matriz, esta operación es equivalente a cambiar un número de la matriz por otro de su misma columna.
\end{enumerate}
\end{defn}

\begin{prop}
Dos códigos equivalentes tienen los mismo parámetros y la misma capacidad de detectar/corregir errores, aunque los algoritmos a emplear serán distintos en cada caso.
\end{prop}

\begin{lemma}
Si $0\in F_q$ cualquier código no vacío $\algbC$ es equivalente a otro $\algbC'$ con $0...0\in \algbC'$
\end{lemma}
\begin{proof}
Dada una palabra cualquiera sobre cada elemento $a_i$ de la palabra aplicamos la permutación $a_i\to 0 \to a_i$, con lo que obtenemos la palabra $0...0$
\end{proof}

Ahora estamos en condiciones de seguir con el ejemplo.

\begin{example}
Si existiera un $(5,5,3)$-código binario, entonces existiría uno equivalente $\algbC$ con $0...0 \in \algbC$.

Además este código debe cumplir que
\[\nexists x \in \algbC \tq w(x)=1,2 \text{ puesto que } d=3\footnote{Si existiera una palabra con peso 1 o 2 estaría a distancia 2 de la palabra 0...0 lo que contradice el concepto de distancia mínima}\]

Por otro lado, si tomamos $w(x)=5$, $w(y)=4$ y $w(z)=4$ tenemos $d(x,y)=1$, $d(y,z)=2$. Por tanto no podemos tener en nuestro código dos palabras de peso 4 ni una de peso 4 y otra 5, puesto que entonces estarían más cerca de lo permitido.

Con esto tenemos que en nuestro código de longitud 5 puede haber como mucho 1 palabra de peso 5 ó 4, otra de peso 0 y ninguna de pesos 1 ni 2. Por tanto, debe haber, como mínimo 3 palabras de peso 3.

Si nos ponemos a buscar estas palabras vemos que sólo hay dos posibles palabras de peso 3 que disten, al menos 3. Estas palabras son $11100$ y $00111$.

Pero entonces aún nos falta una palabra para nuestro código con lo que acabamos de comprobar que no es posible la existencia de un $(5,5,3)$-código binario.
\end{example}

\begin{example}

Vamos a estudiar los dos casos extremos de $A_q$, para los que conocemos el valor.

\begin{itemize}
\item $A_q(n,1)=q^n$

En concreto tenemos el primer código que vimos a principio de la sección $\algbC_0$ que alcanza este máximo con $q=2$.

\item $A_1(n,n)=q$

Si las palabras del código han de estar a distancia $n$ y tener longitud $n$, entonces todas las palabras deben tener distintos todos sus caracteres.

Por tanto, como en cada caracter tenemos un total de $q$ posibles valores, si tenemos la palabra 0...0, otra posible palabra será 1...1 y así sucesivamente hasta los $q$ posibles caracteres.
\end{itemize}

\end{example}

\begin{prop}\label{prop:existencia_codigos_binarios}
Siendo $d$ impar
\[\exists  \ (n,M,d)\text{-código binario} \iff \exists \ (n+1,M,d+1) \text{-código binario}\]
\end{prop}
\begin{corol}
Si $d$ es par
\[A_2(n,d)=A_2(n-1,d-1)\]
\end{corol}

\begin{lemma}
En $F_2^n$ se cumple que
\[d(x,y) = w(x)+w(y)-2w(z\cap y)\]
donde
\[z \cap y = (x_1y_1,x_2y_2,...,x_ny_n)\]
\end{lemma}
\begin{proof}
Ya sabemos que
\[d(x,y) = w(x-y)=w(x+y)\]
Lo que tenemos que hacer es ver cuándo hay 1s en x+y, cosa que ocurre cuando hay un 1 en x, o en y de forma exclusiva.

Cuando hay un 1 en tanto en x como en y, $w(x+y)=0$ mientras que $w(x)+w(y)=2$, por lo que en ese caso deberemos restar 2.

Si leemos con cuidado lo que acabamos de explicar obtenemos exactamente la definición de distancia que queremos demostrar.
\end{proof}

Ahora nos encontramos en condiciones de demostrar la proposición \ref{prop:existencia_codigos_binarios}.
\begin{proof}
Vamos a demostrar los dos sentidos de la implicación

\begin{itemize}
\item $\implies$
Sea $\algbC$ un $(n,M,d)$-código definimos
\[\tilde{\algbC} = \left\{x \in F_2^{n+1}\tq (x_1,...,x_n) \in \algbC \ \y \ \sum_{i=1}^{n+1}x_i = 0 \mod 2\right\}\]

Es decir, añado al código $\algbC$ un bit de control global de paridad.

Es evidente ver que $\tilde{\algbC}$ tiene $n+1$ elementos y que $|\tilde{\algbC}|=M$ pues estamos añadiendo un dígito a cada palabra de $\algbC$.

Comprobemos ahora que $d(\tilde{\algbC})=d+1$.

Por construcción de $\tilde{\algbC}$ ya tenemos que $d+1 \geq d(\tilde{\algbC})\geq d$ puesto que los primeros $n$ caracteres de las palabras no pueden distar menos de $d$ y sabemos que al menos en un caso están a distancia $d$.

Apoyándonos en el lema anterior tenemos que
\[d(\tilde{x},\tilde{y}) = w(\tilde{x})+w(\tilde{y})-2w(\tilde{x}\cap \tilde{y})\]

Por construcción de $\tilde{\algbC}$ tenemos que todas las palabras van a ser de paridad par por lo que a la derecha de la igualdad anterior tenemos combinación de términos pares que deberá ser par.

Pero hemos empezado considerando que $d$ es impar, por tanto $d(\tilde{\algbC})$ no puede ser $d$.

Por tanto queda claro que $d(\tilde{\algbC})=d+1$.

\item $\Longleftarrow$

Para demostrar esta dirección de la implicación vamos a acortar o reducir el código.

Sea $\algbC$ un $(n+1,M,d+1)$-código y sean $x,y \in \algbC$ tenemos que $d(x,y)=d+1$.

Sea $i$ tal que $x_i \neq y_i$ construimos $\tilde{\algbC}$ formado a partir de las palabras de $\algbC$ quitando la coordenada i-ésima, es decir, tomamos una proyección
\[\appl{π}{F_q^{n+1}}{F_q^n}\]

Es evidente que los elementos de $\tilde{\algbC}$ tienen longitud $n$.

Por otro lado podemos ver que $|\tilde{\algbC}| = M$ puesto que en caso de no ser así tendríamos
\[π(x)=π(y) \implies d(x,y) \leq 1 \text{ contradicción pues } d(x,y) > d+1 =2\]

Por último nos queda estudiar la distancia $d(\tilde{\algbC})$.

Es evidente que $d(\tilde{\algbC})\geq d$. Como
\[d(x_0,y_0)=d+1 \implies d(π(x_0),π(x_1))= d \implies d(\tilde{\algbC})=d\]

\end{itemize}
\end{proof}

Hasta ahora sólo hemos visto conceptos muy triviales salvo la cota de Singleton que era algo más compleja.

Vamos a empezar con cosas de verdad.

\section{Cota de Hamming o del empaquetamiento de esferas}

Sea $\algbC$ un $(n,M,2t+1)$-código q-nario es sencillo ver que dados $x,y \in \algbC$ con $x \neq y $ se cumple
\[\overline{B}(x,t) \cap \overline{B}(y,t) = \emptyset\]

Podemos ver, además que la unión disjunta de estas bolas se contiene en $F_q^n$, es decir
\[\coprod_{x \in \algbC} \overline{B}(x,t) \subset F_q^n\]

\obs En cada bola la única palabra que se contiene en el código es el centro de la misma.

Puesto que las bolas son disjuntas, el conjunto de elementos cubiertos por la unión de las bolas es igual al producto del número de palabras cubiertas por una bola multiplicado por el número de bolas.

\begin{defn}[Cota de Hamming]
Sea $\algbC$ un $(n,M,2t+1)$-código q-nario tenemos que:
\[|\algbC| |\overline{B}(x,t)| \leq q^n\]
\end{defn}

No obstante, esta cota es totalmente inútil mientras no seamos capaces de contar los elementos de la bola. Podemos ver que, por definición de bola cerrada:
\[\overline{B}(x,t) = \coprod_{i=0}^t\{y \in F_q^n: \ d(x,y)=i\}\]

Y sabemos que
\[|\{y \in F_q^n \ d(x,y)=i\}| = {n \choose i} (q-1)^i\]
puesto que tenemos ${n \choose i}$ formas de tomar $i$ posiciones de la palabra y para cada una de ellas tendremos $(q-i)^i$ posibles caracteres que utilizar.

Por tanto podemos ver que
\[|\overline{B}(x,t)| = \sum_{i=0}^t {n \choose i} (q-1)^i\]

Por tanto, podemos reescribir la \textbf{cota de Hamming} como:
\[|\algbC|=M \leq \frac{q^n}{\sum_{i=0}^t {n \choose i} (q-1)^i}\]
En particular
\[A_q(n,2t+1) \leq \frac{q^n}{\sum_{i=0}^t {n \choose i} (q-1)^i}\]

\begin{example}
\[A_q(n,3) \leq \frac{q^n}{1+n(q-1)}\]
\end{example}

\begin{defn}[Código perfecto]
Decimos que un código $\algbC$ es perfecto si
\[|\algbC| = \frac{q^n}{\sum_{i=0}^t {y \choose i} (q-1)^i}\]
es decir, si $F_q^n$ se tesela por bolas cerradas de radio $t$ alrededor de las $x \in \algbC$
\end{defn}

Existen una serie de códigos que son perfectos de manera trivial. Vamos a verlos:

\textbf{Códigos perfectos triviales}
\begin{enumerate}
\item $\algbC=F_q$

Estamos tomando todos los posibles términos de $F_q$ por lo que tendremos $d=1$ lo que nos implica $t=0$

\item $\algbC=\{a\}$

Tomamos un código con una sola palabra de forma que $d=\infty$ (realmente no tiene sentido definir la distancia) y por tanto $t=\infty$ aunque, puesto que estamos en un cuerpo finito esto se traduce en considerar una bola de radio $t=n$ que engloba todo $F_q^n$.

\item Código de repetición binaria de longitud $n=2t+1$.

Este código tendrá $d=2t+1 =n$.

Podemos comprobar que el código es perfecto puesto que sabemos que tiene 2 palabras posibles (todo 0s o todo 1s).

\end{enumerate}

\subsection{Código de Hamming}
\begin{defn}[Código de Hamming(3,2)]
El código de Hamming (Hamming(3,2)) es el primer código perfecto que se creó. Es un código binario, lineal, de longitud 7 y formado por 16 palabras.
\end{defn}

\begin{minipage}{0.57\textwidth}
La idea de Hamming consiste en dibujar tres círculos con intersecciones no vacías y distribuir los 7 dígitos entre los círculos. Formarán parte del código únicamente las palabras tales que, al distribuir sus cifras como muestra el esquema de la derecha, cada círculo tiene un número par de 1s.
\end{minipage}
\begin{minipage}{0.42\textwidth}
\begin{center}
\inputtikz{hamming}
\end{center}
\end{minipage}

Esta relación se traduce en que establecemos 3 ecuaciones lineales entre las cifras de un mensaje:
\[\begin{array}{l}x_5=x_1+x_2+x_4 \mod 2 \\ x_6=x_1+x_3+x_4 \mod 2 \\ x_7=x_2+x_3+x_4 \mod 2 \end{array}\]

\begin{prop}
\[d(Hamm(3,2))=3\]
\end{prop}
\begin{corol}
Hamm(3,2) corrige siempre un error.
\end{corol}

La forma de demostrar la proposición pasa por comprobar que este código de Hamming siempre corrige un error, lo que nos garantiza que la distancia mínima será 3.

Cuando recibimos un mensaje distribuimos sus cifras siguiendo el diagrama que representa esta código y procedemos a comprobar cuántos círculos satisfacen la propiedad.

Si algún círculo no satsiface la condición, la regla del vecino más próximo nos que el código original puede obtenerse modificando el bit que se encuentra en la intersección de los círculos que causan el problema.

Es evidente que este método funciona y que siempre nos permite detectar un error, sea cual sea la cifra errónea.

Para entender que está formado por 16 palabras basta con observar que podemos dar valores cualesquiera a las cifras $x_1,x_2,x_3$ y $x_4$ (lo que ya nos da 16 combinaciones posibles) y que el resto de cifras vienen determinadas por estas 4.