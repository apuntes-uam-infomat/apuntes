\chapter{Códigos detectores y correctores de errores}
\section{Motivación}
\begin{example}
Supongamos que tenemos un robot en Marte (o en cualquier lugar muy lejano) y queremos comunicarnos con él, indicándole en cuál de las 4 direcciones naturales queremos que se mueva:
\[\left\{ \begin{array}{l}
N = 00 \\ E = 01 \\ O = 10 \\ S = 11
\end{array}\right.\]

En cualquier momento puede ocurrir un problema que dificulte la recepción del mensje por parte del robot, ya sea el ruido del propio canal de comunicación, o la aparición de un rayo cósmico que modifique algún bit.

El problema de esto es que, en cuanto el robot reciba un par de órdenes perturbadas, habremos perdido el control del mismo, pues no sabremos cuál es su posición.

La \textbf{clave del problema} es que, puesto que todas las palabras de longitud dos son válidas en nuestro código, el robot no puede darse cuenta de si el mensaje ha sido perturbado o no.

Para solventar este problema, una opción es emplear cadenas más largas para codificar cada movimiento, a fin de poder encontrar errores posibles. Por ejemplo, podemos duplicar el código que representa cada posible movimiento.

\[\left\{ \begin{array}{l}
N = 0000 \\ E = 0101 \\ O = 1010 \\ S = 1111
\end{array}\right.\]

Si enviamos al robot el mensaje $0101$ y el robot recibe $0111$, el robot detecta que ha habido un error, pero no puede corregirlo con exactitud.

Por tanto, lo que hará el robot será corregir el mensaje minimizando la probabilidad de fallo, es decir, trata de maximizar la probabilidad condicionada:
\[\mathbb{P} \left(\text{han enviado m} | \text{he recibido }\tilde{m} \right)\]

Esta probabilidad dependerá de las características del canal. De forma general tenemos que un bit muta con probabilidad pequeña $p$ y, consecuentemente, llega de forma adecuada con probabilidad $p-1$. Como hipótesis consideramos que $p$ es pequeño (desde luego $p<\frac{1}{2}$).

Para simplificar el problema supondremos, además, que los errores en los bits son independientes.

Puesto que $p$ es pequeño, es más probable que un bit llegue de forma correcta a que llegue mutado. Por tanto, a la hora de maximizar la probabilidad descrita anteriormente, necesitamos encontrar el mensaje original al que se puede llegar mediante el menor número de cambios posibles.

Así tenemos que, dado $\tilde{m} = 0111$
\[\left\{ \begin{array}{l}
m = N= 0000 \text{ con 3 cambios}\\ m= E = 0101 \text{ con 1 cambio }\\ m= O = 1010 \text{ con 3 cambios}\\ m =S = 1111 \text{ con 1 cambio}
\end{array}\right.\]

Por tanto, lo más probable es que la orden fuese desplazarse hacia el este o el sur, pero no sabemos cuál de las dos es.

\obs Si se produce un sólo error sabemos detectarlo, aunque no corregirlo. No obstante, si se producen dos errores es posible que no seamos capaces ni de detectarlo.

A fin de poder corregir un único error lo que haremos será repetir el código tres veces, de modo que tenemos

\[\left\{ \begin{array}{l}
N = 000000 \\ E = 010101 \\ O = 101010 \\ S = 111111
\end{array}\right.\]

Con este método, seremos capaces de detectar la presencia de hasta dos bits erróneos. 

Supongamos que recibimos el mensaje $\tilde{m}=011101$, mirando, como antes, el código original más probable tenemos:

\[\left\{ \begin{array}{l}
m = N= 000000 \text{ con 4 cambios}\\ m= E = 010101 \text{ con 1 cambio }\\ m= O = 101010 \text{ con 5 cambios}\\ m =S = 111111 \text{ con 2 cambios}
\end{array}\right.\]

En esta ocasión tenemos un \textbf{vecino más próximo} que será el mensaje $m=E$.

Además, si conseguimos garantizar que sólo habrá un error en el mensaje recibido, podemos estar seguros de que hemos encontrado el mensaje original, pues no hay forma de llegar hasta el mensaje que hemos recibido modificando un único bit de alguno de los otros mensajes posibles.

\obs Si se produce un error podemos corregirlo, dos errores podremos detectarlos, a partir de 3 errores es posible que no los detectemos.

Con este mismo esquema, supongamos que recibimos la palabra $\tilde{m} = 11101$. En este caso, como en el anterior, es sencillo detectar que se ha producido un error.

Si empleamos el mismo método que en el caso anterior, tendríamos que el vecino más cercano es $m=S$ pero estamos restingiéndonos a asumir que sólo hay un error. No obstante, el mensaje podría haber sido $m=E$ y encontrarnos con dos errores.

\obs O asumimos que sólo hay un error y lo corregimos o aceptamos que puede haber dos y no podemos corregirlo

\end{example}

Veamos ahora cómo generalizar este tipo de problemas y la solución al mismo.

\begin{defn}[Código q-nario]
Dado un alfabeto con $q$ elementos, $F_q$, un código (de bloques) q-nario de longitud $n$ es $\algb{C}\in F_q^n$.

Los números de teléfono de españa, por ejemplo, constituyen un código 10-ario de longitud 9: $F_{10}^9$
\end{defn}

De manera general, siendo $|\algb{C}| = M$ querremos encontrar un código con $M$ grande, para tener más ordenes posibles para el robot, con un $n$ pequeño, para que no tenga que enviar demasiada información y que permita corregir muchos errores.

\obs Todo a la vez es imposible. \textbf{La teoría de códigos trata de hacerlo lo mejor posible.} 

\section{Distancia de Hamming}
\begin{defn}[Distancia de Hamming]
Dados $x=(x_1,...,x_n)$, $y=(y_1,...,y_n) \in F^n$ se define la \textbf{Distancia de Hamming} como
\[d(x,y)=\#\{i: x_i \neq y_i\}\] 
\end{defn}

\begin{prop}[$d$ es una distancia]
Puesto que somos matemáticos debemos convencernos de la que la función $d$ definida es realmente una distancia. Para ello comprobamos que se cumplen las propiedades de una distancia:
\begin{enumerate}
\item 
\[d(x,y) \geq 0\]

Puesto que estamos mirando el cardinal de un conjunto, trivialmente esta propiedad es cierta

\item
\[d(x,y)=0 \iff x=y\]

Puesto que estamos contando el número de bits diferentes, si el número es 0, evidentemente las dos cadenas son la misma

\item 
\[d(x,y)=d(y,x)\]

Esta propiedad es trivial puesto que el operador $\neq$ es conmutativo

\item 
\[d(x,y) \leq d(x,z)+d(z,y)\]

Tenemos que comparar el número de cambios necesarios para convertir $x$ en $z$ y luego $z$ en $y$. Si tomamos $d=d_H$, donde $d_H$ es la distancia mínima, la desigualdad es obvia.

\begin{defn}[Distancia mínima]
Dado un código $\algb{C}\in F_q^n$, su \textbf{distancia mínima} se define como:
\[d(\algb{C}) = \min\{d(x,y) \ t.q. \ \ x,y \in \algb{C}, \ x \neq y\}\]
\end{defn}
\end{enumerate}
\end{prop}

Si utilizamos $\algb{C} \subset F_q^n$ con $|G|=M$ para transmitir $M$ mensajes, lo que queremos es que las palabras empleadas esten lo más espaciadas posible, de modo que un pequeño cambio en un elemento de una palabra no nos haga caer en otra palabra válida. Es decir, queremos que la distancia mínima del código, es decir, $d(\algb{C})$ sea grande.

\begin{theorem}
Sea $\algb{C} \subset F_q^n$ un código con $d(\algb{C})=d$, podemos \textbf{detectar} hasta $d-1$ errores 
\end{theorem}
\begin{proof}
El procedimiento estándar para detectar errores es:
\begin{enumerate}
\item Recibo $\tilde{x} \in F_1^n$

\item Si $\tilde{x} \in F_q^n \in \algb{C}$ seguimos trabajando. 

\item Si $\tilde{x} \notin F_q^n \in \algb{C}$ el mensaje no es correcto.
\end{enumerate}

Si el número de errores es menor o igual que $d-1$, es imposible que desde un $x\in \algb{C}$ llegemos a otro $\tilde{x} \in \algb{C}$, puesto que, por definición, $d(x,\tilde{x})\geq d$.

Es decir, el algoritmo descrito nunca podrá confundirse si tenemos un máximo de $d-1$ errores.
\end{proof}

\begin{theorem}
Sea $\algb{C} \subset F_q^n$ un código con $d(\algb{C})=d$, podemos \textbf{corregir} hasta $\floor{\frac{d-1}{2}}$
\end{theorem}
\begin{proof}
Desde el punto de vista geométrico (o topológico) podemos ver que, dados dos mensajes $x,y \in \algb{C}$
\[B\left(x,\floor{\frac{d-1}{2}}\right) \cap B\left(y,\floor{\frac{d-1}{2}}\right) = \emptyset\]

Apoyándonos en esta idea, veamos la demostración del teorema.

El procedimiento para corregir errores es:
\begin{enumerate}
\item Dado un mensaje $\tilde{x} \notin \algb{C}$, a fin de corregir el error buscamos $x_0$ tal que
\[d(\tilde{x},x_0)=\min\{d(x,\tilde{x}): \ x\in \algb{C}\}\]

\item Si tenemos un empate, tomamos uno al azar.
\end{enumerate}

Este algoritmo podría fallar si y sólo si 
\[\exists x_0 \in \algb{C} \tq d(x_0,\tilde{x}) \leq d(x,\tilde{x})\]
Pero en este caso tenemos que
\[d(x,x_0) \leq d(x,\tilde{x})+d(\tilde{x},x_0) \leq 2d(x,\tilde{x}) \leq 2 \floor{\frac{d-1}{2}} \leq d-1 < d\]
con lo que llegamos a una contradicción, pues por definición $d(x,x_0)\geq d$.
\end{proof}

\begin{example}
Sea el lenguaje

\[\left\{ \begin{array}{l}
N = 000000 \\ E = 010101 \\ O = 101010 \\ S = 111111
\end{array}\right.\]

Supongamos que se envía ``S=111111'' y recibimos ``010111'' y empleamos un algoritmo de corrección de errores veremos que el mensaje más cercano es ``E=010101'' que implica un único error en el mensaje.

No obstante, en este caso el resultado es incorrecto pues en realidad se produjeron 2 errores.
\end{example}

Con lo visto hasta ahora, es evidente que la forma de conseguir detectar/corregir más errores pasa por emplear códigos con distancias mínimas grandes.

En general, podemos crear códigos con distancias mínimas tan grandes como queramos.

\begin{defn}[Codigo binario de repetición de longitud n]
Este código consiste en repetir cada bit n veces.
\[R_{2,n}=\{\underbrace{111....111}_{n\text{ veces}}, \underbrace{000....000}_{n\text{ veces}}\}\]

Si $F_q=\{a_1,...,a_q\}$ es un alfabeto, el \textbf{código de repetición de longitud n} es:
\[R_{2,n}=\{\underbrace{a_1a_1a_1....a_1a_1a_1}_{n\text{ veces}}, ... , \underbrace{a_qa_qa_q....a_qa_qa_q}_{n\text{ veces}}\}\]
\end{defn}

Podemos ver que las distancias mínimas para un código de repetición de longitud n será siempre $d(R_q,n)=n$.

El problema de este método es que acaba siendo muy costoso cuando queramos hacer $n$ grande. 

Para poder clasificar y estudiar la calidad de los diferentes códigos debemos definir algunos términos:

\begin{defn}[Distancia relativa]
Sea $\algb{C} \subset F_1^n$ con $d(\algb{C})=d$ y $|\algb{C}|=M=q^l$ definimos la \textbf{distancia relativa} como:
\[δ=\frac{d}{n}\]
\end{defn}

\begin{defn}[Tasa de transmisión]
Sea $\algb{C} \subset F_1^n$ con $d(\algb{C})=d$ y $|\algb{C}|=M=q^l$ definimos la \textbf{tasa de transmisión} como:
\[R=\frac{l}{n}\]
\end{defn}

\begin{example}
En un código de repetición siempre tendremos una distancia relativa igual a 1 y una tasa de transmisión inversamente proporcional a $n$. Es decir:
\[δ=1, \ \ R = \frac{1}{n}\]
\end{example}

\obs Dado un código con más de una palabra\footnote{Si sólo tenemos una palabra es un caso trivial}, es decir $|\algb{C}|>1$ tenemos:
\[0 < δ,R \leq 1\]

En general intentaremos que tanto δ cono $R$ sean cercanos a 1. No obstante, es imposible que ambas tiendan a 1 a la vez por lo que se tratará de optimizar, dependiendo de la situación concreta en que vayamos a trabajar.

\begin{defn}[Código lineal]
Diremos que un código $\algb{C}$ es un \textbf{código lineal} si:
\begin{enumerate}
\item $F_q$ es un cuerpo [o un anillo conmutativo si uno sabe lo que es un módulo sobre un anillo]

\item $\algb{C}$ es un espacio vectorial de $F_q^n$ [o es un submódulo si $F_q^n$ es un anillo]
\end{enumerate}
\end{defn}

\begin{example}
Todos los códigos que hemos visto hasta ahora son \textbf{códigos lineales}.

\[\algb{C}_2 = \{\vx\in \mathbb{F}_2^4: x_1-x_3 = 0 , \  x_2-x_3 = 0\}\]
\[\algb{C}_3 = \{\vx \in \mathbb{F}_3^6: x_1=x_3=x_5, \ x_2=x_4=x_6\}\]
\end{example}

\obs Sea $|\algb{C}|=M=q^l$ tenemos que $l=k=dim_{F_q}(\algb{C})$

\begin{defn}[Peso]
Si $F_q$ es un cuerpo tal que $x\in F_q^n$ llamamos \textbf{peso de x} a:
\[w(x) = \# \{i: \ x_i \neq 0\}\]
Es decir, dada una palabra nos cuenta la cantidad de 1s.
\end{defn}

\begin{defn}[Peso mínimo de $\algb{C}$]
\[w(\algb{C}) = \min \{ w(x) : \ \ x \in \algb{C}, x \neq 0\}\]
\end{defn}

\begin{lemma}
Si $\algb{C}$ es un código lineal tenemos que:
\begin{enumerate}
\item \[\forall x \neq y \in \algb{C} \ \ d(x,y)=w(x-y)\]
\item \[d(\algb{C})=w(\algb{C})\]
\end{enumerate}
\end{lemma}