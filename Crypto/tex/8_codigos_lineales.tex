\chapter{Códigos lineales}
\section{¿Por qué códigos lineales?}
Como ya se ha observado en el capítulo anterior hay una gran ventaja en el empleo de códigos lineales y es que la distancia mínima puede calcularse de forma mucho más sencilla, pues podemos calcular los pesos de cada palabra para luego tomar el mínimo en lugar de calcular todas las distancias entre posibles pares de palabras antes de tomar el mínimo.

Otra ventaja de los códigos lineales es que a la hora de definirlos no es necesario listar todas las palabras que componenen el código sino que basta con dar una base del espacio lineal que define el código.

Esta ventaja también se puede aprovechar proporcionando las ecuaciones que definen el código, de nuevo sin tener que listar todas las palabras del mismo. No obstante se tendrán que proporcionar más ecuaciones que elementos de la base aunque en cualquier caso estamos teniendo que redactar muchísima menos información que si diésemos todas las palabras válidas.

Por último, pero no por ello menos improtante, otra ventaja de los códigos lineales es que existen algoritmos rápidos de codificación y descodificación.

No obstante, los códigos lineales también tienen algunas \textbf{desventajas}. En general los códigos lineales son ``mejores'' que los demás aunque esto no siempre es así y nos encontraremos con excepciones en las que es mejor emplear códigos no lineales.

\section{Definición}

\begin{defn}[.$(n,k,d)$-código lineal q-nario]
Un $[n,k,d]$-código lineal q-nario es un subespacio vectorial $\algb{C} \subset F_q^n$ tal que $dim(\algb{C})=k$ y $d(\algb{C})=d$.
\end{defn}

\begin{defn}[B$_q(n,d)$]
\[B_q(n,d)=max \{q^k \tq \ \exists [n,k,d]\text{-código lineal q-nario}\}\]
\end{defn}

\obs $B_q(n,d)\leq A_q(n,d)$ y en ocasiones se tiene la desigualdad escrita.

\begin{defn}[Matriz generadora]
Sea $\algb{C}$ un [n,k,d]-código lineal q-nario decimos que una matriz $M$ es generadora de $\algb{C}$ si y sólo si las filas de $M$ son una base de $\algb{C}$.

Esta matriz deberá ser de dimensión $k\times n$.
\end{defn}

Decimos que $H \in \text{Mat}(F_q)$ será \concept{matriz controladora de paridad} para $\algb{C}$ si dado $x\in F_q^n$ se cumple
\[x \in \algb{C} \iff Hx^t = 0 \]

\begin{example}
Consideramos el código $Ham(3,2)$ como se ha definido anteriormente. En este caso tendremos:
\[H = \left(\begin{array}{ccccccc}
1 & 1 & 1 & 0 & 0 & 0 & 1\\
1 & 1 & 0 & 1 & 0 & 1 & 0\\
1 & 0 & 1 & 1 & 1 & 0 & 0
\end{array} \right), \ \ G=\left(\begin{array}{ccccccc}
1 & 0 & 0 & 0 & 1 & 1 & 1\\
0 & 1 & 0 & 0 & 0 & 1 & 1\\
0 & 0 & 1 & 0 & 1 & 0 & 1\\
0 & 0 & 0 & 1 & 1 & 1 & 0
\end{array}\right) \]
\end{example}

\begin{prop}\label{prop:At-At}
Si la matriz generadora puede escribirse como la matriz identidad complementada con algo, es decir:
\[G=[I_k \ | \ A_{k \times (n-k)}]\]
entonces podemos tomar
\[H = [-A^t_{(n-k) \times k} \ | \ I_{n-k}]\]
donde $k$ es la dimensión del código e $I$ es la matriz identidad.
\end{prop}

\obs Sea $H$ la matriz de control de paridad de un código lineal y sea $G$ su matriz generadora, siempre que satisface que: $HG^t=0$

\begin{proof}
Si escribimos
\[G = \begin{bmatrix} g_1 \\ g_2 \\ \vdots \\ g_k\end{bmatrix}\]
tenemos que, por definición de matriz generadora, cada fila $g_i \in F_q^n$.

Por tanto, por definición de matriz controladora de paridad tenemos que $Hg_i^t = 0$, lo que nos muestra que:
\[HG^t = 0\]
\end{proof}


\obs Las siguientes operaciones en $G$ o en $H$ no cambian el código:
\begin{enumerate}
\item Intercambiar dos filas
\item Multiplicar una fila por un $λ\in F_q$ no nulo.
\item Sumar a una fila un múltiplo de otra
\end{enumerate}

\begin{defn}[Códigos lineales equivalentes]
Dos códigos lineales son equivalentes si uno se obtiene del otro a partir de una combinación de las siguientes operaciones
\begin{enumerate}
\item Intercambiar dos variables.

Esta operación se traduce en intercambiar dos columnas en las matrices $G$ o $H$.

\item Multiplicar los símbolos de la variable $x_i$ por $λ \in F_q$ no nulo.

Esta operación se traduce en multiplicar toda una columna de $G$ por λ o una columna de $H$ por $\frac{1}{λ}$.
\end{enumerate}
\end{defn}

\begin{example}
Dado el código $Ham(3,2)$, del que hemos hallado las matrices $G$ y $H$ en el ejemplo anterior, podemos ver que la matriz
\[\tilde{H} = \left(\begin{array}{ccccccc}
0 & 0 & 0 & 1 & 1 & 1 & 1\\
0 & 1 & 1 & 0 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 1 & 0 & 1
\end{array} \right)\]
define el mismo código.
\end{example}

Podemos observar que la matriz $H$ tiene 7 columnas y todas son no nulas, distintas entre si y de longitud 3. Por tanto está considerando todas las posibles combinaciones de tres bits (sin contar la combinación ``000'') lo que nos garantiza que cualquier matriz que satisfaga esta propiedad definirá el mismo código.

\begin{prop}
Todo código lineal es equivalente a otro de la forma:
\[G=[I \ |\  A ], , H = [-A^t \ | \ I]\]
este formato se denomina \concept{notación estándar}
\end{prop}

\begin{proof}
La idea es que siempre podemos aplicar Gauss forzando a tener columnas que sólo tengan un elemento no nulo.

Una vez tenemos esto, si estas columnas no están formando la matriz identidad, siempre podemos intercambiar columnas para obtener esta matriz.

Para garantizar la presencia de $A^t$ y $-A^t$ nos apoyamos en la propisición \ref{prop:At-At}
\end{proof}

Como ya comentamos al principio del capítulo existen ciertas ventajas en el empleo de códigos lineales. Una de estas ventajas es la facilidad de codificación y descodificación de mensajes cuando empleemos este tipo de códigos.

\section{Codificación con códigos lineales}
Tenemos que, por lo visto en la sección anterior, la matriz $G$ tiene dimensión $k \times n$. Sea
\[G = \left(\begin{array}{l}g_1\\g_2\\ \vdots \\g_k\end{array}\right)\]

A partir de esta matriz podemos deducir el código:
\[\algb{C} = \left\{\sum_{i = 1}^k λ_ig_i \tq λ_i \in F_q \right\} = \{(λ_1, \cdots, λ_k)G \tq (λ_1, \cdots, λ_k) \in  F_q^k\}\]

La forma de codificar pasará por emplear una función de la forma
\[\appl{f}{F_q^k}{\algb{C}}\]
que tomará un vector cualquiera $(λ_1,\cdots,λ_k)$ de $F_q^k$ y nos devolverá una palabra de nuestro código.

Si tenemos que el código es lineal y está en su forma estándar tenemos que las primeras $k$ cifras del código codificado coinciden con el mensaje original, de modo que la decodificación será trivial.

\begin{example}
Si el código no está en forma estandar y ciframos la palabra $(a,b)$ tenemos:
\[(a,b)\left(\begin{array}{cccc} 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1\end{array}\right) = (a,a,b,b)\]
que no nos permite conocer de forma inmediata la palabra original.

Por otro lado, si el código si que se encuentra en formato estándar tendremos:
\[(a,b)\left(\begin{array}{cccc} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1\end{array}\right) = (a,b,a,b)\]

Con lo que al principio de la palabra codificada tendremos la palabra original.
\end{example}

\section{Decodificación con códigos lineales}
Ya hemos visto al final del apartado anterior cómo puede leerse el mensaje original a partir del mensaje codificado. El problema de la decodificación radica en la corrección de los posibles errores que nos encontremos.

Ahora supongamos que tenemos un código lineal $\algb{C} \in F_q^n$ y que hemos recibido un mensaje $y \in F_q^n$. ¿Qué debemos hacer?.

El objetivo es encontrar un $x \in \algb{C}$ lo más próximo posible a $y$, es decir, tendremos que hacer que el error, $y-x=e$, sea mínimo.

Si observamos el conjunto cociente $F_q^n/\algb{C}$, lo que tendremos que hacer es encontrar el elemento de peso mínimo de entre todos los de la clase de equivalencia de $y$.

Esta es la idea de la tabla de \concept{Slepian}. Veamos el funcionamiento de esta tabla con un ejemplo.

\begin{example}
Sea el código dado por la matriz generadora:
\[G = \left( \begin{array}{cccc} 1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 1 \end{array}\right)\].

La tabla se construye por filas. Para cada fila empezamos tomando una cadena de peso mínimo que aún no haya aparecido en la tabla.

Para continuar la construcción de la fila, añadiremos los elementos que pertenecen a la misma familia. Lo haremos escribiendo los resultados de aplicar combinaciones lineales de los generadores sobre el elemento que representa la clase de equivalencia.

Finalmente la tabla queda:
\[\begin{array}{cccc}
0000 & 1010 & 0111 & 1101 \\
1000 & 0010 & 1111 & 0101 \\
0100 & 1110 & 0011 & 1001 \\
0001 & 1011 & 0110 & 1100
\end{array}\]

La idea que hay de fondo es que estamos tomando representantes de las diferentes clases de equivalencia (primera columna) y vemos todos los elementos que pertenecen a esa clase.

Ahora buscamos el mensaje recibido en esta tabla y tomamos su representante con peso mínimo que, si hemos construido la tabla de la mejor forma posible, será el elemento más a su izquierda.\footnote{En la tabla ejemplo y en las que debemos hacer, empezamos tomando un representante de peso mínimo, que escribimos en la primera columna, y a su derecha escribimos el resto de elementos de la clase. No obstante, se obtiene un resultado equivalente si empezamos tomando un representante distinto.}

Una vez tenemos este representante, conocemos $e$, para conocer $x$ simplemente miramos la columna en la que se encuentra $y$ de forma que se satisface
\[y=x+e\]

\obs Puesto que estamos realizando una división del código en clases de equivalencia, no puede haber ningún elemento repetido en la tabla pues estas clases son disjuntas.
\end{example}

Con el procedimiento que acabamos de ver podemos corregir un error en los bits primero, segundo y cuarto, pero no en el tercero.

Por otro lado, podíamos haber construido de forma distinta la segunda fila, cambiando el representante de peso mínimo tomado. En ese caso seríamos incapaces de detectar errores en el primer bit.

Como conclusión podemos ver que este código no es tan bueno para corregir errores. Veamos otro ejemplo con un mejor código, el que en su día llamamos $\algb{C}_5$.

\begin{example}
Tomamos el código dado por la matriz generadora
\[G = \left(\begin{array}{ccccc} 1 & 0 & 1 & 1 & 0 \\ 0 & 1 & 1 & 0 & 1 \end{array}\right) \]

En esta ocasión tenemos $n=5$, $k=2$, $r(=n-k)=3$, $d=3$ y la tabla de Slepian queda:
\[\begin{array}{cccc}
00000 & 10110 & 01101 & 11011 \\
10000 & 00110 & 11101 & 01011 \\
01000 & 11110 & 00101 & 10011 \\
00100 & 10010 & 01001 & 11111 \\
00010 & 10100 & 01111 & 11001 \\
00001 & 10111 & 01100 & 11010 \\
11000 & 01110 & 10101 & 00011 \\
10001 & 00111 & 11100 & 01010
\end{array}\]

Con esta tabla seremos capaces de corregir un total de 7 errores: los 5 errores simples, error en los dos primeros bits, y error en el primer y último bit.

\end{example}

En este tipo de ocasiones en que el número de errores que podemos corregir está limitado, lo que se suele hacer es \concept{decodificación incompleta} que consiste en tomar sólo una parte de la tabla, es decir, nos quedamos solo con aquellas filas en las que el lider $e$ tiene peso menor o igual que $\frac{d-1}{2}$.

Así siempre que recibamos un mensaje $y \in [e]$ con $w(e) \leq \frac{d-1}{2}$ leemos $x=y-e$ y en caso contrario \textbf{PITO}.

Para casos pequeños como el que estamos viendo es muy sencillo encontrar la palabra recibida $y$ en la tabla. No obstante, si trabajamos en un caso más real con palabras de longitud $256$ tendremos tablas de $2^{256}$ elementos donde buscar es bastante complicado.

Sin embargo, no es necesario tener todos los elementos de la tabla calculados.

Para empezar, hay un síndrome clarísimo que nos permite saber si la palabra no está en el código que consiste en emplear la matriz controladora de paridad. Se define así el \concept{síndrome de una palabra} como:
\[S(y) = Hy^t\]
y consideramos que
\[y \in \algb{C} \iff S(y) = 0 \text{ está sano, no tiene síndromes}\]

\obs
\[y_1 \equiv y_2 \mod \algb{C} \iff S(y_1)=S(y_2)\]
\begin{proof}
\[y_1 = y_2 \mod \algb{C} \iff y_1-y_2 \in \algb{C} \iff H(y_1-y_2)^t = 0 \iff Hy_1^t = Hy_2^t \iff S(y_1)=S(y_2)\]
\end{proof}

Por tanto, podemos saber en qué fila nos encontramos sólo con calcular el síndrome de la palabra recibida. Una vez tenemos la fila localizada, basta con que construyamos esta fila, ahorrándonos el proceso de construcción de la tabla completa.

\begin{example}
Retomando el ejemplo anterior, tenemos
\[G = \left(\begin{array}{ccccc} 1 & 0 & 1 & 1 & 0 \\ 0 & 1 & 1 & 0 & 1 \end{array}\right)  \implies H = \left(\begin{array}{ccccc}
1 & 1 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 1
\end{array}\right) \]

Ahora calculamos el síndrome de cada representante de clase de la tabla de Slepian:

\[\begin{array}{cc}
00000 & 000\\
10000 & 110\\
01000 & 101\\
00100 & 100\\
00010 & 010\\
00001 & 001\\
11000 & 011\\
10001 & 111
\end{array}\]

Supongamos que hemos recibido el mensaje $y=01001$ calculamos su síndrome, que nos da $(y)=100$ con lo que asumimos que $e=00100$ con lo que leemos el mensaje $x=01101$ que es una palabra del código.
\end{example}

\subsection{Decodificación por síndrome para Ham(3,2)}
Cuando trabajamos con Ham(3,2), que es un $(7,16,3)$-código binario, tenemos la matriz comprobadora de paridad:
\[H = \left(\begin{array}{ccccccc}
0 & 0 & 0 & 1 & 1 & 1 & 1 \\
0 & 1 & 0 & 0 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 1 & 0 & 1
\end{array}\right) \]

Cuando recibimos una palabra $y$, calculamos su síndrome $S(y)=Hy^t$. Si este síndrome vale 0, entonces leemos la palabra $y$.

Puesto que todas las combinaciones posibles de 3 bits están representadas como columnas de la matriz $H$, en caso de que el síndrome no sea el vector 0, este será igual a una de las columnas de la matriz $H_i$. En esta ocasión leeríamos $\tilde{y}=y-e_i$.

Alternativamente:
\[Sy=\left( \begin{array}{c} s_2 \\ s_1 \\ s_0 \end{array}\right) \implies \text{ leo } \tilde{y} = y-e_{s_0+2s_1+4s_2}\]
siendo esto posible sólamente si escribimos la matriz $H$ como hemos mostrado al inicio de la sección, donde cada columna $i$ se corresponde con la representación binaria del número $i$.

Dado un código lineal con matriz generadora $G_{k \times n}$ y matriz controladora de paridad $H_{r\times n}$ siendo $r=n-k$, el \concept{Código dual $\algb{C}^*$} es aquel que tiene como matriz generadora la $H_{r\times n}$. Por construcción, la matriz controladora del código dual será la matriz $G_{k\times n}$.

A continuación veremos el resultado más importante de lo que resta de curso. La idea es que en un código lineal existen formas alternativas de calcular la distancia mínima.

En general, si $|\algb{C}|=M$, la primera forma de calcular $d(\algb{C})$ que estudiamos requería ${M \choose 2}$ cálculos de distancias entre palabras.

Esta cota ya la mejoramos al estudiar el concepto de los pesos con lo que logramos reducir la cantidad de operaciones necesarias a $M-1$ cálculos de pesos de palabras.

Pero esto puede mejorarse más aún, puesto que $H$ contiene la información necesaria para conocer $d(\algb{C})$.

\begin{obs}
Sea la matriz $H=[H_1,...,H_n]$ y sea $x \in \algb{C}$. Considerando $w=w(x)$ entonces sabemos que $x_{i_1},...,x_{i_w}\neq 0$, por definición de peso de una palabra.

Por definición de la matriz controladora tenemos:
\[Hx^t=0 \implies x_{i_1}H_1 + \cdots + x_{i_w}H_w = 0\]
es decir, tenemos una combinación lineal de $w$ columnas de $H$ en la que ningún coeficiente es 0.
\end{obs}

\begin{prop}
Sea $\algb{C}$ un código lineal con matriz controladora $H$ y $d(\algb{C})=d$, son ciertas las siguientes afirmaciones:
\begin{enumerate}
\item $H$ no puede tener $d-1$ (o menos) columnas linealmente dependientes
\item $H$ tiene $d$ columnas linealmente dependientes
\end{enumerate}
\end{prop}
\begin{proof}
\begin{enumerate}
\item Si tuviéramos $d-1$ columnas linealmente independientes, atendiendo a la observación anterior, tendríamos una palabra de peso $d-1$ que pertenece al código, lo que nos daría que la distancia mínima no podría ser $d$.

\item Sea $x$ una palabra del código con $w(x)=d$, entonces tenemos una relación de dependencia entre $d$ columnas de la matriz $H$.
\end{enumerate}
\end{proof}

\begin{corol}
Sea $\algb{C}$ un código lineal con matriz controladora $H$, entonces $d(\algb{C})=d$ si y sólo si se cumplen las dos propiedades siguientes:
\begin{enumerate}
\item $d-1$ columnas \textbf{cualesquiera} de $H$ sean linealmente independientes
\item Existen $d$ columnas de $H$ linealmente dependientes.
\end{enumerate}
\end{corol}
\begin{proof}
La perimera afirmación nos garantiza que la distancia mínima del código será al menos $d$.

Por otro lado, la segunda afirmación nos garantiza que la distancia mínima del código seá como mucho $d$.

Combinando ambas afirmaciones, queda probado que la distancia mínima es $d$.
\end{proof}

Veamos un pequeño ejemplo en binario para entender estos conceptos.
\begin{example}
Tomamos el código binario $\algb{C}$ con matriz controladora de paridad:
\[H = \left(\begin{array}{cccccc}
1 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 & 1 & 0\\
0 & 1 & 1 & 0 & 0 & 1
\end{array}\right) \]

Vamos a calcular ahora la distancia mínima del código a partir de la matriz $H$.

Comprobamos si la distancia mínima puede ser mayor o igual que 2. Esto implicaría, por el corolario anterior, que toda columna es linealmente independiente es decir, que toda columna es no nula.

Puesto que nuestra matriz cumple esta propiedad ya sabemos que la distancia mínima será, al menos, 2.

Vemos ahora que ocurre para $d\geq 3$. Si esto fuese así, dos columnas cualesquiera de la matriz serían independientes, propiedad que se cumple.

Vamos a ver si $d \geq 4$. Para ello debemos ver si tres columnas cualesquiera son linealmente independientes. En este caso es sencillo ver que
\[H_1=H_4+H_5\]
por lo que la distancia mínima no puede ser 4.

Como conlusión tenemos que el código dado tiene distancia mínima igual a 3 y que carolina es preciosa.
\end{example}

Recordemos que el rango de una matriz nos daba el máximo número de columnas linealmente independientes. Por tanto, siendo $r$ el rango de la matriz controladora de paridad, sabemos que $r+1$ columnas siempre serán dependientes por lo que $d\leq r+1$.

Podemos ver que no tendremos siempre la igualdad (de hecho por eso hemos escrito una desigualdad) puesto que el rango nos garantiza la existencia de $r$ columnas independientes pero no garantiza que todas las combinaciones de $r$ columnas sean independientes.

