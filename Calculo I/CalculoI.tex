\documentclass[palatino, nochap]{apuntes}

\usepackage{fancysprefs}
\usepackage{tikztools}
\usetikzlibrary{arrows}
%
\author{Guillermo Julián Moreno}
\date{2011/2012 - C1}
\title{Cálculo I}
%

\begin{document}
\begin{abstract}
Apuntes de Cálculo I del curso 2011/2012, tomados en las clases del profesor Fernando Soria. Cubren límites, derivadas e integrales en una variable, junto con una cierta base matemática necesaria para poder manejar los conceptos.
\end{abstract}

\maketitle
\newpage

\tableofcontents

\newpage

\section{Fundamentos}
\label{sec:Fundamentos}

Una de las primeras cosas que diferencia las matemáticas de verdad de las matemáticas que uno pueda dar en el instituto o en carreras de ingeniería es que aquí nos interesan las razones. Queremos saber no sólo qué es lo que ocurre, sino también por qué. Nos interesa saber de dónde vienen los conceptos que manejamos, cómo se construyen y qué limitaciones tienen, para poder manejarlos nostoros a nuestra manera y poder construir sobre ello. Por eso, empezamos primero con una sección dedicada a los fundamentos necesarios para poder tratar el cálculo formalmente y no sólo como una colección de fórmulas sin sentido.

\subsection{Pruebas e inducción}

La primera idea que necesitamos manejar es, sin duda, el concepto de prueba. En el fondo no es más que una serie de transformaciones que hacemos para asegurarnos de que una cierta hipótesis es cierta. Por ejemplo, si queremos probar que $2 · 3 = 6$ (y suponemos que no sabemos multiplicar), podemos decir que $2·3 = 2 + 2 + 2 = 6$. A lo largo del curso haremos muchas pruebas de este estilo: partimos de una base que aceptamos como cierta (en el ejemplo tonto que acabamos de poner, suponemos que una multiplicación de un número por $n$ es sumar ese número $n$ veces) y empezamos a desarrollar a partir de ahí hasta llegar a lo que queremos demostrar.

Un tipo de prueba que a veces usaremos y que puede ser un poco contraintuitiva es la \concept{Inducción}. Son muy útiles cuando queremos demostrar que algo se cumple para todos los números naturales, porque nos permite llegar a esa prueba sin necesidad de andar probando para todos los números (cosa imposible, porque hay infinitos naturales). Vamos a ver un ejemplo con el \concept{Binomio\IS de Newton}.

El binomio de Newton, que se estudia en Bachillerato\footnote{Salvo que por enésima vez se haya cambiado el programa de estudios de Bachillerato.}, nos da una fórmula para la potencia $n$-ésima de una suma: \begin{align*} (x+y)^n &= \sum_{k=0}^n \comb{n}{k}x^{n-k}y^k = \\
&= \comb{n}{0} x^ny^0 + \comb{n}{1} x^{n-1}y^1 +  \dotsb + \comb{n}{n-1} x^1y^{n-1} + \comb{n}{n} x^0y^n \end{align*}, donde $\comb{n}{k}$ es un número combinatorio, dado por \[ \comb{n}{k} = \frac{n!}{k!(n-k)!} \]. Por si acaso, recordamos que $k!$ es lo que se llama un \concept{Factorial}, y es igual a la multiplicación de todos los números hasta $k$: $k! = k · (k-1) \dotsb 2 · 1$.

Bien, ¿cómo demostrarmos el binomio de Newton? Hacer el desarrollo puede ser infernal, y no podemos probarlo para todos los $n$ posibles porque hay infinitos. Pero podemos darnos cuenta de una cosa muy tonta: $(x+y)^{n+1}= (x+y)  · (x+y)^{n}$. Si supiésemos que el binomio de Newton se cumple para un cierto $n$, entonces podríamos probarlo para $n + 1$ fácilmente, porque esa cuenta tiene pinta de ser sencilla de hacer. Si repetimos el mismo paso, podríamos hacerlo para $n+2$, después para $n+3$, y así sucesivamente. Es decir, que con probar que podemos pasar de $n$ a $n+1$, podemos deducir que se cumple para todos los $n$. Sólo nos falta, eso sí, saber que tenemos un punto de partida válido.

Vamos a tratar de probar primero que tenemos el \textit{caso base}. Suponemos $n = 1$, y entonces vemos que efectivamente se cumple \[ x+y = \comb{1}{0} x^1y^0 + \comb{1}{1} x^0 y^1 = x + y\]

Ahora que tenemos ese caso base, podemos tratar de probar que podemos dar el salto de $n$ a $n+1$ (esto es lo que se llama la \concept{Hipótesis\IS de inducción}). Suponemos entonces que ya lo tenemos probado para un cierto $n$, y operamos para ver si podemos demostrarlo para $n+1$. Decíamos antes que $(x+y)^{n+1} = (x+y)(x+y)^{n}$. Entonces tenemos que \begin{align*} (x+y)(x+y)^{n} &\eqreasonup{\text{Se cumple para } n} (x+y) · \sum_{k=0}^n \comb{n}{k}x^{n-k}y^k  =  \\
&= x · \sum_{k=0}^n \comb{n}{k} x^{n-k}y^k + y · \sum_{k=0}^n \comb{n}{k} x^{n-k}y^k \\
&= \sum_{k=0}^n \comb{n}{k} x^{n-k + 1}y^k + \sum_{k=0}^n \comb{n}{k} x^{n-k}y^{k+1}
\end{align*}

Para hacer esa suma, vamos a fijarnos en los coeficientes de los polinomios y sumarlos. En la suma de la izquierda, el coeficiente de $x^{n-k+1}y^{k}$ será $\comb{n}{k}$. En el de la derecha, será $\comb{n}{k-1}$, ya que $x^{n-(k-1)}y^{(k-1)+1} = x^{n-k+1}y^k$. Sumando ambos coeficientes nos queda que\footnote{Se trata de la identidad de Pascal, si uno no se fía puede sumar las fracciones y ver qué queda.} \[ \comb{n}{k} + \comb{n}{k-1} = \comb{n+1}{k} \]

Por último, como tenemos todos los posibles monomios $x^{n+1}$, $x^ny^1$, etc, hasta $x^1y^n$ y $y^{n+1}$, nos queda efectivamente lo que estábamos buscando: \[ (x+y)^{n+1} =\sum_{k=0}^{n+1}\comb{n+1}{k} x^{n+1-k}y^{k}\] y ya está la prueba hecha, así que dejamos nuestro símbolo de ``demostrado'': \qed

\subsection{Conjuntos}

Una de las bases de las mátemáticas son los conjuntos, que no son más que colecciones de elementos. Hay tres formas de definirlo:

\begin{enumerate}
\item Enumerar los elementos: $A=\{a,b,c,\dotsc\}$.
\item Operaciones con conjuntos: $A=B\cap C$ son los elementos que están en $B$ y en $C$.
\item A través de una fórmula que ``filtre'' elementos de otro conjunto: $A=\{x\in B \tq x \text{ es par}\}$.
\end{enumerate}

La principal razón para limitarnos a definir los conjuntos así es para evitarnos paradojas como la de Russell. Si pudiésemos definir conjuntos sin construirlos o sin filtrar de conjuntos ya existentes, podríamos definir el conjunto $R$ de los conjuntos que no son miembros de sí mismos. Si $R$ está en $R$, entonces es un miembro de sí mismo y no puede estar en $R$. Por otra parte, si $R$ no está en $R$, entonces no es miembro de sí mismo y sí está en $R$. Como uno puede imaginar, este tipo de paradoja no es muy deseable si queremos hacer matemáticas estrictas que siempre funcionen.

En la asignatura de Cálculo los conjuntos no nos interesarán de momento demasiado salvo por unos conceptos simples:

\begin{defn}[Cardinal\IS de un conjunto] El cardinal de un conjunto es el número de elementos que contiene. Se denota por $\card{A}$.
\end{defn}

También querremos construir conjuntos como pares de elementos de uno y otro. Definiremos un par ordenado o \concept{Tupla} como $(a,b)$, y diremos que un par $(a,b)$ es igual a otro $(c,d)$ si y sólo si $a = c$ y $b = d$. A partir de esto, definimos el producto cartesiano:

\begin{defn}[Producto cartesiano o directo][Producto\IS cartesiano]
Sean $X$ e $Y$ dos conjuntos. Entonces $X\x Y$ es el conjunto de tuplas con un elemento en $X$ y otro en $Y$. Formalmente, \[ X × Y =\set{(a,b)\tq a\in X\; b\in Y} \]

Si $X$ e $Y$ son finitos, $\card{(X\x Y)} =\card{X} \cdot \card{Y}$
\end{defn}

\subsection{Sucesiones y límites}

El cálculo nos lleva a estudiar lo que pasa cuando nos acercamos mucho a un punto. Por ejemplo, la derivada mira qué le ocurre al valor de una función cuando avanzamos o retrocedemos muy poquito. Si primero avanzamos mucho y sucesivamente nos vamos moviendo menos, tendremos una serie de números, y querremos ver si se acercan a un determinado valor o si se van a tomar viento fresco y cada vez se separan más.

Para poder comprender bien esos conceptos, estudiaremos las sucesiones $\set{x_n}_{n≥1} = x_1, x_2, \dotsc, x_n, \dotsc$, que no son más que colecciones ordenadas de números. Ese cierto número al que se acercan, si existe, se llama límite, y nos lleva a definir lo que es una serie convergente.

\begin{figure}[hbtp]
\centering
\begin{tikzpicture}
\draw[->] (-0.1, 0) -- (9,0);
\draw[->] (0,-0.1) -- (0,3);

\fill[blue, fill opacity = 0.2] (8.5,1.8) rectangle (0, 1.2);

\foreach[count=\i] \x in {0.8,1.6,...,8} {
	\pgfmathsetmacro{\cnt}{2 * \i - 1}
	\node[nodepoint, label={above:$x_{\pgfmathprintnumber{\cnt}}$}] at (\x, 0.6 / \x + 1.5) {};
}
\foreach[count=\i] \x in {0.8,1.6,...,8} {
	\pgfmathsetmacro{\cnt}{2 * \i}
	\node[nodepoint, label={below:$x_{\pgfmathprintnumber{\cnt}}$}] at (\x + 0.4, -0.6 / \x + 1.5) {};
}

\node[nodepoint, blue, label={right:$l$}] (L) at (8.5, 1.5) {};
\draw[(-), blue] (8.5,1.8)  node[anchor = south west] {$\varepsilon$} -- (8.5,1.2);

\end{tikzpicture}
\caption{Ilustración de la convergencia: da igual lo pequeño que tomemos ε, porque a partir de un cierto $x_n$ todos los elementos siguientes se quedan dentro de la banda de ancho ε centrada en $l$.}
\label{fig:Convergencia}
\end{figure}

\begin{defn}[Convergencia][Convergencia!de sucesiones]
Una sucesión $x_n$ es convergente a $l$ (o tiene límite $l$) si $\forall \epsilon > 0 \;\exists n_0 \in \nat \tq \forall n > n_0 \;\abs{x_n -l} < \epsilon$. Esto es, si para cualquier ε que tomemos, por pequeño que sea, hay un cierto número $n_0$ para el cual cualquier elemento que vaya por detrás de $x_{n_0}$ está contenido en una banda de ancho $ε$ centrada en $l$ (ver \fref{fig:Convergencia}).

La notación es $\lim_{n\to ∞} x_n = l$. Hay que tener en cuenta que $n_0$ suele depender del $\epsilon$ que tomemos.
\end{defn}

Calcular ese límite require que tengamos ciertas herramientas. Una muy útil (por lo gráfico de su nombre) es el teorema del Sándwhich.

\begin{theorem}[Teorema del Sándwich o principio de comparación][Teorema!del sándwich]
Sean $a_n,\: b_n, \: c_n$ tres sucesiones, y $a_n\leq b_n \leq c_n \:\forall n$. Si $a_n$ y $c_n$ convergen al mismo límite $\alpha$, entonces $b_n$ tiene límite y es $\alpha$.

En otras palabras, si podemos encerrar a una sucesión entre dos sucesiones que convergen a lo mismo, entonces tiene límite y es el mismo.
\end{theorem}

Los límites los podremos calcular también haciendo operaciones, como la suma o producto.

\begin{prop}[Cálculo de límites]
\[ \lim (a_n \pm b_n) = \lim a_n \pm \lim b_n \]
\[ \lim (a_n b_n) = \lim a_n \cdot \lim b_n \]
\[ \lim \frac{a_n}{b_n} = \frac{\lim a_n}{\lim b_n} \; ;\lim b_n \neq 0\]
\end{prop}

\begin{lemma}
Si $A\neq \emptyset$ entonces $\exists\: \{a_n\} \in A$ monótona creciente tal que $\exists \lim_{n\to\infty} a_n= sup(A)$.\end{lemma}

\begin{defn}[Sucesión de Cauchy][Cauchy!Sucesión de]
Una sucesión $\{a_n\}$ es de Cauchy si $\forall \epsilon > 0 \; \exists n_o \in \nat \tq |y_n-y_{n'}| < \epsilon \; \forall n,n' \geq n_o$.\end{defn}

\begin{prop}Toda sucesión de Cauchy tiene límite.\end{prop}

\begin{defn}[Subsucesión] Dada una sucesión $\{x_n\}$ se dice que $\{y_k\}$ es una subsucesión de $\{x_n\}$ si existen índices $n_1 < n_2 < \cdots < n_k < \cdots$ tales que $y_k=x_{n_k}$.\end{defn}

\begin{theorem}[Principio del palomar]
Sean $A$ y $B$ dos conjuntos tales que $\card{(A}) < \card{(B})$. No existe una aplicación inyectiva entre $A$ y $B$.

Es decir, si tenemos $m$ huecos y tenemos que meter $n>m$ elementos, en algún hueco hay más de un elemento.
\end{theorem}

\begin{theorem}[Teorema\IS de Bolzano-Weierstrass]
Toda sucesión $\{x_n\}$ acotada posee una subsucesión convergente.
\end{theorem}

\begin{proof}
Supongamos que $a_0 ≤ x_n ≤ b_0\; \forall n$. Hay infinitos $\x_n$ distintos. Sea $I_0=[a_0, b_0]$: una de sus dos mitades, que llamaremos $I_1=[a_1,b_1]$, contiene infinitos elementos $x_n$. A su vez, una de sus dos mitades de $I_1$, a la que llamamos $I_2=[a_2, b_2]$, contiene infinitos elementos.

Por recurrencia, existen intervalos $I_0 \subset I_1 \subset \cdots \subset I_k=[a_k, b_k]$ cada uno de los cuales contiene infinitos elementos de la sucesión. Además, la longitud de cada uno es la mitad del anterior. Observamos que $a_0 \leq a_1 \leq \cdots \leq a_k \leq a_{k+a} \leq \cdots \leq b_{k+1} \leq b_k \leq \cdots \leq b_1 \leq b_0$. Luego $\exists \lim_{k\to\infty}a_k =\lim_{k\to\infty}b_k$.

Construimos la subsucesión de forma siguiente. $x_{n_1} \in I_1$, $x_{n_2} \in I_2$ pero con $n_1 < n_2$. Luego $\exists \lim_{k\to\infty} x_{n_k} = \lim_{k\to\infty}a_k =\lim_{k\to\infty}b_k$.
\end{proof}

\begin{corol}Toda sucesión de Cauchy es convergente en $\real$.\end{corol}

\begin{proof}Sea $\{x_n\}$ de Cauchy, luego es acotada. Por el teorema de Bolzano-Weierstrass existe una sucesión convergente $\{x_{n_k}\}$. Por lo tanto $\exists L = \lim_{k\to\infty}x_{n_k}$. Entonces, se tiene que $\exists \lim_{n\to\infty} x_n = L$.\end{proof}

\begin{lemma}La serie $\sum^{\infty}a_n$ converge si $\exists\lim_{n\to\infty}S_n$.\end{lemma}

\section{Funciones}

\begin{defn}[Función]
Una relación $f\subset A\x B$ es función si $\forall x \in A \; \uexists y \in B$. Se denota como $\appl{f}{A}{B}$.
\end{defn}

\begin{defn}[Antiimagen]
Sea \stdf. La antiimagen de un conjunto $B\subset Y$ es
\[ f^{-1}(B)=\{ x\in X \tq f(x) \in B\}\].
\end{defn}

\begin{remark}No confundir la antiimagen con la función inversa. La antiimagen es sólo con conjuntos, la inversa con elementos. Por ejemplo, si $x$ es un elemento y $X$ es un conjunto, $f^{-1}(f(x))=x$, pero $f^{-1}(f(X))$ no tiene por qué ser igual a $X$.

Por ejemplo, sea $X=[0,2]$ y $f(x)=x^2$. La imagen de $X$ es $[0,4]$. Sin embargo, la antiimagen de $[0,4]$ es $[-2,2]\neq X$.
\end{remark}

\begin{defn}[Función\IS inyectiva] Una función \stdf es inyectiva si elementos distintos tienen imágenes distintas: $f(x)=f(y) \implies x=y$.
\end{defn}

\begin{defn}[Función\IS sobreyectiva]
Una función \stdf es sobreyectiva si $f(X)=Y$, es decir: $\forall y \in Y \exists x \in X \tq f(x)=y$.
\end{defn}

\begin{defn}[Función\IS biyectiva]
Una funcion es biyectiva si es inyectiva y sobreyectiva.
\end{defn}

\begin{defn}[Función\IS inversa] Sea \stdf una función. La relación inversa es $\appl{f^{-1}}{Y}{X}$, y si es función, se dice que $f^{-1}$ es la inversa de $f$.
\end{defn}

\begin{prop} $f$ es invertible si y sólo si es inyectiva.

\begin{proof}
Para que \stdf sea invertible, $\appl{f^{-1}}{Y}{X}$ tiene que ser función. Es decir, que $\forall y\in Y \uexists x \in X$. Comprobamos primero la existencia de imagen para cualquier elemento de $Y$. Si $f$ es sobreyectiva, entonces tenemos que $Y=f(X)$. Por lo tanto, cualquier elemento de $Y$ tiene una imagen en $X$. Si no fuese sobreyectiva existiría algún elemento en $Y$ que no fuese imagen de un elemento de $X$, así que $f^{-1}$ no sería función.

Pasamos ahora a demostrar la unicidad de la imagen para cualquier elemento de $Y$. Si $f$ es inyectiva, tenemos que $\forall x,x' \in X \: f(x)=f(x') \dimplies x=x'$. Cada elemento de $X$ esta relacionado con un sólo un elemento de $Y$, por lo que cada elemento de $Y$ tiene una sola imagen. Si no fuera inyectiva, algún elemento de $Y$ tendría dos imágenes en $X$ y la relación inversa no sería función.
\end{proof}
\end{prop}

\begin{defn}[Composición][Función\IS composición de] Sean $\appl{f}{A}{B}$ y $\appl{g}{C}{D}$, y $f(A)\subset C$. Entonces se define la composición $f$ compuesto con $g$ como $\appl{g\circ f}{A}{D}$, tal que $(g \circ f)(x)=g(f(x)), x \in A$.

La composición de funciones cumple la propiedad asociativa ($(f\circ g)\circ h=f\circ (g\circ h)$).

Si $f$ y $g$ son sobreyectivas, entonces $g\circ f$ también lo es.
\end{defn}

\begin{defn}[Límite] Sea $\appl{f}{A}{\real}, A\subset\real$ se dice que $f$ tiene límte $L$ en el punto $a$ si $\forall \epsilon > 0 \; \exists \delta >0 \tq (x\in A \y |x-a|<\delta) \implies |f(x)-L|<\epsilon$.

Se escribe como \[\lim_{x\to a}f(x)=L\]
\end{defn}

\begin{remark}
No es necesario que $a\in A$.\end{remark}

\begin{defn}[Límite\IS lateral] Se define el límite lateral por la derecha de $\appl{f}{A}{\real}, A\subset\real$ (con valor $L_d$ como aquel que cumple que $\forall  \epsilon >0 \;\exists\delta >0 \tq( x\in A \y |x-a| <\delta \y x>a) \implies |f(x)-L_d|<\epsilon$. La definición de límite lateral por la izquierda es análoga, salvo que $x<a$.

Los límites laterales por la derecha e izquierda se escriben, respectivamente, como
\[ \lim_{x\to a^{+}}f(x)=L_d\]
\[ \lim_{x\to a^{-}}f(x)=L_i\]
\end{defn}

\begin{defn}[Límite\IS en el infinito] Se dice que $\appl{f}{A}{\real}, A\subset\real$ tiene límite $L$ para $x\to +\infty$ si $\forall \epsilon > 0\; \exists M >0 \tq (x>M\y x\in A)\implies |f(x)-L|<\epsilon$. La definición es análoga cuando $x\to -\infty$, salvo que $x<-M$.

\end{defn}

\begin{theorem}
$f$ tiene límite $L$ en $a$ si y sólo si \underline{toda} sucesión $\{x_n\}\subset dom(f)$ con $\lim_{n\to\infty} x_n=a$ cumple que $\{f(x_n)\}$ forman una sucesión convergente a $L$ ($\lim_{n\to\infty}f(x_n)=L$).
\end{theorem}

\begin{defn}[Continuidad en un punto][Continuidad] Se dice que $\appl{f}{A}{\real}$ es continua en $a$ si cumple que $a\in A \y \exists \lim_{x\to a} f(x) \y \lim_{x\to a} f(x) = f(a)$.\end{defn}

\begin{defn}[Función\IS continua] Una función es continua si lo es en todos los puntos de su dominio.\end{defn}

\begin{theorem}[Lema del sándwich para funciones - Principio de comparación][Teorema\IS del sándwich]
\[(f(x) \leq g(x) \leq h(x) \; \forall x \neq c \y \exists \; \lim_{x\to c}f(x) =\lim_{x\to c}h(x)=L) \implies \exists \lim_{x\to c}g(x) = L\]
\end{theorem}

\begin{proof}
Sea $\{x_n\}$ una sucesión tal que $\neg c \in \{x_n\}$ y $\lim_{n\to\infty}x_n = c$. Entonces, tenemos que $\lim_{n\to\infty}f(x_n)=\lim_{n\to\infty}h(x_n)=L$. Por el lema del sandwich para las sucesiones, tenemos que $f(x_n)\leq g(x_n) \leq h(x_n) \implies \exists \lim_{n\to\infty}g(x_n)=L \implies \exists \lim_{x\to c}g(x) = L$.
\end{proof}

\begin{theorem}[Teorema de los valores intermedios - Teorema de Bolzano][Teorema\IS de Bolzano]
Si $\appl{f}{[a,b]}{\real}$ es continua y $f(a)<f(b)$, $\forall v \in (f(a),f(b)) \:\exists z \in [a,b] \tq f(z)=v$.
\end{theorem}

\begin{proof}
Sea $a_0=a$, $b_0=b$ y $I_0=[a_0, b_0]$. Sea $m_0$ el punto medio de $I_0$. Si $f(m_0)=v$, hemos terminado. Si no, cogemos $I_1$ como la parte izquierda de $I_0$ si $f(m_0) >v$ o la parte derecha si $f(m_0)<v$.

Por recurrencia encontramos intervalos $I_n=[a_n, b_n]$ tal que $I_0\supset I_1 \supset \cdots \supset I_n$, de forma que $f(a_n)<v<f(b_n)$. Se tiene que $a_0 \leq a_1 \leq \cdots \leq a_n \leq \cdots \leq b_n \leq \cdots \leq b_1 \leq b_0$. $z$ es el límite de $a_n$, que coincide con el límite de $b_n$. Por ser $f$ continua, $f(z)=\lim_{n\to\infty}f(a_n) \leq v$ y $f(z)=\lim_{n\to\infty}f(b_n) \geq v$, por lo tanto $f(z)=v$.
\end{proof}

\begin{defn}[Acotación de funciones][Función!acotada]
Se dice que $\appl{f}{A}{\real}$ está acotada superiormente si $\exists M \tq f(x)\leq M \: \forall x\in A$. La definición es análoga para la cota inferior.
\end{defn}

\begin{theorem}[Teorema\IS de Weierstrass]
\label{thmWeierstrass}
Toda función continua en un intervalo cerrado está acotada y alcanza su máximo y mínimo.
\end{theorem}

\begin{theorem}
Sea $\appl{f}{[a,b]}{\real}$ inyectiva y continua. Entonces, $f$ es estrictamente creciente o decreciente y $\inv{f}$ es continua.
\end{theorem}

\begin{remark}
Esto también quiere decir que\[f([a,b])=[f(a),f(b)]\]
\end{remark}

\begin{proof}
Como $f$ es inyectiva, $\forall a\neq b \; f(a)\neq f(b)$, es decir que $f(a)<f(b)$ o $f(a)>f(b)$. Suponemos el primer caso $f(a)<f(b)$.

\textit{Previo:} Sabemos que $a<x<b \implies f(a)<f(x)<f(b)$, ya que si $f(a)<f(b)<f(x)$ dado un $v\tq f(b)<v<f(x)$ por Bolzano $\exists x_1\in [a,x];\: x_2\in [x,b] \tq f(x_1)=f(x_2)=v$, contradicción con que la función es inyectiva.

Si la función no fuese estrictamente creciente, entonces $\exists x,y \in [a,b] \tq x<y \:\y\: f(x)>f(y)$. En ese caso, $a<x<y$ y $f(a)<f(y)<f(x)$, lo que es imposible según el argumento previo.

Demostramos la segunda parte, la continuidad de la inversa. Sea $\epsilon >0, \:x_0\in dom \inv{f} \tq f(x_0)=y_0$, buscamos $\delta \tq |y-y_0|<\delta \implies |\inv{f}(y)-x_0|<\epsilon$. Suponemos que $a<x_0-\epsilon < x_0+\epsilon < b$. Sea $\delta=min(y_0-f(x_0-\epsilon), f(x_0+\epsilon)-y_0)$. Entonces $|y_0-y|<\delta \implies y\in (f(x_0-\epsilon, f(x_0+\epsilon))$ luego $|\inv{f}(y)-x_0|<\epsilon$.
\end{proof}

\subsection{Derivadas}

\begin{defn}[Derivada]
Sea $f$ una función continua. La derivada en un punto $a$ es
\[f'(a)=\frac{df}{dx}(a)=\lim_{x\to a} \frac{f(x)-f(a)}{x-a} \]

Se dice que una $f$ es derivable en $a$ si y sólo si $\exists \lim_{x\to a} \frac{f(x)-f(a)}{x-a}$.

La derivada en un punto marca la pendiente de la recta tangente a la función en ese punto.
\end{defn}

\begin{prop}
Si $H(x)$ es una recta que pasa por el punto de la gráfica de $f$ $(a, f(a))$ y cumple que \[\lim_{x\to a} \frac{f(x)-H(x)}{x-a}=0\] entonces $f$ es derivable en $a$ y su derivada en ese punto es la pendiente de $H$.\end{prop}

\begin{proof}
Sea $H(x)= f(a)+m(x-a)$. Entonces \[ \frac{f(x)-H(x)}{x-a}=\frac{f(x)-f(a)-m(x-a)}{x-a}=\frac{f(x)-f(a)}{x-a}-m\] que tiende a 0 cuando $x\to a$. Por lo tanto, \[\exists \lim_{x\to a}\frac{f(x)-f(a)}{x-a}=m\].
\end{proof}

\begin{prop}[Cálculo operativo]
Sean $f, g$ derivables en $a$. Entonces
\begin{itemize}
\item $(f\pm g)'(a)=f'(a)\pm g'(a)$.
\item $(fg)'(a)=f'(a)g(a)+f(a)g'(a)$.
\item Si $g(a)\neq 0 $, $\left(\frac{1}{g}\right)'(a)=\frac{-g'(a)}{(g(a))^2}$.
\item Si $g(a)\neq 0$, $\left(\frac{f}{g}\right)'(a)=\frac{f'(a)g(a)-f(a)g'(a)}{(g(a))^2}$.
\item $(g\circ f)'(a)= g'(f(a))f'(a)$.
\end{itemize}
\end{prop}

\begin{prop}La derivada de una función derivable en un extremo local (máximo o mínimo) vale cero.\end{prop}

\begin{theorem}[Teorema\IS de Rolle]
Sea $\appl{f}{[a,b]}{\real} \tq f(a)=f(b)$ continua en el cerrado y derivable en el abierto. Entonces $\exists c \in (a,b) \tq f'(c)=0$.
\end{theorem}

\begin{proof}
Sólo es necesario buscar un extremo local en $(a,b)$. Por el teorema de Weierstrass (\ref{thmWeierstrass}) $f$ alcanza el máximo y mínimo $x_1, x_2$ en el intervalo $[a,b]$. Si $x_1\in (a,b)$ o $x_2 \in (a,b)$, hemos terminado. Si no, el máximo y mínimo están en los extremos y como $f(a)=f(b)$ $f$ es constante y su derivada siempre vale 0.
\end{proof}

\begin{theorem}[Teorema\IS del valor medio de Lagrange]
Sea $\appl{f}{[a,b]}{\real}$ continua en el cerrado y derivable en el abierto. Entonces, $\exists t \in (a,b) \tq f'(t)=\frac{f(b)-f(a)}{b-a}$.
\end{theorem}

\begin{proof}
Sea \[h(x)=f(x) - f(a)-\frac{f(b)-f(a)}{b-a}(x-a)\]. Es claro que $h(a)=h(b)$, por lo tanto $\exists c \in (a,b) \tq h'(c)=0$. Derivamos $h(x)$:

\[ h'(x)=f'(x) - \frac{f(b)-f(a)}{b-a} \]

Si $h'(c)=0$
\begin{align*}
h'(c)&=f'(c)-\frac{f(b)-f(a)}{b-a} \\
0 &= f'(c)-\frac{f(b)-f(a)}{b-a} \\
f'(c)&=\frac{f(b)-f(a)}{b-a}
\end{align*}
\end{proof}

\begin{theorem}[Teorema\IS del valor medio de Cauchy]
Sean $f$ y $\appl{g}{[a,b]}{\real}$ continuas en el cerrado y derivables en el abierto. Entonces $\exists t \in (a,b)$ tal que
\[ \left(f(b)-f(a)\right)g'(t)=\left(g(b)-g(a)\right)f'(t) \]

Si $g(b)\neq g(a)$

\[ \frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(t)}{g'(t)} \]

\end{theorem}

\begin{proof}
Sea $h(x)=\left(f(b)-f(a)\right)g'(x)-\left(g(b)-g(a)\right)f'(x)$. Es claro $h(a)=h(b)$, así que $\exists t \in (a,b) \tq h'(t)=0$.
\end{proof}

\begin{lemma}
Sea $\appl{f}{(a,b)}{\real}$ derivable. Si $f'(x)\geq 0$ la función es creciente.
\end{lemma}

\begin{theorem}[Regla de L'Hopital]
Sean $f$ y $g$ derivables definidas sobre un intervalo abierto $I$ salvo quizás en un punto $a\in I$. Supongamos \begin{enumerate}
\item $g'(t)\neq 0 \forall t \in I$.
\item $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$.
\item $\lim_{x\to a}\frac{f'(x)}{g'(x)}=L$
\end{enumerate}

Entonces \[ \lim_{x\to a}\frac{f(x)}{g(x)}=L \]
\end{theorem}

\begin{lemma}[Resolución indeterminaciones 0/0]
Sean $\appl{f,g}{(a,b)}{\real}$ derivables. Supongamos \begin{enumerate}
\item $\lim_{x}{a^+}{f}=\lim_{x\to a^+}g(x)=0$.
\item $g'(x)\neq 0 \; \forall x \in (a,b)$.
\item $\exists\lim_{x\to a^+}\frac{f'(x)}{g'(x)}=L$.
\end{enumerate}

Entonces, \[ \lim_{x\to a^+}\frac{f(x)}{g(x)}=L \]
\end{lemma}

\section{Polinomios y Teorema de Taylor}
\begin{defn}Se dice que $f(x)$ es $o$ de una función $\varphi(x)$ cuando $x\to a$ (notación: $f(x)=o(\varphi(x)),\; x\to a$) si \[ \lim_{x\to a} \frac{f(x)}{\varphi(x)}=0\]
\end{defn}

\begin{defn}Se dice que $f$ y $g$ tienen orden de contacto superior a $n$ cuando \[f(x)-g(x)=o(|x-a|^n); x\to a\], es decir \[ \lim_{x\to a}\frac{f(x)-g(x)}{(x-a)^n}\].
\end{defn}

En particular, $f$ y su recta tangente tienen orden de contacto superior a 1.

\begin{defn}[Derivada\IS superiore]
Si $f$ es una función derivable en un intervalo $I$ podemos considerar la función derivada $\appl{f'}{I}{\real}$. Si ésta es a su vez derivable, la segunda derivada es $f''(x)$, y así sucesivamente para la n-ésima derivada. \end{defn}

\begin{defn}[Polinomio de Taylor] Sea $f$ $n$ veces derivable en el entorno de un punto $a$. Se define el polinomio de Taylor de orden $n$ asociado a $f$ en el punto $x=a$ como \[ P_n(x)=P_{n,a}f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\cdots + \frac{f^{(n)}}{n!}(x-a)^n = \sum_{k=0}^n \frac{d^nf}{dx^{n}} (a)\cdot \frac{(x-a)^n}{n!} \].
\end{defn}

Propiedades del polinomio de Taylor:

\begin{enumerate}
\item $P_n(a)=f(a)$, $P_n^{(k)}(a)=f^{(k)}(a),\; k=1,2,\cdots,n$.
\item $\frac{d}{dx}\left(P_n(f)\right)(x)=P_{n-1}f'(x)$.
\item $f$ y $P_n$ tienen orden de contacto superior a $n$ para $x\to a$.
\item $P_n$ es el único polinomio de grado menor o igual que $n$ con la propiedad anterior.
\end{enumerate}

\begin{proof}
Propiedad 2:
\[\frac{d}{dx}\left(P_n(f)\right)(x)=f'(a)+f''(a)(x-a)+\cdots +\frac{f^{n}(a)}{(n-1)!}(x-a)^{n-1} = P_{n-1,a}f'(x) \].

Propiedad 1: $P_n(a)=f(a)$ es evidente. $P_n^{(k)}=f^{(k)}(a)$ también lo es a partir de la propiedad 2.

\textbf{Propiedad 3}: Usando L'Hopital siempre que el límite exista:

\[ \lim_{x\to a} \frac{f(x)-P_n(x)}{(x-a)^n}=^{0/0}\lim_{x\to a} \frac{f'(x)-P_n'(x)}{n(x-a)^{n-1}}=\cdots=\lim_{x\to a} \frac{f^{(n-1)}(x)-P^{(n-1)}_n(x)}{n!(x-a)} \]

$P_n^{(n-1)}(x)$ es la ecuación de la recta tangente a $f^{(n-1)}(x)$ en $x=a$, así que \[\lim_{x\to a} \frac{f^{(n-1)}(x)-P^{(n-1)}_n(x)}{n!(x-a)}=0\].

\textbf{Propiedad 4}: Sea $Q$ un polinomio de grado menor o igual que $n$ que cumpla la propiedad 3. Entonce $P_n - Q$ tiene orden de aproximación superior a $n$ en $x\to a$ porque \[\lim_{x\to a} \frac{P_n(x)-Q(x)}{(x-a)^n}=\lim_{x\to a} \left(\frac{P_n(x)-f(x)}{(x-a)^n}+\frac{f(x)-Q(x)}{(x-a)^n}\right)=0-0=0 \]

Si $P_n(x) - Q(x)$ es un polinomio de grado menor o igual que $n$, entonces \[0=\lim_{x\to a}\frac{R(x)}{(x-a)^n}=\lim_{x\to a}\frac{A_0+A_1(x-a)+\cdots}{(x-a)^n}\], por lo que $A_0=0$. Como $A_0$ es 0, reducimos una potencia y \[\lim_{x\to a}\frac{A_1+A_2(x-a)+\cdots}{(x-a)^{n-1}}\], de forma que $A_1$ es 0 y así sucesivamente cualquier $A_n$ vale 0. Por lo tanto, $R=0$ y $P_n=Q$.
\end{proof}

\begin{theorem}[Teorema\IS de Taylor]
Sea $f$ derivable $(n+1)$ veces alrededor del punto $a$. Entonces, dado un $x\;\exists t \in (a,x)$ tal que
\[ f(x)-P_n(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1} \].

\end{theorem}

\begin{defn}[Resto] $R_{n,a}f(x)=f(x)-P_{n,a}f(x)$ se denomina resto y el teorema nos da la forma de Lagrange de este resto.\end{defn}

\section{Geometría de gráficas de funciones}

\begin{defn}[Asíntota\IS vertical] Se dice que $f$ tiene una asíntota vertical por la derecha para $x=c$ si $\lim_{x\to c^+}f(x)=\pm \infty$. La definición es análoga para la asíntota vertical por la izquierda.\end{defn}

\begin{defn}[Asíntota\IS oblicua] Se dice que $f$ tiene una asíntota horizontal para $x\to \pm \infty$ si $\lim_{x\to \pm \infty}f(x)=m$.\end{defn}

\section{Integrales}

\subsection{Integral de Riemann}

\begin{defn}[Suma\IS superior e inferior]
La suma superior asignada a una partición $P=\{x_0=a, x_1,\cdots, x_n=b\}$ de un conjunto $[a, b]$ se define como

\[ S_pf =\sum_{k=1}^{n}(\textit{long.}\; I_k)\sup_{I_k} f \]

De forma análoga, se define la suma inferior (notación $s_pf$).\end{defn}

\begin{defn}[Función\IS integrable]
Dada $\appl{f}{[a,b]}{\real}$ acotada, se dice que $f$ es integrable (Riemann) si
\[ \sup s_pf = inf S_pf \] y se denota por
\[\int^b_a f\ \equiv \int^b_a f(x) \,\mathrm{d}x\]
\end{defn}

\begin{prop} \[ f\; \text{integrable} \dimplies \forall \epsilon >0, \exists P \;\mathrm{ particion} \tq S_Pf-s_Pf \leq \epsilon\]\end{prop}

\begin{prop} Si $\appl{f}{[a,b]}{\real}$ es continua, entonces es integrable.\end{prop}

\begin{remark}
Algunas funciones con una cantidad numerable de discontuinidades son integrables.
\end{remark}

Propiedades de la integral: Sean $f$ y $g$ dos funciones integrables y $\alpha \in \real$. Entonces se cumplen las siguientes propiedades: \[\sint (f+g) = \sint g + \sint g \]
 \[ \sint \alpha f = \alpha \sint f \]
 \[ f\leq g \implies \sint f \leq \sint g \]
 \[m\leq f \leq M \implies m \leq \frac{1}{b-a}\sint f \leq M \]
 \[ \left|\sint f\right | \leq \sint |f| \]
 \[ c \in (a,b) \implies \sint  f= \int_a^c f + \int_c^b f \]

\begin{prop} Si $f$ no es positiva, entonces llamamos $f_+ = max(f(x),0)$ y $f_- = - min(f(x),0)$. $f$ es integrable si y sólo si $f_+$ y $f_-$ lo son, y \[\sint  f =\sint f_+ -\sint  f_-\]\end{prop}

\begin{defn}[Función\IS Lipschitziana]
Una función es Lipschitziana si para cierta constante $k$ $|f(x) - f(y)| \leq k |x-y|$.\end{defn}

\begin{theorem}
Si $f$ es integrable y acotada, entonces la función $F(x)=\int_a^x f$ es continua y Lipschitziana.
\end{theorem}

\begin{theorem}[Teorema\IS Fundamental del Cálculo]
Si $F$ es continua en $[a,b]$, entonces $F(x)=\int_a^x f(t) dt$ es derivable y $F'(x) = f(x)$.
\end{theorem}

\begin{proof}
Fijado $x_0\in (a,b)$ queremos ver que \[\lim_{h\to 0}\frac{F(x_0+h)-F(x_0)}{h}=f(x_0)\].

Usamos que $f$ es continua en $x_0$: dado $\epsilon>0, \; \exists \delta >0 \tq |x_0-t| < \delta \implies |f(x_0)-f(t)| < \epsilon$. Sea $|h| < \delta\; , h>0$. Entonces

\[ \frac{F(x_0+h)-F(x_0)}{h} = \frac{1}{h} \int_{x_0}^{x_0+h}f(t)\;dt \] y \[ f(x_0)-\epsilon \leq \frac{1}{h} \int_{x_0}^{x_0+h}f(t)\;dt \leq f(x_0) + \epsilon\] \[-\epsilon \leq \frac{1}{h} \int_{x_0}^{x_0+h}f(t)\;dt - f(x_0) \leq  \epsilon\]

Por lo tanto el límite existe y es $f(x_0)$.
\end{proof}

{\bf Consecuencia}: $F(x)$ es una primitiva de $f$ si es continua y $F(a)=0$.

Si $G(x)$ es otra primitiva ($G' = f$) entonces $F(X) = G(X) + C$, donde $C$ es una constante.

En particular, \[ 0 = F(a) = G(a) + C \implies C = -G(a) \]

Luego \[\int_a^b f(t) dt \eqexpl{TFC} F(b) = G(b) + c = G(b) - G(a)\] y a esto lo llamamos la \concept{Regla de Barrow}.


\subsection{Integrales impropias}

Usamos los métodos de integración más general cuando $f$ no es acotada o cuando la región de integración no es acotada.

\[ \int_a^\infty f = \lim_{R\to\infty}\int_a^R f(t)\;dt \]

Sean $\varphi(x),\; \psi(x)$ derivables. Entonces
\[H(x)=\int_{\varphi(x)}^{\psi(x)}f(t)\,dt \] es derivable y su derivable es fácil de hallar si nos damos cuenta de que, si llamamos $F(x)=\int_a^xf(t)\; dt$ entonces $H(x) = F(\psi(x))-F(\varphi(x))$.
\newpage
\printindex
\end{document}
