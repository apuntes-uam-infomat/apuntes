% -*- root: ../GeometriaDiferencial.tex -*-
\chapter{Formas diferenciales en abiertos de $ℝ^n$}
\label{chapFormasDiferenciales}

\section{Introducción y motivación}

Vamos a hacer un pequeño repaso de formas diferenciales. A lo largo de esta sección vamos a considerar la función $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$.

¿Qué es una forma diferencial? La mejor introducción la da Spivak\footnote{A Comprehensive Introduction to Differential Geometry, volume 1, p. 111, chapter 4. Vía \href{http://math.stackexchange.com/a/450568}{Math.SX}.}

\begin{quotation}\itshape
Los geómetras diferenciales clásicos no tenían reparos en hablar de cambios $\dif x_i$ ``infinitamente pequeños'' de las coordenadas $x_i$, tal y como había hecho Leibnitz. Nadie quería admitir que esto no tenía sentido, porque los resultados verdaderos sólo se obtenían cuando estas cantidades infinitamente pequeñas se dividían en otras, siempre y cuando uno lo hiciese de la manera correcta.

Eventualmente se dieron cuenta de que lo mejor que se podía hacer para describir un cambio pequeñísimo es describir la dirección en la cual el cambio ocurre: el vector tangente. Dado que $\dif f$ debía ser el cambio infinitesimal de $f$ bajo un cambio infinitesimal del punto, $\dif f$ debe de ser una función de ese cambio, luego $\dif f$ debe ser una función que se aplica a vectores tangentes. Los $\dif x_i$ se convirtieron entonces en funciones, y quedó claro que debían distinguirse de los vectores tangentes $\dpa{}{x_i}$.

Una vez que esta idea se instauró, sólo era cuestión de hacer nuevas definiciones que conservaran la vieja notación. En resumen, todas las nociones que requerían cantidades infinitesimales se convirtieron en funciones de vectores tangentes, como $\dif f$, salvo los cocientes de esas cantidades, que se convirtieron en vectores tangentes como $\deriv{c}{t}$.
\end{quotation}

En resumidas cuentas, las formas diferenciales son un lenguaje complicado para permitir algo complicado: análisis y cálculo diferencial independiente de las coordenadas. Una 1-forma se convierte, por ejemplo, en un elemento de longitud en una curva. Una 2-forma es un elemento de área, y una 3-forma un elemento de volumen. Estos elementos nos permitirán definir integrales en curvas, superficies y volúmenes respectivamente.

\subsection{Construyendo las formas diferenciales}

Vamos a empezar por el principio. Hemos dicho que queremos obtener el elemento de longitud/área/volumen de una función, ¿verdad? Lo simple será ver el elemento de longitud. O, dicho de otra forma, cuánto cambia la función cuando nos movemos en una cierta dirección.

\begin{defn}[Diferencial\IS de una función] Dada $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$ y $x_0 ∈ U$, entonces el diferencial en un punto es $\Dif f(x_0) = f'(x_0) = (\Dif f)_{x_0}$, que se considera como una aplicación lineal
\begin{align*}
\appl{\Dif f}{ℝ^n&}{ℝ} \\
(λ_1, \dotsc, λ_n)^T &\longmapsto \sum \dpa{f}{x_i} (x_0) λ_i
\end{align*}
\label{defDiferencialD}
\end{defn}

Para las formas diferenciales vamos a querer una definición algebraica muy bien especificada, así que aquí tenemos que ser claros y formales. Por ejemplo, el espacio de partida de $\Dif f$, $ℝ^n$, no es el mismo espacio de partida de $f$. El $ℝ^n$ de $\Dif f$ es el espacio tangente de $f$ en $x_0$ (denotado por $\tgs_{x_0} U = ℝ^n$), ya que sólo podremos movernos en direcciones tangentes a la superficie (o variedad, o lo que sea) en un cierto punto.

Por ser una aplicación lineal de $ℝ^n$ en $ℝ$, el diferencial está en el espacio dual de $ℝ^n$ o, más concretamente, del espacio tangente de $U$: \[ \Dif f(x_0) ∈ \left(\tgs_{x_0} U\right)^* ≝ \tgs_{x_0}^* U\] donde $\tgs_{x_0}^* U$ será el espacio cotangente.

Para los que no recuerden claramente qué es el espacio dual:

\begin{defn}[Espacio\IS dual]
Dado cualquier espacio vectorial $V$ sobre un cierto cuerpo $F$, definimos el \textbf{espacio dual $V^*$} como el conjunto de todas las funcionales lineales en $F$, es decir, transformaciones lineales en $V$ a valores escalares (en este contexto, un ``escalar'' es un miembro del cuerpo-base $F$). El propio $V^*$ se convierte en un espacio vectorial sobre $F$ bajo la definiciones habituales ('punto a punto') de la adición y de la multiplicación escalar.
\end{defn}

Como el diferencial está en el espacio dual, sus elementos serán llamados \concept[Covector]{covectores}. Esto nos permitirá evitar líos de notación al considerar vectores en el espacio y por otro lado vectores en el espacio tangente.

\seprule

Por otro lado, vamos a tratar de formalizar lo que decíamos en la introducción: que $\dif f$ es el cambio infinitesimal de $f$ y que se aplica a un vector dirección.

\begin{defn}[Diferencial\IS exterior] Dada $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$ y $x_0 ∈ U$, entonces la diferencial en un punto es \[ (\dif f)_{x_0} = \sum \dpa{f}{x_i} (\dif x_i)_{x_0} \] En general, la diferencial será \[ \dif f = \sum \dpa{f}{x_i} \dif x_i \] \label{defDifrenciald}
\end{defn}

En realidad, la única diferencia entre esta definición y la dada en \ref{defDiferencialD} es que cambiamos los $λ_i$, los componentes del vector tangente, por $\dif x_i$. ¿Qué significa eso?

Simplemente, los $\dif x_i$ son ``selectores'' de coordenadas. Esto es, dado un vector $\vv$, se tiene que $\dif x_i (\vv) = v_i$. Así, la diferencial exterior nos queda definida como \[ \appl{(\dif f)_{\vx}}{\tgs_{\vx} U}{ℝ} \] donde $\vx ∈ U$, y se aplicaría a vectores del espacio tangente de la siguiente forma: \[ (\dif f)_\vx (\vv) = \sum \left(\dpa{f}{x_i} (\vx)\right) · v_i \]

En ambos casos tenemos que, por definición, la diferencial es una 1-forma en el abierto $U$, que no es más que un objeto dado de la siguiente forma \[ ω = f_1 \dif x_1 + \dotsb + f_n \dif x_n \] con $f_i = \dpa{f}{x_i} ∈C^∞(U)$.

Una cosa curiosa a tener en cuenta: cada $(\dif x_i)_\vx$ es un covector de $\tgsd_\vx U$, y de hecho $\set{(\dif x_i)_\vx}_{i=1}^n$ es una base del espacio de covectores (lo demostraremos más adelante).

\subsection{Deducción de las reglas de cálculo}

Vamos a considerar ahora una diferencial \[ \dif h = \dpa{h}{x_1} \dif x_1 + \dotsb + \dpa{h}{x_n} \dif x_n \] y una 1-forma \[ ω = f_1 \dif x_1 + \dotsb + f_n \dif x_n \]

Dado ω, nos preguntamos si existe una función $h$ tal que $\dif h = ω$. Es una propiedad muy deseable, porque nos da de alguna manera unas ciertas garantías de ``operación cerrada'' en formas diferenciales. Si esto ocurre, significa que la 1-forma viene de una función y no es un objeto ``raro'' que no viene de ninguna parte.

Esto es lo mismo que plantearse la resolución del sistema de ecuaciones diferenciales dado por \[ \dpa{h}{x_i} = f_i\quad i=1,2,\dotsc,n \]

Esta cuestión está resuelta por el teorema de Poincaré, que veremos más adelante durante el curso.

A primera vista, parece difícil que haya solución. Si uno elije las $f_i$ de forma aleatoria, tendremos demasiadas ecuaciones y pocas incógnitas. Luego debemos esperar condiciones de integrabilidad: las $f_i$ deben cumplir ciertas posibilidades para que la solución $h$ exista. Habitualmente estas condiciones son necesarias pero no siempre suficientes, y una vez que las encontremos suponemos que se cumplen (si no no hay solución) y veremos si bajo esas condiciones el problema tiene solución.

Dado que queremos que exista $h$, vamos a suponer que efectivamente existe $h$. Entonces va a ocurrir que $\dpa{h}{x_i} = f_i$, y si volvemos a derivar tendremos que $\frac{∂h}{∂x_j∂x_i} = \dpa{f_i}{x_j}$. Si derivamos en orden contrario, como el Teorema de Schwarz nos dice que las derivadas cruzadas son iguales, tendremos que tener \[ \dpa{f_i}{x_j} = \dpa{f_j}{x_i} \]

Hay una manera natural de poner toda esta información. Nosotros tenemos la matriz Hessiana de derivadas parciales segundas dada por \[ H = \left(\frac{∂h}{∂x_j∂x_i}\right)_{ij}\] que es simétrica por el Teorema de Schwarz.

Por otra parte, a partir de la forma diferencial $ω = f_1 \dif x_i + \dotsb + f_n \dif x_n$ podemos obtener la matriz $H_1$ dada por \[ H_1 = \del{\dpa{f_i}{x_j}}_{ij} \]

Habíamos visto que la diferencia entre la matriz diferencial y la diferencial exterior era simplemente de notación, luego parece lógico que $H = H_1$. Luego la condición de integrabilidad es que $H_1$ sea simétrica.

A partir de aquí podemos definir más formalmente la diferencial de una 1-forma

\begin{defn}[Diferencial\IS de una 1-forma] Dada una 1-forma ω, tenemos que \[ \dif ω ≝ H_1^Γ = \frac{H_1 - H^T_1}{2} \] donde $H_1^Γ$ es la parte antisimétrica de la matriz $H_1$.
\end{defn}

Esto nos lleva a poder dar una condición concreta para que exista la función $h$ que comentábamos antes. Como queremos que $H_1$ sea simétrica, la parte antisimétrica deberá ser 0. Podemos enunciar entonces el siguiente lema:

\begin{lemma} Si existe $h$ tal que $\dif h = ω$, entonces $\dif ω = 0$. \end{lemma}

Vamos a ver cómo aplicar todo ese tocho de antes para calcular la diferencial de una 1-forma $ω$ dada por \[ ω = f_1 \dif x_1 + f_2 \dif x_2 \] definida en un abierto $U ⊆ ℝ^2$. En este caso, su matriz $H_1$ será \[ H_1 = \begin{pmatrix} \dpa{f_1}{x_1} & \dpa{f_1}{x_2} \\ \dpa{f_2}{x_1} & \dpa{f_2}{x_2} \end{pmatrix} \] así que su parte antisimétrica será

\[ H_1^Γ = \begin{pmatrix} 0 & \displaystyle\frac{\dpa{f_1}{x_2} - \dpa{f_2}{x_1}}{2} \\ \displaystyle\frac{\dpa{f_2}{x_1}- \dpa{f_1}{x_2}}{2} \end{pmatrix} \]

Y como ese 2 molesta multiplicamos por dos y nos lo quitamos de en medio.

Como al diferenciar estamos derivando y la derivada es lineal, deberíamos esperar que \[ \dif ω = \dif (f_1\dif x_1) + \dif (f_2 \dif x_2)\] luego tenemos que ver cuánto vale $\dif(f_1\dif x_1)$.

Dado que la derivada tiene que seguir cumpliendo la regla de Leibniz, tenemos que tener \[ \dif(f_1\dif x_1) = \dif f_1 \dif x_1 + f_1 \dif \dif x_1 \]

Aquí ya empiezan a pasar cosas importantes de este cálculo de diferenciales. Principalmente que $ \dif \dif x_1 = 0$, ya que la matriz $H$ tiene que ser simétrica.

Ahora tenemos que calcular $ \dif f_1 \dif x_1 $, que será \[ \dif f_1 \dif x_1  = \left(\dpa{f_1}{x_1} \dif x_1 + \dpa{f_1}{x_2} \dif x_2\right)\dif x_1 = \dpa{f_1}{x_1}\dif x_1 \dif x_1 + \dpa{f_1}{x_2} \dif x_2 \dif x_1 \] luego sumando nos queda que

\[ \dif ω = \dpa{f_1}{x_1}\dif x_1 \dif x_1 + \dpa{f_1}{x_2} \dif x_2 \dif x_1  + \dpa{f_2}{x_1} \dif x_1 \dif x_2 + \dpa{f_2}{x_2} \dif x_2 \dif x_2 \]

Comparamos esto con la matriz antisimétrica dada por  \[ 2H_1^Γ = \begin{pmatrix} 0 & \dpa{f_1}{x_2} - \dpa{f_2}{x_1} \\ \dpa{f_2}{x_1}- \dpa{f_1}{x_2} & 0\end{pmatrix} \] vemos que los términos $\dpa{f_1}{x_1}$ y $\dpa{f_2}{x_2}$ no aparecen, luego tiene que ser \[ \dpa{f_1}{x_1} \dif x_1 \dif x_1 = \dpa{f_2}{x_2} \dif x_2 \dif x_2 = 0 \] y por otra parte que el producto de diferenciales tiene que ser anticonmutativo, esto es, que $\dif x_1 \dif x_2 = - \dif x_2 \dif x_1$. Dado que este producto (producto exterior) es distinto al habitual, lo denotaremos como $\dif x_1 \y \dif x_2$.

Es decir, hemos extraído las dos reglas siguientes para el producto de diferenciales o \concept[Producto\IS exterior]{producto exterior}: \begin{align*}
\df{x_i, x_j} &= - \df{x_j, x_i} \\
\df{x_i, x_i} &= 0
\end{align*}

Vamos a extendernos un poco en el significado de la expresión $\df{x_i, x_j}$, y por tanto en el de la forma diferencial. En el fondo, no estamos más que definiendo unos objetos (las formas) y aplicando ciertas reglas razonables para calcular ciertas operaciones. Ciertamente esto es cierto.

\paragraph{¿Es $\dif ω = 0$ condición suficiente?} Tenemos que es una condición necesaria para que exista un $h$ tal que $\dif h = ω$. Ahora bien, ¿es condición suficiente?

Para demostrarlo, lo que vamos a hacer es construir esa función $h$. Es trivial ver que \[ h(x) = \int_γ \dif h \] para un cierto camino $\appl{γ}{I}{U}$. En este caso, aplicando la regla de Barrow

\[ \int_γ \dif h = \int_I \dif(h ○ γ) = h(γ(b)) - h(γ(a)) = h(x) - h(x_0)\] donde $x_0$ es el punto de inicio de $γ$. Es decir, que $h$ está definido salvo constante. Habría que ver, eso sí, que la definición que hemos construído no depende de la elección del camino $γ$ y que, además, se cumple efectivamente que $\dif h = ω$.

Vamos a demostrar que la definición no depende del camino. Tomemos $γ_1, γ_2$ dos caminos distintos que empiezan y acaban en el mismo punto. Consideremos entonces $Γ = γ_1 * γ_2^-$, y sea $D$ la región encerrada por Γ.

Por un lado, tenemos que como $\dif ω = 0$, entonces $0 = \int_D \dif ω$. Por otra parte, por el teorema de Stokes, \[ \int_D \dif ω = \int_Γ ω = \int_{γ_1} ω - \int_{γ_2} ω \implies \int_{γ_1} ω = \int_{γ_2} ω\] luego la integral no depende del camino

Lo único que necesitamos para aplicar Stokes es que $U$ sea simplemente conexo, es decir, que no haya agujeros y que siempre podamos deformar un camino a otro, de tal forma que el borde de $D$ sea Γ.

En el fondo, esto es un reflejo de lo que habíamos visto en cursos anteriores de cálculo con campos de vectores conservativos: cuando eran conservativos ($\grad V = 0$) la integral sobre caminos cerrados era cero y además existía una función potencial.

\section{Estudio formal de las formas diferenciales: álgebra diferencial}

Reconozcamos una cosa: las derivadas son feas. Así que vamos a buscar una definición algo más elegante de las formas diferenciales, y lo vamos a hacer a través del álgebra\footnote{Porque el álgebra es mucho más bonito que el cálculo, claro que sí, campeón.}. Será importante ya que en futuros temas nos permitirá manejar más fácilmente los conceptos sin tener que dar vueltas por el cálculo.

\subsection{Espacio tangente y cotangente}

Lo primero es lo primero: las formas diferenciales se basan en el espacio tangente a variedades, así que vamos a necesitar una definición algebraica. Suponemos un abierto $U ⊆ ℝ^n$ y un punto $p ∈ U$.

\begin{defn}[Espacio\IS tangente] Se define el espacio tangente de $U$ en un punto como $\tgs_{p} U$, un espacio vectorial sobre $ℝ$ de dimensión $n$. Los elementos $D^\vv_{p} ∈ \tgs_{p} U $ son las derivadas direccionales (en la dirección $\vv$) locales (en el punto $p$), que operan las funciones y dan números.
\end{defn}

Vamos a tratar de construir este espacio tangente. Empezaremos primero tratando de ver cuál es el espacio de funciones al que vamos a aplicar la derivada direccional.

\subsubsection{Anillo de funciones}

Si tenemos dos funciones $\appl{f,g}{U}{ℝ}$ definidas en el abierto $U$ con $f,g ∈ C^∞ (U)$, podemos operar con ellas de forma sencilla:

\begin{align*}
(f+g)(x) &≝ f(x) + g(x) \\
(f·g)(x) &≝ f(x) · g(x)
\end{align*}

Es decir, que las funciones definidas en el abierto tienen estructura de anillo: todas tienen inverso con la suma. Tenemos una unidad y además es anillo conmutativo. Así, podemos definir el anillo de funciones de un abierto:

\begin{defn}[Anillo\IS de funciones] El anillo de funciones de un abierto $U$ se define como

\[ \mathcal{A}(U) = C^∞ (U) = \set{\appl{f}{U}{ℝ},\; f∈C^∞}\]

, que es anillo conmutativo y con unidad.\end{defn}

De hecho, $\mathcal{A}(U)$ no es sólo un anillo: también es una \concept{{$\real$}-álgebra}: es un anillo conmutativo y además espacio vectorial sobre $ℝ$, cumpliendo la igualdad

\[ (λ· f) · (β·g) = (λ · β) · (f·g) \]

donde $f,g ∈ A(U)$, $λ,β ∈ ℝ$ y teniendo cuidado de usar los productos que correspondan\footnote{$(λ·_{E.V.} f) ·_{A(U)} (β·_{E.V.}g) = (λ ·_{\real} β) \cdot_{E.V.} (f·_{A(U)}g)$.} (el producto de elementos de $ℝ$ no es lo mismo que de elementos del anillo de funciones).

\subsubsection{Gérmenes de funciones en $p$}

La idea de los gérmenes es que sólo nos interesa la función en un entorno del punto. En espacios normales, no hay diferencia entre estudiar sólo el entorno o toda la función, pero en otros espacios\footnote{Ver explicación que no he entendido en \href{http://en.wikipedia.org/wiki/Tangent_space\#Definition_via_derivations}{Wikipedia}.} pueden ocurrir cosas extrañas.

Por tanto, necesitamos deshacernos de todo lo superfluo: queremos sólo lo básico. Podemos definir así lo que llamáramos el germen de una función.

\begin{defn}[Germen\IS de función] Un germen de una función en un punto $p$ es un par $(V,f)$ de un abierto $p ∈ V ⊆ U$ y una función $\appl{f}{V}{ℝ}$ con $f∈C^∞$. \label{defGermenFuncion}
\end{defn}

De momento esto no nos simplifica mucho. Lo que haremos será trabajar con relaciones de equivalencia para simplificar. Diremos que dos gérmenes $(V,f)$, $(W,g)$ están relacionados $(V,f) \sim (W,g)$) si y sólo si $p∈V∩W$ y además existe un abierto $V' ⊆ V∩W$ que contiene a $p$ y para el cual las restricciones de $f$ y $g$ coinciden, esto es $\restr{f}{V'} = \restr{g}{V'}$.

Tomando los gérmenes módulo esta relación de equivalencia, tendremos igualmente una $\real$-álgebra.

La ventaja de buscar ese abierto más pequeño es que no tenemos que decir exactamente cómo de pequeño es, nos basta simplemente que las funciones sean iguales en un pequeño entorno del punto.

A partir de esto, denotaremos como $\mathcal{A}_p$ la $\real$-álgebra de gérmenes de funciones $C^∞$ definidas en un entorno de $p$. Por comodidad, para denotar un germen nos bastará con la función, $f∈\mathcal{A}_p$, por ejemplo.

\subsubsection{Derivaciones}

Una vez que ya tenemos las funciones que vamos a manejar, vamos a definir lo que es una derivada. Ahora bien, no vamos a buscar exactamente la derivada sino sólo algo que se le parezca:

\begin{defn}[Derivación] Una derivación en $p$ es una aplicación
\begin{align*}
\appl{D}{\mathcal{A}_p&}{ℝ} \\
f &\longmapsto D(f)
\end{align*}

Queremos que esta función conserve de alguna forma la noción de derivada en una dirección, así que buscaremos varias propiedades:

\begin{enumerate}
	\item $D$ es lineal.
	\item $D(λ) = 0$, donde $λ$ es una función constante $λ(x) = λ ∈ ℝ$.\footnote{En realidad, esta propiedad es consecuencia de las otras dos pero viene bien tenerla presente.}
	\item Si esto se parece a una derivada, además de definir cómo se derivan las sumas\footnote{Por esto forzamos que $D$ sea lineal.} definiremos cómo se derivan los productos, según la regla de Leibniz: \[ D(f·g) = f · D(g) + D(f) · g\]
\end{enumerate}\label{defDerivacion}
\end{defn}

Una vez que hemos emulado las derivadas, ahora sí podemos definir el espacio tangente:

\begin{defn}[Espacio\IS tangente] Diremos que el espacio tangente a $p$ en $U$ se define como

\[ \tgs_p U ≝ \set{\appl{D}{\mathcal{A}_p}{ℝ}\tq D \text{ es derivación }} \]
\end{defn}

Fijémonos que el espacio tangente es un subespacio vectorial del espacio dual, ya que $\mathcal{A}^*_p ≝ \set{\appl{D}{\mathcal{A}_p}{ℝ} \tq D \text{ lineal }}$. De hecho, tenemos bien definido el producto por escalares y la suma de elementos de la forma habitual.

Al final, hemos logrado llegar a una definición de espacio tangente sin tener que usar derivadas, una definición puramente algebraica.

Ahora bien, ¿es este el espacio tangente que de verdad buscamos? Vamos a comprobarlo, y empezaremos por buscar su dimensión para ver que cuadra con lo que buscamos.

\subsubsection{Cálculo de la dimensión del espacio tangente}
\label{secDimTangente}

Vamos a demostrar que la dimensión del espacio tangente de un abierto de $ℝ^n$ es $n$, más que nada porque queremos asegurarnos de que este espacio tangente que hemos definido es lo mismo que el espacio tangente habitual.

Suponemos $p ∈ U ⊆ ℝ^n$, y $p = (p_1, \dotsc, p_n)$. Supongamos también que tenemos una derivación $\appl{D}{\mathcal{A}_p}{ℝ}$. Vamos a ver qué podemos obtener de aquí sabiendo las propiedades de la derivación.

La observación fundamental es que en el conjunto de gérmenes $\mathcal{A}_p$ tenemos un cierto subconjunto en el que los gérmenes son 0: \[ \mathfrak{m}_p ≝ \set{f ∈ \mathcal{A}_p \tq f(p) = 0}\]

$\mathfrak{m}_p$ se podría ver como el conjunto de curvas, superficies y en general conjuntos $X ⊂ U$ dados por $X = \inv{f}(\set{0})$ que pasan por $p$.

Vamos con un poco de álgebra. $\mathfrak{m}_p$ es un ideal por la propia construcción: el producto de dos funciones de $\mathfrak{m}_p$ está en $\mathfrak{m}_p$, y lo mismo con la suma.

Además, resulta ser maximal\footnote{Aunque todos nos acordamos de Estructuras Algebraicas, recordamos por si acaso: un ideal es maximal si no hay ningún ideal propio que lo contenga.}, ya que el cociente $\quot{\mathcal{A}_p}{\mathfrak{m}_p}$ es un cuerpo. Veamos por qué: los elementos de ese cociente son las clases de equivalencia dadas por la siguiente relación: \[ f \sim g \iff f - g ∈ \mathfrak{m}_p \], esto es, estarán formadas por gérmenes de funciones cuya diferencia sea constante en el entorno en el que están definidas. Así, podemos identificar cada clase de equivalencia con un número real (la diferencia entre dos gérmenes cualquiera de la clase), y por lo tanto \[ \quot{\mathcal{A}_p}{\mathfrak{m}_p} = ℝ \], que es un cuerpo.

Consideramos\footnote{Aviso para navegantes: todo esto es una idea feliz que no sé de dónde sale.} ahora $\mathfrak{m}_p^2$, el ideal generado por productos de elementos de $\mathfrak{m}_p$, esto es, \( \label{eqMp2} \mathfrak{m}_p^2 ≝ \set{\sum g_{ij} f_i f_j \tq f_i, f_j ∈ \mathfrak{m}_p; \; g_{ij} ∈ \mathcal{A}_p }\). Es fácil ver que esto es un subconjunto de $\mathfrak{m}_p$.

Vamos a seguir con las ideas felices. Consideremos el espacio cociente $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$, que es lo mismo que las clases de equivalencia de $\mathfrak{m}_p$ dadas por la relación $\rel$ de la siguiente forma: $f \rel g \iff f - g ∈ \mathfrak{m}_p^2$.

Dicho de otra forma, $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$ es el conjunto de todos los posibles comportamientos ``lineales'' de funciones en pequeños entornos de $p$. Cada clase de equivalencia consistirá de todas las funciones que se comportan igual cuando sólo nos fijamos en la parte lineal (veremos esto mejor en un ejemplo más adelante).

La cuestión es que si tienen la misma ``parte lineal'', deberían tener la misma derivada. Si la tienen, entonces cualquier derivación $D ∈ \tgs_p U$ está determinada por su valor en $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$. Dado que las derivaciones son aplicaciones lineales de $\mathcal{A}_p$ a $ℝ$, por lo que acabamos de decir lo son también de $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$ a $ℝ$. En otras palabras, podemos identificar $\tgs_p U$ con el espacio dual $\left(\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}\right)^*$.

Así, podríamos decir que $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2} \simeq \tgsd_p U$ es el espacio cotangente de $U$ en $p$. Lo malo es que tenemos que pulir algunas cosillas para asegurarnos de esto.

Lo primero es que no hemos probado que dos funciones con la misma ``parte lineal'' tienen la misma derivación. Consideramos dos funciones $f,g ∈ \mathfrak{m}_p$ con la misma parte lineal, esto es, $f \rel g$ con la relación de equivalencia definida antes. Tenemos entonces que $f - g = h ∈ \mathfrak{m}_p^2$. Podemos simplificar y decir que $h = φ · γ$ con $φ, γ ∈ \mathfrak{m}_p$. Ahora podemos aplicar la derivación en ambos lados, usando la regla de Leibniz en la izquierda
\begin{align*}
f- g &= φ · γ \\
D(f-g) &= D(φ·γ) \\
D(f) - D(g) &= φ·D(γ) + γ· D(φ)
\end{align*}

Ahora bien, como  $φ, γ ∈ \mathfrak{m}_p$, se tiene que $φ(p) = γ(p) = 0$, y como la derivación se aplica siempre en $p$\footnote{Estamos omitiéndolo pero $D \equiv D_p$.}, entonces $ D(φ·γ) = 0$. Esto es, que $D(f) = D(g)$, así que efectivamente dos funciones con la misma ``parte lineal'' tienen la misma derivación.

Si derivamos su producto, tenemos que $D(f·g) = f·D(g) + g· D(f)$ por la regla de Leibniz. Ahora bien, como $f(p) = g(p) = 0$, entonces $D_p(f·g) = 0$.

El otro punto que tenemos que pulir es qué ocurre con las funciones en $\mathcal{A}_p \setminus \mathfrak{m}_p$, que no hemos considerado arriba. Si tenemos $f∈\mathcal{A}_p$ con $f(p) ≠ 0$, entonces consideramos la función $f_1(x) ≝ f(x) - f(p)$, que está en $\mathfrak{m}_p$ por construcción\footnote{Recordamos que $\mathfrak{m}_p$ son las funciones que se anulan en $p$.}. Es claro que yo debo definir \[ D(f) ≝ \adh{D}\left([f_1]_{\mathfrak{m}^2_p}\right) \]

Es decir, que sólo con saber lo que vale $\adh{D}$ nos vale ya que simplemente trasladamos las funciones $f$ con valores desconocidos $f ∉ \mathfrak{m}_p$ a una clase de equivalencia de la cual sí sabemos el valor. Sólo tenemos que ver que efectivamente cumple la propiedades de derivación que habíamos pedido (\ref{defDerivacion}). Es trivial ver que se cumple la linealidad, pero Leibniz es más interesante. La demostración de eso se deja como ejercicio para el lector.

¿Qué hemos logrado hasta aquí? Pues algo realmente importante: hemos unido el concepto de derivación al concepto de comportamiento ``lineal'' o de primer orden de funciones. Es decir, que a través del álgebra hemos llegado a algo que efectivamente se parece mucho a las derivadas. Para manejar mejor todos estos conceptos, vamos a ver un ejemplo.

\begin{example}[Estudio del espacio tangente con funciones polinomiales]

Vamos a suponer que trabajamos con polinomios (espacio $ℝ[x,y]$), que son más sencillos: con ellos podemos derivar formalmente sin definiciones de límites ni nada, y además, los gérmenes son los mismos polinomios: si tenemos todos los valores que toma en un abierto de $ℝ$, tenemos completamente determinado el polinomio.

Supongamos $p = (0,0)$. Entonces, ¿qué es $\mathfrak{m}_p ⊆ ℝ[x,y]$? Será\footnote{Teniendo en cuenta que $p(x,y) = a_0 + a_1 x + a_2 y + \dotsb$} \[\mathfrak{m}_p = \set{p(x,y) ∈ ℝ[x,y] \tq p(0,0) = 0} = \set{p(x,y) \tq a_0 = 0} \]

Queremos ver qué es $\mathfrak{m}_p^2$. Está claro que $x^2, y^2, xy ∈ \mathfrak{m}_p^2$. Afirmamos que $\mathfrak{m}_p^2 = \gen{x^2, y^2, xy}$, y vamos a demostrarlo. Si fuese así y tuviésemos $F(x,y) ∈ \mathfrak{m}_p^2$, podríamos escribirlo como \[ F(x,y) = \sum A_{ij}(x,y) · p(x,y) · q(x,y) \] siguiendo la definición que teníamos en \eqref{eqMp2}, y con $p, q ∈ \mathfrak{m}_p$. Por lo tanto, $p,q$ sólo pueden ser polinomios con término independiente 0 y entonces nos quedaría que efectivamente

\[ \mathfrak{m}_p^2 = \gen{x^2 + y^2 + xy} = \set{A(x,y) x^2 + B(x,y) y^2 + C(x,y) xy} \]

Una vez que tenemos $\mathfrak{m}_p$, queremos ver quién es $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$, que es el espacio cotangente a $\tgs_{(0,0)}^* ℝ$. Si $p(x) ∈ \mathfrak{m}_p$, entonces \[ p(x) = a_1 x + a_2 y + a_3 x^2 + a_4 xy + \dotsb \] Si $p, q$ están en la misma clase de equivalencia, su resta tiene que estar en $\mathfrak{m}_p^2$, luego los polinomios $r(x) ∈ \quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$ son de la forma \[ r(x) = a_1 x + a_2 y\] o, dicho de otra forma, \[ \quot{\mathfrak{m}_p}{\mathfrak{m}_p^2} \equiv ℝ·[x] + ℝ· [y] \]

A lo que hemos llegado es al mismo concepto que comentábamos antes: que $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$ es el conjunto de todos los comportamientos lineales posibles de funciones en entornos de $p$ (en el caso de polinomios, los comportamientos lineales son todos los polinomios lineales en $x$ y/o en $y$) y que las derivaciones no son más que aplicaciones lineales (y por lo tanto del espacio dual de $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$) de esos comportamientos lineales a los números reales.
\end{example}

Una vez que hemos logrado manejar algo mejor y ver la aplicación de todos estos conceptos, podemos pasar a la formalización de la demostración de la dimensión del espacio tangente.

\begin{theorem} \label{thmDimQuotMp} Sea $p ∈ U ⊆ ℝ^n$, con $p = (p_1, p_2, \dotsc, p_n)$. Entonces el espacio $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$ es un R-espacio vectorial de dimensión $n$.\end{theorem}

Hasta ahora todo lo que hemos visto han sido manipulaciones algebraicas, aunque para demostrar este teorema hace falta usar análisis.

\begin{proof}[Idea] Suponemos que $n = 1$, y consideramos el desarrollo de Taylor de una función $f ∈ \mathcal{A}_p$:

\[ f(x) - f(p) = f'(p)(x - p) + \frac{1}{2!} f''(p)(x - p)^2 + \dotsb\]

Vemos que $f''(p)(x - p)^2  ∈ \mathfrak{m}_p^2$. Dado que trabajamos en módulo $\mathfrak{m}_p^2$, tenemos que $[f(x) - f(p)] = f'(p) [ x- p]$. Esto demuestra que $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$ tiene base $[x - p]$, luego efectivamente la dimensión es 1.

Vamos ahora con la demostración más rigurosa. Para una función $f ∈ \mathcal{A}_p$, tenemos\footnote{ Por el teorema de Taylor e incluyendo el término de error, en este caso con la formulación integral } que

\[ f(x) - f(p) = \sum_{i=1}^n \dpa{f}{x_i} (x_i - p_i) + \sum_{\substack{i=1 \\ j=1}}^n \frac{1}{2} (x_i - p_i)(x_j - p_j) ·\int_0^1 (1-t) \left(\frac{∂^2f}{∂x_i ∂x_j}(p + tx)\right) \dif t \]

Entonces nos queda que \[ [f(x) - f(p)] = \sum_{i=1}^n \dpa{f}{x_i} (p) [x_i - p_i] \mod \mathfrak{m}_p^2 \] así que está claro que $\set{[x_i-p_i]}_{i=1}^n$ son generadores de $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$. No hemos demostrado que sean base, eso sí.

Para terminar, demostramos que ese conjunto de generadores son linealmente independientes y que por lo tanto serán base. Lo haremos por demostración al absurdo: si no fueran independientes, podríamos tener una función \[ F = \sum λ_i (x_i - p_i) ∈\mathfrak{m}_p^2\]

Esto es un poco absurdo ya de por sí\footnote{El profesor ha dicho por qué pero yo no lo veo nada claro.}, pero vamos a demostrarlo.

Sabemos que $∀D$ $D(F) = 0$. Entonces, elegimos un conjunto de derivaciones $D_j$ tales que $D_j ( x_i - p_i) = δ_{ij}$ con $δ$ la delta de Kronecker (ver \ref{defDeltaKronecker}). En ese caso, \[ D_j(F) = \sum λ_i D_j (x_i - p_i) = λ_j\] pero hemos dicho que $D(F) = 0$, contradicción.

¿Qué $D_j$ nos valen? Si tomamos $D_j = \left(\dpa{}{x_j}\right)_p$, cumplen las propiedades de derivación y también que $D_j (x_i - p_i) = \dpa{(x_i - p_i)}{x_j} = δ_{ij}$\footnote{Es fácil ver que $D_j(x_i - p_i)$ será 1 cuando $j = i$.}.

En ese caso, tenemos efectivamente que $\set{[x_i-p_i]}_{i=1}^n$ es base.
\end{proof}

Con esta demostración hemos llegado a algo interesante, que es ver qué es un diferencial de cada coordenada \[ (\dif x_i)_p ≝ [x_i - p_i]_{\mathfrak{m}_p^2} \] y, de la misma forma, el diferencial de una función \[ (\dif f)_p ≝ \left[f - f(p)\right]_{\mathfrak{m}_p^2} ∈ \quot{\mathfrak{m}_p}{\mathfrak{m}_p^2} \]

De esta notación y del teorema anterior \ref{thmDimQuotMp} podemos llegar a otra notación:

\( (\dif f)_p = \sum \dpa{f}{x_i}(p) (\dif x_i)_p \label{eqDiferencialNotacion} \)

En realidad, lo que estamos haciendo es similar a un desarrollo de Taylor despreciando los términos de segundo orden (que es lo que logramos con el módulo $\mathfrak{m}_p^2$): aproximaciones lineales de funciones.

De momento estamos trabajando localmente, siempre con la diferencial definida en un punto $p$. Pronto pasaremos a definir las diferenciales en abiertos o incluso en variedades directamente.

\subsubsection{Construcción del espacio tangente}

Recapitulemos: hemos demostrado hasta ahora el siguiente isomorfismo entre el espacio tangente y el dual  \[ \tgs_p U \simeq \left(\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}\right)^* \] que $\dim_{ℝ} \quot{\mathfrak{m}_p}{\mathfrak{m}_p^2} = n$ y que además hay una base natural \[ \quot{\mathfrak{m}_p}{\mathfrak{m}_p^2} = \gen{(\dif x_1)_p, \dotsc, (\dif x_n)_p} \]

A partir de aquí también podemos escribir una base del tangente $\tgs_p U$. Como podemos identificar el tangente con el dual de $\quot{\mathfrak{m}_p}{\mathfrak{m}_p^2}$, si definimos $\set{(D_i)_p}_{i=1}^n$ como la base de $\tgs_p U$, tenemos que cada $D_i$ tiene que ser un elemento del dual: \[ (D_i)_p (x_j - x_j(p)) = δ_{ij}\] aunque por notación más cómoda tomaremos \[ (D_i)_p ≝ \left(\dpa{}{x_i}\right)_p \] y por lo tanto podemos reescribir la base del espacio tangente en un punto como \[ \tgs_pU = \gen{\left(\dpa{}{x_1}\right)_p, \dotsc, \left(\dpa{}{x_n}\right)} \] y entendiendo cada $\left(\dpa{}{x_i}\right)_p$ como derivaciones lineales que se aplican a gérmenes de funciones de $p$ y que cumplen la regla de Leibniz.

Así, cualquier elemento $D_p ∈ \tgs_p U$ se puede expresar como  \[ D_p = \sum a_i \left(\dpa{}{x_i}\right)_p \] con $a_i ∈ ℝ$.

De hecho, como cada $D_p$ sólo tiene la información de los $a_i$, podemos considerar los $D_p$ como vectores del espacio ambiente $ℝ^n$. Análogamente, se puede obtener cada coordenada del vector tangente derivando con respecto a las coordenadas en $U$: \[ D_p (x_j) = a_j \]

Podemos preguntarnos por qué tanto lío para llegar al final a que los elementos del espacio tangente son vectores de $ℝ^n$. La respuesta es que de esta forma tenemos una definición estricta y formal de las formas diferenciales que nos permitirá extenderlas naturalmente a variedades.

Otra forma de definir esto sería usar clases de equivalencia de curvas, construyendo vectores tangentes sin acabar de hablar de tangentes. Veremos (y el profesor espera convencernos de eso) que esta forma que hemos desarrollado es en realidad mejor, ya que se comportan mejor respecto a ciertas operaciones. No es lo estándar, desde luego, pero de hecho se va imponiendo que es mejor.\footnote{Resumen: las formas molan y los tensores son caca.}

\subsection{Relación con el cálculo}

Una cosa que no hemos logrado (de momento) con nuestras derivaciones algebraicas ha sido aplicarlas a cambios de coordenadas o a aplicaciones entre abiertos de $ℝ^n$, cosa que sí habíamos hecho con las definiciones de derivada y diferencial que vimos en su momento en cálculo y análisis. Vamos a ir a por ello.

\subsubsection{Aplicaciones entre abiertos}

Suponemos que tenemos abiertos $U⊆ℝ^n, V⊆ℝ^m$, y una aplicación $\appl{F}{U}{V}$ con $F ∈ C^∞$. Sea $p ∈U$ y $F(p) ∈V$, y supongamos $D_p$ una derivación en $p$. La pregunta es si le podemos asociar una derivación en $F(p)$, o dicho de otra forma, si $F$ manda vectores tangentes en vectores tangentes.

Esto puede ocurrir o no, pero como pregunta es importante. Lo que haremos será estudiar variedades y funciones entre variedades, y ver si las propiedades del cálculo diferencial se mantienen por funciones.

Definiremos entonces $F_{*,p}(D_p) ∈ \tgs_p V$ como un vector tangente a $F(p)$ en $V$. Este elemento existe, se podrá definir con una fórmula, y se podrá aplicar a funciones $g ∈ \mathcal{A}_{F(p)}$ donde $\mathcal{A}_{F(p)}$ es el conjunto de gérmenes de funciones en $F_{(p)}$ (ver definición en \ref{defGermenFuncion}).

Sabemos derivar en $U$ pero no en $V$, entonces lo que vamos a hacer va a ser una permutación de símbolos que tiene sentido\footnote{Metageometría diferencial.}. En este caso, nos queda \[ F_{*,p}(D_p)(g) = D_p (g ○ F)\] donde $g ○ F ∈ \mathcal{A}_p$ y sí sabremos derivarlo. Queda demostrar que efectivamente $F_{*,p}(D_p)$ es una derivación\footnote{Se deja como ejercicio.}. Con la suma, tendríamos algo como esto:

\[ F_{*,p}(D_p + D_p')(g) = [D_p + D_p'](g○F) \]

Querríamos escribir ahora  $[D_p + D_p'](g○F) = D_p (g○F) + D_p'(g○F)$, y de hecho podemos hacerlo por ser $D_p, D_p'$ elementos del dual de los gérmenes y por la definición de la suma en el dual\footnotemark\ y por lo tanto nos quedaría efectivamente que \[ F_{*,p}(D_p + D_p')(g) = F_{*,p}(D_p)(g) + F_{*,p}(D_p')(g) \]

\footnotetext{La suma de dos elementos del espacio dual aplicada a un elemento es la suma de sus aplicaciones al mismo elemento. Es decir, si $A, B ∈ E^{*}$ entonces $(A+B)(x) = A(x) + B(x)$ con $x ∈ E$.}

Bien, ¿quién es esa función $F_{*,p}$ en el cálculo diferencial (lo que vimos en Análisis Matemático)? No es más que la diferencial de $F$, que recordemos que era una aplicación \[ F_{*,p} = \appl{DF(p)}{\tgs_p U} {\tgs_{F(p)}V}\] cuya matriz era la de derivadas parciales, \[ DF(p) = \left(\dpa{F_j}{x_i}\right)_{ij} \]

Vamos a ver que efectivamente eso es lo que sale. Si tenemos coordenadas $x_1, \dotsc, x_n$ en $\tgs_pU$ y $y_1, \dotsc, y_m$ en $\tgs_{F(p)} V$, con $F = (F_1, \dotsc, F_m)$ y $y_j = F_j (x_1, \dotsc, x_n)$; nos queda que \[ F_{*, p} \left(\dpa{}{x_i}_p\right)(y_j) = \dpa{F_j}{x_i} (p) \] efectivamente la matriz jacobiana.

Con esto, nos queda que podemos usar los teoremas de análisis que ya conocemos: regla de la cadena, teorema de la función inversa (\ref{thmInv}) y teorema de la función implícita (\ref{thmFImp}). Son teoremas demostrados por definición de límite pero que usaremos libremente.

\subsubsection{Coordenadas locales}

En Geometría nos interesará cambiar las coordenadas para adaptarlas al problema. Supongamos que tenemos las coordenadas $(x_1, \dotsc, x_n)$ para un abierto $U⊆ℝ^n$, y una serie de funciones $f_i$ tales que \[ y_i = f_i(x_i, \dotsc, x_n)\] son otro conjunto de coordenadas.

La pregunta que nos podemos hacer es, si dado un $p ∈ U$, las diferenciales \[ (\dif y_j)_p = \sum \dpa{f_j}{x_i}(p) \dif x_i (p)\] forman una base de tal forma que \[ \gen{(\dif y_1)_p, \dotsc, (\dif y_n)_p} = \tgs_p^* U \]

Es claro que la condición necesaria es que la matriz jacobiana formada por las derivadas parciales $\dpa{f_j}{x_i}(p)$ del cambio de coordenadas sea invertible o de rango máximo ($n$).

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\inputtikz{I_EspiralDifeomorfismo}
\caption{La aplicación $F$ que lleva la espiral a la circunferencia cumple el T.F.Inv. en todo punto, pero por no ser inyectiva no hay un difeomorfismo.}
\label{figEspiralDifeomorfismo}
\end{wrapfigure}

¿Es esta condición también suficiente? Aquí podemos usar el teorema de la función inversa (\ref{thmInv}) nos dice que es ``casi'' suficiente, ya que nos dice que en un entorno $V$ de $p$ quizás más pequeño que $U$ la función $F$ del cambio de coordenadas (la formada por las $f_i$) es un difeomorfismo (\ref{defDifeomorfismo}) con su imagen.


Para completarlo, suponemos que las condiciones del teorema de la función inversa se cumplen $∀p ∈ U$. En este caso, nos preguntamos si $F$ define un difeomorfismo con su imagen.

Dado que la diferencial es local, no se ``entera'' de lo que ocurre lejos del punto que estemos considerando. Si $F$ no es inyectiva, nos estropea el difeomorfismo global aunque en cada entorno sí que sea inyectiva sobre su imagen (por ejemplo, una espiral que llevamos a una circunferencia, figura \ref{figEspiralDifeomorfismo}). Luego para que $F$ defina un difeomorfismo con su imagen en todo $U$ tiene que ser inyectiva.

\section{Álgebra multilineal o teoría de tensores}

\subsection{Motivación}

Ya tenemos una definición algebraica de las derivaciones, que no son más que aplicaciones lineales. Pero el álgebra que tenemos hasta ahora se nos queda un poco corta.

Pongamos un ejemplo usando poca matemática. Recordando el principio de esta sección, lo que queríamos no era más que hacer cálculo (derivar e integrar) en espacios extraños, más allá de $ℝ^n$. Para ello necesitábamos los conceptos de elemento de longitud/área/volumen. Esos conceptos vienen dados por las diferenciales, que acabamos de definir de forma algebraica como aplicaciones lineales. Por ejemplo, un elemento de área viene dado por dos vectores tangentes (derivaciones) que definen un plano.

Querríamos manejar estas aplicaciones lineales de manera natural. Por ejemplo, si uno de los vectores del elemento de área se duplica, lo normal sería que el elemento de área se duplicase también. En definitiva, vamos a querer aplicaciones que nos permitan manejar múltiples vectores, y todos ellos de forma lineal. Necesitamos algo más que el álgebra lineal, y en un alarde de originalidad lo vamos a llamar ``álgebra multilineal''. A por ello.

\subsection{Definición}

Consideraremos un espacio vectorial $E$ sobre un cuerpo $\kbb$ con dimensión finita $n$, y su espacio dual $E^*$ de la misma dimensión con una base $e^*_i$ tal que $e_i^*(e_j) = δ_{ij}$.

\begin{defn}[Forma\IS multilineal] Una forma\footnote{``Forma'' indica que los valores son escalares} multilineal es una función \[
\appl{F}{\underbrace{E^* × \dotsb × E^*}_{a} × \underbrace{E × \dotsb × E}_{b}}{\kbb} \] lineal en cada variable. Por convención, se dirá que es $a$ veces contravariante y $b$ veces covariante.
\end{defn}

Por ejemplo, el determinante de una matriz se puede considerar como una forma multilineal: es una aplicación de $n$ veces $E$ a $\kbb$.

Una cosa interesante es que el conjunto de todas las formas multilineales $a$ veces contravariantes y $b$ veces covariantes, que denotaremos por $\tens_b^a(E)$ es un espacio vectorial con las operaciones naturales:

\begin{gather*}
(Φ_1 + Φ_2)(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) = Φ_1(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) + Φ_2(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) \\
(λ ·Φ_1)(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) = λ (Φ_1(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b))
\end{gather*}

También podemos definir el producto de tensores.

\begin{defn}[Producto\IS tensorial] Si tenemos $Φ_1 ∈ \tens_a^b(E), Φ_2 ∈ \tens_c^d(E)$, tenemos que

\begin{multline*} Φ_1 \otimes Φ_2 (x_1^*,\dotsc, x_{a+c}^*, x_1, \dotsc, x_{b+d}) = \\ Φ_1(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) ·_\kbb Φ_2(x_{a+1}^*,\dotsc, x_{a+c}^*, x_{b+1}, \dotsc, x_{b+d}) \end{multline*}

En este caso, tenemos que $Φ_1 \otimes Φ_2 ∈ \tens_{a+c}^{b+d}(E)$. \label{defProdTensorial}
\end{defn}

Este producto tiene propiedades deseables, como el hecho de que sea distributivo ($Φ_1 \otimes (Φ_2 + Φ_3) = Φ_1 \otimes Φ_2 + Φ_1 \otimes Φ_3$) al ser $\kbb$ un cuerpo con el producto distributivo respecto a la suma.

Lo que nos interesará es ver bases en estos espacios de tensores de dimensión finita, y también operaciones naturales con los tensores.

\subsection{Bases del espacio de tensores}

Es un proyecto complicado, así que vamos a dividir la tarea de escribir una base en tres partes. Primero, buscaremos la base para tensores covariantes ($(b,0)$ ó $\tens_0^a (E)$), después para tensores contravariantes ($(0,a)$ ó $\tens_0^a (E)$)) y después para covariantes y contravariantes.

\subsubsection{Base de tensores covariantes $(b,0)$}

Partimos de algo simple: tenemos $E$ un espacio vectorial sobre $\kbb$ de dimensión finita $n$, con una base $\base = \set{e_i}_{i∈I} \equiv \set{e_1, \dotsc, e_n}$. Sea $Φ ∈ \tens_0^b(E)$ un tensor covariante $(0,b)$, es decir, una aplicación multilineal que lleva $b$ vectores de $E$ a un valor en $\kbb$.

Queremos saber entonces cómo caracterizar $Φ$ como combinación lineal de elementos de la base. Más concretamente, querríamos poder escribir \[ Φ = \sum λ_m ρ_m \] donde $ρ_m$ serían los elementos de la base y $λ_m$ las coordenadas.

Para llegar a esa notación, vamos a empezar estudiando la imagen $Φ(\vx_1, \dotsc, \vx_b)$.Sabemos que cada $\vx_j$ se puede expresar como combinación lineal de elementos de la base de $E$, es decir, \[ \vx_j = \sum_{i = 1}^n x_i^j e_i\] donde $x_i^j$ es la coordenada $i$ del vector $j$.

Entonces, podemos escribir \[ Φ(\vx_1, \dotsc, \vx_b) = Φ\left(\sum_{i = 1}^n x_i^1 e_i, \dotsc, \sum_{i = 1}^n x_i^b e_i\right)\]

Por ser $Φ$ multilineal, podemos ver que esos sumatorios y productos por escalares se van a poder sacar. La cuestión es cómo. Veamos cómo sale el primer sumatorio:

\[ Φ\left(\sum_{i = 1}^n x_i^1 e_i, \dotsc, \sum_{i = 1}^n x_i^b e_i\right) = \sum_{{j_1}=1}^n x_{j_1}^1 · Φ\left(e_{j_1}, \sum_{i = 1}^n x_i^2 e_i, \dotsc, \sum_{i = 1}^n x_i^b e_i\right) \]

Parece claro entonces que a partir de aquí nos van a quedar $b$ sumatorios que van desde $1$ hasta $n$. Es decir, \[ Φ(\vx_1, \dotsc, \vx_b) = \sum_{{j_1}=1}^n \dotsb \sum_{{j_b}=1}^n x_{j_1}^1 · \dotsb x_{j_b}^b Φ(e_{j_1}, \dotsc, e_{j_b})\]

Vemos que, independientemente de los vectores $\vx_i$ que cojamos, hay una parte que se queda constante, que sólo depende de cómo es $Φ$: los escalares $Φ(e_{j_1}, \dotsc, e_{j_b}) ∈ \kbb$. Podríamos pensar entonces que eso son las coordenadas de $Φ$ en $\tens_0^b(E)$. La cuestión es, ¿quiénes son los elementos de la base?

Los elementos de la base deberían ser elementos que nos den $x_{j_1}^1 \cdot \dotsb x_{j_b}^b$ cuando sean aplicados a $\vx_1, \dotsc, \vx_b$. Por así decirlo, necesitamos ``selectores de coordenadas'', que cuando apliquemos a los vectores nos den un producto de determinadas coordenadas.

Haciendo un ejercicio de intuición matemática, podemos pensar en los elementos del espacio dual $E^*$. Consideramos la base $\base^* = \set{e_1^*, \dotsc, e_2^*}$ del espacio dual, donde $e_i^*(e_j) = δ_{ij}$. Visto de otro modo, $e_i$ es una aplicación lineal tal que $e_i^*(\vv) = v_i$, es decir, que nos da la coordenada $i$-ésima del vector que le pasemos como argumento\footnote{Algo más formalmente: si $\vv = \sum_{j=1}^n v_j e_j$, entonces $e_i^*(\vv) = \sum_{j=1}^n v_j e_i^*(e_j) = v_i e_i^*(e_i) = v_i$.}.

Parece que los elementos de la base del dual son lo que necesitamos, lo que actuará de ``selector de coordenadas''. ¿Cómo los componemos para poder aplicarlo a $b$ vectores? Usando lo que vimos antes del producto tensorial (\ref{defProdTensorial}). El truco es que podemos considerar un elemento del dual como un tensor $(0,1)$, y entonces el producto tensorial de $b$ tensores es un tensor $(0,b)$.

Es decir, que los elementos de la base del espacio de tensores $\tens_0^b(E)$ serían los elementos \begin{gather*}
ρ_{j_1,\dotsc,j_b} = e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^* \\
\appl{ρ_{j_1,\dotsc,j_b}}{\underbrace{E×\dotsb×E}_{b}}{ℝ}
\end{gather*} donde $ρ_{j_1,\dotsc,j_b}$ es un tensor $(0,b)$ que coge la coordenada $j_1$ del vector $1$, la $j_2$ del vector $2$, etc\footnote{Por ejemplo, si tomamos $b=2$, tendríamos que $ρ_{1,2}((1,2), (3,4)) = 1 · 4$.}, es decir, con
\[
ρ_{j_1,\dotsc,j_b} (\vx_1, \dotsc, \vx_b) = \left(e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^*\right)(\vx_1, \dotsc, \vx_b) = x_{j_1}^1 \dotsb x_{j_b}^b
\]
donde $x_{j_b}^k$ es la coordenada $j_b$ del vector $k$, con $j_b = 1, \dotsc, n$ y $k = 1, \dotsc, b$.

Así, nos quedaría que podemos expresar un tensor $Φ$ como \[ Φ = \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n \underbrace{Φ(e_{j_1}, \dotsc, e_{j_b})}_{\text{Coordenada}} · \underbrace{e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^*}_{\text{Elemento de la base}} \] de tal forma que la imagen de $\vx_1, \dotsc, \vx_b$ por $Φ$ se puede calcular como
\begin{align*}
Φ(\vx_1, \dotsc, \vx_b) &= \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n Φ(e_{j_1}, \dotsc, e_{j_b}) · \left(e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^*(\vx_1, \dotsc, \vx_b)\right) = \\
&= \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n Φ(e_{j_1}, \dotsc, e_{j_b}) · x_{j_1}^1 \dotsb x_{j_b}^b
\end{align*}
donde, de nuevo, $x_{j_b}^k$ es la coordenada $j_b$ del vector $k$.

\paragraph{Ejemplo: base de un tensor $(0,2)$} Un tensor $Φ∈\tens_0^2(ℝ^2)$ se podrá descomponer de la siguiente forma: \[ Φ = λ_{1,1} · e_1^* \otimes e_1^* + λ_{1,2} · e_1^* \otimes e_2^* + λ_{2,1} · e_2^* \otimes e_1^* + λ_{2,2} · e_2^* \otimes e_2^*\] donde $λ_{j_1, j_2} ∈ ℝ$ y $e_1, e_2$ son una base de $ℝ^2$, que en este caso tomaremos como la canónica $e_1 = (1,0),\;e_2=(0,1)$. Así, los tensores $e_{j_1} \otimes e_{j_2}$ serán matrices cuadradas $2 × 2$. Por ejemplo, \[ e_1 \otimes e_2 = \begin{pmatrix} 1 & 0 \\ 0 & 1\end{pmatrix}\]

Entonces, los tensores $Φ∈\tens_0^2(ℝ^2)$ no son más que matrices $2 × 2$, que se pueden expresar como combinación lineal de cuatro matrices base, cada una obtenida con los productos tensoriales de los elementos de la base de $ℝ^2$.

\paragraph{El determinante como tensor $(0,b)$} Un ejemplo más concreto: antes decíamos que el determinante es un tensor, así que vamos a ir a ello. Sea $Ψ_b$ la operación que nos da el determinante de una matriz $b × b$, es decir, \[ \appl{Ψ_b}{M_{b×b}}{ℝ} \]. ¿Cómo lo expresamos como un tensor?

Lo primero es pasar del espacio de matrices al espacio en el que se definen los tensores. Podemos ver que una matriz $b × b$ no es más que un conjunto de $b$ vectores de dimensión $b$, es decir, que \[ M_{b×b} = \underbrace{E × \dotsb × E}_{b \text{ veces}}\] donde $E$ es el espacio vectorial que estemos trabajando. Tomemos por comodidad y por seguir con el ejemplo de antes $b = 2$ y $E = ℝ^2$, por lo que vamos a buscar el determinante de matrices de dimensión $2$.

Sea \[ A = \begin{pmatrix} a & c \\ b & d \end{pmatrix} = \begin{pmatrix} \vx_1 & \vx_2 \end{pmatrix} \] con $\vx_1 = \begin{pmatrix}a \\ b\end{pmatrix}$ y $\vx_2 = \begin{pmatrix}c \\ d\end{pmatrix}$. Entonces $Ψ_2(A) = ad - cb$. Mirándolo desde el punto de vista de ``selección de coordenadas'' que comentábamos antes, seleccionamos la coordenada 1 del primer vector y la 2 del segundo multiplicando por 1; y luego seleccionamos la coordenada 2 del primer vector y la 1 del segundo multiplicando por -1.

Expresando esto con los elementos de la base, tendremos que \( Ψ_2 = 0 · e_1^* \otimes e_1^* + 1 · e_1^* \otimes e_2^* - 1 · e_2^* \otimes e_1^* + 0 · e_2^* \otimes e_2^* \label{eqDet2Tensor} \)

Bajo esta notación, el determinante de una matriz $A$ sería \begin{align*}
Ψ_2(A) &= 0 · \left(e_1^* \otimes e_1^* (\vx_1, \vx_2) \right)+ 1 · \left(e_1^* \otimes e_2^* (\vx_1, \vx_2) \right) - 1  · \left(e_2^* \otimes e_1^* (\vx_1, \vx_2) \right) + 0 · \left(e_2^* \otimes e_2^* (\vx_1, \vx_2) \right) = \\
&= 0 · x_1^1 x_2^1 + 1 · x_1^1 x_2^2  - 1 · x_1^2 x_2^1 + 0 · x_2^2 x_2^2\end{align*}

Podríamos ver, por ampliar un poco, cómo escribiríamos el tensor que nos da el determinante de matrices de dimensión 3 (omitiendo las coordenadas que son 0)\footnote{Y probablemente con algún índice que me ha bailado también.}:
\begin{multline*} Ψ_3 = e_1^* \otimes e_2^* \otimes e_3^* + e_2^* \otimes e_3^* \otimes e_1^* + e_3^* \otimes e_1^* \otimes e_2^*  \\
 - e_3^* \otimes e_2^* \otimes e_1^* - e_2^* \otimes e_1^* \otimes e_3^* - e_2^* \otimes e_1^* \otimes e_3^* \end{multline*}

\subsubsection{Base de tensores contravariantes $(0,a)$}

La construcción de la base de tensores contravariantes es análoga a la de covariantes. Estudiaremos la imagen $Φ(\vx_1^*, \dotsc, \vx_a^*)$, que nos quedará como
\[ Φ(\vx_1^*, \dotsc, \vx_a^*) = \sum_{{j_1}=1}^n \dotsb \sum_{{j_b}=1}^n x_{j_1}^1 · \dotsb x_{j_b}^b Φ(e_{j_1}^*, \dotsc, e_{j_b}^*)\] donde $\base^* = \set{e_1^*,  \dotsc, e_n^*}$ es una base del dual $E^*$.

Igualmente, necesitaremos unos ``selectores de coordenadas'' que nos lleven los $\vx_1^*, \dotsc, \vx_a^*$ a los distintos productos $x_{j_1}^1 · \dotsb x_{j_b}^b$. Esos selectores de coordenadas serán aplicaciones lineales $\appl{e_{i}^{**}}{E^*}{\kbb}$, es decir, $e_{i}^{**} ∈ E^{**}$, elementos del dual del dual. Así, la base serían los elementos $e_{j_1}^{**} \otimes \dotsb \otimes e_{j_b}^{**}$. Por suerte para nosotros, se puede identificar naturalmente\footnote{Habría que decir bien por qué.} $E^{**}$ con $E$ a través de un isomorfismo, así que nuestra base serían los productos tensoriales de elementos de $E$, es decir, elementos de la forma $e_{j_1} \otimes \dotsb \otimes e_{j_b}$.

En definitiva, que podemos expresar tensores $(0,a)$ $Φ∈\tens_0^a$ como combinación lineal de la siguiente forma:

\[ Φ = \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n \underbrace{Φ(e^*_{j_1}, \dotsc, e^*_{j_b})}_{\text{Coordenada}} · \underbrace{e_{j_1} \otimes \dotsb \otimes e_{j_b}}_{\text{Elemento de la base}} \]

\subsubsection{Base de tensores generales $(a,b)$}

Una vez que ya tenemos la bases de tensores covariantes y contravariantes, por separado, tenemos que juntarlos. Problema número uno: el infierno de notación que nos podemos encontrar.

\paragraph{Notación: dual arriba, normal abajo} Vamos a jugar con índices y subíndices. Cuando hablemos de vectores en el espacio vectorial $E$, usaremos subíndices tanto en los vectores de la base como en las coordenadas: \[  \vv = \sum_{i=1}^n λ_i e_i ∈ E\]

Análogamente, usaremos superíndices cuando trabajemos con el espacio dual \[ \vv^* = \sum_{i=1}^n λ^i e^i ∈ E^* \] con la ventaja de que nos evita tener que poner la estrella al referirnos a vectores del dual.

Bien, con esto vamos a ver cómo expresar un tensorial $Φ ∈ \tens_a^b (E)$. Podríamos seguir lo que hemos visto en las anteriores secciones, sacando los sumatorios usando la propiedad de mulitilinealidad de los tensores. Al final, nos quedaría que los elementos de la base serían los productos tensoriales \[ e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b} \] de tal forma que podemos expresar un tensor como  \[ Φ = \sum_{i_1 = 1}^n \dotsc \sum_{i_b = 1}^n \sum_{j_1 = 1}^n \dotsb \sum_{j_b = 1}^n Φ(e^{i_1}, \dotsc, e^{i_a}, e_{j_1},  \dotsc, e_{j_b}) · e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b} \]

Por comodidad, denotaremos las coordenadas $Φ(e^{i_1}, \dotsc, e^{i_a}, e_{j_1},  \dotsc, e_{j_b})$ con una letra mayúscula $T^{i_1, \dotsc, i_a}_{j_1, \dotsc, j_b}$, esto es, \[ T^{i_1, \dotsc, i_a}_{j_1, \dotsc, j_b} ≝ Φ(e^{i_1}, \dotsc, e^{i_a}, e_{j_1},  \dotsc, e_{j_b}) \]

Sólo tendremos que especificar estas coordenadas cuando hablemos un vector. Por ejemplo, siguiendo con el ejemplo que habíamos descrito en la ecuación \eqref{eqDet2Tensor} del determinante de matrices de orden 2 como tensor, tendríamos que dar las cuatro coordenadas como $T_{1,1} = 0,\, T_{1,2} = 1,\, T_{2,1} = -1,\, T_{2,2} = 0$ (no ponemos superíndices porque las entradas del determinante son vectores del espacio base, no hay nada de dual).

\subsection{Contracciones}

En el álgebra lineal, las aplicaciones lineales se pueden representar por matrices. Y una propiedad interesante de las matrices es la traza. Por ejemplo, la traza de una matriz identidad es la dimensión del espacio en el que vive esa matriz, \href{http://mathoverflow.net/questions/13526/geometric-interpretation-of-trace}{entre otras cosas}. Querremos generalizar esta noción a los tensores, y lo hacemos a través de la contracción.

\begin{defn}[Contracción] La contracción, denotada como $C_1^1$, es una función lineal \[ \appl{C_1^1}{\tens_a^b(E)}{\tens_{a-1}^{b-1}(E)} \] que reduce en una unidad cada una de las componentes de la aplicación multilineal. Si $Φ ∈\tens_a^b(E)$ es una forma multilineal con coordenadas $T_{j_1, \dotsc, j_a}^{i_1, \dotsc, i_b}$ entonces las coordenadas $\adh{T}$ de $C_1^1(Φ)$ se definen de la forma \[ \adh{T}_{j_2, j_3, \dotsc, j_a}^{i_2, i_3, \dotsc, i_b} ≝ \sum_{α = 1}^n T_{α,j_2, j_3, \dotsc, j_a}^{α, i_2, i_3, \dotsc, i_b} \]

Es obvio que para que esta definición tenga sentido han de ser $a,b ≥ 1$.
\end{defn}

Por ejemplo, la contracción de una forma multilineal $Φ ∈\tens_2^2(E)$ dada por las coordenadas \[ \begin{matrix}
T_{1,1}^{1,1} = 1 & T_{1,2}^{1,1} = 1 & T_{2,1}^{1,1} = 1 & T_{2,2}^{1,1} = 0 \\
T_{1,1}^{1,2} = 2 & T_{1,2}^{1,2} = 0 & T_{2,1}^{1,2} = 4 & T_{2,2}^{1,2} = 0 \\
T_{1,1}^{2,1} = 3 & T_{1,2}^{2,1} = 1 & T_{2,1}^{2,1} = 0 & T_{2,2}^{2,1} = 1 \\
T_{1,1}^{2,2} = 0 & T_{1,2}^{2,2} = 2 & T_{2,1}^{2,2} = 1 & T_{2,2}^{2,2} = 0 \\
 \end{matrix} \]
 estaría dada por las siguientes coordenadas\footnote{Me ha bailado algún índice ssssseguro.}:
\begin{align*}
\adh{T}_1^1 &= \sum_{α = 1}^2 T_{α,1}^{α,1} = T_{1,1}^{1,1} + T_{2,1}^{2,1} = 1 + 0 = 1 \\
\adh{T}_1^2 &= \sum_{α = 1}^2 T_{α,1}^{α,2} = T_{1,1}^{1,2} + T_{2,1}^{2,2} = 2 + 2 = 4 \\
\adh{T}_2^1 &= \sum_{α = 1}^2 T_{α,2}^{α,1} = T_{1,2}^{1,1} + T_{2,2}^{2,1} = 1 + 1 = 2 \\
\adh{T}_2^2 &= \sum_{α = 1}^2 T_{α,2}^{α,2} = T_{1,2}^{1,2} + T_{2,2}^{2,2} = 0 + 0 = 0
\end{align*}

Podemos preguntar de forma más general cómo actúan las contracciones sobre los vectores de la base de tensores $e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b}$. Ese elemento de la base se puede considerar como un tensor $Φ$ con coordenada $T_{j_1, \dotsc, j_a}^{i_1, \dotsc, i_b} = 1$ y resto de coordenadas 0. Entonces, $C_1^1(Φ)$ será un tensor con todas las coordenadas $0$ salvo $R_{j_2, \dotsc, j_a}^{i_2, \dotsc, i_b}$, dada por \[ R_{j_2, \dotsc, j_a}^{i_2, \dotsc, i_b} = \sum_{α = 1}^n T_{α, j_2, \dotsc, j_a}^{α, i_2, \dotsc, i_b} = T_{j_1, j_2, \dotsc, j_a}^{i_1, i_2, \dotsc, i_b} = 1 \]

Esta coordenada corresponde además al vector de la base de $\tens_{a-1}^{b-1}(E)$ $e_{i_2} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_2} \otimes \dotsb \otimes e^{j_b}$. Es decir, que la actuación de la contracción sobre vectores de la base es la siguiente: \[
C_1^1(e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b}) ≝ e^{j_1}(e_{i_1}) · \left(e_{i_2} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_2} \otimes \dotsb \otimes e^{j_b}\right)\]


\paragraph{Algo de magia} Si tenemos un tensor $(1,1)$, un endomorfismo\footnote{¿Ah sí?}, su contracción es un tensor $(0,0)$, un número\footnote{WTF $×2$.}, y vemos, escribiendo su definición, que es exactamente la traza del endomorfismo\footnote{Zasca final.}

Mi intento de explicación: un tensor $(1,1)$ es una aplicación bilineal $\appl{Φ}{E^*×E}{ℝ}$. Uno puede tratar de representarlo como una matriz $A$ de tal forma que \[ Φ(\vv^*, \vv) = \begin{pmatrix} v_1^* & \cdots & v_n^* \end{pmatrix} · \begin{pmatrix} A \end{pmatrix} · \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \] que se puede ver que es un escalar. Ahora bien, que eso sea endomorfismo es difícil. Lo que sí es claro es que, viendo cómo actúa la contracción, tendríamos que \[ C_1^1 (e_{i_1} \otimes e^{j_1}) =  e^{j_1}(e_{i_1}) \] es decir, que los elementos de la base son escalares.

\seprule

Vamos a recuperar un poco la perspectiva: queremos buscar una función $f$ tal que $\dif f = ω$, donde ω es una 1-forma. Buscábamos también el significado de la diferencial de una variable $(\dif x_i)_{x_0}$ o de una función $(\dif f)_{x_0}$, que veíamos en la sección \ref{secDimTangente}. Ahora, con las definiciones del álgebra multilineal, lo que estamos haciendo es darle sentido a la expresión del producto exterior $\dfl{x_{i_1}}{x_{i_a}}$.

Ahora cosas sobre isomorfismos raros. Identificación entre $E×E\dotsb × E^* \dotsb $ y $E\otimes$ etc con una aplicación lineal $U_Φ$ entre el tensorial y $\kbb$.


¿Cuánto vale su contracción? Lo primero es ver que será un tensor $(0,0)$, que ya habíamos dicho que eran escalares. Aplicando la definición, tenemos que $C^1_1(Φ)$ es un tensor dado por una única coordenada $\adh{T}$ que se define como \[ \adh{T} = \sum_{i=1}^n T_i^i\] donde $T_i^j$ son los elementos de la matriz de $Φ$, y por lo tanto esa ecuación no es más que la suma de los elementos de la diagonal de la matriz.

Es decir, que la contracción de una matriz vista como un tensor $(1,1)$ es su traza, llegando efectivamente a la generalización de este concepto que queríamos antes.

% Nota: aquí iba algo de funtorialidad pero paso bastante de poner cosas que no sirven para nada. Si alguien lo quiere recuperar que mire el git y los apuntes de Moodle.

\subsection{Cambios de base}

De álgebra lineal sabíamos que los cambios de base se hacían con una matriz de cambio. ¿Cómo extendemos esto a las formas multilineales?

Es algo que nos interesará mucho hacer, ya que más tarde o más temprano tendremos que pasar de una variedad a otra y cambiar de coordenadas. Tenemos, por lo tanto, que entender bien cómo funcionan los cambios de base.

Partimos de dos bases $\base = \set{e_1, \dotsc, e_n}$, $\base' = \set{e_1', \dotsc, e_n'}$. Entonces, sabemos que si tenemos $\vv ∈ E$, lo podemos escribir de dos formas:
\[\vv = \sum_{i=1}^n λ^ie_i = \sum_{i=1}^n μ^i e_i' \] y que además podemos pasar de una base a la otra \[ e_i = \sum_j p_i^j e_j' \qquad e_i' = \sum_j q_i^j e_j\] es decir, multiplicando por las matrices de cambio de base $P = (p_i^j), Q=(q_i^j)$, que cumplen que $PQ=QP=I$.

Nos podemos preguntar también cómo cambian las cosas si tenemos dos bases del espacio dual $E^*$: $\base^* = \set{e_1, \dotsc, e_n}$, $\base'^* = \set{e'^1, \dotsc, e'^n}$. No hay duda de que tiene que existir una forma de cambiar entre las dos bases tal que \[ e'^j = \sum_i a_i^j e^i \] con $A = (a_i^j)$ es una matriz invertible. Para calcularlo, querremos evaluar esos vectores del dual en vectores de la base de $E$ (los $e_t$) y ver qué sale\footnote{Usamos que $e^j(e_i) = δ_i^j$, esto es, $1$ si los índices son iguales y 0 si no (ver \ref{defDeltaKronecker})}:
\begin{align*}
e'^j (e_t) &= \left(\sum_i a_i^j e^i \right) (e_t) \\
e'^j \left( \sum_r p_t^r e_r' \right) &= a_t^j \\
p_t^j &= a_t^j \\
\end{align*}

En definitiva, yendo al cambio de coordenadas tendremos que, en $E$, \[ μ^j = \sum p_i^j λ^i\] y en el dual $E^*$ se usa la otra matriz, \[ μ_j = \sum q_i^j λ_i\]

\paragraph{Cambio de coordenadas en matrices} Supongamos que tenemos una aplicación lineal $\appl{U}{E}{E}$ dada por una matriz, de la siguiente forma: \[ U(e_i) = \sum a_i^j e_j\]

Podemos decir que $U ∈ \tens_1^1(E)$, espacio que es isomorfo a $E\otimes E^*$. Es decir, que podemos escribir \[ \sum_{ij} a_j^i e_i \otimes e^j = \sum_{ij} a_j^i \left(\sum_l p_i^le'_l\right) \otimes \left(\sum_t q^j_t e'^t\right)\]

Ahora sólo hay que usar que el producto es distributivo respecto de la suma, por lo tanto nos quedaría \[ \sum_{ij} a_j^i \left(\sum_l p_i^le'_l\right) \otimes \left(\sum_t q^j_t e'^t\right) = \sum_{lt}\left(\sum_{ij} a_j^1 p_i^l q_t^j \right)e_l' \otimes e'^t\]

\paragraph{Cambio de coordenadas en tensores en general} Tomamos un tensor $Φ =T = (T_{j_1,\dotsc, j_b}^{i_1,\dotsc, i^a}) ∈ \tens_b^a(E)$, donde podemos identificar al espacio de tensores con \[ \tens_b^a(E) \simeq \underbrace{E\otimes \dotsb \otimes E}_{a} \otimes \underbrace{E^* \otimes \dotsb \otimes E^*}_{b} \equiv E^{\otimes a} \otimes E^{*\otimes b}\]

Sabemos que \[ T = \sum T_{j_1,\dotsc, j_b}^{i_1,\dotsc, i^a} e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b} \]

Siguiendo los mismos cálculos que antes, nos quedaría que el cambio de coordenadas a $T'$ estaría dado por la fórmula \[ T'^{l_1, \dotsc l_a}_{k_1,\dotsc, k_b} = \sum_{\substack{i_a, \dotsc, i_a \\ j_1, \dotsc, j_b}} T_{j_1,\dotsc, j_b}^{i_1,\dotsc, i^a} p_{i_1}^{l_1} \dotsb p_{i_a}^{l_a} q_{j_1}^{k_1} \dotsb q_{j_b}^{k_b} \]

De aquí podemos retomar una definición algo antigua en la historia de las matemáticas:

\begin{defn}[Tensor\IS clásico] Un tensor clásico es una función $\appl{T}{\mathfrak{B}}{\kbb^{n^{a+b}}}$, donde $\mathfrak{B}$ es el conjunto de todas las bases en $E$, de tal forma que \begin{gather*} T(\base) = \left(\tc\right) \\ T(\base') = \left(\tc[k][l]\right) \end{gather*}

\end{defn}

\subsection{Potencias y álgebra tensorial}

\begin{defn}[Potencia] Se define la potencia tensorial como \[ \otimes^n E ≝ \underbrace{E\otimes \dotsb \otimes E}_{n} \]
\end{defn}

\begin{defn}[{Á}lgebra\IS tensorial] Dado un espacio vectorial $E$, se define el álgebra tensorial $\tens^\bullet E$ como \[ \tens^\bullet E ≝ \bigoplus_{n∈ℕ} \left(\otimes^n E\right) \]
\end{defn}

\begin{defn}[{Á}lgebra\IS pinchorial]\footnote{No soy capaz de escribir pinchorial sin reírme.} Se define el álgebra pinchorial como el conjunto de todos los tensoriales antisimétricos
\end{defn}

%No sé qué mierdas ha escrito aquí. Pinchorial. Desisto.Λ

\subsection{Tensores antisimétricos}

¿Qué ocurre con un tensor antisimétrico? Si tenemos $Φ(\vx^*, \vy^*) = -Φ(\vy^*, \vx^*)$, con $Φ∈Λ^2 E$, entonces operando \begin{align*}
Φ(\vx^*, \vy^*) &= -Φ(\vy^*, \vx^*) \\
\sum_{ij} A^{ij} x_i y_j &= - \sum_{ji} A^{ji} x_j y_i \\
\end{align*} nos quedan dos condiciones sobre la matriz $A$: $A^{ii} = 0$ y $A^{ij} = - A^{ji}$. En este caso, podemos simplificar y nos quedaría \[\sum_{i<j} A^{ij} (x_i y_j - x_j y_i) \] y, por lo tanto, podemos escribir \[ Φ = \sum_{i<j} A^{ij} e_i \y e_j \]

En resumidas cuentas, lo que concluimos es que $\dim Λ^2 E = \comb{n}{2}$.

En general, tenemos que $Λ^p E ⊂ \otimes^p E$ con $e_{i_1} \y \dotsb \y e_{i_p}$ los elementos de la base definidos tal que \[ e_1 \y \dotsb \y e_p (f^1, \dotsc, f^p) ≝ \det \left(f^i(e_j)\right)\] y con \[ \dim Λ^p E = \comb{n}{p} \]

Como ejercicio, habría que ver que si $Φ ∈ Λ^p E$ entonces se puede escribir como \[ Φ = \sum_{i_1, \dotsc, i_p} A^{i_1, \dotsc, i_p} e_{i_1} \y \dotsb \y e_{i_p} \]

Ahora vamos a ir a lo que nos interesa. Si tenemos $α∈ Λ^p E, β∈ Λ^q E$, ¿qué es $α\y β$? Uno querría poder escribir $α\y β = k (α\otimes β)$, pero eso no es antisimétrico. Por lo tanto, tendremos que ``antisimetrizarlo''. Recordamos que si tenemos una aplicación $\appl{Φ}{\underbrace{E^* × \dotsb × E^*}_{m}}{\kbb}$, su antisimetrización es \[ Φ_a ≝ \frac{1}{m!}\sum_{σ∈S_m} (-1)^{\mop{sig} σ} Φ(f^{σ(1)}, \dotsc, f^{σ(m)}) \]

Así, justificamos que \[  e_1 \y \dotsb \y e_p (f^1, \dotsc, f^p) = \frac{p!}{p!} \sum_{σ∈S_p} (-1)^{\mop{sig} σ} \prod_{j=1}^p f^{σ(j)}(e_j)\]

\begin{defn}[Producto\IS exterior] Dados dos tensores antisimétricos \begin{gather*} α = \sum A^{i_1, \dotsc, i_p} e_{i_1} \y \dotsb \y e_{i_p} \\ β = \sum B^{j_1, \dotsc, j_q} e_{j_1} \y \dotsb \y e_{j_q} \end{gather*}, su producto se define gracias a la propiedad distributiva como \[ α \y \beta = \sum A^{i_1, \dotsc, i_p} B^{j_1, \dotsc, j_q} e_{i_1} \y \dotsb \y e_{i_p} \y e_{j_1} \y \dotsb \y e_{j_q}  \]
\end{defn}

En cuanto al cambio de signo, es fácil ver que \[ α \y β = (-1)^{pq} β \y α \]


\subsection{Aplicaciones en el espacio vectorial libre}

\begin{figure}[hbtp]
\centering
\inputtikz{I_EspacioVectorialLibre}
\caption{Diagrama de las aplicaciones del espacio vectorial libre.}
\label{imgVecLibre}
\end{figure}

Tenemos un conjunto $X$ y una aplicación $\appl{f}{X}{F}$. Tomando una cierta aplicación $\appl{u}{X}{\kbb^X}$,\footnote{$\kbb^X$ es el espacio vectorial libre, ver \ref{secEspacioVectorialLibre}.} demostrar que existe una aplicación $\appl{\hat{f}}{\kbb^X}{F}$ tal que $f = u ○ \hat{f}$ (ver figura \ref{imgVecLibre}).

Tomaremos $u$ de forma natural, es decir, que $u(x) = g_x ∈ \kbb^X$, $\appl{g_x}{X}{\kbb}$ definida como \[ g_x(α) =
\begin{cases}
1_\kbb & α = x \\
0_\kbb & α ≠ x \end{cases} \]

Basta con encontrar ahora la función $\hat{f}$ que cumpla las propiedades indicadas. Esta función deberá tomar una función y devolver un valor en $F$.

Podemos definir $\hat{f}(g_x)=f(x)$ y $\hat{f}{h}=0$ para toda función $h$ que no sea de la forma $g_x$.

Con esta $\hat{f}$ podemos comprobar de manera sencilla que se cumplen todas las condiciones pedidas.

\section{Formas diferenciales}

Con toda la base del álgebra multilineal podemos definir más prácticamente las formas diferenciales.

\begin{defn}[p-forma] Una p-forma es un tensor contravariante que expresamos de la siguiente manera: \[ ω_p = \sum a_{i_1, \dotsc, i_p} (x_1, \dotsc, x_n) \dfl{x_{i_i}}{x_{i_p}} = \sum a_I \dif x_I \]

con $a_I ∈ C^∞(U)$, donde $U ⊆ ℝ^n$.
\end{defn}

Así, si tenemos dos p-formas \begin{align*}
ω &= \sum a_I \dif x_I \\
τ &= \sum b_J \dif x_J
\end{align*}

podemos operarlas con el \concept[Producto\IS exterior]{producto exterior} de la siguiente forma: \[ ω \y τ ≝ \sum a_I b_J \dif x_I \y x_J \] teniendo en cuenta que $\dif x_i \y \dif x_i = 0$.

Este producto cumple las propiedades distributiva y asociativa.

\subsection{Pullback}

También podemos ver qué ocurre con las formas diferenciales cuando tenemos aplicaciones entre espacios tangentes. Suponemos que tenemos $\appl{F}{U}{V}$, con $x_1, \dotsc, x_n$ coordenadas en $U$ y $y_1, \dotsc, y_m$ coordenadas en $V$, y con \[ F = (y_1(x_1, \dotsc, x_n), \dotsc, y_m(x_1, \dotsc, x_n)) \]

Por otra parte, tenemos una forma diferencial $ω = \sum a_I \dif y_I$ definida en $V$. ¿Existe una forma diferencial equivalente en $U$? Pues sí, existe. Podemos ``traer de vuelta'' ω a $U$ simplemente sustituyendo:

\begin{align*}
ω &= \sum a_I(y_1, \dotsc, y_m) \dif y_I \\
&= \sum a_I(y_1(x_1, \dotsc, x_n), \dotsc, y_m(x_1, \dotsc, x_n)) \dif y_I (x_1, \dotsc, x_n) \\
&= \sum_i a_i(y_1(x_1, \dotsc, x_n), \dotsc, y_m(x_1, \dotsc, x_n)) \left( \sum_j \dpa{y_i}{x_j} \dif x_{j} \right)
\end{align*}

\subsubsection{Campos como formas diferenciales y relación con el pullback}

Vimos en algún momento el operador de campo \[ D_p = \sum a_1(x_1, \dotsc, x_m) \left(\dpa{}{x_i}\right)_p \] que se se aplica a fuciones, de tal forma que \[ D_p(f) = \sum_i a_i \left(\dpa{f}{x_i}\right)_p ∈ C^∞ (U) \]

Una forma de ver estos campos vectoriales es como vectores que nos indican la velocidad de un fluido en cada punto (suponiendo que esta no dependa del tiempo). Veremos que en Geometría nos interesará buscar curvas tales que en todo punto su vector tangente sea el vector del campo, cosa que haremos con EDOs.

Si consideramos una 1-forma \[ ω_p = \sum b_i(x_1(p), \dotsc, x_m(p)) (\dif x_i)_p\] no es más que una aplicación lineal $\appl{ω_p}{\tgs_p U}{ℝ}$. Podemos pensar en el núcleo $N_p = \ker ω_p ⊂ \tgs_p U$ con codimensión\footnote{La codimensión es la dimensión del núcleo menos la del total, esto es, $\dim N_p = n - 1$.} 1\footnote{Por no sé qué teorema.}.

En este caso, en cada punto tendríamos un hiperplano $N_p$ diferenciable que varía con el punto.

Es decir, que cuando tenemos una 1-forma diferencial buscaremos encontrar las hipersuperficies resolviendo la ecuación $ω = 0$.

Sabemos que $F^*(ω)$, con $ω$ una $p$-forma y $\appl{F}{U}{V}$, actúa sobre $p$ vectores $D_1, \dotsc, D_p$ del espacio tangente $\tgs_{x_0} U$ en $x_0$. Entonces, definiremos \[ F^*(ω)(D_1, \dotsc, D_p)≝ ω(F_* D_1, \dotsc, F_* D_p)\] donde cada $F_* D_i ∈ \tgs_{F(x_0)} V$ se define como \[ F_* D(g) ≝ D(g○F)\]

Las propiedades de esta imagen inversa son las siguientes (por definición):

\begin{enumerate}
\item $F^*(ω + τ) = F^* ω + F^* τ$.
\item $F^*(gω) = F^*(g) F^*(ω)$ donde $g$ es una función en $U$ y $F^*(g) = F ○ g $ es una función en V.
\item $F^*(ω_1 \y \dotsb \y ω_k) = F^*ω_1 \y \dotsb \y F^*ω_k$ para $ω_i$ 1-formas.
\item $F^*(ω \y τ) = F^*ω \y F^* τ$ para formas en general.
\item Si tenemos dos aplicaciones $G, F$, tenemos que $(G ○ F)^*ω = F^*(G^*(ω))$.
\end{enumerate}

Vamos a hacer la demostración de la propiedad 3.

\begin{proof}[Propiedad 3] $F^*(ω_1 \y \dotsb \y ω_k)$ va a ser una $k$-forma en $U$, luego va a actuar sobre $k$ vectores tangentes. Entonces, por definición, tenemos que \[ F^*(ω_1 \y \dotsb \y ω_k)(D_1, \dotsc, D_k) = ω_1 \y \dotsb \y ω_k(F_*D_1, \dotsc, F_* D_k)\] y entonces por definición\footnote{Decimoquinta vez que en los apuntes pongo ``por definición''. Estoy seguro de que lo ha dicho muchas veces más.} del producto pinchorial o exterior esto es igual a \[ \det\left(ω_i (F_*D_j)\right)\]

Volviendo a aplicar la definición\footnote{Decimosexta.} de $F^*$ nos quedaría que
\[ \det\left(ω_i (F_*D_j)\right) = \det (F^*ω_i (D_j)) = F^* ω_1 \y \dotsb \y F^* ω_k (D_1, \dotsc, D_k) \] y ya está.
\end{proof}

Veamos un ejemplo. Supongamos que tenemos una $p$-forma diferencial $ω = \sum a_I \dif y_I$ en $V$ donde $a_I ∈ C^∞(V)$ y $I = i_1, \dotsc, i_p$, de tal forma que $\dif y_I = \dif y_{i_1} \y \dotsb \y \dif y_{i_p}$. En este caso, la función inversa nos quedaría \[ F^* ω = F^*\left(  \sum a_I \dif y_I \right) = \sum F^*(a_I) F^*(\dif y_I) \] usando las propiedades que hemos visto antes. Esto demuestra que las dos definiciones que hemos visto de qué es $F^*$ son la misma: la primera era sustituir y operar y la segunda es la aplicación a los vectores tangentes.


La propiedad cuatro también es interesante, veámosla.

\begin{proof}[Propiedad 4] Queremos ver que si $ω$ es una $p$-forma y $τ$ una $q$-forma, entonces $F^*(ω \y τ) = F^*ω \y F^* τ$. Si partimos de que \begin{gather*}
ω = \sum a_I \dif y_I \\
τ = \sum b_J \dif y_J
\end{gather*}, entonces tenemos que \[ ω \y τ = \sum a_I b_J \dif y_I \y \dif y_J \] por definición del producto exterior.

Operamos entonces: \[ F^*\left(\sum a_I b_J \dif y_I \y \dif y_J\right) = \sum F^*(a_I) F^*(b_J) F^*(\dif y_I \y \dif y_J) \]

Tenemos que $\dif y_I \y \dif y_J$ son 1-formas, luego podemos escribir $F^*(\dif y_I \y \dif y_J) = F^*(\dif y_I) \y F^*(\dif y_J)$ y entonces nos queda directamente que eso es igual a $ F^*ω \y F^* τ$.
\end{proof}

\subsection{Derivada exterior}

La derivada exterior nos lleva de una $p$-forma a una $(p + 1)$-forma, como si derivásemos una vez más. Veamos la definición formal.

\begin{defn}[Derivada\IS exterior] Dada $ω_p = \sum a_I \dif x_I$, entonces se define la derivada exterior como \[ \dif ω_p ≝ \sum \dif a_I \y \dif x_I \] teniendo en cuenta que \[ \dif f ≝ \sum \dpa{f}{x_i} \dif x_i \]\label{defDerivadaExterior}
\end{defn}

Vamos a ver sus propiedades:

\begin{enumerate}
\item Linealidad: $\dif (ω_1 + ω_2) = \dif ω_1 + \dif ω_2$.
\item \concept{Regla\IS de Leibniz}: $\dif (ω_1 \y ω_2) = \dif ω_1 \y ω_2 + (-1)^p ω_1 \y \dif ω_2$, donde $ω_1$ es una $p$-forma.
\item \concept{Lema\IS de Poincaré}: $\dif (\dif ω) = 0$.
\item $F^*(\dif ω) = \dif F^*(ω)$.
\end{enumerate}

Vamos a ver las demostraciones de las propiedades 2,3 y 4.

\begin{proof}[Propiedad 2] Partimos de que $ω_1 \y ω_2 = \sum a_I b_J \dif x_I \y \dif x_J$, entonces \[ \dif (ω_1 \y ω_2) = \dif\left(\sum a_I b_J \dif x_I \y \dif x_J\right) = \sum \dif(a_I b_J) \y \dif x_I \y \dif x_J \]

Con $\dif(a_I b_J)$ aplicamos la regla de Leibniz de funciones así que $\dif(a_I b_J) = \dif a_I · b_J + a_I · \dif b_J$. Podemos separar entonces y \[ \sum \dif(a_I b_J) \y \dif x_I \y \dif x_J = \sum b_J \dif a_I \y \dif x_I \y \dif x_J + \sum a_I \dif b_J \y \dif x_I \y \dif x_J \]

Por definición, el primer sumando es lo que queremos: $\sum b_J \dif a_I \y \dif x_I \y \dif x_J = \dif ω_1 \y ω_2$. El segundo casi lo es, salvo porque $\dif x_I$ no está donde debe.Tenemos que ``mover'' $\dif x_I$ a la izquierda, y eso son $p$ cambios. Es decir, que $\dif b_J \y \dif x_I = (-1)^p \dif x_I \y \dif b_J$ así que $\sum a_I \dif b_J \y \dif x_I \y \dif x_J = (-1)^p ω_1 \y \dif ω_2$.h
\end{proof}

\begin{proof}[Propiedad 3]
Queremos demostrar que el diferencial de la diferencial es 0. Ya lo teníamos en el caso de las 0-formas, y ahora vamos a verlo en general para $ω = a_I \dif x_I$. En este caso, tenemos que \[\dif ω = \dif a_I \y \dif x_I + a_I \dif(\dif x_I) \]

Sabemos que $\dif (\dif x_I) = 0$\footnote{Es fácil verlo así: $\dif (\dif x_I) = \dif (1 · \dif x_I) = \dif 1 \y \dif x_I = 0$, que es $0$ porque la derivada de una constante ($1$) es $0$.}, así que nos queda $\dif ω = \dif a_I \y \dif x_I$\footnote{Esto no es más que llegar de forma algo más detallada a la derivada exterior que ya hemos definido en \ref{defDerivadaExterior}.}.

Tomamos de nuevo la derivada exterior: \[ \dif(\dif ω) = \dif(\dif a_I \y \dif x_I) = \underbrace{\dif(\dif a_I)}_{0} \y \dif x_I - \dif a_I\y \underbrace{\dif(\dif x_I)}_{0} = 0\] y ya lo tenemos.
\end{proof}

\begin{proof}[Propiedad 4]
Vamos a demostrar que $F^*(\dif ω) = \dif F^*(ω)$. Empezaremos primero viendo qué ocurre cuando $ω$ es una 0-forma y por lo tanto sólo tenemos el diferencial de una función. En este caso, \[ \dif ω = \dif f = \sum_j \dpa{f}{y_j}\dif y_j \] y aplicando el pullback
	\begin{multline*} F^* (\dif ω )
		= F^*\left(\sum_j \dpa{f}{y_j}\dif y_j \right)
		= \sum_{i,j} \dpa{f}{y_j} \dpa{y_j}{x_i} \dif x_i = \\
		\eqreason{Usando la regla de la cadena.} \sum_i \dpa{(f○F)}{x_i}
		\eqreason{Por definición de $\dif$.}\dif (F○f) = \dif F^*(f)
	\end{multline*}

Con esto podemos saltar al caso general, cuando $ω = \sum a_I \dif y_I$. Necesitaremos un lemilla que nos dice que \[ \dif\left(F^* (\dif y_I)\right) = \dif(\dif F_{i1} \y \dif F_{i2}\y \dotsb \y \dif F_{ip}) = 0\] y que no vamos a demostrar.

Siguiendo con la demostración, por definición de pullback tenemos que  \[ \dif (F^* ω) = \dif \left(\sum_I F^*(a_I) F^*(\dif y_I)\right) \]. Con la propiedad distributiva, tenemos que eso es igual a \[ \sum_I \left(\dif F^*(a_I) \y F^*(\dif y_I) + F^*(a_I) \dif\left(F^* (\dif y_I)\right)\right)\]. Aquí vemos rápidamente que $\dif \left(F^*(\dif y_I)\right) = \dif \dif F^* y_I = 0$ (acabamos de ver que el pullback y la diferencial se pueden intercambiar con 0-formas). Luego, operando, \[ \dif (F^* ω) = \sum_I \dif F^*(a_I) \y F^*(\dif y_I) = F^*\left(\sum_I \dif a_I \y \dif y_I\right) \eqreason{Por la propia definición del pullback.} F^* (\dif ω) \]
\end{proof}

Dos definiciones rápidas:

\begin{defn}[p-forma\IS cerrada] Se dice que $ω$ es una p-forma cerrada si $\dif ω = 0$.\end{defn}

\begin{defn}[p-forma\IS exacta] Se dice que $ω$ es una p-forma exacta si existe una $(p-1)$-forma $α$ tal que $ω = \dif α$.\end{defn}

Con las propiedades demostradas, vamos a ir a por el primer teorema importante: el de Poincaré, que nos relaciona p-formas cerradas y exactas

\begin{theorem}[Teorema\IS de Poincaré]
Sea $ω$ una p-forma en un abierto $U ⊂ ℝ^n$, con coordenadas $x_1, \dotsc, x_n$ en $U$. Vamos a suponer $\dif ω = 0$ (o, dicho de otra forma, que $ω$ sea cerrada).

Bajo ciertas hipótesis topológicas, existe $α$ una $(p-1)$-forma en $U$ tal que \[ ω = \dif α\]. Esto se denota como que $ω$ es una p-forma\IS exacta.
\end{theorem}

Para la demostración necesitaremos una definición previa

\begin{defn}[Abierto\IS contractible] Un abierto $U ⊂ ℝ^n$ es contractible si existe una función continua $\appl{Φ}{I×U}{U}$ con $Φ(1,x) = x\; ∀ x ∈ U$ y $Φ(0,x) = x_0 ∈ U$.\label{defContractible}
\end{defn}

\begin{proof}
La demostración usa un truco con mucha utilidad en topología. En lugar de trabajar en $U$, vamos a considerar $I = [0,1)$ y trabajar en $I × U = U'$, una especie de cilindro, con coordenadas $t, x_1, \dotsc, x_n$.

Definiremos dos aplicaciones, $j_0, j_1$, de la siguiente forma:
\begin{align*}
\appl{j_1}{U&}{U'} & \appl{j_0}{U&}{U'} \\
x &\longmapsto (1,x) & x &\longmapsto (0,x)
\end{align*}

El pullback de ambas se haría cambiando $t = 1$ en $ω$ y $\dif t = 0$ con $j_1^*(ω)$, y cambiando $t = 0$ en $ω$ y $\dif t = 0$ con $j_0^*(ω)$. El truco es ahora usar las dos copias de $U$ (cada tapa del cilindro) y usar estas $j_i^*$ de una forma que vamos a ver ahora.

Sea $Ω^p(U)$ el espacio vectorial de todas las $p$-formas en $U$. Definimos la aplicación $\appl{K}{Ω^{p+1}(U')}{Ω^p(U)}$ de la siguiente forma: si $ω = a_I \dif x_I$, sin que aparezca la $t$, definimos $K(a_I \dif x_I) ≝ 0$ ya que en la derecha tengo $p$ diferenciales y no hay otra forma de quitar una.

Cuando $ω = a_I(t,x) \dif t \y dif x_I$, entonces $K(ω) ≝ \left(\int_0^1 a_I(t,x) \dif t\right) \dif x_I $. Los elementos son suma de estos dos tipos y como es lineal con definirlo en los ya se queda definido.

Una propiedad clave es que \( \label{eqPropKdif} K(\dif ω) + \dif (K(ω)) = j_1^* ω - j_0^* ω \), y vamos a demostrarlo.

Cualquier $ω$ se va a poder ver como suma de elementos de la forma $a_I(t,x)  \dif x_I$ y $a_I(t,x) \dif t \y \dif x_I$, así que vamos a escribir la fórmula según estos dos elementos. Si $ω = a_I (t,x) \dif x_I$, tenemos que $K(ω) = 0$ por la construcción de $K$. Por otra parte, como $\dif ω = \dpa{a_I}{t} \dif t \y \dif x_I + θ$ donde $θ$ son otros términos que no tienen $\dif t$, tenemos que \[ K(\dif ω ) = \left(\int_0^1 \dpa{a_I}{t} \dif t\right) \dif x_I = \left(a_I(1,x) - a_I(0,x) \right) \dif x_I \] que, por definición, es lo mismo que $j_1^*ω - j_0^*ω$ por la propia definición de $j_0$ y $j_1$.

Nos queda ahora demostrar \eqref{eqPropKdif} para $ω = a_I(t,x) \dif t \y \dif x_I$. Por la construcción de los $j_i$, el lado de la derecha de la igualdad es $0$ ya que ambas aplicaciones nos llevaban $\dif t$ $0$. Así, tenemos que demostrar que $K(\dif ω ) = \dif (K(ω))$.

Calculamos $K(\dif ω)$, que será
\begin{align*}
K(\dif ω) &= K \left[\left(\dpa{a_i}{t}\dif t + \sum_i \dpa{a_I}{x_i} \dif x_i\right) \y \dif t \y \dif x_I \right] \eqreason{$\dpa{a_i}{t}\dif t$ se va por repetir $\dif t$, y después cambiamos de signo por la permutación de una diferencial.} \\
& =
K \left( - \sum_i \dpa{a_I}{x_i} \dif t \y \dif x_i \y \dif x_I \right) = \\
&= - \sum_i\left(\int_0^1 \dpa{a_I}{x_i} \dif t\right) \dif x_i \y \dif x_I
\end{align*}

Calculando luego el $\dif K(ω)$ nos saldrá lo mismo pero con el signo cambiado:
\begin{align*}
\dif (K ω) &= \dif \left(\int_0^1 \left( a_I(t,x) \dif t \right) \dif x_I\right) = \\
&= \sum \dpa{\int_0^1 a_I(t,x) \dif t}{x_I} \dif x_i \y \dif x_I = \\
&= \sum \left(\int_0^1 \dpa{a_I}{x_I} \dif t\right) \dif x_i \y \dif x_I
\end{align*}

Cogemos ahora la contracción $Φ$ de \ref{defContractible}. Entonces, \[ K(\dif Φ^*ω) + \dif (K(Φ^*ω) = j_1^* ω - j_0^*ω = ω - 0 = ω \] luego el α que buscamos es algo.
\end{proof}

Vamos a ver un ejemplo de esto. Sea $ω = P \dif x + Q \dif y + R \dif z$ definida en todo $ℝ^3$. Entonces, \[ \dif ω = \left(\dpa{R}{y} - \dpa{Q}{x}\right) \dif y \y \dif z + \left(\dpa{P}{z} - \dpa{R}{x}\right) \dif z \y \dif x + \left(\dpa{Q}{x} - \dpa{P}{y}\right) \dif x \y \dif y\]

Si igualamos $\dif ω = 0$, eso es equivalente a un sistema de ecuaciones: \begin{align*}
\dpa{R}{y} &= \dpa{Q}{x} \\
\dpa{P}{z} &= \dpa{R}{x} \\
\dpa{Q}{x} &= \dpa{P}{y}
\end{align*}

Queremos obtener ahora el potencial $α$. Definiremos la contración $Φ(t,x,y,z) = (tx, ty,tz)$. Lo primero es calcular $Φ^*ω$, que es puramente un cambio de variable:
\begin{multline*}
Φ^* ω = P(tx,ty,tz) (t\dif x + x \dif t) + \\ Q(tx,ty,tz) (t\dif y + y \dif t) + R(tx,ty,tz) (t\dif z + z \dif t)
\end{multline*}

Sólo interesarán los términos $x \dif t, y \dif t, z \dif t$: los otros no tienen $\dif t$ y por $K$ se irán a cero de cabeza. Consideramos ahora $F(x,y,z) ≝ K(Φ^*ω(x,y,z))$, y vemos que nos queda \[ F(x,y,z) = x \int_0^1 P(tx,ty,tx) \dif t +  y \int_0^1 Q(tx,ty,tx) \dif t +  z \int_0^1 R(tx,ty,tx) \dif t \]

Ahora deberíamos comprobar que $\dif F = ω$, que como no es muy horroroso podemos hacerlo. Empezamos por la derivada parcial con respecto a $x$, que es\footnote{Omitimos argumentos por facilidad.} \[ \dpa{F}{x} = \int_0^1 P \dif t + x \int_0^1 \dpa{P}{x} t \dif t + y \int_0^1 \dpa{Q}{x}t \dif t + z \int_0^1 \dpa{R}{x}t \dif t \]

Tenemos que llegar ahora a que $\dpa{F}{x} = P$, que es el coeficiente de $\dif x$ en $ω$. Nos fijamos en las ecuaciones que teníamos antes y podemos cambiar:
\[ \dpa{F}{x} = \int_0^1 P \dif t + x \int_0^1 \dpa{P}{x} t \dif t + y \int_0^1 \dpa{P}{y}t \dif t + z \int_0^1 \dpa{P}{z}t \dif t\]

Eso se puede poner como una sola integral, que es
\begin{align*}
\dpa{F}{x} &= \int_0^1 \left(P + xt \dpa{P}{x} + yt \dpa{P}{y} + zt \dpa{P}{z} \right)\dif t = \\
&= \int_0^1 \left(P + t \deriv{}{t} P(tx,ty,tz)\right) \dif t = \\
&= \int_0^1 \deriv{P · t}{t} \dif t = \\
&= P(x,y,z)
\end{align*}

Haríamos esto con el resto de parciales y nos saldría efectivamente ω.

Si considerásemos $ω = A \dif y \y \dif z + B \dif z \y \dif x + C \dif x \y \dif y$, la derivada exterior nos saldría como la divergencia: \[ \dif ω = \left(\dpa{A}{x} + \dpa{B}{y} + \dpa{C}{z}\right) \dif x \y \dif y \y \dif z \]


La 1-forma $α$ que cumple $\dif α = ω$ sería de la forma $α = P \dif x + Q \dif y + R \dif z$. Usando la misma contracción de antes, tendríamos que \[ Φ^* ω = A(tx,ty,tz) \dif (ty) \y \dif (tz) + \dotsb  = A(tx,ty,tz) \left((y\dif t +t\dif y) \y (z \dif t + t \dif z)\right) + \dotsb \] donde de nuevo sólo nos importan los términos con $\dif t$, y donde $y \dif t \y z \dif t$ se van porque repiten diferencial. Entonces \[ Φ^* ω = A(tx,ty,tz) (yt \dif t \y \dif z - zt \dif t \y \dif y) + \dotsb \] de tal forma que \[ α = K(Φ^* ω) = \left(\int_0^1 At \dif t\right) (y \dif z - z \dif y)\]

Aquí podríamos ponernos de nuevo a derivar para ver que sale lo que sale, pero ahora sí es un engendro y no es plan.

\section{Integral de línea}

Ya mencionamos que si teníamos $ω = \sum a_i \dif x_i$ una 1-forma en $U ⊂ ℝ^n$, definíamos su integral sobre una curva $\appl{γ}{I = [a,b]}{U}$ diferenciable como \[ \int_γ ω ≝ \int_a^b γ^* ω \] que se puede escribir con más detalle como \( \int_a^b \left(\sum_i a_i(t) \deriv{x_i}{t}\right) \dif t \label{eqIntLinea} \)

Si ahora reparametrizamos la curva en un intervalo $J = [c,d]$ con una reparametrización creciente biyectiva $φ(τ)$ definida como $\appl{φ}{J}{I}$, tomamos $γ'= γ ○ φ$, querríamos ver que la integral no cambia.

Lo que hacemos es seguir reescribiendo \eqref{eqIntLinea} como \[ \int_γ ω = \int_a^b \left(\sum_i a_i(t) \deriv{x_i}{τ}\deriv{τ}{t}\right) \dif t = \int_c^d \left(\sum_i a_i(τ) \deriv{x_i}{τ}\right) \dif τ = \int_{γ'} ω \]

Esto también nos dice que la integral depende sólo del valor en los extremos de la forma diferencial, esto es, si $ω = \dif f$, entonces \[ \int_γ ω = f(γ(b)) - f(γ(a)) \]

Hay una proposición muy interesante que nos dice lo siguiente:

\begin{prop} Son equivalentes
\begin{enumerate}
\item ω es exacta en un abierto $V ⊂ U$.
\item $\int_γ ω$ depende sólo del valor de $γ$ en los extremos.
\item $\int_γ ω = 0$ si y sólo $γ(a) = γ(b)$.
\end{enumerate}
\end{prop}

\begin{proof}
Ya hemos visto que $1 \implies 2$ y $2 \implies 3$. La implicación $3 \implies 2$ es inmediata por lo que sólo nos queda probar que $2 \implies 1$.

Vamos a ello.

Tomemos un punto $p \in V$. Para cada $x\in V$ llamemos $c$ a la curva diferenciale que une $p$ con $x$ y definimos $\appl{f}{V}{\real}$ como
\[f(x) = \int_c ω\]
que sabemos está bien definida por el punto $2$ de la proposición.

Podemos ver que si demostramos que $\dif f = ω$ habremos concluido la demostración (hemos encontrado la $f$ que implica la definición de forma exacta).

Dado que $\dif f = \sum_i \frac{\partial f}{\partial x_i}\partial x_i$ debemos comprobar que $a_i =  \frac{\partial f}{\partial x_i}$.

Vamos a considerar el caso concreto en que la curva $c$ que une los puntos $p$ y $x$ es de la forma
\[\appl{c_i}{t}{x+te_i} \ \ \ t \in (-ε, ε)\]

En este caso tenemos que
\begin{align*}
\frac{\partial f}{\partial x_i} &= \lim_{t \to 0} \frac{1}{t}\{f(x+te_i)-f(x)\}\\
&= \lim_{t \to 0} \frac{1}{t}\{\int_{c+c_i}w-\int_c w  \}= \lim_{t \to 0} \frac{1}{t}\int_{c_i}w\\
&=  \lim_{t \to 0} \frac{1}{t} \int_0^t a_i(s)ds = a_i(0)=a_i(x)
\end{align*}
Puesto que estas cuentas valen para cualquier $e_i$ queda probado que
\[\frac{\partial f}{\partial x_i} = a_i \ \ \forall i\]
\end{proof}

\begin{example}
Sea \[ ω = \frac{-y}{x^2 + y^2} \dif x + \frac{x}{x^2 + y^2} \dif y\] una 1-forma definida en $ℝ^2 \setminus \set{0}$. Haremos un cambio de variable $f$ a polares $(r, θ)$ desde el dominio $U = (0, ∞) × (0, 2π)$ (esto es, $r > 0$ y $0 < θ < 2π$).Entonces, tendremos que
\begin{align*}
\dif x &= \cos θ\, \dif r - r \sin θ\, \dif θ \\
\dif y &= \sin θ\, \dif r + r \cos θ\, \dif \theta
\end{align*}

Sustituyendo en la definición de ω nos queda:
\[ω = \frac{-r \sin \theta}{r^2}\left( \cos \theta \dif r-r\sin \theta \dif \theta\right)+\frac{r \cos \theta}{r^2} \left(\sin θ\, \dif r + r \cos θ\, \dif \theta \right)=\]
\[= \frac{-r\sin θ \cos θ \dif r+r^2\sin^2θ \dif θ +r \cos θ \sin θ \dif r + r^2 \cos^2 θ \dif θ}{r^2} = \dif θ\]

Es interesante porque nos sale que $f^* ω = \dif θ$. Entonces, si llamamos a una circunferencia $γ$, tendremos que \[ \int_γ ω = \int_0^{2π} f^*ω = \int_0^{2π} \dif θ = 2 π\]

¿Qué ocurre aquí? Podemos ver que $\dif ω = 0$ pero sin embargo la integral no es cero. Esto ocurre porque el dominio $ℝ^2 \setminus \set{0}$ no es contractible\footnote{Creo que esto tiene algo que ver con que el grupo fundamental del dominio sea el trivial, es decir, que se pueda contraer a un punto.}, así que no se puede aplicar el teorema de Poincaré.

Esto es un ejemplo de que las formas ``ven'' la topología que hay por debajo del espacio en el que se encuentran. En el fondo, la geometría se basa en montar estructuras diferenciables sobre espacios topológicos (la principal estructura son las p-formas) y esas estructuras dependen de la topología subyacente.
\end{example}

\section{Cohomología de DeRhan}

Tenemos $X$ una variedad diferenciable. Definimos $Ω^p_X$ cono el espacio vectorial de p-formas en $X$. Ya veremos cómo se define esto ya que hasta ahora sólo hemos trabajado en abiertos de $ℝ^n$. La idea básica será tener p-formas definidas en abiertos que funcionan bien en las intersecciones.

Dentro de $Ω^p_X$ definimos $Z_X^p$ como las p-formas cerradas en $X$. Dado que $Ω_X^p$ es espacio vectorial y $Z_X^p$ su núcleo, $Z_X^p$ es subespacio vectorial. Además, tenemos que $\dif Ω^{p-1}_X$, el conjunto de las p-formas exactas, es subconjunto de $Z_X^p$.

La observación de DeRhan es tomar el cociente de grupos:

\begin{defn}[Espacio\IS de cohomología de De Rhan] \label{defCohomologiaDeRhan} Se define el espacio de cohomología de DeRhan como el cociente de espacios vectoriales dado por \[ H^p(x) ≝ \quot{Z_X^p}{\dif Ω_X^{p-1}} \]
\end{defn}

Lo curioso de ese espacio es que si $X$ es compacto, su dimensión es finita. De hecho, su dimensión tiene nombre: es el \concept[Número\IS de Beti]{número p-ésimo de Beti de X}. Con esto se pueden hacer construcciones algebraicas de la topología sin pasar por el cálculo diferenciable, y más cosas que ha dicho pero de las que no me he enterado.

Por ejemplo, si uno se pregunta si, dadas una variedad $X$ y una 1-forma $ω$ en $X$, es cierto que $ω = \dif f$ para una cierta función $\appl{f}{X}{ℝ}$. Con esta notación y definiciones, lo único que se está preguntando es en el fondo si $H^1(X) = 0$. Si esto es cierto, lo otro también lo es para cualquier 1-forma ω. Muchas veces no estamos interesados en lo que pasa para todo ω, sólo en una concreta, pero en cualquier caso lo de la dimensión es muy fuerte.

En otras palabras, para saber si el problema de Poincaré tiene solución sólo hay que estudiar un grupo. Hay que estudiar la clase de ω en el grupo cociente. Si su clase módulo $\dif Ω^{p-1}x$ es cero, por definición\footnote{Pues eso.} existe esa $f$.

En esta notación, el teorema de Poincaré lo que dice es que si $U⊂ℝ^n$ es contractible entonces $H^p(U) = 0$ para todo $p ≥ 1$. Es decir, que los abiertos contractibles son topológicamente triviales (o cohomológicamente triviales).

Siguiendo con la relación con topología, suponer que $H^1(X) = 0$ es más débil que suponer que el primer grupo fundamental de homotopía $\tgs^1(X)$ es $0$ (i.e., $X$ es simplemente conexo).

Pura magia matemática.

\seprule

Un ejemplo de esto que fue históricamente interesante e importante es el de la termodinámica en física. Un sistema termodinámico está definido por una serie de variables en $ℝ^n$, como puedan ser presión, volumen o temperatura (las más importantes). Ocurre que hay relaciones entre ellas: por ejemplo, se sabe que en un gas perfecto $PV = k T$. El sistema se desarrolla dentro de un dominio $X ⊂ ℝ^n$, y los procesos pueden ser como curvas $γ ⊂ X$ que definen cómo van variando presión, volumen y temperatura a lo largo del proceso.

Entonces, se define asociada la variedad una 1-forma $ω$ (que en los libros se denota como $ω = \dif Q$, donde la $\dif$ no es la misma que antes), de tal forma que $\int_γ ω$ es el calor transferido. Dado que la 1-forma no es exacta, no hay un calor definido para cada punto del sistema (un gas no tiene un ``calor'') sino que mide la diferencia de calor transferido a lo largo de un proceso. Análogamente, hay otra 1-forma no exacta que define el trabajo para cambiar un sistema, pero que no asigna un trabajo a cada estado.
