% -*- root: ../GeometriaDiferencial.tex -*-
\chapter{Introducción a la Geometría Riemanniana}

\hfill \textit{Prometo que yo he hecho lo mejor que he podido con esto.}

Tomaremos $X$ una variedad compacta y una métrica Riemanniana en $X$ dada por el producto escalar $\pesc{}_p$ en $\tgs_p X$. La condición es que dados $D, D'$ campos en $X$, la aplicación \begin{align*}
p & \longmapsto \pesc{D_p, D_p'}_p \\
x & \longmapsto ℝ
\end{align*} es infinitamente diferenciable en $X$.

% Autonota: Creo que lo que este hombre está tratando de explicarnos es cómo la métrica riemanniana permite replicar las nociones que ya teníamos de ℝ^n.

En una variedad riemanniana\footnote{Variedad compacta con métrica riemanniana.} algo.

En geometría de primero hacíamos cosas como medir la longitud de una curva $\appl{γ}{I}{X}$ dada por \[ \mop{long} γ ≝ \int_I \md{γ'(t)} \dif t \]

Dado que esta longitud no dependía de la parametrización, es una propiedad intrínseca de la imagen. Por alguna razón, se tiene que la distancia entre dos puntos es \[ \dst(x, x') ≝ \inf_{γ\, curva} \mop{long} γ \], que es una distancia que nos da la misma topología que teníamos antes\footnote{La topología de la variedad es la topología de subespacio inducida por el espacio ambiente.}. Esto será lo que nos dé una distancia riemanniana.

Para demostrar que es una distancia lo más difícil es ver que si la distancia es cero entonces los dos puntos son el mismo.

La métrica riemanniana también nos da una noción de ángulo. No sé cómo lo hace pero lo hace, parece ser.

Otra cosa que querríamos hacer es definir una isometría.

\begin{defn}[Isometría] Dadas dos variedades $X, X'$ y un difeomorfismo $\appl{Φ}{X}{X'}$, se dice que Φ es isometría si y sólo si para todo $p∈X$ y para todo par $D_p, D_p'$ de vectores tangentes se tiene que \[ \pesc{D_p, D_p'} = \pesc{Φ_{*,p}(D), Φ_{*,p}(D')} \]
\end{defn}

Dos variedades diferenciales isomorfas son la misma, aunque no tienen por qué tener la misma métrica riemanniana. En cambio, dos variedades isométricas sí que tienen la misma geometría riemanniana.

Las isometrías de una variedad forman un grupo y, en fin.

% TODO: Esto es una proposición.

Un último comentario: si hay particiones de la unidad para una variedad diferenciable compacta $X$ entonces hay una métrica. Tenemos la partición $(U_i, ρ_i)$. En cada abierto $U_i$ la métrica se puede definir como \[ \pesc{\dpa{}{x_j}, \dpa{}{x_k}}_i = δ_ij \], tomando esas parciales como los elementos de la base del tangente. Así, podemos combinarlo todo y decir que, dados dos vectores $D,D'$, su producto escalar es\footnote{No estoy muy seguro de esto porque lo ha puesto sin $D,D'$, pero a mí me gustan las ecuaciones sin vacíos.} \[ \pesc{D, D'} = \sum_i ρ_i \pesc{D, D'}_i \]

La cuestión es que puede haber muchas.

\section{Ecuaciones de estructura}

Nosequé de un libro del hijo de Cartan que se llama formas diferenciales y que está bien explicado.

La idea es construir ciertas formas diferenciales que son más o menos canónicaas y, usando esas formas diferenciales y las operaciones estándar construir nuevas formas y se ve que así aparecen muchas formas y llega un momento en el que no pueden ser indepentientes, hay tantas que tiene que haber relaciones entre ellas. Las relaciones entre las formas vendrán dadas por coeficientes que darán invariantes de la variedad, como curvatura o algo.

La idea va a ser coger formas, y aplicarles operaciones hasta que aparezcan estas relaciones porque no queda otra.

Lo primero que hacemos es definir una transformación afín $T$, que es un vector $M$ (una traslación) y una parte lineal $L$. Por notación, llamaremos \[ e_i ≝ L((0,\dotsc, 0, 1, 0, \dotsc, 0))\] donde el $1$ está en la posición $i$-ésima, y además \[ M ≝ T((0,\dotsc, 0))\], con $\det(e_1, \dotsc, e_n)$. Con esto diremos que tenemos una referencia afín dada por $(M, e_1, \dotsc, e_n)$. El conjunto de todas las referencias afines será $\mathcal{R} ⊆ ℝ^n \underbrace{× \dotsb ×}_{n+1} ℝ^n ⊂ ℝ^{(n+1)n}$. Es un abierto por alguna razón. % Creo que porque es todo el espacio.

Un nosequé móvil es un $S ⊂ \mathcal{R}$ es una referencia parametrizada de los puntos de $S$.En particular, uno puede parametrizar las referencias por el mismo punto que estamos... No sé ni cómo escribirlo.

De aquí viene el nombre: el método de la referencia móvil consiste en usar referencias móviles\footnote{Capitán obvio al rescate.} que varían de punto a punto.

Podemos considerar funciones $\mathcal{R}\longmapsto ℝ^n$, donde las funciones serán $M, e_1, \dotsc, e_n$, que a cada elemento de $\mathcal{R}$ le manda un elemento de la referencia. Por ejemplo, \begin{align*}
\appl{M}{\mathcal{R}&}{ℝ^n} \\
(M, e_1, \dotsc, e_n) &\longmapsto M
\end{align*}

Aquí por alguna razón se puede derivar $M$ con la diferencial exterior. Para funciones a valores vectoriales derivar dos veces da cero porque se hace componente a componenente \[ \dif(\dif M) = 0\]

Entonces ahora empieza a aparece ya estas uno-formas y dos-formas que son las que van a dar lugar a las ecuaciones de estructura. Yo considero $\dif M$, que se obtiene derivando las componentes de $M$. Y esto, va a ocurrir digamos en una referencia $r$ y se va a aplicar a un vector tangente $ξ ∈ ℝ^{n+1}$, luego \[ (\dif M)_r\, ξ = \sum_i ω_{i,r} (ξ) (e_i)_r\] porque estamos escribiendo un vector en la base y un vector son coordenadas por coeficientes numéricos, los $ω_{i,r}(ξ)$. Al final no se van a escribir casi nunca, no hace falta, y esto al final se va a escribir como  \[ \dif M = \sum_i ω_i e_i \] y sobreentendemos los puntos y los vectores.

Los coeficientes actúan sobre vectores tangentes y dan números, luego los $ω_i$ son 1-formas en $\mathcal{R}$. Los mismos cálculos se hacen para las otras funciones $e_j$, y tenemos que \( \dif e_j = \sum_k υ_{j,k} e_k \label{eqDerivadaE} \), donde los $ω_{j,k}$ son igualmente $1$-formas.

Así, tenemos $n^2 + n$ 1-formas que nos vienen de gratis. Son 1-formas normales. Lo que hacemos es derivarlas\footnote{Espíritu Guijarro: no sé qué hacer, luego derivo.} y ver qué pasa.

Lo que sabemos es que $\dif(\dif M) = 0$ y que $\dif(\dif e_i) = 0$. Por otro lado, tenemos que $\dif M = \sum ω_i e_i$. Ahora tenemos que volver a derivar, así que calculamos: \[ \dif(ω_i e_i) = \dif ω_i e_i - ω_i \y \dif e_i \]

Ahora, la derivada de $\dif e_i$ la tenemos en \eqref{eqDerivadaE}, luego sustituimos y nos queda que eso es igual a \[ \dif ω_i e_i - ω_i \y \left(\sum_k ω_{i,k} e_k\right) \]. Sumando todas estas cosas nos queda que \( 0 = \dif(\dif M) = \sum_i \dif ω_i e_i - \sum_i ω_i \y \sum_k ω_{i,k} e_k \label{eqRiemann2} \)

¿Qué quiero hacer con esto? Ya hemos escrito como algo que tiene formas diferenciales por los elementos de la base, luego vamos a usar que los $e_k$ son independientes, por lo que nos quedaría lo siguiente \[ 0 = \sum_k \left(\dif ω_k -\sum_i ω_i \y ω_{i,k} \right) e_k \], de donde se concluye ya que los $e_k$ son base que \[ \dif ω_k = \sum_i ω_i \y ω_k \]

Lo que se ve es que cuando se deriva, los coeficientes de $M$ que es lalalalalala traslación en la referencia en la que estamos, se obtienen unas $ω_i$ que al volverlas a derivar salen cosas que siguen siendo dependientes de la $ω_i$. Ahora, el mismo cálculo se puede hacer para las otras $e_i$ y lo que queda es que \[ \dif ω_{i,j} = \sum_k ω_{i,k} \y ω_{k,j} \] haciendo el mismo cálculo.

A estas ecuaciones se le llama las ecuaciones de estructura. A las $ω_{i,j}$ se les llama la forma de conexión.

Queda una cosa más, que es que usando las referencias afines se puede hacer geometría afín, pero normalmente querremos hacer geometría euclídea (Riemanniana). Luego lo que habrá que suponer es que la referencia sea ortonormal, luego querremos obligar a que $\pesc{e_i, e_k} = δ_{ij}$.

Las referencias ortonormales $\mathcal{O} ⊂^i \mathcal{R}$ no es un abierto pero es una subvariedad de $\mathcal{R}$. En este conjunto se pueden definir $\gor{ω}_i = i^* ω_i$ y $\gor{ω}_ij = i^* ω_{ij}$ usando el cambio de variable $i$ de la inmersión (creo), y van a definir las mismas ecuaciones de estructura. Y entonces es lo mismo hacer la derivada y luego la imagen y nosequé cosas. Cuando estamos en geometría riemanniana les quitamos la barra porque es un abuso razonable.

La cuestión es que además de estas ecuaciones de estructura hay otra ecuación más que viene de aquí (no sé qué es aquí) ya que $\dif (e_i · e_j) = 0$ por ser base ortonormal, pero por Leibiniz \[ \dif (e_i · e_j) = \dif e_i e_j + e_i \dif e_j \] y sustituyendo nos va a quedar que $ω_{ij} = -ω_{ji}$. Entonces en el caso de referencias ortonormales se añade otra ecuación a las ecuaciones de estructura. Esto sguiere que con estas ω se va a poder hacer una matriz antisimétrica de 1-formas que se le llama Ω, matriz de conexión.

Con esto lo único que se ha hecho es la base digamos en $ℝ^n$ de la geometría que se va a hacer. La base viene luego de considerar variedades de $ℝ^n$. Lo que veremos el próximo día es qué ocurre cuando en $ℝ^n$ hay una subvariedad. De hecho, consideraremos el caso de $n=3$ con una subvariedad como superficie.

\begin{theorem}[Lema\IS de Cartan I] (doCarmo, p. 80) Sea $E$ un espacio vectorial de dimensión $n$. Sean $ω_1, \dotsc, ω_r$ aplicaciones lineales de $E$ en $ℝ$ linealmente independientes\footnote{Es decir, linealmente independientes como covectores del espacio dual $E^*$.}. Supongamos que existen otras formas lineales de $E$ en $ℝ$ $θ_1, \dotsc, θ_r$ tal que \[ Λ^2 E^* \ni \sum ω_i \y θ_i = 0\]

Entonces, existen $a_{ij}$ tales que \[ θ_i = \sum_j a_{ij} ω_j \] y con $a_{ij} = a_{ji}$.\label{thmCartanI}
\end{theorem}

\begin{proof} Dado que las $ω_i$ que tenemos son independientes, podemos completarlas a una base $ω_1, \dotsc, ω_r, \dotsc, ω_n$ del espacio vectorial. Por lo tanto, podremos reescribir las $θ_i$ como \[ θ_i = \sum_{j≤r} a_{ij} ω_j + \sum_{l>r} b_{il} ω_l \]

Queremos buscar que los $b_{il}$ son todos cero para que se cumpla el teorema. Usamos que $\sum ω_i \y θ_i = 0$, y operamos: \[
0 = \sum_{i ≤ r} ω_i \y θ_i = \sum_{\substack{i ≤ r \\ j ≤ r}} a_{ij} · ω_i \y ω_j + \sum_{\substack{i ≤ r \\ l > r}} b_{il} · ω_i \y ω_l \eqreason{Agrupamos las formas usando la antisimetría del producto exterior.} \sum_{i < j} (a_{ij} - a_{ji}) ω_i \y ω_j + \sum b_{il} ω_i \y ω_l
\]

Ahora usamos que el conjunto de los $ω_i \y ω_j, ω_i \y ω_l$ es linealmente independiente y concluimos: como todo eso tiene que ser cero, por un lado $a_{ij} = a_{ji}$ y $b_{il} = 0$.
\end{proof}

\begin{theorem}[Lema\IS de Cartan II] Sea un abierto $U ⊂ ℝ^n$ y $ω_1, \dotsc, ω_n$ 1-formas linealmente independientes. Supongamos que existen 1-formas $ω_{ij}$ con $1≤i,j≤n$ tales que \[ ω_{ij} = - ω_{ji},\qquad \dif ω_j = \sum ω_k \y ω_{kj} \]

Entonces, las $ω_{ij}$ son únicas.
\end{theorem}

\begin{proof} Vamos a demostrarlo por reducción al absurdo, suponiendo que existen otras $\gor{ω}_{ij}$ que cumplen las condiciones del teorema. En concreto, dado que la diferencial de $\dif ω_j$ tiene que ser la misma, si restamos vamos a tener que  \[ \dif ω_j - \difω_j = \sum_k ω_k \y (\gor{ω}_{kj} - ω_{kj}) = 0\]

Usando el lema anterior \eqref{thmCartanI}, tenemos una suma de productos diferenciales que es nula, luego sabemos que existen unos coeficientes simétricos, que llamaremos $B_{ki}^j$, tales que \[ \gor{ω}_{kj} - ω_{kj} = \sum_i B_{ki}^j ω_i \] y \( B_{ki}^j = B_{ik}^j \label{eqCartanII_1}\). Querremos demostrar que esos coeficientes son 0.

El truco es usar la antisimetría del producto de formas, y entonces \[ \gor{ω}_{kj} - ω_{kj} = -(\gor{ω}_{jk} - ω_{jk}) \]

Por otra parte, cambiando el signo al sumatorio tendríamos que \[-(\gor{ω}_{jk} - ω_{jk}) = - \sum B_{ji}^k ω_i \]

De ahí sacamos que \( B_{ji}^k = - B_{ki}^j \label{eqCartanII_2} \). Ahora operamos rotando los índices para conseguir demostrar que un coeficiente es igual a menos él mismo, y que por lo tanto sólo puede ser cero:
\[ B_{ji}^k \eqexpl{\eqref{eqCartanII_2}} -B_{ki}^j \eqexpl{\eqref{eqCartanII_1}} -B_{ik}^j \eqexpl{\eqref{eqCartanII_2}} B_{jk}^i \eqexpl{\eqref{eqCartanII_1}} B_{kj}^i \eqexpl{\eqref{eqCartanII_2}} -B_{ij}^k \eqexpl{\eqref{eqCartanII_1}} -B_{ji}^k  \]

Por lo tanto, nos queda que $\gor{ω}_{kj} = ω_{kj}$ y entonces las formas son únicas.
\end{proof}

Ahora vamos a aplicar esto a nosequé. Superficies, creo. Recordamos el conjunto de las referencias ortonormales \[ \mathcal{O} = \set{ r = (M, e_1, \dotsc, e_n) \tq e_i · e_j = δ_{ij} } \]

%% Dibujo del marco de referencia aquí

Entonces, cuando en $ℝ^3$ tenemos una superficie $S$, en cada punto vamos a tener el vector $M$, que nos dice qué punto es, y luego tendremos un montón de referencias. Nosotros estaremos interesados en las referencias adaptadas.

Suponemos que tenemos la superficie $S \overset{i}{⊂} ℝ^3$, donde $i$ es una inmersión compatible con la topología. Entonces, para cualquier $p∈S$, tendremos $U,V$ entornos de $p$ en $S$ y $ℝ^3$ respectivamente tales que $V∩S = U$.

En cada punto de la superficie vamos a tener un punto y un espacio tangente $\tgs_p S$. En cada uno de esos puntos elegiremos una referencia ortonormal que esté adaptada. ¿Qué quiere decir eso?

Para elegir una referencia adaptada, elegiremos un campo normal\footnote{Módulo 1, con el producto escalar inducido por el de $ℝ^3$.} $e_1 ∈ \tgs_p S$ que no se anule, que será el primer vector de la referencia. Lo que tiene sentido ahora es tomar el ortogonal a $e_1$. La cuestión es que hay dos ortogonales. Para elegir entre ellos, lo que haremos será usar la orientación inducida del plano. Así, $e_2$ será unitario, perpendicular a $e_1$ y además, para la orientación inducida $Ψ_{\tgs_p S}(e_1, e_2) > 0$ donde $Ψ_{\tgs_p S}$ es la aplicación bilineal inducida de $ℝ^3$\footnote{No sé qué es esto.}. Por último, elegiremos un último vector ortogonal a esos tres tal que \[ Ψ_{ℝ^3} (e_1, e_2, e_3) > 0\]

A esto le llamaremos una referencia adaptada. Esta referencia definirá una función $\appl{σ}{U⊂S}{\mathcal{O}(U)}$ donde $\mathcal{O}(U) = \set{r∈\mathcal{O} \tq M ∈ U}$. La aplicación inversa será la proyección $π$, tal que $π○σ = I_U$.

La σ asocia una referencia a cada punto de $U$. Se puede ver como una referencia móvil porque cambia según el punto.

Ahora, en $\mathcal{O}(U)$ teníamos 1-formas $ω_1, ω_2, ω_3$ y 1-formas $ω_{12}, ω_{13}, ω_{23}$. Uno puede definir entonces sus antiimágenes por $σ$, y veremos que esas formas $σ^*(ω_i), σ^*(ω_{ij})$ cumplen las mismas ecuaciones de estructura que habíamos visto antes, dado que el pullback es compatible con el producto exterior y la diferencial. Por abuso de notación, muchas veces se omitirá el $σ^*$ y se hablará simplemente de las $ω_i, ω_{ij}$: el contexto nos dirá de cuáles estamos hablando.

Nos damos cuenta de que en una superficie (dimensión 2), las tres 1-formas que tenemos no son independientes. Tiene que haber una relación de dependencia. En la referencia adaptada, esta relación es bastante tonta: $ω_3 = 0$. Lo interesante es lo que se deduce de esto: si $ω_3$ es $0$, entonces cuando escribimos las ecuaciones de estructura tendremos que, como $\dif ω_3 = 0$, ocurrirá que \[ \dif ω_3 = ω_1 \y ω_{13} + ω_2 \y ω_{23} = \]. Usando el lema de Cartan \eqref{thmCartanI}, podremos escribir \begin{align*}
ω_{13} &= h_{11} ω_1 + h_{12} ω_2 \\
ω_{23} &= h_{12} ω_1 + h_{22} ω_2
\end{align*}

Podemos escribir esos $h_{ij}$ como una matriz simétrica \[ \begin{pmatrix} h_{11} & h_{12} \\ h_{12} & h_{22}\end{pmatrix} \], que tendrá como invariantes el determinante y la traza. Entonces, definiremos la \concept[Curvatura\IS Gaussiana]{curvatura gaussiana} \[ K = h_{11} h_{22} - h_{12}^2 \] y la \concept[Curvatura\IS Media]{curvatura media} \[ H = \frac{h_{11} + h_{22}}{2} \]

Lo interesante será ver que no dependen de la referencia que hemos usado, ya que el cambio de referencia es simplemente un cambio de base que mantiene los invariantes de la matriz de antes.
