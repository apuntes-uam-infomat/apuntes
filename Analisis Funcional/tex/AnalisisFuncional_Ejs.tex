% -*- root: ../AnalisisFuncional.tex -*-
\newcommand{\hard}{\hspace{-3pt}(\dag)\hspace{5pt}}

Los ejercicios marcados con (\dag) están marcados como de ``dificultad especial'' en las hojas.

\section{Hoja 1}


\begin{problem}
\ppart Probar, usando el \nref{thm:CategoriaBaire}, que $I = \set{x ∈ ℝ \tq x ∉ ℚ} ≠ ∅$ y que, de hecho, $I$ es un \nlref{def:ConjuntoGDelta}.

\ppart Probar que $ℝ$ no es numerable.

\ppart Sea $X = ℤ$ con $\dst(x,y) = \abs{x-y}$. Probar que \sdst es un espacio métrico completo y que, sin embargo, es numerable. ¿Por qué no contradice esto al Teorema de Baire?

\solution

\spart

Sabemos que $ℚ$ es numerable, así que podemos enumerar todos los racionales en una serie $q_1, q_2, \dotsc, q_n, \dotsc ∈ ℚ$. Definimos entonces $X_n = ℝ \setminus \set{q_n}$ como una serie de conjuntos abiertos y densos. La intersección de todos ellos son todos los $x ∈ ℝ$ no racionales, que es el conjunto $I$ que buscábamos. Además, por el \nref{thm:CategoriaBaire}, esa intersección es un $G_δ$ denso en $ℝ$, y por eso mismo es no vacío.

\spart

Si $ℝ$ fuese numerable, entonces podríamos enumerarlo: $ℝ \equiv\set{ x_1, x_2, \dotsc, x_n, \dotsc}$. Por otra parte, los conjuntos formados por un único punto son diseminados, por lo que podríamos definir que $ℝ = \bigcup_{n≥1} \set{x_n}$. Sin embargo, esto entraría en contradicción con el Teorema de Baire, que nos dice que no podemos escribir $X$ como una unión numerable de conjuntos diseminados.

\spart

Para que \sdst sea un espacio métrico completo, toda \nlref{def:SucesionCauchy} ha de converger en el espacio. Que una sucesión sea de Cauchy implica que $∀ε> 0$ existe un $N ∈ ℕ$ tal que si $m,n ≥ N$, entonces $\dst(x_m, x_n)$. La cuestión es que, como estamos en $ℤ$, si tomamos\footnote{Cosa que no sé si podemos hacer.} un $ε < 1$, entonces $\dst(x_m, x_n) = 0$ (no podemos tener distancias fraccionarias entre elementos de $ℤ$). Por lo tanto, por ser de Cauchy llega un momento en el que la sucesión se repite constantemente. El límite será entonces es elemento que se repite, que por ser parte de la misma sucesión está en $ℤ$.

% Triplazo.
Esto no contradice el Teorema de Baire porque en $ℤ$ no hay conjuntos densos, y lo vamos a demostrar. Sea $Y \subsetneq ℤ$ un conjunto cualquiera de $ℤ$, y sea $z ∈ ℤ \setminus Y$. La bola de radio $\sfrac{1}{2}$ centrada en $z$ tiene intersección vacía con $Y$ (no hay ningún entero a distancia $\sfrac{1}{2}$ de $z$), por lo que $Y$ no puede ser denso.

Como no hay conjuntos densos, no puede haber tampoco conjuntos diseminados y por lo tanto sigue cumpliéndose el Teorema de Baire: no podemos escribir $ℤ$ como unión numerable de conjuntos diseminados.
\end{problem}

\begin{problem} \hard Sean $\set{a_n}_n≥1$, $\set{b_n}_{n≥1}$ dos sucesiones de números reales y $a_n$ absolutamente convergente. Probar que:

\ppart Sea $f$ dada por \[ f(x) = \sum_{n≥1} a_n φ(b_n) \] con \[ φ(x) = \begin{cases} [x] & 0 ≤ [x] ≤ \frac{1}{2} \\ 1 - [x] & \frac{1}{2} ≤ [x] ≤ 1 \end{cases} \] siendo $[x]$ la parte decimal de $x$.

Demostrar que $f$ es continua y $f ∈ C_{[0,1]}$. Además, la serie que define a $f(x)$ es absolutamente convergente.

\ppart Sea $a_n = 2^{-n}$, $b_n = 2^n$, y $h_m = ε_n 2^{-m}$ con $ε_m = \pm 1$ para todo $m$. Probar que \[ \frac{f(x + h_m) - f(x)}{h_m} = ε_m \sum_{n=1}^{m-1} 2^{m-n} \left(φ(2^n(x+h_m)) - φ(2^nx)\right)\]

\ppart Si escribimos $x = [x] + \sum_{k>0} α_k 2^{-k}$ con $α_k ∈ \set{0,1}$, entonces \[ φ(2^nx) = φ\left(\sum_{l≥1} α_{n +l} · 2^{-l}\right)\] y además $\sum_{l≥1}α_{n +l} · 2^{-l} ∈ [0,1]$. Del mismo modo, \[ φ(2^n(x+h_m)) = φ(ε_m2^{2-m} + \sum_{l≥1} α_{n +l} · 2^{-l})\] y \[ ε_m2^{n-m} + \sum_{k≥1} α_{n +l} · 2^{-l} = \sum_{l≥1} α_{n +l}' · 2^{-l}\], siendo $α_{n+l}' = α_{n+l} + δ_{m-n, k} ε_m$, con $δ_{i,j}$ la delta de Kronecker\footnote{Esto es, $δ_{i,j} = 1$ si $i = j$, y $0$ si $i ≠ j$.}.

\ppart Tomamos $ε_m = (-1)^{α_m}$. Entonces $α_{n+l}' ∈ {0,1}\;∀l≥1$. Fijemos $m > 1$. Entonces $\sum_{l≥1} α_{n+l} 2^{-l}$ y $\sum_{l≥1}α_{n+l} 2^{-l}$ están ambos en la misma mitad del intervalo $[0,1]$. Usar esto para probar que \[ \frac{f(x+h_m) - f(x)}{h_m} = m-1\]

\ppart Del apartado anterior se sigue que $f(x)$ no es diferenciable en ningún $x ∈ ℝ$. Sin embargo, $f$ no está muy lejos de serlo en el sentido siguiente: si $\abs{h} ≤ 1$, entonces $∃ C ∈ (0,∞)$ independiente de $x$ y $h$ tal que \[ \abs{f(x+h) - f(x)} ≤ C\abs{h} \left(1 + \log \frac{1}{\abs{h}}\right) \]

\textbf{Indicación}: Dado $0 < \abs{h} ≤ 1$, existe un único $k ∈ ℕ$ con $2^{-k-1} < \abs{h} ≤ 2^{-k}$. Entonces estimar $f(x+h) - f(x)$ dividiendo la suma en los términos $n < k$ y $n ≥ k$ y estimando cada suma por separado.

\solution

\spart

\spart

\spart

\spart

\spart

Lo que está diciendo es que el módulo de continuidad de esta función es sólo un poquito peor que Lipschitz.

Para demostrarlo, vemos que existe un único $k ∈ ℕ$ tal que $2^{-k-1} < \abs{h} ≤ 2^{-k}$. En otros términos, $k$ es la parte entera de $\log_2 \frac{1}{\abs{h}}$. Entonces podemos escribir \begin{align*}
\abs{f(x+h) - f(x)} &=
	\abs{\sum_{n≥1} 2^{-n} \left([2^nx + 2^n h] - [2^nx]\right)}\\
&≤ \sum_{n≥1} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]} \\
&= \underbrace{\sum_{1≤n≤k} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]}}_A + \underbrace{\sum_{n≥k} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]}}_B
\end{align*}

Vamos a estimar ambos sumatorios por separado. El más simple es el $B$, que haciendo cuentas sale que $B ≤ 2 \abs{h}$. Para la estimación de $A$, usamos que $\abs{[x] - [y]} ≤ \abs{x-y}$ y haciendo todavía más cuentas nos queda que $A ≤ k\abs{h}$.

\end{problem}

\begin{problem} Sea \sdst un espacio métrico compacto. Probar que es completo. ¿Es cierto el recíproco?
\solution

Sea $\set{x_n}$ una sucesión de Cauchy en $X$, y sea $\set{ε_n}$ otra sucesión que tiende a $0$. Por ser $\set{x_n}$ de Cauchy, para cada $ε_n$ existe un $M_n ∈ ℕ$ tal que si $m,n ≥ M_n$ entonces $\dst(x_m, x_n) < ε_n$. Equivalentemente, para todo $n ∈ N$ tendremos que $x_n ∈ \bola_{ε_n} (x_{M_n}) = B_n$. Entonces, por ser $X$ compacto existe un subrecubrimiento finito $B_{n_i}$ de $\bigcup B_n$.

Por ser un subrecubrimiento, tendremos que a partir de un cierto $n$ suficientemente grande, las bolas $\bola_{ε_n} (x_{M_n})$ que definíamos antes están estrictamente contenidas en él, por lo que el límite debe de estar ahí también. % Me convence esto más bien poco.

\end{problem}

\begin{problem} \hard Sea $\set{f_n}_{n∈ℕ}$ una familia uniformemente acotada de funciones diferenciables en $[a,b]$, un intervalo compacto de $ℝ$, y cuyas derivadas están uniformemente acotadas.

\ppart Probar que cierta subsucesión $\set{f_{n_j}}$ converge uniformemente a una función $f$ Lipschitz. ¿Es cierto que $f$ es necesariamente diferenciable en todo $[a,b]$?

\ppart Probar que si se omite la condición de que la familia sea uniformemente acotada, existe una sucesión de constantes $c_n$ tales que $f_n - c_n$ converge uniformemente a una función $f$ Lipschitz continua.

\solution

\begin{figure}[hbtp]
\centering
\inputtikz{FuncionLipschitz}
\caption{La idea de la función Lipschitz es que siempre hay un doble cono (el verde) que contiene a toda la función.}
\label{fig:FuncionLipschitz}
\end{figure}

Recordamos la definición de función continua Lipschitz (\fref{fig:FuncionLipschitz}):

\begin{defn}[Función\IS Lipschitz continua] Una función $\appl{f}{(X, \dst_X)}{(Y, \dst_Y)}$ entre dos espacios métricos se dice Lipschitz continua si existen una constante $C ∈ ℝ$, $C ≥ 0$ tal que para todos $x_1, x_2 ∈ X$ se tenga que \[ \dst_Y(f(x_1), f(x_2)) ≤ C \dst_X(x_1, x_2) \]
\end{defn}

\spart

Si la familia de funciones está uniformemente acotada, eso significa que tenemos una cota $0 ≤ C_0 < ∞$ tal que $C_0 > f_n\; ∀n∈ℕ$. Igualmente, si sus derivadas están uniformemente acotadas tenemos que $f_n' < C_1$.

El espacio $C_{[a,b]}$ con la norma del supremo, que es donde vive esta sucesión, es un espacio métrico completo, por lo que existe una subsucesión $\set{f_{n_j}}$ convergente uniformemente a $f ∈ C_{[a,b]}$. Vamos a demostrar ahora que esa $f$ es Lipschitz.

Por otra parte, como las $f_n$ tienen derivada acotada por $C_1$, podemos decir que son funciones Lipschitz con cota $C_1$. Vemos ahora qué ocurre con $\abs{f(x) - f(y)}$ para $x,y ∈ [a,b]$. Fijando un $ε>0$, podemos encontrar un $n_0 ∈ ℕ$ tal que si $n > n_0$ entonces $\abs{f_n(x) - f(x)} < ε$ por ser las $f_{n_j}$ convergentes a $f$. Sumando y restando: \begin{align*}
\abs{f(x) - f(y)} &= \abs{f(x) - f_n(x) + f_n(x) - f(y) + f_n(y) - f_n(y)} \\
&≤ \abs{f(x) -f_n(x)} + \abs{f(y) - f_n(y)} + \abs{f_n(x) - f_n(y)} \\
&≤ ε + ε + C_1\abs{x-y}
\end{align*}, por lo que haciendo tender $ε \to 0$ tenemos que $f$ es Lipschitz con la misma cota $C_1$ de las funciones de la sucesión.

Sin embargo, $f$ no tiene por qué ser necesariamente diferenciable: $f$ sólo es Lipschitz pero Lipschitz no implica diferenciable en todo punto. Por ejemplo, $f(x) = \abs{x}$ es continua Lipschitz pero no es derivable en $x = 0$.

\spart

Tomamos $c_n = f_n(a)$, ya que por el teorema del valor medio tenemos que \[ \abs{f_n(x) - c_n} = \abs{f_n(x) - f_n(a)} ≤ C\abs{x-a} ≤ C\abs{b-a} < ∞ \], luego $f_n - c_n$ está equiacotada y estamos en las condiciones del teorema anterior.

\end{problem}

\begin{problem} Construir en $C_{[0,1]}$ un subconjunto acotado de funciones que no sea un \nlref{def:ConjTotalmenteAcotado}.

\solution

Nos basta aplicar \nref{thm:AscoliArzela}. El ejemplo canónico es $f_n(x) = x^n$ para $x ∈ [0,1]$, que converge a $0$ cuando $x ∈ [0,1)$ y $1$ si $x = 1$.

\end{problem}

\begin{problem} Sea \sdst un espacio métrico completo. Demuestra que:

\ppart Si $K ⊂ X$ es cerrado, entonces $K$ es completo.
\ppart Si $K$ es completo y totalmente acotado, entonces es compacto.

\solution

\spart

Fácil: una sucesión de Cauchy en $K$ es convergente en $X$ por ser $X$ completo. Como $K$ es cerrado, el límite de la sucesión ha de estar en $K$, así que $K$ es completo.

\spart

Recordamos el \nref{thm:HeineBorel}, que nos dice que un subconjunto $K ⊂ X$ es compacto si y sólo si es cerrado y totalmente acotado. Partimos de que $K$ es completo y totalmente acotado, por lo que sólo necesitamos probar que además es cerrado.

Ahora bien, esto es sencillo de probar: por ser $K$ completo, podemos tomar una sucesión de Cauchy $\set{x_n} ⊂ K$, que convergerá a un $x ∈ K$. Por lo tanto $K$ es cerrado y podemos demostrar el ejercicio aplicando Heine-Borel.

\end{problem}


\begin{problem} Sea $f ∈ C_c(ℝ)$ una función continua en $ℝ$ y de soporte compacto. Fijamos $η ∈ C_c^1 (ℝ)$ con $\int η = 1$, y sea $η_t(x) = tη(tx)$ para $t > 0$. Demuestra que:

\ppart Sea $f_t ≝ f * η_t$. Entonces $f_t \convs[][t] f$ uniformemente.
\ppart $\norm{f_t'}_∞ ≤ C(1+t)$ con $t > 0$ y $C$ finita e independiente de $t$.

\solution

Este ejercicio es una demostración de un teorema conocido sobre familias de aproximaciones de la identidad \citep[Def. III.11, Teorema III.20]{ApuntesVarReal} que viene bien demostrado en \citep[Teoremas 8.14, 8.15]{folland99}. Vamos a demostrarlo todo formalmente.

\begin{defn}[Familia\IS de aproximaciones de la identidad] \label{def:AproxIdentidad} Una familia de aproximaciones de la identidad es una sucesión $\set{ρ_n}_{n∈ℕ}$ de funciones $\appl{ρ_n}{ℝ^N}{ℝ}$ no negativas, infinitamente derivables, de soporte compacto, con $\sop ρ_n ∈ \bola_{\frac{1}{n}} (0)$ y con norma $\norm{ρ_n}_1 = \int_ℝ ρ_n = 1$.
\end{defn}

Esta definición nos vale igualmente para una familia no numerable. Es fácil ver que $η_t$ son una familia de aproximaciones de la identidad, aunque en este caso miramos el límite $t \to ∞$ en lugar de $t \to 0$. No es un cambio relevante.

Vamos ahora con la demostración.

\spart

Demostramos primero que las $η_t$ mantienen la misma integral sobre $ℝ$ que η: \[ \int_{ℝ} η_t(x) \dif x = \int_ℝ tη(tx) \dif x \eqreasonup{$y = tx$} \int_ℝ t · \frac{1}{t} · η(y) \dif y = \int_{ℝ} η(y) \dif y = 1\]

Una vez hecho esto, calculamos la convolución: \[ f_t(x) = \int_ℝ f(x-y) η_t(y) \dif y \]

Por ser $f$ continua podremos acotar su valor en entornos pequeños, esto es, que $∀ ε > 0$ existe un $δ > 0$ tal que $f(y) ∈ \bola_ε(f(x))$ si $x - y ∈ \bola_δ(0)$. Además, dado que las $η_t$ tienen un soporte cada vez más pequeño, esto es, $\sop η_t \convs[][t] \set{0}$, podemos encontrar un $t_ε$ suficientemente grande tal que $\sop η_t ⊆ \bola_δ(0)$ si $t > t_ε$. En este caso podemos acotar y ver que, si $t > t_ε$, entonces \begin{align*}
f_t(x) &= \int_ℝ f(x-y) η_t(y) \dif y \\
&= \int_{\bola_δ(0)} f(x-y) η_t(y) \dif y \\
&≤ \int_{\bola_δ(0)} (f(x) \pm ε) η_t(y) \dif y \\
&= (f(x) \pm ε) \int_{\bola_δ(0)} η_t (y) \dif y = f(x) \pm ε \end{align*}

Haciendo tender $ε \to 0$, $t$ se irá a infinito y tendremos la convergencia uniforme que buscábamos.

\spart

Vamos a usar una propiedad de la convolución, y es la siguiente: dadas $f,g$, entonces \[ \od{f*g}{x} = \od{f}{x} * g = f* \od{g}{x}\]

Esto nos permitirá sacar la derivada de la convolución aprovechando que η es derivable. Calculamos su derivada: \[ η_t'(x) = t^2 η'(tx) \], lo que nos deja que \[ f_t' (x) = f * η_t'(x) = \int_{ℝ} f(x-y) t^2 η'(tx) \dif y \]

Ahora vamos a tratar de acotar esta integral acotando las dos funciones que tenemos. $η'$ está acotada por ser $η$ derivable, y $f$ también está acotada por ser continua en un compacto. Llamaremos $K_f, K_η$ a las respectivas cotas para $f$ y $η'$.

El siguiente paso es ver que, dado que tanto $f$ como η (y por lo tanto $η_t'$) tienen soporte compacto, no tenemos que evaluar la integral en todo $ℝ$ sino sólo donde alguna de las dos funciones sea distinta de cero. Podemos suponer que siempre hay un soporte contenido en el otro (si no, la intersección será más pequeña que cualquiera de los dos y la cota será menor).

Empezaremos viendo qué ocurre cuando $\sop η_t' ⊆ \sop f(x-y)$. En este caso, tendremos que \[ \int_{ℝ} f(x-y) t^2 η'(tx) \dif y = \int\limits_{\sop η_t'} f(x-y) t^2 η'(tx) \dif y ≤ t^2 K_f K_η \int\limits_{\sop η_t'} \dif y \]

Sabemos que $\int_{\sop η_t'} \dif y = m(\sop η_t')$, es decir, es la medida del soporte. Sin embargo, es fácil ver que $m(\sop η_t') ≤ \frac{m(\sop η)}{t}$ (el soporte de la derivada está contenido en el soporte de la función, y al multiplicar por $t$ el parámetro estamos contrayendo el soporte). Como η tiene soporte compacto, su medida es finita y lo que nos queda es que \[ K_f K_η t^2 \int\limits_{\sop η_t'} \dif y = K_f K_η t · m(\sop η_t') ≤ C_1 t \]

Nos falta ver el caso contrario: qué ocurre cuando  $\sop f(x-y) ⊆ \sop η_t'$. De manera análoga, nos quedaría que \[ \int\limits_{\sop f(x-y)} f(x-y) t^2 η'(tx) \dif y ≤ K_f K_η t^2 · m(\sop f(x-y)) \]

Ahora bien, este caso ($\sop f(x-y) ⊆ \sop η_t'$) ocurre sólo cuando $t$ es pequeño (para $t$ grande el soporte de $η_t$ se hace tan pequeño como queramos y acabará estando contenido en el de $f(x-y)$). Luego en este caso tendremos $t$ acotado y por lo tanto podremos decir que \[ K_f K_η t^2 · m(\sop f(x-y)) ≤ C_2 \] con $C_2 ∈ ℝ$ independiente de $t$. Sumando ambas cotas, tenemos que $f_t' ≤ C_1 t + C_2$, y tomando $C = \max \set{C_1, C_2}$ nos queda que $f_t' ≤ C t + C = C(1+t)$.

\end{problem}

\section{Hoja 2}

\begin{problem} Sea $(X, \norm{·})$ un espacio vectorial normado. Demuestra que

\ppart  Dado $\mathcal{U} ⊂ X$ abierto, el conjunto $x + \mathcal{U} = \set{ x+ y \tq y ∈ \mathcal{U}}$ es abierto, así como $r\mathcal{U} = \set{r · x \tq x ∈ \mathcal{U}}$ para $r > 0$. En otras palabras, demuestra que la topología de un espacio normado es compatible con la estructura de espacio vectorial.

\ppart La norma es una función Lipschitz continua de constante $1$: \[ \abs{\norm{x} - \norm{y}} ≤ \norm{x-y} \quad ∀x,y ∈ X \]

\ppart Si $x_n \to x$ en $X$, entonces $\norm{x_n} \to \norm{x}$.

\ppart Dado $E ⊂ X$ definamos $\dst(x,E) = \inf \set{\norm{x-y} \tq y ∈ E}$. Entonces $\dst(x,E)$ es también Lipschitz continua de constante $1$.

\solution

\spart \label{ej:Hoja2:1A}

Sea $\mathcal{U} ⊂ X$. Ya hemos visto que $x + \mathcal{U}$ es un abierto para un $x ∈ V ⊂ \mathcal{U}$. Si tomamos entonces $z ∈ x + \mathcal{U}$, existirá un $y ∈ \mathcal{U}$ con $z = x + y$. Como $\mathcal{U}$ es abierto, podemos tomar una bola $\bola_δ(y) ⊂ \mathcal{U}$ y entonces $x + \bola_δ(x) ⊂ x + \mathcal{U}$, luego efectivamente $x + \mathcal{U}$ es abierto.

Un argumento similar valdrá para demostrar que $r\mathcal{U}$ es abierto. No merece demasiado la pena pararse en ello.

\spart

Sumando y restando, podemos ver que \[ \norm{x} = \norm{(x+y) - y} ≤ \norm{x-y} + \norm{y} \], luego $\norm{x} - \norm{y} ≤ \norm{x-y}$.

Eso sí, no hemos conseguido el valor absoluto. Sin embargo, si repetimos el mismo proceso desde la norma de $y$ tenemos que \[\norm{y} = \norm{(y-x) + x} ≤ \underbracket{\norm{y-x}}_{= \norm{x-y}} + \norm{x} \], luego $\norm{x} - \norm{y} ≥ - \norm{x-y}$. Juntando ambas conclusiones, \[ \abs{\norm{x} - \norm{y}} ≤ \norm{x-y} \] tal y como nos pedían.

\spart

Se resuelve simplemente por continuidad de la norma.

\spart

Si $\dst(x,E)$ es Lipschitz de constante 1, entonces lo que queremos ver es que \[ \abs{\dst(x,E) - \dst(x',E)} ≤ \abs{x - x'} \]

Esto se puede entender como una generalización del apartado B, si en ese caso tomábamos $E = \set{0}$.

Para demostrarlo, tomamos $x,x' ∈ V$ y $ε > 0$. Entonces existen $y, y' ∈ E$ con \begin{gather*}
 \dst(x,E) ≤ \norm{x-y} ≤ \dst(x,E) + ε\\
 \dst(x',E) ≤ \norm{x'-y'} ≤ \dst(x',E) + ε\\
\end{gather*}, de tal forma que nos acercan todo lo que queramos a esa distancia\footnote{No tiene por que ser igual porque está definido como el ínfimo, no tiene por qué alcanzarse.}. Además, podemos hacer dos estimaciones adicionales por ser la distancia el ínfimo: \[ \dst(x', E) ≤ \norm{x' - y} \qquad \dst(x,E) ≤ \norm{x-y'} \]

Supongamos además sin pérdida de generalidad que $\dst(x,E) ≥ \dst(x',E)$. Entonces \begin{multline*}
\abs{\dst(x,E) - \dst(x',E)} =
	\underbracket{\dst(x,E)}_{≤\norm{x-y'}} - \underbracket{\dst(x', E)}_{≥ \norm{x' - y'} - ε} ≤ \norm{x + y'} - \norm{x'-y'} + ε ≤ \\ ≤ \abs{ \norm{x + y'} - \norm{x'-y'} } + ε
	≤ \norm{(x-y') - (x'-y')} + ε = \norm{x-x'} + ε
\end{multline*}, y haciendo tender $ε \to 0$ ya tenemos lo que buscábamos.

\end{problem}

\begin{problem} \label{ej:Hoja2:2} Sea $V$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$. Diremos que $A ⊂ V$ es convexo si $∀x,y ∈ A$ y $∀t ∈ [0, 1]$, entonces $(1-t)x + ty ∈ A$ (en otros términos, el segmento $[x,y] ⊂ A$ si $x,y ∈ A$).

\ppart Demuestra que \[ A + A = \set{x + y \tq x,y ∈ A} = 2A = \set{2x \tq x ∈ V} \]

\ppart Si $W$ es otro espacio vectorial sobre \kbb y $\appl{T}{V}{W}$ es lineal, entonces $T(A)$ es convexo en $W$.

\ppart Si $(V, \norm{\cdot})$ es normado, entonces dados $x ∈ V$ y  $r > 0$, los conjuntos \begin{align*}
\bola_r (x) &= \set{x ∈ V \tq \norm{x} < r} \\
\adh{\bola_r}(x) &= \set{x ∈ V \tq \norm{x} ≤ r}
\end{align*} son, respectivamente, la bola abierta y la cerrada, centradas en $x ∈ V$ y de radio $r > 0$. Demuestra que ambas son convexas.

\ppart Si $(V,\norm{\cdot})$ es normado y $A ⊂ V$ es convexo, entonces $\adh{A}$ es convexo.

\solution

\spart

Está claro que $2 A = \set{x + x \tq x ∈ A} ⊂ A + A$, y para esto no necesitamos para nada la convexidad. La inclusión no será demasiado complicada.

Sea $z ∈ A + A$. Entonces $z = x + y$ con $x,y ∈ A$. Dividiendo por dos, \[ \frac{1}{2} z = \frac{1}{2} x + \frac{1}{2} y\], que está en $A$ por ser convexo. Luego si $\frac{1}{2} z ∈ A$, sólo queda ver que $z ∈ 2A$.

\spart

Tomamos $T$ en $(1-t)x + ty ∈ A$ y listos, podemos hacerlo por linealidad.

\spart

Empezamos primero con la bola cerrada. Tomamos $p,q ∈ \adh{\bola_r}(x)$ y $ 0 ≤ t ≤ 1$. Entonces \begin{multline*} \norm{(1-t)p + t q - x} = \norm{(1-t)(p - x) + t(q - x)} ≤ \\ ≤ (1-t)\norm{p - x} +(t) \norm{q - x} ≤ (1-t) r + t (r) = r \end{multline*}, así que como $\norm{((1-t)p + t q) - x} ≤ r$, tenemos que $(1-t)p + t q ∈ \adh{\bola_r}(x)$.

Para la bola abierta se hace igual, sólo que aparece un menor estricto en $\norm{p-x} < r,\norm{q-x} < r$ y nos sale.

\spart

Sean $x,y ∈ \adh{A}$ y $t ∈ [0,1]$. Por ser $\adh{A}$ cerrado, existirán sendas sucesiones $\set{x_n}, \set{y_n} ⊂ A$  que convergen respectivamente a $x$ e $y$. Consideramos ahora los puntos $(1-t)x_n + ty_n ∈ A$ por ser $A$ convexo. Como $\adh{A}$ es cerrado, la sucesión de esos puntos tenderá a $(1-t)x + t y$ que está en $\adh{A}$.

\end{problem}

\begin{problem} Sea $X = (C[0,1], \norm{·}_∞)$. Entonces, si $f ∈ L^1[0,1]$ y \[ λ_f(g) ≝ \int_0^1 f(x) g(x) \dif x\], entonces $λ_f$ define un funcional acotado en $X$ cuya norma es exactamente $\norm{f}_1$. Demuéstralo siguiendo los siguientes pasos:

\ppart La desigualdad $\norm{λ_f}_{X^*} ≤ \norm{f}_1$ es trivial (justificarlo).
\ppart Si $f ∈ C[0,1]$ probar la desigualdad opuesta; junto con el apartado anterior esto prueba igualdad en este caso.
\ppart Si $f ∈ L^1[0,1]$ es cualquiera, usar que hay una sucesión de funciones continuas $f_n ∈ C[0,1]$ con $f_n \to f$ en $L^1[0,1]$

\solution

\spart

Podemos hacer la siguiente estimación: \[
\abs{λ_f(g)} = \abs{\int_0^1 f(x) g(x) \dif x} ≤ \int_0^1 \abs{f(x)} \abs{g(x)} \dif x ≤ \norm{g}_∞ \int_0^1 \abs{f(x)} \dif x = \norm{f}_1 \norm{g}_∞
\], luego $λ_f ∈ X^*$.

\spart

Si $f ∈ C[0,1]$, entonces $\norm{λ_f}_{X^*} ≥ \norm{f}_1$. Supongamos que $f ∈ C[0,1]$ y $f \not\equiv 0$. Consideramos \[ g (x) = \sign f(x) = \begin{cases} \frac{\abs{f(x)}}{f(x)} & f(x) ≠ 0 \\ 0 & f(x) = 0 \end{cases} \]

Entonces \[ λ_f(g) = \int_0^1 f(x) \sign f(x) \dif x = \int_0^1 \abs{f(x)} \dif x = \norm{f}_1 \]

Además, como $f \not \equiv 0$, $g(x) ≠ 0$ y $\abs{g(x)} = 1$, si $g(x) ≠ 0$, por lo que $\norm{g}_∞ = 1$ y entonces \[ \frac{\abs{λ_f (g)}}{\norm{g}_∞} = \frac{\norm{f}_1}{1} = \norm{f}_1 \implies \norm{λ_f}_{X^*} ≥ \norm{f}_1 \]

Ahora bien, tenemos un problema con esto, y es que $g$ no es continua y por lo tanto $g ∉ X$. Para arreglar el argumento vamos a regularizar $g$ con $\set{θ_t}_{t>0}$ una serie de  \textit{mollifiers}\footnote{\nref{def:AproxIdentidad}}. Para construirlos, fijamos $θ ∈ C_c(ℝ)$ par con $\sop θ ⊆ [-1, 1]$, $\norm{θ}_1 = 1$ y $θ ≥ 0$; y entonces \[ θ_t(x) = \frac{θ(\inv{t} x)}{t} \]

Tenemos que ampliar la definición de $g$ a $ℝ$, así que lo que haremos será extenderla con ceros a todo $ℝ$. Definimos entonces ahora \[ g_t = g * θ_t = \int_{ℝ} g(y) θ_t(x-y) \dif x\]

Supongamos adicionalmente que $\sop f ⊆ [ε, 1-ε]$ para cierto $0 < ε < \sfrac{1}{2}$, y que entonces tendremos $\sop g = \sop f$. Ahora vemos que $\sop g_t = \sop g + \sop θ_t = [ε - t, 1 - ε +t] ⊂ [0,1]$ si $0 < t ≤ ε$.

Calculamos ahora $λ_f(g_t)$ con $t ∈ (0,ε]$, ya que esta $g_t$ siempre será continua para $t > 0$. \begin{align*}
λ_f(g_t) &= \int_0^1 f(x) (g * θ_t)(x) \dif x = \\
	&= \int_{ℝ} f(x) (g * θ_t)(x) \dif x = \\
	&= \int_ℝ f(x) \left(\int_{ℝ} g(y) θ_t(x-y) \dif y \right) \dif x = \\
	&= \int_{ℝ} g(y) \left(\int_{ℝ} f(x) \underbracket{θ_t (x-y)}_{ = θ_t(y-x)} \dif x \right) \dif y = \\
	&= \int_{ℝ} g(y) (f*θ_t)(y) \dif y
\end{align*}

Como $f ∈ C[0, 1]$, entonces $f * θ_t \convs[L^∞][t][0] f$, luego podemos la integral converge igualmente: \[ \int_{ℝ} g(y) (f*θ_t)(y) \dif y \convs[][t][0] \int_{ℝ} g(y) f(y) \dif y \], así  que $λ_f(g_t) \convs[][t][0] λ_f (g) = \norm{f}_1$. Entonces \[ \norm{λ_f}_{X^*} ≥ \frac{\abs{λ_f(g_t)}}{\norm{g_t}_∞} \quad ∀t > 0\], ya que la definición del supremo era que  \[ \norm{λ_f}_{X^*} = \sup_{g ∈ X \setminus\set{0}} \frac{\abs{λ_f(g)}}{\norm{g}_X} \]

Lo único que nos falta ver es que $\norm{g_t}_∞ \convs[][t][0] 1$. Sabemos que $\norm{g_t}_∞ ≤ \norm{g}_∞ = 1$, como consecuecnia de que $θ ≥ 0$ y $\norm{θ}_1 = 1$. Pero si $f \not \equiv 0$, entonces $g$ es de módulo $1$ y continua en cualquier abierto donde $f ≠ 0$, y entonces $g_t(x) \convs[][t][0] g(x)$. Para verlo, si $g$ es continua en $x$ pues sale, y si no el módulo es exactamente $1$, y juntándolo tenemos que la norma infinito converge hacia uno cuando $t$ se hace pequeñito. Y juntando de nuevo esto con otra cosa, juraría que lo de la norma de antes, entonces nos queda que \[ \norm{λ_f}_{X^*} ≥ \liminf_{t \to 0} \frac{\abs{λ_f(g_t)}}{\norm{g_t}_∞} = \frac{\liminf_{t \to 0} \abs{λ_f(g_t)}}{\liminf_{t \to 0} \norm{g_t}_∞} = \norm{f}_1 \]

Y ya casi lo hemos probado si $\sop f ∈ [ε, 1-ε]$. Nos queda probarlo para el último caso de $f ∈ C[0,1]$. Sea $ε > 0$. Consideramos $f_ε $ como una versión truncada, formalmente \[ f_ε(x) = \begin{cases}
f(x) & x ∈ [ε, 1-ε] \\
f(ε)\left(1 - 2\frac{ε-x}{ε}\right) & x ∈ [\sfrac{ε}{2}, ε] \\
f(1-ε)\left(1 - 2\frac{1-ε-x}{1-ε}\right) & x ∈ [1- ε, 1- \sfrac{ε}{2}] \\
0 & x ∈ [0, \sfrac{ε}{2}] ∪ [ 1- \sfrac{ε}{2}, 1]
\end{cases}
\]

Vamos a ver entonces que \[ \norm{f-f_ε}_1 ≤ \dotsb ≤ 2ε\norm{f}_1 \], y entonces podemos escribir \[ λ_f(g) = λ_{f_ε}(g) + λ_{f-f_ε}(g) \], y como $\abs{λ_{f-f_ε}(g)} ≤ \norm{f-f_ε}\norm{g}_1 \convs[][ε][0] 0$, y luego \[\norm{λ_f}_{X^*} = \norm{λ_{f-f_ε} + λ_{f_ε}} ≥ \norm{λf_ε}_{X^*} - \norm{λ_{f-f_ε}}_{X^*} = \norm{f_ε}_1 - \norm{f - f_ε}_1 \convs[][ε][0] \norm{f} \], y esto ya sí termina la prueba cuando $f$ es continua. La idea sería la misma si tuviésemos $f ∈ L^1[0,1]$, que es lo que queríamos probar, usando una aproximación por continuas.


\end{problem}

\begin{problem}[4] Dada $f ∈ C([-\sfrac{1}{2}, \sfrac{1}{2}])$ (que entenderemos extendida por periodicidad de período 1 a todo $ℝ$), sus coeficientes de Fourier son \[ \hat{f}(n) ≝ \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(x) e^{-2πinx} \dif x\quad n ∈ ℤ\] y las sumas parciales de su serie de Fourier formal $\sum_{n∈ℤ} \hat{f}(n) e^{2πinx}$ como \[ S_Nf(x) ≝ \sum_{\abs{n} ≤ N} \hat{f}(n) e^{2πinx} \quad N ∈ ℕ \]

Demostrar que:

\ppart Dado $D_N(t) ≝ \sum_{\abs{n}≤N} e^{2πint}$ el llamado \textbf{Núcleo de Dirichlet}.\index{Núcleo!de Dirichlet}, entonces  \[ S_N f(x) = \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(y) D_n (x-y) \dif y = (D_N * f) (x) \]

\ppart Probar que \[ D_N(t) = \frac{\sin 2πt(N + \sfrac{1}{2})}{\sin πt} \]

Indicación: las sumas $\sum_{1≤ n ≤ N} e^{2πint}$ y $\sum_{-N ≤ n ≤ -1} e^{2πint}$ son ambas geométricas y conjugadas entre sí. Además, según la notación del \fref{ej:Hoja2:2}, $S_N(f)(0) = λ_{D_N} (f)$.

\ppart Probar que existe $C > 0$ tal que \[ \norm{D_N}_{L^1} = C \log N + \mathcal{O}(1), \; N \to ∞ \]

Indicación: si $\abs{t} ≤ \sfrac{1}{2}$, entonces $\frac{1}{\sin πt} - \frac{1}{πt} = \mathcal{O}(1)$.

\ppart Usar el \fref{ej:Hoja2:2} y los apartados anteriores para concluir que existe una $f ∈ C([-\sfrac{1}{2}, \sfrac{1}{2}])$ cuya serie de Fourier diverge en $x = 0$; y que de hecho existe todo un $G_δ$ denso de funciones con esa propiedad.

\solution

\spart

Los núcleos de Dirichlet están dados por \[ D_N(t) ≝ \sum_{\abs{n} ≤ N} e^{2πint} \]

Operando tenemos que \[
S_Nf(x) = \sum_{\abs{n}≤N} \hat{f}(n) e^{2πinx} = \sum_{\abs{n}≤N} \left(\int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(y) e^{-2πiny} \dif y \right) e^{2πinx}
\]

Podemos pasar $e^{2πinx}$ dentro de la integral por ser constante con respecto a $y$, y dado que la suma que estamos haciendo es finita, podemos pasar el sumatorio dentro de la integral por linealidad de ésta, y lo que nos queda es que \begin{gather*}
\sum_{\abs{n}≤N}
	\left(\int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(y) e^{-2πiny} \dif y \right)
	e^{2πinx} =
\int_{-\sfrac{1}{2}}^{\sfrac{1}{2}}
	\sum_{\abs{n}≤N} f(y)·e^{-2πiny}·e^{2πinx} = \\
= \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}}  \sum_{\abs{n}≤N} f(y) e^{2πn(x-y)} \dif y
= \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}}  f(y) \underbracket{\sum_{\abs{n}≤N} e^{2πn(x-y)}}_{D_N(x-y)}
= (D_N * f)(x) \end{gather*}
, que es lo que se pedía probar.

\spart

Definimos \[
D_N^+(t) ≝ \sum_{1≤n ≤N} e^{2πint} \qquad
D_N^-(t) ≝ \sum_{-N ≤ n ≤ -1} e^{2πint} \]
de tal forma que \[ D_N(t) = D_N^+(t) + D_N^-(t) + e^{2πit·0} = D_N^+(t) + \conj{D_N^+(t)} + 1 = 2 \Re D_N^+(t) + 1 \]

Dado que $D_N^+(t)$ es una suma geométrica, podemos hallar su valor: \[ D_N^+(t) = e^{2πit} \frac{1 - e^{2πiNt}}{1-e^{2πit}} = \frac{e^{2πit} - e^{2πi(N+1)t}}{1-e^{2πit}} \]

Multiplicando arriba y abajo por $e^{-πit}$ tenemos que \[ D_N^+(t) = \frac{e^{2πit} - e^{2πi(N+1)t}}{1-e^{2πit}} = \frac{e^{πit} - e^{2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}\], de forma que \[ \conj{D_N^+(t)} = \frac{e^{-πit} - e^{-2πi(N + \sfrac{1}{2})t}}{-e^{-πit} + e^{πit}} = \frac{-e^{-πit} + e^{-2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}} \]

Ahora sólo queda sumar y ver que \begin{align*}
D_N(t) &=
	\frac{e^{πit} - e^{2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}
+ 	\frac{-e^{-πit} + e^{-2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}
+	\frac{e^{-πit} - e^{πit}}{e^{-πit} - e^{πit}} = \\
&= 	\frac{e^{-2πi(N + \sfrac{1}{2})t} - e^{2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}
=	\frac{\sin -2π(N + \sfrac{1}{2})t}{\sin -πt} = \\
&= 	\frac{\sin 2πt(N + \sfrac{1}{2})}{\sin πt}
\end{align*} tal y como se pedía.

\spart


Una primera observación es que podemos reducirnos a estudiar la integral en el intervalo $[0,\sfrac{1}{2}]$, ya que el valor absoluto del seno es una función par: \(
\norm{D_N}_{L^1[-\sfrac{1}{2}, \sfrac{1}{2}]}
	= \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} \abs{\frac{\sin 2πt(N+\sfrac{1}{2})}{\sin πt}} \dif t
	= 2 \int_{0}^{\sfrac{1}{2}} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} \dif t \label{eq:NormaDNPrev} \)

Usando la sugerencia, sabemos que $\frac{1}{\sin πt} - \frac{1}{πt} = \mathcal{O}(1)$ cuando $\abs{t} ≤ \sfrac{1}{2}$, luego

Hay dos problemas que sortear en la integral: por un lado, la indeterminación cuando $t \to 0$ y el valor absoluto en el numerador. Podemos dividir el intervalo de integración en subintervalos en los que el numerador no cambie de signo, y además ``aislaremos'' la indeterminación y podremos tratarla por separado. La división será la siguiente: \begin{gather*}
[0, \sfrac{1}{2}] = \left(\bigcup_{k=0}^{N-1} I_k \right) ∪ I_E \\
I_k = \left[ \frac{k}{2(N+\sfrac{1}{2})},  \frac{k+ 1}{2(N+\sfrac{1}{2})} \right) \qquad
	I_E = \left[ \frac{N}{2(N+\sfrac{1}{2})}, \frac{1}{2} \right]
\end{gather*}

Por comodidad, denotaremos $f(t) = \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} $.

\paragraph{Integral en $I_0$} El primer intervalo a resolver será $I_0 = \left[0, \frac{1}{2(N+\sfrac{1}{2})}\right)$, que es donde tenemos la indeterminación cuando $t \to 0$. Pero si resolvemos ese límite tenemos que \[ \lim_{t \to 0}  \frac{\sin 2πt(N+\sfrac{1}{2})}{\sin πt} \eqreasonup{L'Hopital} \lim_{t \to ∞} \frac{2π(N + \sfrac{1}{2}) · \cos 2πt(N+\sfrac{1}{2})}{π · \cos πt} = 2(N + \sfrac{1}{2}) \]

Además, derivando $f(t)$ tenemos que \begin{align*}
f'(t) &= \frac{\sin πt ·\cos 2πt(N + \sfrac{1}{2}) - \cos πt · \sin 2πt(N + \sfrac{1}{2})}{\sin^2 πt} = \frac{\sin πt(1 - 2N - 1)}{\sin^2 πt} \\
&= \frac{\sin -2πtN}{\sin^2 πt} = \frac{- \sin 2πtN}{\sin^2 πt} ≤ 0 \quad ∀t ∈ \left[0, \frac{1}{2(N+\sfrac{1}{2})}\right)
\end{align*}, por lo que $f$ es decreciente en el intervalo, luego su máximo ocurre en $t = 0$ y finalmente podemos acotar y ver que \( 0 ≤ \int_{I_0} f(t) \dif t ≤ \int_0^{\frac{1}{2(N+\sfrac{1}{2})}} 2(N + \sfrac{1}{2}) \dif t = 1 = \mathcal{O}(1) \label{eq:CotaI0} \)

\paragraph{Integral en $I_k$} Para resolver esta integral, usamos que $\frac{1}{\sin πt} - \frac{1}{πt} ≤ M$ para $M < ∞$ cuando $\abs{t} ≤ \sfrac{1}{2}$. Entonces podemos separar la integral: \begin{align*}
\int_{I_k} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} \dif t &= \int_{I_k}\underbracket{\sin (2πt(N+\sfrac{1}{2}))}_{≤ 1}·\left(\underbracket{\frac{1}{\sin πt} - \frac{1}{πt}}_{≤M} + \frac{1}{πt} \right) \dif t = \\
 &≤ \int_{\frac{k}{2(N+\sfrac{1}{2})}}^{\frac{k+ 1}{2(N+\sfrac{1}{2})}} M + \frac{1}{πt} \dif t = \frac{M}{2(N+\sfrac{1}{2})} + \left(\frac{\log t}{π}\right|_{t = \frac{k}{2(N+\sfrac{1}{2})}}^{\frac{k+ 1}{2(N+\sfrac{1}{2})}} = \\
 &= \frac{M}{2(N+\sfrac{1}{2})} + \frac{\log (k+1) - \log k}{π}
\end{align*}

Como los logaritmos tienen signo alternado, se van a cancelar al sumar las integrales de intervalos sucesivos y por lo tanto quedará \( \label{eq:CotaIkSup}
0 ≤ \sum_{k = 1}^{N-1} \int_{I_k} f(t) \dif t ≤ \frac{M(N-1)}{2N + 1} + \frac{\log N - \log 1}{π} = \mathcal{O}(1) + \frac{1}{π}\log N
	\)

Falta ahora una cota inferior para comprobar que efectivamente $\sum_{k=1}^{N-1} = \mathcal{O}(1) + C \log N$. Para ello, observamos dos cosas: que $\sin πt ≤ πt$ cuando $\abs{t} ≤ \sfrac{1}{2}$ y que $\frac{1}{πt}$ es monótona decreciente. Entonces podemos operar y ver que \begin{align*}
\int_{I_k} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} \dif t
	&≥ \int_{I_k} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{πt} \dif t
	≥ \int_{I_k} \frac{\abs{\sin2πt(N+\sfrac{1}{2})}}{π\left(\frac{k+ 1}{2(N+\sfrac{1}{2})} \right)} \dif t = \\
	&= \frac{1}{π^2k} \abs{\left(\cos 2πt(N+\sfrac{1}{2})\right|_{t=\frac{k}{2(N+\sfrac{1}{2})}}^{\frac{k + 1}{2(N+\sfrac{1}{2})}}} = \frac{2}{π^2k}
\end{align*}

Sumando ahora todas esas integrales, vemos que \( \sum_{k=1}^{N-1} \int_{I_k}f(t) \dif t ≥ \sum_{k=1}^{N-1} \frac{2}{π^2k} > \frac{2\log N}{π^2} \label{eq:CotaIkInf}\), luego juntando esta cota y la de \eqref{eq:CotaIkSup} lo que tenemos es que efectivamente \( \sum_{k=1}^{N-1} \int_{I_k} f(t) \dif t = C \log N + \mathcal{O}(1) \label{eq:CotaIk} \) para alguna constante $0 < C < ∞$ (de hecho, según estos cálculos, $C$ debería de estar entre $\frac{2}{π^2}$ y $\frac{1}{π}$).

\paragraph{Integral en $I_E$} La función en este intervalo $I_E = \left[ \frac{N}{2(N+\sfrac{1}{2})}, \frac{1}{2} \right]$ es continua y por lo tanto acotada, y como la medida del intervalo $m(I_E) \convs[][N] 0$ se va a cero, es fácil ver que \( \label{eq:CotaIE} \int_{I_E} f(t) \dif t = \mathcal{O}(1) \)

\paragraph{Conclusión} Dividiendo el intervalo de integración en tres ``zonas'' \eqref{eq:CotaI0}, \eqref{eq:CotaIk} y \eqref{eq:CotaIE} hemos logrado las cotas que buscábamos (un logaritmo y funciones $\mathcal{O}(1)$), así que sólo falta juntarlas volviendo a la integral original \eqref{eq:NormaDNPrev}, de tal forma que \( \norm{D_N}_{L^1} = 2 \int_{0}^{\sfrac{1}{2}} f(t) \dif t = C \log N + \mathcal{O}(1), \; N \to ∞ \label{eq:CotaFinal}\), con $C ∈ [\sfrac{4}{π^2}, \sfrac{2}{π}]$.

\spart

Según la notación del \fref{ej:Hoja2:2}, $λ_{D_N} (f) = S_Nf(0)$, con $λ_{D_N}$ un operador acotado. De hecho, $\norm{λ_{D_N}}_{\mathcal{L}(X, ℝ)} = \norm{D_N}_1$, y $λ_{D_N} ∈ \mathcal{L}(X, ℝ)$ donde $X = (C([-\sfrac{1}{2}, \sfrac{1}{2}], \norm{·}_∞)$ que es un espacio Banach y $ℝ$ normado.

Como los funcionales $\set{λ_{D_N}}_{N ≥ 1}$ no están acotados uniformemente ($\norm{D_N}_1 \convs[][N] ∞$ según hemos demostrado en \eqref{eq:CotaFinal}), el Teorema de acotación uniforme de Banach-Stainhaus nos dice que existe un conjunto $B$ $G_δ$ denso en $X$ tal que \( \label{eq:BanachStainhaus} \sup_{N ≥ 1} \norm{λ_{D_N} (f)}_ℝ = \sup_{N ≥ 1} \norm{S_Nf(0)}_ℝ = ∞ \quad ∀f ∈ B \), y como $λ_{D_N}$ está acotado, \[ \norm{λ_{D_N} (f)}_ℝ ≤ \norm{λ_{D_N}}_{\mathcal{L}(X, ℝ)} · \norm{f}_ℝ < ∞ \quad ∀N ∈ ℕ \], luego la única posibilidad para que \eqref{eq:BanachStainhaus} se cumpla es que $\norm{S_Nf(0)}_ℝ$ diverja para $f ∈ B$, siendo $B$ un $G_δ$ denso en $X$, tal y como se pedía demostrar.

\end{problem}

\begin{problem}[4] Sea $X = (C([0,1]), \norm{·}_∞)$ u espacio y $δ_{x_0} = f(x_0)$ el funcional evaluativo o Delta de Dirac. En la norma del supremo, ese funcional es continuo.

\solution

El núcleo de $δ_{x_0}$ es $\ker δ_{x_0} = \set{ f ∈ C([0,1]) \tq f(x_0) = 0}$, que es cerrado porque $f(x_0) = 0$ es cerrado y el funcional es continuo. Además, $\quot{X}{\ker δ_{x_0}} \cong ℂ$.

\end{problem}

\begin{problem}[5] \label{ej:Hoja2:5} Sean $X$ e $Y$ dos espacios normados. Demostrar que:

\ppart Si $\dim X < ∞$ y $A$ es una aplicación lineal $\appl{A}{X}{Y}$, entonces es continua.

\ppart Si $X$ es de dimensión finita, existe una aplicación lineal $\appl{A}{X}{Y}$ discontinua. Indicación: Usar que todo espacio vectorial admite una base de Hamel, esto es, un conjunto $B ⊂ X$ linealmente independiente y maximal para esa propiedad.

\solution

\spart

Como la dimensión de $X$ es finita, podemos fijar $\mathcal{B} = \set{\ve_1, \dotsc, \ve_n}$ base de $X$ y la base dual algebraica $\mathcal{B}^* = \set{\ve_1^*, \dotsc, \ve_n^*}$, ambas ortonormales. Con estas dos bases podemos expresar cualquier elemento $x ∈ X$ como \[ x = \sum_{i = 1}^n \ve_i^*(x) \ve_i \]

Con eso podremos acotar $A$ de la siguiente forma:
\begin{align*}
A(x) &=
	A(x) = A\left(\sum_{i=1}^n \ve_i^* (x) \ve_i \right) = \\
	&= \sum_{i=1}^n \ve_i^* (x) A(\ve_i)
	≤ \sum_{i=1}^n \underbracket{\norm[0]{\ve_i^*}_{\dual}}_{=1} \norm{x}_X A(\ve_i) \\
\norm{A(x)} &≤ \norm{x}_X \norm{\sum_{i=1}^n A(\ve_i)}
\end{align*}

Dado que la suma de los $A(\ve_i)$ es finita y cada uno de los sumandos es finito (si no, $A$ no sería lineal) esa norma es finita y ya tenemos la cota que buscábamos.

\spart

% TODO.

\end{problem}

\begin{problem}[6] \label{ej:Hoja2:InversaBiyeccion} Sean $X$ e $Y$ espacios de Banach.

\ppart Si $T ∈ \linapp$ es una biyección, entonces $\inv{T} ∈ \linapp$ (\fref{crl:AppAbiertInversa} del \nref{thm:AppAbierta}).

\ppart Si $V$ es un espacio vectorial con dos normas $\norm{·}_1$ y $\norm{·}_2$ para las cuales $V$ es completo, con la primera dominando a la segunda (\fref{def:NormaDominante}), entonces la segunda domina también a la primera (\fref{crl:AppAbiertaAcotacionNormas}). \textit{Indicación: usar el apartado anterior con $T$ la identidad}.

\solution

\spart

La existencia de la inversa viene por el hecho de que la aplicación sea biyectiva. Tenemos que demostrar que es lineal y acotada.

Para la linealidad, por un lado $\inv{T}(x+y) = c \iff T(c) = x + y$. Como $T$ es biyectiva, existen $a,b ∈ X$ tales que $T(a) = x, T(b) = y$, luego $T(c) = T(a) + T(b) = T(a+b)$, luego $c = a+ b$ por ser inyectiva y $\inv{T}(x+y) = a + b = \inv{T}(x) + \inv{T}(y)$.

Por otro, $\inv{T}(λy) = a \iff λy = T(a)$. Como $T$ es biyectiva, existe $b ∈ X$ tal que $T(b) = y$, luego $λT(b) = T(a) \implies \inv{T}(λy) = λ\inv{T}(y)$.

Para la acotación, por el \nref{thm:AppAbierta} sabemos que $∃ δ  > 0$ tal que $T(\bola_1(0)) ⊃ \bola_δ(0)$. Sea ahora $y ∈ Y$, y queremos demostrar que $\norm{T(y)} < C$ para una cota $C > 0$. Lo que hacemos es ``reescalar'' $y$: $y' = \frac{δ}{2\norm{y}} y$, de tal forma que $y' ∈ \bola_δ(0) ⊂ Y$. Así, $\inv{T}(y') ∈ \bola_1(0) ⊂ X$. Ahora bien, como $\inv{T}$ es lineal entonces $\inv{T}(y') = \frac{δ}{2\norm{y}} \inv{T}(y)$, luego \begin{align*}
\frac{δ}{2\norm{y}} \norm{\inv{T}(y)} &≤ 1  \\
\norm{\inv{T}(y)} &≤ \frac{2\norm{y}}{δ} \\
\norm{\inv{T}} &≤ \frac{2}{δ}
\end{align*} así que ya tenemos la acotación que buscábamos.

\spart

Si $\norm{·}_1$ domina a $\norm{·}_2$, entonces (\fref{prop:NormaDominante}) existe una constante $C < ∞$ tal que $∀x ∈ X$ $\norm{x}_1 ≤ C \norm{x}_2$. Denotamos entonces $X = (V, \norm{·}_1)$ e $Y = (V, \norm{·}_2)$, y definimos la aplicación identidad entre ellos, que es obviamente una biyección y una aplicación lineal acotada (norma $1$).

Ahora sólo falta ver que $\norm{T(x)}_2 = \norm{x}_1 = \norm{x}_2$ y listos.

\end{problem}

\begin{problem}[7] Probar que el \nref{thm:GraficaCerrada} es equivalente a la siguiente afirmación: Dados $X$, $Y$ espacios de Banach y $\appl{T}{X}{Y}$ lineal, si $T$ tiene la propiedad de que siempre que $x_n \to x$ en $X$ y $T(x_n) \to y$ en $Y$ se cumple que $T(x) = y$, entonces $T$ es acotado.

\solution

Que ese enunciado implica el \nref{thm:GraficaCerrada} es sencillo: tomamos una sucesión $\set{(x_n, T(x_n))}_{n ≥ 1} ⊂ G(T)$ de Cauchy. Sabemos que $x_n \to x ∈ X$ y $T(x_n) \to y = T(x) ∈ Y$ por ser $X, Y$ Banach. Por un lado, esto implica que $G(T)$ es cerrado (el límite de sucesión de Cauchy se queda dentro del conjunto) y por otro implica que es acotado según el enunciado anterior, luego $T$ será continua y por lo tanto $T ∈ \linapp$, que es lo que dice el teorema de la gráfica cerrada.

En el otro sentido, sabemos que si $G(T)$ es cerrado eso implica que $T ∈ \linapp$. Dado que $G(T)$ es cerrado, podemos tomar una sucesión de Cauchy $(x_n, T(x_n)) \to (x, y) ∈ G(T)$. Como $T∈ \linapp$, está acotado y además $T(\lim x_n) = \lim T(x_n) \implies T(x) = T(y)$.
\end{problem}


\begin{problem}[8] Una isometría $I ∈ \linapp$ con $X,Y$ espacios normados es una aplicación con la propiedad de que $∀ x ∈ X$ $\norm{I(x)}_Y = \norm{x}_X$. Probar:

\ppart Si $X$ es Banach, $Y$ es normado e $\appl{I}{X}{Y}$ es una isometría sobreyectiva, entonces $Y$ también es de Banach.
\ppart Toda isometría es una aplicación inyectiva: $\ker I = \set{0}$.
\ppart Si $I$ es isometría sobreyectiva, entonces es un isomorfismo lineal entre $X$ e $Y$ y su inversa $\appl{I}{Y}{X}$ es también una isometría.
\ppart Si $I$ es una isometría de $X$ y $\dim X < ∞$ entonces $I$ es automáticamente sobreyectiva y, por tanto, un isomorfismo de $X$.
\ppart Construir un ejemplo de isometría no sobreyectiva de un espacio de Banach. Indicación: Tomar $X = \ell^2$ siendo \[ \ell^2 = \set{\set{a_n}_{n ≥ 1} \tq \norm{\set{a_n}}_2 ≝ \left(\sum_{n ≥ 1} \abs{a_n}^2 \right)^{\frac{1}{2}} < ∞ } \]

\solution

\spart
\label{ej:Hoja2:8}

Sea $\set{y_n}_{n ≥ 1} ⊂ Y$ una sucesión de Cauchy. Como $I$ es sobreyectiva, existe otra sucesión $\set{x_n}_{n ≥ 1} ⊂ X$ tal que $I(x_n) = y_n\;∀n ≥ 1$. Vamos a ver que $x_n$ es de Cauchy: sabemos que $∀ ε > 0$ existe un $N > 1$ tal que si $n,m ≥ N$ entonces $\norm{y_n - y_m} < ε$. Ahora podemos ver que \[ \norm{x_n - x_m} = \norm{I(x_n - x_m)} = \norm{I(x_n) - I(x_m)} = \norm{y_n - y_m} < ε\] luego $\set{x_n}$ es de Cauchy.

Como $X$ es Banach, entonces $x_n \to x ∈ X$. Afirmamos que $y = I(x)$ es el límite de $\set{y_n}_{n ≥ 1}$. Para eso vemos que, dado $ε > 0$ existe un $N ≥ 1$ tal que si $n > N$ entonces $\norm{y - y_n} < ε$. Análogamente a como lo hemos hecho antes, tomamos el $N$ que nos haga falta para conseguir la acotación con ε en la sucesión $\set{x_n}$ y nos queda \[ \norm{y - y_n} = \norm{I(x) - I(x_n)} = \norm{I(x - x_n)} < ε \]

\spart

Por ser espacios normados, $\norm{x} = 0$ si y sólo si $x = 0$. Luego si $I(x) = 0$, $\norm{I(x)} = 0 = \norm{x} \implies x = 0$, luego efectivamente $\ker I = \set{0}$.

\spart

Dado que las isometrías son inyectivas y nos dicen que es sobreyectiva, entonces $I$ es biyectiva y por lo tanto es un isomorfismo lineal. Que la inversa es isometría es trivial.

\spart

\textit{Nota: Voy a suponer que dice isometría de $X$ en $X$}

Tenemos dimensión $n$ finita, luego podemos dar una base $\set{e_1, \dotsc, e_n}$ para $X$.

Para que sea sobreyectiva, vemos que podemos construir una base de forma recursiva. Como $I(e_1) ∈ X$ podemos expresarlo como suma de elementos de la base, esto es, \[ I(e_1) = α_1 e_1 + \dotsb + α_n e_n \] y despejar: \[ e_1 = \frac{I(e_1) - α_2 e_2 - \dotsb - α_n e_n}{α_1} \]

Repitiendo, llegaremos a que $\set{I(e_1), \dotsc, I(e_n)}$ es una base y por lo tanto ya tenemos la sobreyectividad.

Por el camino me he dejado ver qué pasa si alguno de los coeficientes es 0 y no podemos despejar.

\spart

La aplicación que ``desplaza'' una sucesión es lineal e isometría pero no sobreyectiva: \[ T(x_1, x_2, \dotsc) = (0, x_1, x_2, \dotsc)\]
\end{problem}

\begin{problem}[9] Sean $X$ e $Y$ espacios de Banach y $\set{T_n}_{n ≥ 1} ⊂ \linapp$ tales que $∀x ∈X$ existe el límite $\lim_{n \to ∞} T_n(x) ≝ T(x)$. Demuestra que

\ppart $\sup_{n ≥ 1} \norm{T_n} < ∞$.
\ppart $T ∈ \linapp$.
\ppart $\norm{T} ≤ \limsup_{n \to ∞} \norm{T_n}$.

\solution

\end{problem}

\begin{problem}[10] Sea $X$ un espacio de Banach real, $0 < ε_n \to 0$ y $f_n ∈ X^*$ con la propiedad de que para todo $x ∈ X$ existe un $r > 0$ tal que $\norm{x}_X < r$ y una constante $C(x) ∈ ℝ$ tal que \( \label{eq:Hoja2:Ej10} \pesc{f_n, x} ≤ ε_n \norm{f_n}_{X^*} + C(x) \)

Entonces probar que $\set{f_n}$ es un subconjunto acotado en $X^*$ del modo siguiente:

\ppart Probar que \eqref{eq:Hoja2:Ej10} es una acotación válida para $\abs{\pesc{f_n,x}}$.
\ppart Considerar $g_n = \left(1 + ε_n \norm{f_n}_{X^*}\right)^{-1} f_n$, y probar que $\set{g_n}$ es acotado en $X^*$.
\ppart Deducir una contradicción si $\sup \norm{f_n}_{X^*} = ∞$

\solution

\spart

\spart

Tomamos un $x ∈ X$ con $\norm{x}_X < r$, y suponemos sin pérdida de generalidad que $C(x)$. Por el apartado anterior, \begin{align*}
\pesc{g_n, x} &≤ \frac{ε_n \norm{f_n}_X + C(x)}{1 + ε_n \norm{f_n}_{X^*}} =
	\underbracket{\frac{ε_n \norm{f_n}_X}{1 + ε_n \norm{f_n}_{X^*}}}_{< 1} +
	\underbracket{\frac{C(x)}{1 + ε_n \norm{f_n}_{X^*}}}_{< C(x)} < \\
& < 1 + C(x) < ∞
\end{align*}, luego hemos acotado $\abs{\pesc{g_n, x}}$ para los $x ∈ X$ con norma menor que $r$. Ahora bien, como todo $x ∈ X$ es proporcional a otro $x' ∈ X$ con $\norm{x'} < r$ tenemos que $\sup \abs{\pesc{g_n, x}} < ∞ \; ∀x ∈ X$.

Entonces podemos aplicar el \nref{thm:AcotacionUniforme}, que nos dice que existirá una constante $C < ∞$ tal que $\norm{g_n}_{X^*} ≤ C\; ∀n$, luego $\set{g_n}$ está acotado.

\spart

Si efectivamente las $f_n$ no estuviesen acotadas, entonces existiría una sucesión $n_1 < n_2 < \dotsb$ con $\lim_{j \to ∞} \norm[1]{f_{n_j}}_{X^*} = ∞$. Así, para un $x ∈ X$ podemos ver que $\pesc{f_{n_j}, x} = (1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) \pesc{g_{n_j}, x}$. Tomando valores absolutos, podemos estimar y ver que
\begin{align*}
\abs{\pesc{f_{n_j}, x}}
	&= \abs{(1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) \pesc{g_{n_j}, x}} ≤ \\
	&≤ (1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) \norm[1]{g_{n_j}}_{X^*} \norm{x}_X ≤ \\
	&≤ (1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) C \norm{x}_X  \\
\frac{\abs{\pesc{f_{n_j}, x}}}{1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}}
	&≤ C\norm{x}_X  \\
\frac{\abs{\pesc{φ_{n_j}, x}}}{1 + ε_{n_j} \frac{1}{\norm[1]{f_{n_j}}_{X^*}}}
	&≤ C\norm{x}_X  \\
\end{align*} definiendo \[
	φ_{n_j} ≝ \frac{f_{n_j}}{\norm[1]{f_{n_j}}_{X^*}} ∈ \adh{\bola}_1(0)  ⊂ X^*
\]

El denominador de ahí se irá a cero,a así que para cada $j$ elegimos un $x_j$ con $\norm{x_j}_{X} = 1$, y el producto escalar es mayor que un medio y al final contradicción.

\end{problem}

\begin{problem}[11] Consideramos $X$ un espacio de Banach real, un operador $\appl{A}{D(A)}{X^*}$ en general no lineal con $D(A) ⊂ X$ y monótono: $∀x,y ∈ D(A)$ se tiene que $\pesc{A(x) - A(y), x - y} ≥ 0$. Demuestra que:

\ppart Sea $x_0 ∈ \inter D(A)$. Entonces existen dos constantes $R, C > 0$ tales que si $x ∈ \bola_R(x_0) ⊂ D(A)$, entonces $\norm{A(x)}_{X^*} ≤ C$. \textit{Indicación: Argumentar por reducción al absurdo suponiendo que $x_n \to x_0$ y $\sup_{n}  \norm{A(x_n)}_{X^*} = ∞$ y aplicar el ejercicio anterior.}

\ppart Deducir que si $\appl{T}{X}{\dual[X]}$ es lineal con la propiedad \[ \pesc{T(x), x)} ≥ 0 \quad ∀x ∈ X \] entonces $T$ es acotado.

\ppart Mostrar mediante un ejemplo que si $T$ es como en el apartado anterior pero sólo en un subespacio $D(T)$ la conclusión ya no es válida, incluso aunque $D(T)$ sea denso.

\solution

\spart

\end{problem}

\begin{problem}[12] Consideremos el espacio \[ \ell^1 = \set{\set{a_n}_{n ≥ 1} \tq \sum_{n≥1} \abs{a_n} < ∞ }\] de las sucesiones numéricas sumables. Sea así mismo \[ \ell^∞ = \set{\set{μ_n}_{n ≥ 1} \tq \sup_{n ≥ 1} \abs{μ_n} < ∞ } \] el espacio de las sucesiones numéricas acotadas. Probar que:

\ppart $\ell^∞ ⊂ \dual[\ell^1]$ en el siguiente sentido: si $\set{μ_n} ∈ \ell^∞$ está fijada y $\set{a_n} ∈ \ell^1$, entonces \[ λ(\set{a_n}) ≝ \sum_{n ≥ 1} a_n μ_n = \pesc{λ, \set{a_n}}\] cumple $\abs{λ(\set{a_n})} ≤ \norm{μ_n}_∞ \norm{a_n}_1$; identificamos entonces $\set{μ_n}$ con el funcional acotado λ.

\ppart Sea $e_n = \set{δ_{nj}}_{j ≥ 1}$ para $n = 1, 2, \dotsc$ y sea $μ_n = λ(e_n)$ con $λ ∈ \dual[\ell^1]$. Entonces $\set{μ_n} ∈ \ell^∞$ y si $\set{a_n} ∈ \ell^1$ entonces $λ(\set{a_n}) = \sum_{n≥1} a_n μ_n$. De este modo todo funcional en $\dual[\ell^1]$ se representa por una sucesión acotada y por lo tanto $\dual[\ell^1] = \ell^∞$.

\ppart $\ell^1$ es separable pero $\ell^∞$ no lo es. \hint{Construir en $\ell^∞$ un conjunto no-numerable de sucesiones de norma $1$ y tales que la diferencia de dos de ellas distintas sea también de norma $1$.}

\solution

\spart

Es claro que \[ \abs{λ(\set{a_n})} = \abs{\sum_{n≥1} a_n μ_n} ≤ \sum_{n≥1} \abs{a_n} \abs{μ_n} ≤ \sum_{n ≥ 1} a_n \sup μ_n = \norm{a_n}_1 \norm{μ_n}_∞ \]

\spart

Operando por linealidad de $λ$, vemos que \[ \sum_{n ≥ 1} a_n μ_n = \sum_{n≥1} λ(e_n a_n) = λ \left(\sum_{n≥1} e_n a_n \right) = λ(\set{a_n})\] ya que si sumamos las sucesiones $a_0 e_0 = \set{a_0, 0, \dotsc}$, $a_1 e_1 = \set{0, a_1, 0, \dotsc}$ al final nos sale $\set{a_n}$.

\spart

\end{problem}


\begin{problem}[13] Sean $E, F$ espacios de Banach y $\appl{a}{E×F}{\kbb}$ una aplicación bilineal tal que \begin{align*}
a(x,·) &∈F^*\;∀x ∈ E \\
a(·,y) &∈E^*\;∀x ∈ F
\end{align*}

Demuestra que entonces existe una constante $C < ∞$ tal que \[ \abs{a(x,y)} ≤ C \norm{x}_E \norm{y}_F \quad ∀x ∈ E,\, ∀y ∈ F \] (en otras términos, bilineal y continua en cada variable implica continua conjuntamente).

\solution

Para la resolución de este ejercicio trataremos de aplicar el principio de acotación uniforme. Sea \[ \mathcal{F} = \set{ a(x, ·) \tq x ∈ \adh{\bola_1}(0) ⊂ E} ⊂ F^* \]

Dados $x ∈ E\setminus\set{0}$ y $w ∈ F$, queremos ver cuánto vale $\abs{a(x,w)}$. Dado que $a(x,·) ∈ F^*$, tendremos que \[ \abs{a(x,w)} ≤ \norm[0]{a(x, ·)}_{F^*} \norm{w}_F \]

Dado que $x ≠ 0$ y $a$ es lineal en $x$, podemos ver que $a(x, ·) = \norm{x}_E a\left(\frac{x}{\norm{x}_E}, ·\right)$ y entonces lo que nos queda es que  \[ \abs{a(x,w)} ≤ \norm{a\left(\frac{x}{\norm{x}_E}, ·\right)}_{F^*} \norm{x}_E \norm{w}_F \]

Podremos estimar esa cantidad tomando entonces el supremo para todos los $z ∈ \adh{\bola_1}(0)$, y entonces tendremos que \( \label{eq:H2:E13} \abs{a(x,w)} ≤ \left(\sup \set{ \norm{a(z,·)}_{F^*} \tq z ∈ \adh{\bola_1}(0) ⊂ E}\right) · \norm{x}_E \norm{w}_F \)

Supongmaos que ese supremo de ahí sea infinito. Como $F$ es completo, usando el \nref{thm:AcotacionUniforme} existiría un cierto conjunto $F_0 ⊂ F$, un $G_δ$ contenido en $F$ y tal que para cualquier $w ∈ F_0$ se tenga que $\sup \set{ \norm{a(z,w)}_{F^*} \tq z ∈ \adh{\bola_1}(0) ⊂ E} = ∞$. Pero dado un $w_0 ∈ F_0$, si consideramos la aplicación $a(·, w_0) ∈ E^*$ tendríamos que $\sup \set{\abs{a(z, w_0)} \tq z ∈ \adh{\bola_1}(0) ⊂ E} = ∞$, contradicción porque entonces $a(·,w_0)$ no sería funcional lineal y continuo en $E^*$.

Así, existe una constante $C < ∞$ que acota uniformemente a todos los operadores y que además vale también para $x = 0$.

\end{problem}

\begin{problem}[16] Demostrar que
\ppart Si $X$ es uniformemente convexo entonces también lo es estrictamente.
\ppart Si $X = (L^p(Ω,\algbA, μ), \norm{}_2)$ con $p = 1$ ó $p = ∞$ y $(Ω,\algbA, μ)$ un espacio de medida, entonces $X$ no es uniformemente convexo (excluir trivialidades como espacios Ω que consten de un único punto).
\ppart Si $X = (L^2(Ω, \algbA, μ), \norm{}_2)$ con $(Ω, \algbA, μ)$ un espacio de medida, demostrar que para $f,g ∈ X$ entonces \[ \norm{\frac{f+g}{2}}_2^2 + \norm{\frac{f-g}{2}}_2^2 = \frac{\norm{f}_2^2 + \norm{g}_2^2}{2} \]

Deducir que $X$ es uniformemente convezo.
\solution

Primero, las definiciones:

\begin{defn}[Espacio\IS estrictamente convexo] Un espacio de Banach $X$ se dice estrictamente convexo si dados $x, y ∈ X$ con $\norm{x} = \norm{y} = 1$ y $x ≠ y$, entonces $\norm{(1-t)x + ty)} < 1\; ∀t ∈ (0,1)$.
\end{defn}

\begin{defn}[Espacio\IS uniformemente convexo] \label{def:EspacioUnifConvexo} Un espacio de Banach $X$ se dice uniformemente convexo si dados $x, y ∈ X$ con $\norm{x} = \norm{y} = 1$ y $x ≠ y$ y $∀ε > 0$, si $\norm{x-y} > ε$ entonces existe un $δ > 0$ tal que \[ \norm{\frac{x+y}{2}} < 1 - δ\]
\end{defn}

Y un lema que nos será útil.

\begin{lemma} \label{lem:ConvexaMaximo} Sea $\appl{φ}{[a,b]}{ℝ}$ convexa con $φ(a) = φ(b) = L$. Si  $∃c ∈ (a,b)$ con $φ(c) ≥ L$, entonces $φ$ es constante en $[a,b]$.
\end{lemma}

\begin{proof}
Lo primero que vamos a ver es que no puede ser que $φ(c) > L$, sólo que $φ(c) = L$. Dado que $c ∈ (a,b)$ podemos escribirlo como $c = (1-t) a + tb$ para algún $t ∈ (0,1)$.

Como $φ$ es convexa, entonces \[ L ≤ φ(c) = φ((1-t) a + tb) \eqreasonup[≤]{φ convexa} (1-t) φ(a) + tφ(b) = L\], luego $φ(c) = L$.

Sea ahora $a < x < c$, y supongamos que $φ(x) < L$. Como $c ∈ (x,b)$, entonces podemos escribir $c = (1-s)x + sb$ para $s ∈ (0,1)$ y haciendo lo mismo que antes \[ L= φ(c) = φ((1-s)x + sb) = (1-s)φ(x) + sφ(b) < L \], contradicción luego $∀x ∈ (a,c)$ tenemos que $φ(x) = L$. Haciendo lo mismo con $x ∈ (c,b)$ llegaríamos a lo mismo y listos.
\end{proof}

En otras palabras, este lema dice que si la función es convexa (es decir, que se va hacia abajo y luego sube) y en algún punto alcanza $L$, entonces la única posibilidad es que sea constante. Ver dibujito correspondiente.

\spart

Consideramos $φ(t) = \norm{z(t)}$ con $t ∈ [0,1]$ y $z(t) =(1-t)x + ty$. Está claro que $φ(0) = φ(1) = 1$. Vamos a demostrar que φ es convexa.

Sean $0 ≤ t ≤ x ≤ s ≤ 1$. Como  $x ∈ [t,s]$ existe un $α ∈ [0,1]$ con $x = (1-α)t + αs$, de tal forma que $z(x) = (1-α)z(t) + α z(s)$. Entonces, usando la desigualdad triangular \[ φ(x)  ≤ (1-α)\norm{z(t)} + α \norm{z(s)} = (1-α)φ(t) + αφ(s) \] luego efectivamente φ es convexa.

Ahora vemos que si $X$ es uniformemente convexo y $φ(t) ≥ 1$ para algún $0 < t < 1$, aplicando el \fref{lem:ConvexaMaximo} tendríamos que $φ(t) = 1$. En particular, $1 = φ(\sfrac{1}{2}) < 1 - δ$ por ser $X$ uniformemente convexo, lo que es una contradicción por alguna razón.

\spart

\spart

Vemos que \[ \norm{f\pm g}^2_2 = \int_Ω \abs{f\pm g}^2 = \int_Ω (f \pm g)\conj{(f \pm g)} = \int \abs{f}^2 + \abs{g}^2 \pm (g\conj{f} + \conj{g}f)\]

Sumando ambos, se nos va el $\pm$: \[ \norm{f+g}_2^2 + \norm{f-g}_2^2 = 2(\norm{f}_2^2 + \norm{g}_2^2)\]

Si $w > \norm{f-g}_2^2 > ε > 0$ y $\norm{f}_2^2 = \norm{g}_2^2 = 1$, entonces \[ \norm{\frac{f+g}{2}}_2^2 = \frac{\norm{f}_2^2 + \norm{g}_2^2}{2} - \frac{\norm{f-g}_2^2}{4} < 1 - \frac{ε^2}{4} \], y entonces $\norm{\frac{f+g}{2}} < \sqrt{1 - \frac{ε^2}{4}} = 1 - δ$ con $0 < δ = 1 - \left(1 - \frac{ε^2}{4}\right)^{\sfrac{1}{2}}$.

\end{problem}

\section{Hoja 3}

\begin{problem} Sea $X$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ y $\appl{p}{X}{ℝ^+}$ una función en $X$ que cumple \begin{align*}
p(tx) &= tp(x) & x ∈ X, t≥ 0 \\
p(x+y) ≤ p(x) + p(y) & x,y ∈ X
\end{align*}

En adelante, a $p(x)$ lo llamaremos \index{Funcional!de gauge}\textbf{funcional de gauge}.

Sea $E ⊂ X$ un subespacio y $\appl{λ}{E}{ℝ}$ un funcional lineal \textit{real} que cumple $λ(x) ≤ p(x)\;∀x ∈ E$. Probar que $λ$ admite una extensión a un funcional lineal real en $X$ tal que $λ(x) ≤ p(x) \;∀x ∈ X$.

\textit{Indicación: La prueba del \nref{thm:HahnBanach} se puede repetir punto por punto con ligeras modificaciones.}

\solution

\end{problem}

\begin{problem}[2] \label{ej:Hoja3:2}
Sea $X = (V, \norm{·})$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ y normado. Sea λ un funcional lineal no nulo en $X$. Un hiperplano afín es un subconjunto $E_α$ de $V$ de la forma $E_α = \inv{λ}(α)$ para algún $α ∈ \kbb$ (esto es, una hipersuperficie de nivel de ese funcional). Demostrar:

\ppart $E_α$ es convexo.
\ppart Siempre podemos suponer $α ∈ ℝ$.
\ppart Si λ es continuo, $E_α$ es cerrado.
\ppart Si $E_0$ es cerrado, $E_α$ es cerrado para todo α.
\ppart Si $X' = (V', \norm{·})$ es una compleción de $X$ (esto es, $V ⊂ V'$ con $V$ denso en $V'$ y $X'$ de Banach) y $E_α$ es cerrado en $V$, lo sigue siendo en $V'$.

\solution

\spart

Sean $x, y ∈ E_α$. Para demostrar que $E_α$ es convexo hay que demostrar que $∀t ∈ [0,1]$ se tiene que $(1-t) x + ty  ∈ E_α$. Si $x, y ∈ E_α$ entonces $λ(x) = λ(y) = α$, y por lo tanto \[ λ\left((1-t) x + ty\right) = λ((1-t)x) + λ(ty) = (1-t) λ(x) + t λ(y) = α \], por lo que todo el segmento $[x,y]$ está en $E_α$ y por lo tanto es convexo.

\spart

Si $α ∉ ℝ$, entonces es de la forma $α = r e^{iθ}$ con $r > 0$ y $θ ∈ (0, 2π)$. En ese caso, podemos multiplicar el funcional λ por el escalar $τ = e^{iθ}$, de tal forma que $\tilde{λ}(x) = τλ(x)$ y $\inv{\tilde{λ}}\left(\frac{α}{τ}\right) = \inv{λ}(α)$, con $\frac{α}{τ} = r e^0 ∈ ℝ$.

El funcional $\tilde{λ}$ es simplemente una rotación de λ: sigue manteniendo la misma norma: $\norm[1]{\tilde{λ}(x)} = \norm[1]{e^{iθ} λ(x)} = 1 · \norm{λ(x)}$. Al ser una multiplicación por un escalar sigue manteniendo las propiedades de linealidad de λ.

\spart

Para demostrar que $E_α$ es cerrado, se puede ver que su complementario $E_α^c$ ha de ser abierto. Tomamos $x ∈ E_α^c$ tal que $λ(x) ≠ α$. Sea $ε > 0$ tal que $ε < \abs{λ(x) - α}$. Por ser λ continuo, existe un $δ > 0$ tal que si $y ∈ \bola_δ(x)$ entonces $\abs{λ(x) - λ(y)} < ε < \abs{λ(x) - α}$. Es decir, que $λ(y) ≠ α$ (si fuera igual, ε sería 0 y hemos dicho que es estrictamente positivo) y entonces $y ∉ E_α$.

En resumen, para cualquier $x ∈ E_α^c$ podemos encontrar una bola abierta $\bola_δ(x)$ con $δ > 0$ tal que $\bola_δ(x) ⊂ E_α^c$, luego $E_α^c$ es abierto y por lo tanto $E_α$ es cerrado.

\spart

Si $E_α = ∅$, entonces es cerrado y no hace falta probar nada. Si no es vacío, entonces $∃ a ∈ E_α$ con $λ(a) = α$, y podemos expresar $E_α = E_0 + a$, que es cerrado.

\spart

Vamos a hacer la caracterización por sucesiones, buscando ver que no existen sucesiones cuyo límite está fuera de $E_α$ cuando lo miramos dentro de $V'$. Las únicas sucesiones problemáticas son las que convergen fuera de $V$ (si alguna convergiese con $V$ y no tuviese límite en $E_α$ entonces éste no sería cerrado).

\spart


\end{problem}

\begin{problem} Sea $V$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ y $C ⊂ V$ convexo. Decimos que $C$ es un \index{Espacio!balanceado}\textbf{espacio balanceado} si dados $x ∈ C$, $λ ∈ \kbb$ con $\abs{λ} = 1$ entonces $λx ∈ C$. Diremos que será un \index{Espacio!absorbente}\textbf{espacio absorbente} si $\bigcup_{t > 0} tC = V$. El \index{Funcional!de Minkowsky}\textbf{funcional de Minkowsky} o gauge de $C$ es \( \label{eq:FuncMinkowsky} p(x) ≝ \inf \set{λ > 0 \tq x ∈ λC} \)

Demuestra que:

\ppart Si $t ≥ 0$, $p(tx) = tp(x)$ para el gauge de un conjunto $C$ absorbente con $0 ∈ C$.
\ppart

\solution

\spart

\spart

\end{problem}

\begin{problem} Sea $X$ como en el \fref{ej:Hoja3:2} y sean $A$, $B$ dos subconjuntos de $V$ no vacíos. Consideramos un hiperplano $E = E_α = \inv{λ}(α)$ con $α ∈ℝ$ para λ un funcional real, tanto si $\kbb = ℝ$ como si es $ℂ$. Decimos que $E$ separa $A$ y $B$ si \begin{align*}
λ(x) ≤ α & \quad ∀x ∈ A \\
λ(x) ≥ α & \quad ∀x ∈ B \\
\end{align*}

Si en cambio para cierto $ε > 0$ se cumple que  \begin{align*}
λ(x) ≤ α - ε & \quad ∀x ∈ A \\
λ(x) ≥ α + ε& \quad ∀x ∈ B \\
\end{align*} decimos que $E$ separa estrictamente $A$ y $B$.

Queremos probar el \nref{thm:HahnBanachGeom1} siguiendo los siguientes pasos.

\ppart Si $E = \inv{λ}(α)$ con $α ∈ ℝ$ es un hiperplano cerrado, entonces los conjuntos \begin{align*}
A_α &= \set{x ∈ V \tq λ(x) ≤ α} \\
B_α &= \set{x ∈ V \tq λ(x) ≥ α}
\end{align*} son ambos convexos y cerrados con frontera común $E$.

\solution

Reproducimos el enunciado del teorema:

\begin{theorem}[Teorema\IS de Hahn-Banach (Primera forma geométrica)] \label{thm:HahnBanachGeom1} Sea $X = (V, \norm{·})$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ normado, y sean $A,B ⊂ V$ convexos, disjuntos y uno de ellos abierto. Entonces, existe un hiperplano cerrado $E$ que los separa.
\end{theorem}

\spart

Veamos que son convexos: sean $x, y ∈ A_α$ y $t ∈ [0,1]$. Queremos ver que $tx + (1-t)y ∈ A_α$. Operamos y vemos que \[ λ(tx + (1-t)y) = tλ(x) + (1-t) λ(y) ≤ tα + (1-t) α = α \] y se puede probar análogamente que $B_α$ es convexo.

Para ver que son cerrados, sea $\set{x_n}_{n ∈ ℕ} ⊂ A_α$ de Cauchy con $x_n \to x ∈ V$. Queremos ver que $x ∈ A_α$. Sabemos que $\set{λ(x_n)}_{n ∈ ℕ}$ es igualmente de Cauchy en $ℝ$ así que $α ≥ c \lim λ(x_n) = λ(\lim x_n) = λ(x)$, listos.

Lo de la frontera me parece demasiado obvio.

\end{problem}


\begin{problem}[5] Sea $E ⊂ V$ con $X = (V, \norm{·})$ un espacio normado. Demostrar que si $E$ es convexo, su cierre en la topología débil y en la fuerte coinciden. \hint{Basta probar que si $\adh{E}$ es el cierre de $E$ en la topología fuerte, su complementario $E^c$ es abierto en la topología débil, para lo cual basta aplicar el \nref{thm:HahnBanachGeom1}}

Probar, sin embargo, que la afirmación anterior es falsa si $\dual$ es un espacio dual en el cual damos la topología débil-$*$. \hint{¿Qué ocurre con $\bola_X$ si $X$ no es reflexivo?}

\solution


\end{problem}
