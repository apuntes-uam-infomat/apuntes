% -*- root: ../EstadisticaII.tex -*-

\chapter{Regresión}
El objetivo de la regresión es predecir una/s variable/s en función de la/s otra/s.


\section{Regresión lineal}

Observamos dos variables, X e Y , el objetivo es analizar la relaci´on existente entre ambas, de forma que podamos predecir o aproximar el valor de la variable Y a partir del valor de la variable X.

\begin{itemize}
\item La variable Y se llama variable respuesta.
\item La variable X se llama variable regresora o explicativa.
\end{itemize}

Por ejemplo:
\begin{center}
\includegraphics[scale=0.5]{img/RentaVsFracaso.png}
\end{center}

Queremos predecir el fracaso escolar en función de la renta. La variable respuesta es el fracaso escolar, mientras que la variable regresora es la renta. 

\subsection{Regresión lineal simple}

Frecuentemente existe una relación lineal entre las variables. En el caso del fracaso escolar,queremos construir una recta $Y_i = β_0 X_i + β_1\; i=1,...,n$ que minimice el error.

El problema es estimar los parámetros $β_0,β_1$. Una manera de hacer esto es:

\subsubsection{Recta de mínimos cuadrados}

\begin{defn}[Recta de mínimos cuadrados]
Estimando $β_i$ por $\hat{β_i}$ obtenemos: \[\hat{Y_i} = \hat{β}_0 + \hat{β}_1 x_i\]

La reca viene dada por los valores $\hat{β_0}, \hat{β_1}$ para los que se minimiza el error cuadrático, es decir:
\[\sum_{i=1}^n \left(Y_i - \hat{Y_i}\right)^2 =  \sum_{i=1}^n \left[ Y_i - (\hat{β_0} + \hat{β_1}x_i) \right]^2\]
\end{defn}

\begin{example}
\begin{center}
\includegraphics[scale=0.6]{img/ejemploRectaRegresionLineal.png}
\end{center}
\end{example}

\paragraph{Cómo calcular la pendiente} de la recta de mínimos cuadrados.


Vamos a ver unas pocas maneras de calcular la recta de mínimos cuadrados.

\begin{itemize}

	\item El sistema habitual:

	\[ \hat{β_1} = \frac{\sum_{i=1}^n(x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{S_{xy}}{S_{xx}} \]
	Donde 
		\[S_{xy} = \sum_{i=1}^n(x_i - \bar{x})(Y_i - \bar{Y}) \] 
		\label{Ssubxx}
		\[S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2\]
	\subitem \[β_0 = \bar{Y} - β_1\bar{x}\]

	\textbf{Entonces:}
	\[\text{recta} \equiv y - \bar{y} = \frac{S_{xy}}{S_{xx}}(x - \bar{x} ) \]

	\item Mínimos cuadrados como promedio de pendientes:
	\label{rmc::promediopendientes}
	\[
	\hat{β_1} = \frac{S_{xy}}{S_{xx}} = \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{S_{xx}} \left( \frac{(Y_i - \bar{Y})}{x_i - \bar{x}} \right) = \sum_{i=1}^n ω_i \left( \frac{(Y_i - \bar{Y})}{x_i - \bar{x}} \right)
	\]

	Vemos que hemos ponderado la pendiente de cada recta que une cada punto con la media. Este peso es mayor cuanto mayor es la distancia \textbf{horizontal}. 

	\item Mínimos cuadrados como promedio de respuestas:

	\[
	\hat{β_1} = \frac{\sum_{i=1}^n  (x_i - \bar{x}) (Y_i - \bar{Y})}{S_{xx}} = \sum α_i Y_i
	\]
	Es interesante ver unas propiedades de estos $α_i$
	\begin{prop}


		\begin{itemize}
			\item[] $\sum α_i = 0$
			\item[] $\sum α_ix_i = 1$
			\item[] $\sum α_i^2 = \frac{1}{S_{xx}}$
		\end{itemize}

	\end{prop}
	\begin{proof}
	\textcolor{red}{Por hacer}
	\end{proof}

\end{itemize}


\begin{defn}[Residuo]
En una recta de mínimos cuadrados: Sea $y_i = β_1x_i - β_0$ y sea $\hat{y}_i = \hat{β}_1x_i - \hat{β}_0$, llamamos residuo a $$e_i = y_i - \hat{y}_i$$

Los residuos cumplen:

\[
\sum_{i=1}^n e_i = 0
\]

Esto es intuitivo, ya que los errores se compensan y además es una buena propiedad.
\end{defn}



\begin{prop}
Sean $\{e_i\}$ una variable aleatoria que cumple \footnote{Se ha utilizado la $e$ porque es útil en cuanto a los residuos de la recta de mínimos cuadrados}:
\[\sum e_i = 0\]

Entonces:
\[\sum e_i x_i = 0 \implies \cov(e,x) = 0\]
\end{prop}

\begin{proof}
\[\sum (e_i - \vec{µ}) x_i = \sum (e_i - \vec{µ}) (x_i - \vx) \]
Por otro lado:

\[
\sum e_ix_i = \sum e_ix_i - \vx \sum e_i = \sum e_i(x_i - \vx)
\]
\end{proof}


\begin{example}
\[
\sum (x_i - \vx)(y_i -\vy) = \sum (x_i - \vx)y_i - \sum \vy\sum(x_i - \vx) \overset{(1)}{=} \sum(x_i - \vx y_i)
\]
\[
(1) \to \sum(x_i - \vx) = 0
\]
\end{example}

Esto tiene la siguiente explicación ``intuitiva'': La recta de mínimos cuadrados contiene toda la información lineal que $X$ puede dar sobre $Y$ (debido a que la covarianza entre los residuos y $X$ es 0).


\subsubsection{Fallos de la recta de mínimos cuadrados}

Vamos a ver un par de ejemplos ilustrativos:

\begin{example}[Sobre los datos atípicos]

Esta es una recta de mínimos cuadrados calculada para una nube de puntos a la que se ha añadido un punto atípico. Se ve una cierta tendencia de que la pendiente debería ser positiva, pero el dato atípico provoca un cambio brusco.
\begin{center}
%\includegraphics[scale=0.9]{img/rmc_atipico1.png}
\includegraphics[scale=0.9]{img/rmc_atipico2.png}
\end{center}

\end{example}

\begin{example}[Sobre la distancia horizontal]

¿Y da igual lo atípico que sea un dato? La respuesta es que no. Si el dato es muy atípico en la variable respuesta ($Y$), pero es muy \textit{típico} en la variable regresora, la recta no se devía tanto. Vamos a verlo y después explicamos la razón.

Esta es la recta, en la que hemos ignorado los 3 datos que parecen ``atípicos''.
\begin{center}
\includegraphics[scale=0.9]{img/sobredistanciahorizontal1.png}
\end{center}

Ahora calculamos las rectas teniendo en cuenta sólo uno de los puntos.

\begin{center}
\includegraphics[scale=0.4]{img/sobredistanciahorizontal1.png}
\includegraphics[scale=0.4]{img/sobredistanciahorizontal2.png}
\end{center}

Vemos que la recta azul no se desvía apenas de la original, mientras que la recta verde si se desvía un montón. ¿Esto a qué se debe? A que importa más la distancia horizontal de la media que la distancia vertical. Si vamos a la expresión de la recta de mínimos cuadrados como promedio de las pendientes \label{rmc::promediopendientes} vemos que hay un término $\frac{(x_i - \gor{x})}{S_{xx}}$ que hemos tomado como pesos para ponderar y en este caso, la distancia horizontal $(x_i - \gor{x})$ está multiplicando en el numerador.



\end{example}





\subsubsection{Introduciendo ``aleatoreidad'' para poder hacer IC}

Sea $\{ε_i\}$ siendo $ε_i \sim N(0,σ^2)$. Lo habitual es no saber cómo han sido generados los datos y es probable que no vayamos a conocer con exactitud absoluta la recta de mínimos cuadrados. Es por ello que suponemos el siguiente modelo para la variable respuesta:

\[
Y_i = β_1 x_i + β_0 + ε_i
\]


Tenemos que $\bar{y}_i \sim N$, ya que es una combinación lineal de variables normales \textbf{independientes} (como vimos en el Tema 1). 


\begin{example}
Sea $σ=1, β_0 = 0 y β_1 = 1$.

Entonces el modelo es:

\[
Y_i = x_i + ε_i
\]

Fijamos $n=10$ y generamos las respuestas para $x_i = i$. Además, repetimos el experimento 6 veces y calculamos las rectas de mínimos cuadrados, obteniendo:

\begin{center}
\includegraphics[scale=0.6]{img/6ejemplosRegresion.png}
\end{center}

Vemos que obviamente las rectas no son las mismas. Esto se debe al $ε_i$ introducido. ¿Cuáles son los valores que toman $β_1$ y $β_0$? Habiendo repetido el experimento 1000 veces, obtenemos los siguientes histogramas:

\begin{center}
\includegraphics[scale=0.3]{img/1000vecesb0.png}
\includegraphics[scale=0.3]{img/1000vecesb1.png}
\end{center}

Vemos que no siempre es el mismo valor. Sabemos (por cómo hemos construido los datos) que $β_0 = 0$ y $β_1 = 1$, pero nuestra manera de calcularos (debido a $ε_i$) no siempre nos da el valor concreto. 


\end{example}

El ejemplo anterior nos muestra que en realidad, estamos estimando $β_i$, aunque no nos guste y ahora tenemos que planternos ¿cómo de buenos son nuestros estimadores? Tal vez son una mierda, o tal vez son insesgados. 

Para ello, vemos que al haber añadido un error $\epsilon_i \sim N(0,σ^2)$, tenemos:

\[
Y_i = β_0 + β_1x + ε_i \implies Y_i \equiv N(β_0 + β_1X_i, σ^2)
\] 


\subsubsection{Estimando $β_1$}

\begin{prop}
Nuestro estimador ``pendiente de la recta de mínimos cuadrados:'' $\hat{β_1}$  cumple

\[
\hat{β_1} \equiv N\left(β_1,\frac{σ^2}{S_{xx}} \right)
\]

\end{prop}

\begin{proof}
Él en clase lo ha hecho al revés. Muchos cálculos para llegar a la conclusión, pero aquí molamos más. En algún momento \textcolor{red}{revisará} alguien los apuntes y completará.

\begin{itemize}
	\item $\esp{\hat{β_1}} = β_1$
	\item $\var{\hat{β_1}} = ... = \displaystyle\frac{σ^2}{S_{xx}}$
\end{itemize}
\end{proof}

\subsubsection{Estimando $β_0$}

\begin{prop}
Nuestro estimador ``término independiente de la recta de mínimos cuadrados:'' $\hat{β_0}$  cumple

\[
\hat{β_0} = N\left(β_0 , σ^2 \left( \frac{1}{n} + \frac{\gor{x}^2}{S_{xx}}\right)  \right)
\]
\end{prop}

\begin{proof}
\begin{itemize}
	\item $\esp{\hat{β_0}} = β_0$
	\item 
	$\var{\hat{β_0}} = \var{\gor{Y}} + \var{\hat{β_1}\gor{X}} - 2 \cov\left(\gor{Y},\hat{β_1}\gor{X}\right)$

 	\subitem Calculamos: $\cov\left(\gor{Y},\hat{β_1}\gor{X}\right)$ utilizando cosas del tema 1

 	\[
		\cov\left(\gor{Y},\hat{β_1}\gor{X}\right) = \cov\left(\frac{1'_n \gor{Y}}{n},α\gor{Y}\right) = \frac{1}{n}1'_nσ^2
 	\]
 	debido a que $α = 0$.

 	Ademas de ser incorrelados, son \textbf{independientes}. ¿Porqué? Porque conjutamente son normales, es decir \[
 		\begin{pmatrix} \gor{Y} \\ \hat{β_1} \end{pmatrix} \equiv A\gor{Y} \equiv N_2
 	\]
\end{itemize}

\end{proof}


\textbf{Conclusiones:}
\begin{align*}
\gor{Y} &\text{ es indepediente de } \hat{β_1}\\
\hat{β_1} &\equiv \left(β_1,\frac{σ^2}{S_{xx}}\right)\\
\hat{β_0} &\equiv \left(β_0,σ^2 \left( \frac{1}{n} + \frac{\vec{x}^2}{S_{xx}}\right)\right)
\end{align*}

¿Son estas las variables $\hat{β_1} $ y $\hat{β_2}$ normales una normal conjunta? No, \textbf{no son una normal conjunta} ya que no son independientes. Intuitivamente es fácil de ver. En una recta, si aumentamos la pendiente (y estamos en el primer cuadrante) entonces el término independiente disminuye. Esta dependencia tiene que aparecer. Vamos a estudiar la covarianza entre los estimadores:

\[
\cov\left(β_1,β_2\right) = \cov\left(\gor{Y} - \hat{β_1}\gor{x}, \hat{β_0}\right) = ... = -\gor{x}\frac{σ^2}{S_{xx}}
\]



\subsubsection{IC y Contrastes para $β_1$}

Recordamos que \[ \hat{β}_1 \equiv N\left(β_1,\frac{σ^2}{S_{xx}}\right)\]

Podemos normalizar y buscar una catidad pivotal (como hacíamos en estadística I)

\[
\frac{\hat{β_1} - β_1}{\frac{σ}{S_{xx}}} \equiv N\left(0,1\right)
\]

Pero aquí nos encontramos con que necesitamos $σ$, la varianza de los errores. Esta varianza a menudo no es conocida (porque no sabemos con exactitud cuál es la recta verdadera) y tenemos que estimarla. 

Para estimarla, parece razonable usar \[ \hat{σ} = S_R =\frac{\sum_{i=1}^n e_i^2}{n-2}\] 

\begin{expla}
Recordamos que para que estimar la varinza, utilizamos (por el lema de fisher) $n-1$ de denominador para que el estimador sea insesgado. Esto sale de que en la demostración, hay una matriz de rango $n-1$ ya que existe una restricción.

Siguiendo este razonamiento, en este caso tenemos 2 restricciones\footnote{$\sum e_i = 0$ y $\sum e_ix_i = 0$}, por lo que si lo demostráramos rigurosamente, aparecería una matriz de rango $n-2$ y por eso es el denomiador. De esta manera, conseguimos un estimador insesgado.

\end{expla}

Además, $S_R$ se denomina \concept{varianza residual}

\begin{prop}
Una pequeña generalización del lema de Fisher:
\[
\frac{(n-2)S_{R}^2}{σ^2} \equiv \chi_{n-2}^2
\]

Además, es independiente de $\hat{β_1}$

\end{prop}



\begin{proof}
Esta proposición es un caso particular de un teorema que veremos más adelante.
\end{proof}


Ahora que ya tenemos estimada la varianza, podemos calcular:


\[
\frac{\hat{β_1}-β_1}{\frac{S_R}{\sqrt{S_{xx}}}} = \frac{\hat{β_1}-β_1}{\displaystyle\frac{\frac{σ}{\sqrt{S_{xx}}}}{\frac{S_R}{σ}}}
\]

En el numerador tenemos una $N(0,1)$ y en denominador una $\chi^2$ dividida por sus grados de libertad. Esto es por definición de $\mathcal{T}$ \footnote{T de Student} es una $\mathcal{T}$ (T-Student) con $n-2$ grados de libertad.

\begin{prop}
Ahora que conocemos la distribución, podemos calcular el intervalo de confianza  para la pendiente de la recta.

\textcolor{red}{No entiendo nada de esto.}
\[
IC_{1-α}(β_1) \equiv \left[ \hat{β_1} \pm \mathcal{T}_{n-2,\frac{α}{2}}\frac{S_R}{\sqrt{S_{xx}}}\right] \equiv \left[ \gor{Y} \pm \mathcal{T}_{n-1,\frac{α}{2}}\frac{S_R}{\sqrt{n}} \right]
\]
\end{prop}

\subsubsection{Contraste en R}


\begin{lstlisting}
> # Ajusta el modelo
> regresion = lm(Fracaso~Renta)
> summary(regresion)

> lm(formula = Fracaso ~ Renta)
Residuals:	Min	1Q	Median	3Q	Max
		-7.8717 -3.7421	0.5878	3.0368	11.5423
---
Coefficients:	Estimate Std.	Error 	t-value	Pr(>|t|)
(Intercept)	38.4944		3.6445	10.562	7.37e-10 ***
Renta 		-1.3467		0.2659	-5.065	5.14e-05 ***
---
Signif. codes: [...]
Residual standard error: 4.757 on 21 degrees of freedom
Multiple R-Squared: 0.5499,
Adjusted R-squared: 0.528
\end{lstlisting}


Aquí, la fila de intercept es el término independiente y renta es la pendiente. Además, los p-valores son para el contraste $\hat{β_i} = 0$, dentro de la hipótesis $β_i \geq 0$. \footnote{Si queremos contrastar si es positivo, nos vamos al caso límite que lo separa y contrastamos eso}.

En este caso, el p-valor para $\hat{β_1} = 7.37e-10$, con lo que no podemos rechazar la hipótesis.


\subsubsection{Predicciones}

Sea $(x_1,y_1),...,(x_n,y_n) \to y_i = β_0 + β_1x_i + ε_i$.

Dado una nueva observación $x_0$, tenemos 2 problemas para predecir:

\begin{itemize}
	\item \extbf{Inferencia sobre $m_0 \equiv \esp{y_0 | x_0} = β_0 + β_1x_0$}

	En este caso, $$\hat{m_0} = \hat{β_0} + \hat{β_1}x_0$$

	¿Cómo es este estimador?

	\[\esp{\hat{m_0}} = β_0 + β_1x_0 = m_0\]
	\[\var{\hat{m_0}} = ... = σ^2\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}} \right] \]

	\subitem Intuitivamente, lo que significa el segundo sumando de la varianza es que ``cuanto más cerca esté $x_0$ de la media, mejor será la estimación''.

	\textbf{Conclusión:}

	\[
		\hat{m_0} \sim N\left( m_0, σ^2\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}} \right]\right)
	\]



	\subitem \textbf{Intervalo de confianza} para $m_0$ utilizando la fórmula de intervalos de confianza:

	\[
IC_{1-α}(m_0) \equiv \left[ \hat{m_0} \pm \mathcal{T}_{n-2,\frac{α}{2}}S_R\sqrt{\frac{1}{n} + \frac{(x - \gor{x})^2}{S_{xx}}}\right] 
\]

	\item \textbf{Predecir $Y_0$} usamos de nuevo:

	\[
\hat{Y_0} = \hat{β_0} + \hat{β_1}x \to Y_0 - Y \equiv N\left( 0, σ^2\left( 1 + \frac{1}{n}+  \frac{(x-\gor{x})^2}{S_{xx}}\right) \right)
	\]

	Donde la varianza ha sido calculada:

	\[
	\var{Y_0 - \hat{Y_0}} = \underbrace{\var{Y_0}}_{σ^2} - \var{\hat{Y_0}} + \underbrace{2 \cov{Y_0,\hat{Y_0}}}_{ = 0 \text{ (indep.) }} = σ^2 + σ^2\left( \frac{1}{n}+  \frac{(x-\gor{x})^2}{S_{xx}} \right)
	\]


	Este es un problema más complicado, ya que tenemos que tener en cuenta el término de error $ε_i$ y es por esto que aparece el 1 en la varianza. Tenemos que tener en cuenta la incertidumbre.

	Estandarizando y cambiando $σ$ por $S$, tenemos:

	\[
	\frac{Y_0 - \hat{Y_0}}{S_r \sqrt{1 + \frac{1}{n} + \frac{(x-\gor{x})^2}{S_{xx}}}} \equiv \mathcal{T}_{n-2} 
	\]

	Ya que tenemos una normal estandarizada dividida por su .... que por definición, es una $\mathcal{T}$ de student.

	Ahora, vamos a construir el \concept{intervalo de predicción} (cambia ligeramente la interpretación)

	\[
1 - α = P\left\{ -\mathcal{T}_{n-2;\frac{α}{2}} < \frac{Y_0 - \hat{Y_0}}{...} < \mathcal{T}_{n-2;\frac{α}{2}}    \right\} = P \left\{ Y_0 \in \left[ \hat{Y_0} \pm \mathcal{T}_{n-2;\frac{α}{2}} S_R \sqrt{1+\frac{1}{n}+...} \right]  \right\}
	\]
\end{itemize}

Ahora vamos a hacer unos ejemplos numéricos.

\begin{example}Seguimos con el ejemplo de la renta.

\begin{center}
\begin{tabular}{c|c|c}
&media&desviación típica\\\hline
\% fracaso & 20.73 & 6.927\\
renta &13.19 $·10^{3}$ & 3.814
\end{tabular}
\end{center}

\setcounter{part}{0}
\ppart IC para $β_1$ de nivel $95\%$.
\ppart IC para \% de fracaso medio si la renta es de $14.000$ euros.

\spart

\[
-1.3467 \pm \mathcal{T}_{21;0.025} · (0.2659)
\]

Donde el $-1.3467$ es el estimador $\gor{m_0}$ que obtenemos de la salida de $R$. Lo mismo el $0.2659$, que es el error típico.

\spart
\[ \gor{Y_0} = 38.49 - (0.3467) · \underbrace{14}_{x_0} = 19.64\%\]

Siendo este el estimador, vamos a construir el intervalo de confianza. \footnote{Podría ser que nos pidieran el intervalo de predicción, pero en ese caso estarían pidiendo el intervalo de ...... para predecir.}

\[
IC = 19.64 \pm (2.06)(4.757)\sqrt{\frac{1}{23}+\frac{(14-13.19)^2}{S_{xx}}}
\]
Donde $S_{xx} = 320.06$ y podemos calcularlo despejando de cualquiera de las fórmulas:

\[
E.T.(\gor{β_1}) = \frac{S_R^2}{S_{xx}} 
\]
\[
6. = \frac{S_{xx}}{n-1}
\]

\end{example}


\obs Todos estos cálculos y todas estas fórmulas se basan en muchas hipótesis (como que la distribución del error sigue una distribución normal). Pero podría ser que esto no ocurriera y estuviéramos suponiendo un modelo falso. Para ello, en estadística existe el \concept{Diagnóstico del modelo}. Este diagnóstico, consiste en comprobar si las hipótesis del modelo son \textbf{aceptables} para los datos disponibles. ¡Ojo! Aceptable... Puede haber muchos modelos aceptables para un mismo conjunto de datos.

Este diagnóstico se suele basar en el análisis de los residuos del modelo.

\begin{example}
Vamos a ver a ojo unos cuantos ejemplos. Vamos a utilizar que $\cor(e,\gor{y}) = 0$ bajo el modelo (como calculamos anteriormente)

\begin{center}
\includegraphics[scale=0.6]{img/diagmodelo.png}
\end{center}

De estos 4 gráficos, el bueno es el primero, ya que los demás no complen alguno.
\end{example}

\begin{example}
Vamos a ver otro ejemplo, donde arriba están los datos y abajo los residuos. Mirando sólo la fila de arriba podríamos saber si nuestro modelo para la regresión se cumple o sino.


\begin{center}
\includegraphics[scale=0.6]{img/diagmodelo_2.png}
\end{center}

Vemos que el primero y el último si tienen este modelo como aceptable, ya que en los residuos no hay ningún patrón (y se cumple que la correlación es 0).

En el segundo, podríammos suponer que es bueno, pero al diagnosticar el modelo mirando los residuos, vemos que no. El diagnóstico del model \textbf{magnifica los errores}.

En el cuarto, vemos más claro que es heterocedástico y que no se cumple el modelo supuesto.
\end{example}

En regresión múltiple veremos que no podemos ver los datos, ya que son demasiadas variables, pero sí podemos estudiar los residuos como acabamos de hacer en los ejemplos anteriores.

\subsection{Regresión lineal múltiple}

El ejemplo que vamos a estudiar en regresión múltiple es el consumo de gasolina en EEUU intentando predecirlo a partir de unas cuantas variables. Las variables regresoras son:

\begin{center}
\begin{tabular}{ccccccc}
State&Drivers&FuelC&Income&Miles&MPC&Pop&Tax\\\hline
AL&3559897&2382507&23471&94440&12737.00&3451586&18.0\\
AK&472211&235400&30064&13628&7639.16&457728&8.0\\
AZ&3550367&2428430&25578&55245&9411.55&3907526&18.0
\end{tabular}
\end{center}

\subsubsection{Notación}


\begin{itemize}
	\item $n$ es el número de observaciones, en este caso, el número de estados.
	\item $k$ es el número de atributos. 
	\item $ε_i \sim N(0,σ^2)$
	\item $n\geq k+2$: esta hípótesis  es muy necesaria.\footnote{En la estadística, habría que rehacer el modelo para cuando $k>n$. ¿Y cuándo $k>n$? ¿Cuándo puede ocurrir esto? Cada vez más hay más información para cada individuo. En estudios genéticos por ejemplo, que hay millones de genes pero no se pueden hacer el estudio con millones de personas... \textbf{LA MALDICIÓN DE LA DIMENSIONALIDAD} que decimos en Introducción previa a los Fundamentos Básicos del Aprendizaje Automático.\\ Una posible solución al problema es un algoritmo que filtre los atributos que son importantes.}
\end{itemize}

Regresión simple es un caso particular de múltiple, tomando $k=1$.

\subsubsection{Modelo}

El modelo es:

\textcolor{red}{Completar de las traspas}

Podemos agruparlo en forma matricial:

\textcolor{red}{Completar de las traspas}

\paragraph{Recordamos} que en el tema 1 vimos unas cuantas formas cuadráticas útiles para normales multivariantes con matriz de variazas $σ^2I_n$ y media arbitraria.

\textcolor{red}{Completar de las traspas}

¿Cómo estimarías $β$ a partir de $Y$ y $X$?

Podemos hacer la proyección de $Y$ sobre $\mathcal{V}$ 

Con esto, parece razonable estimar $µ$ mediante la proyección ortogonal de $T$ sobre $\mathcal{V}$ para obtener $\gor{Y} = X\gor{β}$. Equivalentemente: $||Y-X\gor{β}||^2 \leq || Y-Xβ||^2, ∀β\in ℝ^{k+1}$


\textcolor{red}{completar cosas que faltan}

\paragraph{Resumen}
Si 
\[ Y \equiv N_n (Xβ,σ^2I_n)\]

entonces, la proyección sobre \_\_\_\_\_\_\_ es:
\[
\hat{Y} = X\hat{β} = HY
\]
donde $H = X(X'X)^{-1}X^{-1}$. Además, \[\hat{β} = (X'X)^{-1}X'Y\]


Esto tiene como consecuencia que el vector de residuos es: $e = Y-\hat{Y} = (I-H) Y$

En cuanto a la interpretación geométrica, los residuos es la recta vertical que une la proyección ($\hat{Y}$) con el vector real ($Y$).

\subsubsection{Distribución de $\hat{β}$}
\[
β \equiv N_{k+1}\left(β,σ^2(X'X)^{-1}\right)
\]

Y la regresión simple, es un caso particular de esta fórmula.

\paragraph{Consecuencias:}

\begin{itemize}
	\item ¿Cuál es la distribución marginal de $\hat{β_j}$ a partir de la que hemos visto de la conjunta? Como vimos en el tema 1, es también una normal, con el correspondiente valor del vector $β$ como media y el elemento $j,j$ de la diagonal.
	\[ β_j \equiv N\left(β_j, σ^2q_{jj}\right)\]

	Ahora, podemos estandarizar:

	\[
	\frac{\hat{β_j}-β_j}{σ\sqrt{q_{jj}}} \equiv N(0,1)
	\]

	Y utilizando que $S_R$ es independiente de $σ$ y la definición de $t-$student tenemos:

	\[
	\frac{\hat{β_j}-β_j}{S_R\sqrt{q_{jj}}} \equiv \mathcal{T}_{n-k-1}	
	\]

	¿Cuál es el intervalo de confianza?

	\[
		IC_{n-α}(β_j) \equiv \left[\hat{β_j}\pm \mathcal{T}_{n-k-1}\underbrace{S_R\sqrt{q_{jj}}}_{\text{Error típico de }β_j} \right]
	\]

	Y, como en regresión simple, estudiamos $H_0 : β_j = 0$:
	\[
		R = \left\{ \frac{|β_j|}{S_R\sqrt{q_{jj}}} > \mathcal{T}_{n-k-1;\frac{α}{2}} \right\}
	\]
\end{itemize}

En las traspas encontramos una salida de regresión múltiple de $R$. La columna estimate es el vector $\hat{β}$, el p-valor es del contraste $H_0 : β_j = 0$.

\subsection{Estimadores de mínimos cuadrados}
\subsection{Inferencia sobre los parámetros del modelo}
\subsection{Análisis de la varianza}
\subsection{Contrastes de hṕótesis lineales}
\subsection{Modelo unifactorial}