% -*- root: ../EstadisticaII.tex -*-
\section{Introducción}
Se presentan apuntes de Estadística II, tomados de la clase dada por José Berrendero.

El profesor nos facilita unas diapositivas a las que se harán referencia alguna vez. 

\subsection{Vectores aleatorios}


Sea $X = (X_1, \dots , X_p)'$ un vector aleatorio p-dimensional.


\begin{defn}[Esperanza]
	Definimos la esperanza como el vector de medias, es decir:
	\[
	\mathbb{E}(X) = μ = (μ_1,\dots,μ_p)'
	\]
	donde $μ_i = \mathbb{E}(X)$.
\end{defn}


Ejemplo p=3:
\[
\mathbb{E}(X)=
\mathbb{E}\left[ \left( \begin{array}{c} X_1\\ X_2\\ X_3 \end{array} \right) \right]=
\left( \begin{array}{c} \mathbb{E}(X_1)\\ \mathbb{E}(X_2)\\ \mathbb{E}(X_3) \end{array} \right)=
\left( \begin{array}{c} \mu_1\\ \mu_2\\ \mu_3 \end{array} \right)= \mu
\]


\paragraph{Propiedades:}
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$. Como en el caso de variables aleatorias.
\item $\mathbb{E}(AX) = A\mathbb{E}(X)$. Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.

Lo vemos para p=3:

\[
\mathbb{E}(AX)=
\mathbb{E}\left[
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right) \right]=
\mathbb{E}\left[
\left(
\begin{array}{c}
a_{1,1}X_1 - a_{1,2}X_2 - a_{1,3}X_3\\
a_{2,1}X_1 - a_{2,2}X_2 - a_{2,3}X_3\\
a_{3,1}X_1 - a_{3,2}X_2 - a_{3,3}X_3
\end{array}
\right)
\right]=
\]

\[
=\left(
\begin{array}{c}
a_{1,1}\mathbb{E}(X_1) - a_{1,2}\mathbb{E}(X_2) - a_{1,3}\mathbb{E}(X_3)\\
a_{2,1}\mathbb{E}(X_1) - a_{2,2}\mathbb{E}(X_2) - a_{2,3}\mathbb{E}(X_3)\\
a_{3,1}\mathbb{E}(X_1) - a_{3,2}\mathbb{E}(X_2) - a_{3,3}\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\]
\[
=A\mathbb{E}(X)
\]

\end{enumerate}


Vamos a recordar lo que era la covarianza, ya que esta juega un papel importante en los vectores aleatorios.

\begin{defn}[Covarianza]
\[\sigma_{i,j} = Cov(X_i,X_j) = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)\]

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item $Cov(X,X)= Var(X)$
\item $Cov(X,Y)=Cov(Y,X)$
\end{enumerate}

\end{defn}

Al tener varias variables, ya no podemos hablar de varianzas. Definimos el correspondiente p-dimensional de la varianza.


\begin{defn}[Matriz de covarianzas]
	Llamamos $Var(X) = Σ$ a la matriz de covarianzas, cuya posición $(i,j)$ es $σ_{ij} = Cov(X_i,X_j)$.


	Curiosidades:

	\begin{itemize}
		\item Por la definición de covarianza, la diagonal de esta matriz es el vector p-dimensional cuya entrada $i$ es la varianza de $X_i$.
		\item Es una matriz \textbf{simétrica} ya que $Cov(X_i,X_j) = Cov(X_j,X_i)$

	\end{itemize}

	Además: \[Var(X) = \mathbb{E}[(X-μ)(X-μ)'] = \mathbb{E}(XX')-μμ'\]

\end{defn}

Vamos a demostrar esta útlima afirmación.

\begin{proof}
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')=
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

Ejemplo p=3:

\[
Var(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1-\mu_1\\
X_2-\mu_2\\
X_3-\mu_3
\end{array}
\right)
(X_1-\mu_1, X_2-\mu_2, X_3-\mu_3)\right]=
\left(
\begin{array}{ccc}
\sigma_{1,1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \sigma_{2,2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \sigma_{3,3}
\end{array}
\right)=
\]

\[
=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right) = \Sigma
\]

Donde se cumple que $\sigma_{1,2}=\sigma_{2,1}$, $\sigma_{1,3}=\sigma_{3,1}$ y $\sigma_{3,2}=\sigma_{2,3}$. Y por tanto $\Sigma$ es simétrica.

\paragraph{Propiedades:}

Sea $X$ un vector aleatorio p-dimensional, $A$ una matriz $p\times p$ y $b∈ℝ$

\begin{enumerate}
\item $Var(AX+b) = \mathbb{E}\left[ A(X-\mu)(X-\mu)'A' \right]=A \Sigma A'$.
\begin{proof}
\[
Var(AX+b) = \mathbb{E}\left[ (AX+b-A\mu-b)(AX+b-A\mu-b)' \right] =
\]
\[
 =\mathbb{E}\left[ (AX-A\mu)(AX-A\mu)' \right] = \mathbb{E}\left[A(X-\mu)(X-\mu)'A'\right] = A\mathbb{E}\left[(X-\mu)(X-\mu)'\right]A' =
 \]
 \[
 =A \Sigma A'
\]
\end{proof}

\subitem Con esta propiedad podemos deducir más fácilmente expresiones como $Var(X_1 - X_2)$ de la siguiente manera:
\[Var(X_1 - X_2) = (1,-1) \begin{pmatrix}{cc}σ_1^2&σ_{12}\\ σ_{21} & σ_2^2\end{pmatrix} \begin{pmatrix}{c}1\\-1\end{pmatrix} = σ_1^2+σ_2^2 - 2σ_{12}\]

\item Si recordamos, $Var(X) > 0$. La versión matricial dice $Σ$ es semidefinida positiva.
\begin{proof}
Sea $a_i\in ℝ$ y $X = (X_1,\dots X_n)$ un vector aleatorio.

\[
0 ≤ Var(\sum_{i=1}^p a_iX_i) = Var(a'X)
\]

Por la propiedad anterior, tenemos: \[ Var(a'X) = a'Σa\], con lo que $Σ$ tiene que ser semidefinida positiva.
\end{proof}

\subitem Si $Σ$ no es definida positiva, $\implies \exists a\in ℝ^p \tq a'Σa = 0 \implies V(a'X) = 0 \implies \exists c\in ℝ \tq P(a'X = c) = 1$. Si esto se da el vector $X$ toma valores con probabilidad 1 en un subespacio de dimensión inferior a p. En el caso de $p=2$, las variables se situarían sobre una recta.

\end{enumerate}


\subsection{Función característica}
La función característica de un vector aleatorio $X$ es:

\[
\phi_X(t)=\mathbb{E}(\exp^{it'X})\; t\in ℝ^p
\]

\paragraph{Propiedades:} Lo interesante de esta función (como pasaba en el caso unidimensional) es lo siguiente:

\begin{prop} Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]

\end{prop}


\begin{prop}[Mecanismo de Cramer-Wold]
\[a'X \overset{d}{\equiv} a'Y, \; ∀a\in ℝ^p \dimplies X \overset{d}{=} Y\]
\end{prop}

\begin{proof}
\paragraph{$\implies$} es trivial.


\paragraph{$\impliedby$} se demuestra utilizando funciones características y tomando  $t = 1$.
\end{proof}

También se cumple que:

\[X_n \convs[d] X  \dimplies a'X_n \convs[d] a'X\; ∀a∈ℝ^n\]


\subsection{Normal multivariante}

Habiendo definido lo que es un vector aleatorio, vamos a definir la distribución normal multivariante, que aparecerá continuamente a lo largo del curso.

\begin{defn}[Normal p-dimensional]El vector aleatorio $X$ es normal p-dimensional  con vector de medias μ y vector de covarianzas Σ si tiene densidad dada por:
\[
f(x) = |Σ|^\frac{-1}{2}(2π)^{\frac{-p}{2}}exp\left\{ -\frac{1}{2}(x-μ)'Σ^{-1}(x-μ) \right\}\; x∈ℝ^p
\]

\begin{example}
Vamos a ver un ejemplo en dimensión 2 para ilustrar cómo reconocer si un conjunto de datos tiene una distribución normal.

Sean $μ = (0,0)'$ y $Σ = \begin{pmatrix} λ_1 & 0 \\ 0 & λ_2 \end{pmatrix}$

Vamos a ver sus conjuntos de nivel tomando:

\[(X_1,X_2) Σ^{-1} (X_1,X_2)' = cte \implies \frac{x_1^2}{λ_1} + \frac{x_2^2}{λ_2} = cte\]


Dependiendo de los valores de $λ_1,λ_2$ tendremos casos distintos.

\begin{itemize}
	\item Si $λ_1 = λ_2$, entonces tendremos circunferencias.
	\item $λ_1 ≠ λ_2$ entonces tendremos elipses.
	\subitem
	Estas elipses tendrán como eje mayor uno de los 2 ejes, ya que las variables son independientes ($Cov(X_1,X_2) = 0$).
	\subitem Si por el contrario, $Σ$ no fuera diagonal, entonces las variables no serían independientes y tendríamos una correlación entre las variables provocando que el eje mayor de la elipse fuera una recta.
\end{itemize}


Para más información consultar las transparencias de Berrendero, en las que hay un ejemplo.


\end{example}


\paragraph{Notación:} $X \equiv N_p(μ,Σ)$ significa: $X$ es normal p-dimensional con media μ y matriz de covarianzas Σ.
\end{defn}

\subsection{Estandarización multivariante}


Al igual que en el caso unidimensional, nos interesaría poder transformar una normal de media $μ$ y varianza $σ$ en una $N(0,1)$. A continuación vamos a ver ese proceso con una normal multivariante.


\begin{prop}[Estandarización multivariante] Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).\end{prop}

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

También hay que considerar la exponencial de la fórmula de la densidad, ahi hacemos el cambió de variable de:

$$e^X \text{por} e^{h^{-1}(Y)}=e^{\Sigma^{1/2}Y+\mu}$$

Y el Jacobiano sería $e^{\Sigma^{1/2}Y}$:


Por tanto nos quedaría:
\[
f(X) = f(h^{-1}(Y))*\abs{Jh(x)} = \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y+\mu-\mu)'  \right) \exp\left( \Sigma^{1/2}Y \right) \Sigma^{1/2}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y)' \right) \exp\left( \Sigma^{1/2}Y \right) \abs{\Sigma}^{1/2} =
\]
\[
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'\Sigma^{-1/2}\Sigma^{1/2}Y \right) \abs{\Sigma^{1/2}} = (2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'Y) \right)
\]
\end{proof}


Vamos a ver un ejemplo para profundizar en la distribución. Este ejemplo es el ejercicio 1 de la hoja 1. 
\begin{example}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), Var(X_i))$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right]
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?

No son independientes ya que el vector de covarianzas entre ambas variables no es 0. Como en el caso anterior, tomamos como el elemento que ocipa la fila 3 y las columnas 1 y 2, es decir, el vector $(-1,0)$, que al no ser idénticamente nulo, concluimos que $X_3$ no es independiente del vector $(X_1,X_2)$


\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$.

Procedemos de la siguiente manera:

\[
(2X_1-X_2+3X_3)=(2,-1,3)\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)\equiv
N\left( 0,  \right)
\]

\end{example}

\subsection{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1, X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$.

\begin{gather*}
µ = (µ_1, µ_2)\\
Σ = \left(\begin{array}{c|c} Σ_{11} & Σ_{12} \\\hline Σ_{21} & Σ_{22}
\end{array}\right)
\end{gather*}
\label{form::EspVarCondicionada}


\textbf{entonces: }  $X_2 | X_1 \sim N_{p-q}\left(µ_{2.1},Σ_{2.1}\right)$, donde

\begin{equation}
µ_{2.1} = µ_2 + Σ_{2.1}Σ_{11}^{-1}(X_1 - µ_1) = \esp{X_2|X_1}
\end{equation}

\begin{equation}
	Σ_{2.1} = Σ_{22} - Σ_{21}Σ_{11}^{-1} Σ_{12} = \var{X_2 | X_1}
\end{equation}

\end{prop}

\begin{proof}
Definimos $X_{2.1} = X_2 - Σ_{21}Σ_{11}^{-1}X_1$.

\[
\begin{pmatrix}
X_1\\
X_{2.1}
\end{pmatrix} =
\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]

Como es una combinación lineal de $(X_1,X_{2.1})'$, entonces $X_{2.1}$ es normal multivariante.

Vamos a calcular la media y la matriz de covarianzas de $X_{2.1}$

$X_{2-1} = N\left( µ_2-Σ_{21}Σ_{11}^{-1}µ_1 , \begin{pmatrix} Σ_{11} &|&0\\\hline 0&|&Σ_{2.1} \end{pmatrix} \right)$

Donde las covarianzas se calculan: $AΣA'$, siendo $A$ la matriz de la combinación lineal, es decir:

\[
A=\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]



\paragraph{Conclusiones:}

\begin{itemize}
	\item $X_1$ es independiende de $X_{2.1}$
	\item $X_{2.1}$ es normal, con media y varianza calculadas anteriormente.
	\subitem $X_{2.1}|X_1$, al ser independientes, también se distribuye normalmente, con los mismos parámetros.
	\item Dado $X_1$, los vectores $X_{2.1}$ y $X_2$  difieren en el vector constante $Σ_{21}Σ_{11}^{-1}X_1 \implies X_2|X_1 = N\left( µ_{2.1}, Σ_{2.1} \right)$
\end{itemize}

\end{proof}

\begin{example}
Vamos a considerar $X_1, X_2$ como escalares, para entender la proposición. Este ejemplo le surgió a un investigador que quería predecir la estatura de los hijos en función de la de los padres (que no padres y madres, sólo padres).


\[
\begin{pmatrix}
X\\Y
\end{pmatrix} \equiv N_2\left( \begin{pmatrix} µ_x \\ µ_y \end{pmatrix}, \begin{pmatrix}
σ_x^2&σ_{xy}\\σ_{xy}&σ_y^2
\end{pmatrix} \right)
\]
Definimos \[\gor{Y} = \esp{Y|X} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x)\]. La esperanza de la altura del hijo condicionada a la altura del padre será la media de las alturas de los hijos corregida por un factor en el que influye la diferencia de altura del padre con respecto a su media. Es de esperar que si Yao Ming tiene un hijo, sea más alto que la media.

El factor de corrección $\frac{σ_{xy}}{σ_x^2}$ es importante y no me he enerado bien de dónde sale.

Ahora vamos a calcular $\var{Y|X} = σ_{y}^2 - \frac{σ_{xy}^2}{σ_x^2} = σ_y^2 \left( 1- \rho^2\right)$ donde $\rho = \frac{σ_{xy}^2}{σ_x^2σ_y^2}$, el coeficiente de correlación.

Ha dicho algo así como \textit{La única relación que puede existir entre 2 variables normales es una relación lineal.}


Este coeficiente de correlación aparece también en la expresión de la esperanza. Vamos a verlo:

 \[\gor{Y} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x) \dimplies \frac{\gor{Y}-µ_y}{σ_y} = \frac{σ_{xy}}{σ_xσ_y}\frac{x-µ_x}{σ_x}\]

 Es decir:

 \[
\frac{\gor{Y}-µ_y}{σ_y} = \rho \frac{x-µ_x}{σ_x}
 \]

Aplicado a la estatura de los hijos respecto de los padres, se interpreta como: ``Si un padre es muy alto, su hijo será alto pero no destacará tanto como el padre''. Este fenómeno lo definió como \concept{Regresión a la mediocridad}.

\end{example}

\begin{defn}[Homocedásticidad]\label{defn::Homocedasticidad}
Sea $X=(X_1 ,X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Entonces son vectores \textbf{homocedásticos} $\dimplies Σ_{2.1}$ es constante.

Ya veremos más adelante este concepto con mayor detalle.
\end{defn}


\begin{example}

Ahora vamos a ver un par de ejemplos numéricos:

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 \left( \begin{pmatrix}0,0\end{pmatrix}, \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]

\paragraph{Distribución $Y|X$:}

Utilizando las fórmulas definidas en \ref{form::EspVarCondicionada} para $X_i$ unidimensionales:

\[\esp{Y|X} =  µ_{2-1} = 0 + 3·\frac{1}{10}(X-0) = \frac{3}{10}x\]
\[\var{Y|X} = Σ_{2.1} = 1-\frac{3}{10}·3 = \frac{1}{10}\]

\paragraph{Distribución $X|Y$:}

\[E(X|Y) = 3y\]
\[V(X|Y) = 1\]

Ambas son normales unidimensionales ya que $(X,Y)$ es normal multivariante.

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 ...\]

Sea $Z_1 = X+Y$ y $Z_2 = X-Y$.

\[
\begin{pmatrix}Z_1//Z_2\end{pmatrix} = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix} \implies \begin{pmatrix}Z_1\\Z_2\end{pmatrix} = N_2\left(\begin{pmatrix}2\\0\end{pmatrix},\begin{pmatrix}7&1\\1&3\end{pmatrix}\right)
\]

Ahora vamos a calcular lo que nos piden: $E(Z_1|Z_2=1)$.

\[E(Z_1|Z_2=1) = 2 + \frac{1}{3}(1-0) = \frac{7}{3}\]

Es importante destacar que la distribución no depende del valor concreto por ser homocedásticas .
\end{example}

\begin{example}
Sea \[\begin{pmatrix}{c}X\\Y\end{pmatrix} \sim
N_2\left( \begin{pmatrix}1\\1\end{pmatrix}, \begin{pmatrix} 3&1\\1&2 \end{pmatrix}\right)\]

Queremos calcular la distribución de $(X+Y) | (X-Y) = 1$

Para ello, definimos 2 variables, $Z_1 = X+Y$ y $Z_2 = X-Y$, con lo que ahora tenemos que calcular $Z_2 | Z_1 = 1$

Lo primero es hallar la relación matricial entre $X,Y$ y $Z_i$

\begin{equation*}
	\begin{pmatrix}Z_1 \\ Z_2 \end{pmatrix} = \begin{pmatrix}X+Y\\X-Y\end{pmatrix} = \begin{pmatrix} 1 & 1\\1&-1 \end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix}
\end{equation*}

¿Cuáles son la esperanza y la matriz de covarianzas de el vector aleatorio $(Z_1,Z_2)$? Para ello necesitamos la matriz de la combinación lineal que ya tenemos:

\[
µ_Z = A·µ_{xy} = \begin{pmatrix}1&1\\1&-1\end{pmatrix} \begin{pmatrix}1\\1\end{pmatrix} = \begin{pmatrix} 2\\0 \end{pmatrix}
\]

\[
Σ_Z = AΣ_{xy}A' = \begin{pmatrix}1&1\\1&-1\end{pmatrix} \begin{pmatrix} 3&1\\1&2 \end{pmatrix}  \begin{pmatrix}1&1\\1&-1\end{pmatrix} = \begin{pmatrix}7&1\\1&3\end{pmatrix}
\]

Ahora ya podemos calcular la distribución como en el ejemplo anterior:

\begin{gather*}
	\esp{Z_1 | Z_2 = 1} = 2+1·\frac{1}{3}(1-0) = \frac{7}{3}\\
	\var{Z_1 | Z_2 = 1} = 7 - 1·\frac{1}{3} · 1 = \frac{20}{3}
\end{gather*}

En este caso, al ser homocedásticas (\ref{defn::Homocedasticidad}) entonces $\var{Z_1 | Z_2 = 1} = \var{Z_1 | Z_2 = n} ∀n∈ℕ$

\end{example}


\subsection{Formas cuadráticas bajo normalidad}

Sea $B$ una matriz simétrica e idempontente, $$Y\sim N_2(µ,σ^2I_n)$$ y \[µ'Bµ = 0 y p = Rg(B)\]

\textbf{Entonces: } \[\frac{Y'BY}{σ^2} \equiv \chi_p^2 \]


\obs
\begin{itemize}
	\item La única matriz idempontente de rango completo es $I_n$
	\item $λ = 0,1 ∀λ$ autovalor de $B$.
	\begin{proof}
\[\left.\begin{array}{c} Bu = λu\\ Bu=B^2u = λBu = λ^2u \end{array}\right\} λu = λ^2u \implies λ=0,1\]
	\end{proof}

	\subitem Este último hecho permite calcular los grados de libertad de la distribución más fácilmente, ya que $p = Rg(B) = tr(B) = \#\{i \tq λ_i = 1\}$
\end{itemize}


\begin{lemma}[Lema de Fisher]
Sean $Y_1,..,Y_n \overset{iid}{\sim} N(µ,σ)$. Vamos a considerar el vector cuyas marginales son estas $Y \equiv (Y_1,...,Y_n) = N(µ1_n,σ^2I_n)$\footnote{$1_n = (1,1,...,1)$ n veces.}

\textbf{Entonces: } $\gor{Y}, S^2 = \frac{\sum (Y_i - \gor{Y})^2}{n-1}$ son independientes. Además,
\[\frac{(n-1)S^2}{σ^2} \equiv \chi^2_{n-1}\]

\end{lemma}

\begin{proof}
Lamentándolo mucho, la prueba será ignorada por el momento.
\end{proof}


\begin{theorem}[TCL Multivariante]
Sean $X_1,...,X_n$ vectores aleatorias independientes e idénticamente distribuidas (vec.a.i.i.d.) con $X_i \sim N(µ,Σ)$, con $Σ$ definida positiva.

\textbf{Entonces:}

\[\sqrt{n} Σ^{\frac{-1}{2}} (\gor{X_n} - µ) \convs[d] N(0,I) \dimplies \sqrt{n}(\gor{X_n}-µ) \convs[d] N_p(0,Σ)\]

La velocidad a la que $\gor{X_n}$ converge a $µ$ es del orden de $\frac{1}{\sqrt{n}}$

\end{theorem}

\begin{proof}
Lamentándolo mucho, la prueba será ignorada por el momento.
\end{proof}