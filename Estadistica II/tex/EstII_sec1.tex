% -*- root: ../EstadisticaII.tex -*-
\section{Introducción}
Se presentan apuntes de Estadística II, tomados de la clase dada por José Berrendero.

El profesor nos facilita unas diapositivas a las que se harán referencia alguna vez.

\subsection{Vectores aleatorios}


Sea $X = (X_1, \dots , X_p)'$ un vector aleatorio p-dimensional.


\begin{defn}[Esperanza]
	Definimos la esperanza como el vector de medias, es decir:
	\[
	\mathbb{E}(X) = μ = (μ_1,\dots,μ_p)'
	\]
	donde $μ_i = \mathbb{E}(X_i)$.
\end{defn}


\paragraph{Propiedades:}
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$.
\item Sea A una matriz cuadrada de dimensión $n\times p$ siendo $p$ la dimensión de $X$ \[\mathbb{E}(AX) = A\mathbb{E}(X)\]

\end{enumerate}



\begin{defn}[Covarianza]
\[\sigma_{i,j} = \cov{X_i,X_j} = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)\]

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item $\cov{X,X}= \var{X}$
\item $\cov{X,Y}=\cov{Y,X}$
\end{enumerate}

\end{defn}

Al tener varias variables, ya no podemos hablar de varianzas. Definimos el correspondiente p-dimensional de la varianza.


\begin{defn}[Matriz de covarianzas]
	Llamamos $\var{X} = Σ$ a la matriz de covarianzas, cuya posición $(i,j)$ es $σ_{ij} = \cov{X_i,X_j}$.


	Curiosidades:

	\begin{itemize}
		\item Por la definición de covarianza, la diagonal de esta matriz es el vector p-dimensional cuya entrada $i$ es la varianza de $X_i$.
		\item Es una matriz \textbf{simétrica} ya que $\cov{X_i,X_j} = \cov{X_j,X_i}$
	\end{itemize}

	Además: \[\var{X} = \mathbb{E}[(X-μ)(X-μ)'] = \mathbb{E}(XX')-μμ'\]

\end{defn}

Vamos a demostrar esta útlima afirmación.

\begin{proof}
\[
\var{X}=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')=
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

\paragraph{Propiedades:}

Sea $X$ un vector aleatorio p-dimensional, $A$ una matriz $n\times p$ y $b∈ℝ^n$

\begin{enumerate}
\item $\var{AX+b} = \mathbb{E}\left[ A(X-\mu)(X-\mu)'A' \right]=A \Sigma A'$.
\begin{proof}
\[
\var{AX+b} = \mathbb{E}\left[ (AX+b-A\mu-b)(AX+b-A\mu-b)' \right] =
\]
\[
 =\mathbb{E}\left[ (AX-A\mu)(AX-A\mu)' \right] = \mathbb{E}\left[A(X-\mu)(X-\mu)'A'\right] = A\mathbb{E}\left[(X-\mu)(X-\mu)'\right]A' =
 \]
 \[
 =A \Sigma A'
\]
\end{proof}
\label{propiedades:esperanzaYvarianza}

\subitem Con esta propiedad podemos deducir más fácilmente expresiones como $\var{X_1 - X_2}$ de la siguiente manera:
\[\var{X_1 - X_2} = (1,-1) \begin{pmatrix}σ_1^2&σ_{12}\\ σ_{21} & σ_2^2\end{pmatrix} \begin{pmatrix}1\\-1\end{pmatrix} = σ_1^2+σ_2^2 - 2σ_{12}\]

\item Si recordamos, $\var{X} > 0$. La versión matricial dice $Σ$ es semidefinida positiva.
\begin{proof}
Sea $a_i\in ℝ$ y $X = (X_1,\dots X_n)$ un vector aleatorio.

\[
0 ≤ \var{\sum_{i=1}^p a_iX_i} = \var{a'X}
\]

Por la propiedad anterior, tenemos: \[ \var{a'X} = a'Σa\], con lo que $Σ$ tiene que ser semidefinida positiva.
\end{proof}

\subitem Si $Σ$ no es definida positiva, $\implies \exists a\in ℝ^p \tq a'Σa = 0 \implies V(a'X) = 0 \implies \exists c\in ℝ \tq P(a'X = c) = 1$. Si esto se da el vector $X$ toma valores con probabilidad 1 en un subespacio de dimensión inferior a p. En el caso de $p=2$, las variables se situarían sobre una recta.

\item Sea $\vec{Y}$ un vector aleatorio $\vec{Y}\inℝ^p$con matriz de covarianzas $Σ$, distribuido normal-multidimensionalmente. Sean $A\vec{Y}$ y $B\vec{Y}$ 2 combinaciones lineales, con $A\in ℝ^q\times ℝ^p$ y $B\in ℝ^r\times ℝ^p$. Entonces:
\label{propiedad:CovCombinacionLineal}
\[\cov{A\vec{Y},B\vec{Y}} = AΣB' = BΣA'\]

\begin{proof}
\[\begin{pmatrix}A\vec{Y}B\vec{Y}\end{pmatrix} = (A,B)' \vec{Y}\equiv N_{q+r}\left(\begin{pmatrix}Aµ\\Bµ\end{pmatrix} , \begin{pmatrix}A\\B\end{pmatrix}Σ\begin{pmatrix}A'&B'\end{pmatrix} \right)
\]

Desarrollando la matriz de covarianzas: \[
\begin{pmatrix}A\\B\end{pmatrix}Σ\begin{pmatrix}A'&B'\end{pmatrix} =
\begin{pmatrix}
AΣA' & AΣB' \\ BΣA' & BΣB'
\end{pmatrix}
\]

Y la covarianza es cualquiera de los elementos que no está en la diagonal (que como es simétrica, deberían ser iguales)
\end{proof}

\end{enumerate}


\subsection{Función característica}
La función característica de un vector aleatorio $X$ es:

\[
\phi_X(t)=\mathbb{E}(\exp^{it'X})\; t\in ℝ^p
\]

\paragraph{Propiedades:} Lo interesante de esta función (como pasaba en el caso unidimensional) es lo siguiente:

\begin{prop} Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]

\end{prop}


\begin{prop}[Mecanismo de Cramer-Wold]
\[a'X \overset{d}{\equiv} a'Y, \; ∀a\in ℝ^p \dimplies X \overset{d}{=} Y\]
\end{prop}

\begin{proof}
\paragraph{$\implies$} es trivial.


\paragraph{$\impliedby$} se demuestra utilizando funciones características y tomando  $t = 1$.
\end{proof}

También se cumple que:

\[X_n \convs[d] X  \dimplies a'X_n \convs[d] a'X\; ∀a∈ℝ^n\]


\subsection{Normal multivariante}

Habiendo definido lo que es un vector aleatorio, vamos a definir la distribución normal multivariante, que aparecerá continuamente a lo largo del curso.

\begin{defn}[Normal p-dimensional]El vector aleatorio $X$ es normal p-dimensional  con vector de medias μ y vector de covarianzas Σ si tiene densidad dada por:
\[
f(x) = |Σ|^\frac{-1}{2}(2π)^{\frac{-p}{2}}exp\left\{ -\frac{1}{2}(x-μ)'Σ^{-1}(x-μ) \right\}\; x∈ℝ^p
\]
\label{def:Normal_multivariante}


\paragraph{Notación:} $X \equiv N_p(μ,Σ)$ significa: $X$ es normal p-dimensional con media μ y matriz de covarianzas Σ.
\end{defn}

\begin{prop}
Sea $\vec{X}\in ℝ^p$ un vector aleatorio.

\[\vec{X} \sim N_p(µ,Σ) \implies ∀\vec{a}\in \real^p, \vec{a}\vec{X}' \sim N_1(\vec{a}\vec{µ},\vec{a}Σ\vec{a}')\]
\end{prop}

\begin{proof}

Que la media y la varianza son esas, está calculado anteriormente en \ref{propiedades:esperanzaYvarianza}.

Lo de que sean una normal, \href{https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent}{Wikipedia}

Corroborado por correo con José R.
\end{proof}


Vamos a profundizar en esta definición porque es clave, como veremos más adelante en la \fref{sec:incorrnotindep} entre otras.

Sean \[X_1 \equiv N(0,1)\;\; X_2 = N(0,2)\] ¿El vector $\vec{X}= (X_1,X_2)$ cumple $\vec{X}\sim N_2 (µ,Σ)$?

\begin{prop}
\label{prop:NormalidadConjuntaIncorrelacionIndependencia}
Sean $X_i \sim N(µ_i,σ_i)$
\begin{itemize}
	\item $X_i$ independientes \textbf{entonces} $\vec{X} = (X_1,...,X_n) \sim N_n(µ,Σ)$
	\subitem Al ser independientes son incorreladas y por tanto la matriz de covarianzas será diagonal.
	\item $\corr{X_i,X_j} = 0$ y $\vec{X} = (X_1,...,X_n) \sim N_n(µ,Σ)$ \textbf{entonces} son independientes.
\end{itemize}
\end{prop}


\begin{example}
Vamos a ver un ejemplo en dimensión 2 para ilustrar cómo reconocer si un conjunto de datos tiene una distribución normal.

Sean $μ = (0,0)'$ y $Σ = \begin{pmatrix} λ_1 & 0 \\ 0 & λ_2 \end{pmatrix}$

Vamos a ver sus conjuntos de nivel tomando:

\[(X_1,X_2) Σ^{-1} (X_1,X_2)' = cte \implies \frac{x_1^2}{λ_1} + \frac{x_2^2}{λ_2} = cte\]


Dependiendo de los valores de $λ_1,λ_2$ tendremos casos distintos.

\begin{itemize}
	\item Si $λ_1 = λ_2$, entonces tendremos circunferencias.
	\item $λ_1 ≠ λ_2$ entonces tendremos elipses.
	\subitem
	Estas elipses tendrán como eje mayor uno de los 2 ejes, ya que las variables son independientes ($\cov{X_1,X_2} = 0$).
	\subitem Si por el contrario, $Σ$ no fuera diagonal, entonces las variables no serían independientes y tendríamos una correlación entre las variables provocando que el eje mayor de la elipse fuera una recta que no corresponde con ninguno de los ejes.
\end{itemize}


Para más información consultar las transparencias de Berrendero, en las que hay un ejemplo.

\end{example}


\subsection{Incorreladas no implica independientes}
\label{sec:incorrnotindep}
\textcolor{gray}{Esta sección sale de \href{https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent}{Wikipedia}}


\begin{defn}[Correlación]
Sean $X,Y$ dos variables aleatorias. Se define el coeficiente de correlación:
\[\rho_{X,Y}=\corr{X,Y}={\cov{X,Y} \over \sigma_X \sigma_Y}\]

\paragraph{Curiosidades:}

\begin{itemize}
	\item $\rho_{X,Y} ∈ [-1,1]$ Así, podemos comparar todas las correlaciones independientemente de las variables.
	\item 2 variables se llaman \concept{Variables incorreladas} si y sólo si su coeficiente de correlación es $0$.
	\item Obviamente, $\cov{} = 0 \implies \corr{} = 0$
\end{itemize}
\end{defn}


\paragraph{¿Incorrelación implica independencia?}

En general es un problema muy interesante y útil la independencia o no de variables y la correlación es algo fácil de calcular, pero \textbf{incorrelación \underline{NO} implica independencia}. Esto sólo ocurre en algunos casos.

\begin{itemize}
	\item Cuando las variables $X,Y$ son Bernoulli, entonces incorrelación si implica independencia.\footnote{Esto para el curso da igual, pero es interesante de saber}
	\item Si $\vec{X} = (X_1,X_2) \sim N_2(µ,Σ)$ y $\corr{X_1,X_2} = 0$, entonces $X_1$ es independiente de $X_2$.

	\subitem La condición de $\vec{X} \sim N_2(µ,Σ)$ es muy importante. Si sólo tuviéramos $X_i \sim N_1(µ,σ)$, entonces incorrelación \textbf{no} implica independencia.

	Es por ello que este comentario se sitúa después de la definición de normal multivariante (\fref{def:Normal_multivariante})
\end{itemize}

\begin{prop}
Sea $\vec{Y}$ un vector distribuido normalmente.

\textbf{Entonces:} un vector cualquiera $\vec{X}$ se distribuye normalmente si lo podemos escribir en la forma $A\vec{Y}$, para una matriz A.
\end{prop}

\begin{proof}
Nos lo creemos del correo electrónico.
\end{proof}



\subsection{Estandarización multivariante}


Al igual que en el caso unidimensional, nos interesaría poder transformar una normal de media $μ$ y varianza $Σ$ en una $N(0,1)$. A continuación vamos a ver ese proceso con una normal multivariante.


\begin{prop}[Estandarización multivariante] Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).\end{prop}

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

También hay que considerar la exponencial de la fórmula de la densidad, ahi hacemos el cambió de variable de:

$$e^X \text{ por } e^{h^{-1}(Y)}=e^{\Sigma^{1/2}Y+\mu}$$

Y el Jacobiano sería $e^{\Sigma^{1/2}Y}$:


Por tanto nos quedaría:
\[
f(X) = f(h^{-1}(Y))·\abs{Jh(x)} = \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y+\mu-\mu)'  \right) \exp\left( \Sigma^{1/2}Y \right) \Sigma^{1/2}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y)' \right) \exp\left( \Sigma^{1/2}Y \right) \abs{\Sigma}^{1/2} =
\]
\[
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'\Sigma^{-1/2}\Sigma^{1/2}Y \right) \abs{\Sigma^{1/2}} = (2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'Y) \right)
\]
\end{proof}



Vamos a ver un ejemplo para profundizar en la distribución.
\begin{example}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), \var{X_i})$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
\var{X_1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \var{X_2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \var{X_3}
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right]
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?

No son independientes ya que el vector de covarianzas entre ambas variables no es 0. Como en el caso anterior, tomamos como el elemento que ocipa la fila 3 y las columnas 1 y 2, es decir, el vector $(-1,0)$, que al no ser idénticamente nulo, concluimos que $X_3$ no es independiente del vector $(X_1,X_2)$


\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$.

Procedemos de la siguiente manera:

\[
(2X_1-X_2+3X_3)=(2,-1,3)\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)\equiv
N\left( 0,  \right)
\]

\end{example}



\subsection{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1, X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$.

\begin{gather*}
µ = (µ_1, µ_2)\\
Σ = \left(\begin{array}{c|c} Σ_{11} & Σ_{12} \\\hline Σ_{21} & Σ_{22}
\end{array}\right)
\end{gather*}
\label{form::EspVarCondicionada}


\textbf{entonces: }  $X_2 | X_1 \sim N_{p-q}\left(µ_{2.1},Σ_{2.1}\right)$, donde

\begin{equation}
µ_{2.1} = µ_2 + Σ_{21}Σ_{11}^{-1}(X_1 - µ_1) = \esp{X_2|X_1}
\end{equation}

\begin{equation}
	Σ_{2.1} = Σ_{22} - Σ_{21}Σ_{11}^{-1} Σ_{12} = \var{X_2 | X_1}
\end{equation}

\end{prop}

\begin{proof}
Definimos $X_{2.1} = X_2 - Σ_{21}Σ_{11}^{-1}X_1$.

\[
\begin{pmatrix}
X_1\\
X_{2.1}
\end{pmatrix} =
\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]

Como es una combinación lineal de $(X_1,X_2)'$, entonces $X_{2.1}$ es normal multivariante.

Vamos a calcular la media y la matriz de covarianzas de $X_{2.1}$

$X_{2-1} = N\left( µ_2-Σ_{21}Σ_{11}^{-1}µ_1 , \begin{pmatrix} Σ_{11} &|&0\\\hline 0&|&Σ_{2.1} \end{pmatrix} \right)$

Donde las covarianzas se calculan: $AΣA'$, siendo $A$ la matriz de la combinación lineal, es decir:

\[
A=\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]



\paragraph{Conclusiones:}

\begin{itemize}
	\item $X_1$ es independiende de $X_{2.1}$
	\item $X_{2.1}$ es normal, con media y varianza calculadas anteriormente.
	\subitem $X_{2.1}|X_1$, al ser independientes, también se distribuye normalmente, con los mismos parámetros.
	\item Dado $X_1$, los vectores $X_{2.1}$ y $X_2$  difieren en el vector constante $Σ_{21}Σ_{11}^{-1}X_1 \implies X_2|X_1 = N\left( µ_{2.1}, Σ_{2.1} \right)$
\end{itemize}

\end{proof}

\begin{example}
Vamos a considerar $X_1, X_2$ como escalares, para entender la proposición. Este ejemplo le surgió a un investigador que quería predecir la estatura de los hijos en función de la de los padres (que no padres y madres, sólo padres).


\[
\begin{pmatrix}
X\\Y
\end{pmatrix} \equiv N_2\left( \begin{pmatrix} µ_x \\ µ_y \end{pmatrix}, \begin{pmatrix}
σ_x^2&σ_{xy}\\σ_{xy}&σ_y^2
\end{pmatrix} \right)
\]
Definimos \[\gor{Y} = \esp{Y|X} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x)\]. La esperanza de la altura del hijo condicionada a la altura del padre será la media de las alturas de los hijos corregida por un factor en el que influye la diferencia de altura del padre con respecto a su media. Es de esperar que si Yao Ming tiene un hijo, sea más alto que la media.

El factor de corrección $\frac{σ_{xy}}{σ_x^2}$ es importante y no me he enerado bien de dónde sale.

Ahora vamos a calcular $\var{Y|X} = σ_{y}^2 - \frac{σ_{xy}^2}{σ_x^2} = σ_y^2 \left( 1- \rho^2\right)$ donde $\rho = \frac{σ_{xy}^2}{σ_x^2σ_y^2}$, el coeficiente de correlación.

Ha dicho algo así como \textit{La única relación que puede existir entre 2 variables normales es una relación lineal.}


Este coeficiente de correlación aparece también en la expresión de la esperanza. Vamos a verlo:

 \[\gor{Y} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x) \dimplies \frac{\gor{Y}-µ_y}{σ_y} = \frac{σ_{xy}}{σ_xσ_y}\frac{x-µ_x}{σ_x}\]

 Es decir:

 \[
\frac{\gor{Y}-µ_y}{σ_y} = \rho \frac{x-µ_x}{σ_x}
 \]

Aplicado a la estatura de los hijos respecto de los padres, se interpreta como: ``Si un padre es muy alto, su hijo será alto pero no destacará tanto como el padre''. Este fenómeno lo definió como \concept{Regresión a la mediocridad}.

\end{example}

\begin{defn}[Homocedásticidad]\label{defn::Homocedasticidad}
Sea $X=(X_1 ,X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Entonces son vectores \textbf{homocedásticos} $\dimplies Σ_{2.1}$ es constante.

Ya veremos más adelante este concepto con mayor detalle.
\end{defn}


\begin{example}

Ahora vamos a ver un par de ejemplos numéricos:

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 \left( \begin{pmatrix}0,0\end{pmatrix}, \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]

\paragraph{Distribución $Y|X$:}

Utilizando las fórmulas definidas en \ref{form::EspVarCondicionada} para $X_i$ unidimensionales:

\[\esp{Y|X} =  µ_{2-1} = 0 + 3·\frac{1}{10}(X-0) = \frac{3}{10}x\]
\[\var{Y|X} = Σ_{2.1} = 1-\frac{3}{10}·3 = \frac{1}{10}\]

\paragraph{Distribución $X|Y$:}

\[E(X|Y) = 3y\]
\[V(X|Y) = 1\]

Ambas son normales unidimensionales ya que $(X,Y)$ es normal multivariante.

\end{example}

\begin{example}
Sea \[\begin{pmatrix}X\\Y\end{pmatrix} \sim
N_2\left( \begin{pmatrix}1\\1\end{pmatrix}, \begin{pmatrix} 3&1\\1&2 \end{pmatrix}\right)\]

Queremos calcular la distribución de $(X+Y) | (X-Y) = 1$

Para ello, definimos 2 variables, $Z_1 = X+Y$ y $Z_2 = X-Y$, con lo que ahora tenemos que calcular $Z_2 | Z_1 = 1$

Lo primero es hallar la relación matricial entre $X,Y$ y $Z_i$

\begin{equation*}
	\begin{pmatrix}Z_1 \\ Z_2 \end{pmatrix} = \begin{pmatrix}X+Y\\X-Y\end{pmatrix} = \begin{pmatrix} 1 & 1\\1&-1 \end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix}
\end{equation*}

¿Cuáles son la esperanza y la matriz de covarianzas de el vector aleatorio $(Z_1,Z_2)$? Para ello necesitamos la matriz de la combinación lineal que ya tenemos:

\[
µ_Z = A·µ_{xy} = \begin{pmatrix}1&1\\1&-1\end{pmatrix} \begin{pmatrix}1\\1\end{pmatrix} = \begin{pmatrix} 2\\0 \end{pmatrix}
\]

\[
Σ_Z = AΣ_{xy}A' = \begin{pmatrix}1&1\\1&-1\end{pmatrix} \begin{pmatrix} 3&1\\1&2 \end{pmatrix}  \begin{pmatrix}1&1\\1&-1\end{pmatrix} = \begin{pmatrix}7&1\\1&3\end{pmatrix}
\]

Ahora ya podemos calcular la distribución como en el ejemplo anterior:

\begin{gather*}
	\esp{Z_1 | Z_2 = 1} = 2+1·\frac{1}{3}(1-0) = \frac{7}{3}\\
	\var{Z_1 | Z_2 = 1} = 7 - 1·\frac{1}{3} · 1 = \frac{20}{3}
\end{gather*}

En este caso, al ser homocedásticas (\ref{defn::Homocedasticidad}) entonces $\var{Z_1 | Z_2 = 1} = \var{Z_1 | Z_2 = n} ∀n∈ℕ$

\end{example}


\subsection{Formas cuadráticas bajo normalidad}

\begin{prop}[]
Sea $B$ una matriz simétrica e idempontente, $$Y\sim N_2(µ,σ^2I_n)$$ y \[µ'Bµ = 0 \text{  y  } p = Rg(B)\]

\textbf{Entonces: } \[\frac{Y'BY}{σ^2} \equiv \chi_p^2 \]
\end{prop}

\obs
\begin{itemize}
	\item La única matriz idempontente de rango completo es $I_n$
	\item $λ = 0,1 ∀λ$ autovalor de $B$.
	\begin{proof}
\[\left.\begin{array}{c} Bu = λu\\ Bu=B^2u = λBu = λ^2u \end{array}\right\} λu = λ^2u \implies λ=0,1\]
	\end{proof}

	\subitem Este último hecho permite calcular los grados de libertad de la distribución más fácilmente, ya que $p = Rg(B) = tr(B) = \#\{i \tq λ_i = 1\}$
\end{itemize}

\begin{prop}[Formas cuadráticas bajo normalidad]
\label{prop:tema1_pepino}
Sea $\vec{Y}\equiv = N_n (µ,σ^2I_n)$ y sean $A_{p\times n},B_{q\times n},C_{n\times n},D_{n\times n}$ con $C,D$ simétricas.

\textbf{Entonces:}
\begin{itemize}
	\item $AY$ y $BY$ son independientes $\dimplies$ $AB' = 0$
	\item $Y'CY$ e $Y'DY$ son independientes $\dimplies CD = 0$
	\item $AY$ e $Y'CY$ son independientes $\dimplies AC=0$
\end{itemize}

\end{prop}

\begin{lemma}[Lema de Fisher]
Sean $Y_1,..,Y_n \overset{iid}{\sim} N(µ,σ)$. Vamos a considerar el vector cuyas marginales son estas $Y \equiv (Y_1,...,Y_n) = N(µ1_n,σ^2I_n)$\footnote{$1_n = (1,1,...,1)$ n veces.}

\textbf{Entonces: } $\gor{Y}, S^2 = \frac{\sum (Y_i - \gor{Y})^2}{n-1}$ son independientes. Además,
\[\frac{(n-1)S^2}{σ^2} \equiv \chi^2_{n-1}\]

\end{lemma}

\begin{proof}
Lamentándolo mucho, la prueba será ignorada por el momento.
\end{proof}


\begin{theorem}[TCL Multivariante]
Sean $X_1,...,X_n$ vectores aleatorias independientes e idénticamente distribuidas (vec.a.i.i.d.) con $X_i \sim N(µ,Σ)$, con $Σ$ definida positiva.

\textbf{Entonces:}

\[\sqrt{n} Σ^{\frac{-1}{2}} (\gor{X_n} - µ) \convs[d] N(0,I) \dimplies \sqrt{n}(\gor{X_n}-µ) \convs[d] N_p(0,Σ)\]

La velocidad a la que $\gor{X_n}$ converge a $µ$ es del orden de $\frac{1}{\sqrt{n}}$

\end{theorem}

\begin{proof}
Lamentándolo mucho, la prueba será ignorada por el momento.
\end{proof}
