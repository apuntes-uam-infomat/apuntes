\documentclass[nochap]{apuntes}

\usepackage{hyperref}

\usepackage{tikztools}
\usepackage{fastbuild}
\usepackage{tikz-3dplot}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{latexsym, amsfonts, amsmath, amssymb, amscd, epsfig,amsthm}
\input xy
\xyoption{all} %%!!
\usetikzlibrary{calc, intersections}
\author{Alberto Parramón}
\date{2014/2015 2º cuatrimestre}

\renewcommand*{\arraystretch}{1.5}
\title{Estadística II}
\precompileTikz

\begin{document}

\pagestyle{plain}
\maketitle

\tableofcontents
\newpage

\section{Introducción}
Se presentan apuntes de Estadística II, tomados de la clase dada por José Berrendero.

El profesor nos facilita unas diapositivas, por tanto, se mostrarán las mismas y se explicaran con detalle.

\section{Distribución normal multivariante}

\includepdf[frame=true, noautoscale=true, delta=10 10, nup=1x2,pages={2-3}, scale=1]{pdf/_tema1.pdf}

\subsection{Esperanza, varianza y covarianza de variables aleatorias}
Dada una variable aleatoria definimos:
\begin{itemize}
\item Esperanza: $\mu = \mathbb{E}(X) = \int_{-\infty}^{\infty}x\cdot f_P(x) dx$

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(aX) = a\mathbb{E}(X)$
\item $\mathbb{E}(X+Y) = \mathbb{E}(X)-\mathbb{E}(Y)$
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$ (La esperanza de una constante es la propia constante)
\end{enumerate}
\item Varianza: $Var(X) = \mathbb{E}((X-\mathbb{E}(X))^2) =\mathbb{E}((X-\mu)^2) = \mathbb{E}(X^2)-\mu^2$

Propiedades:
\begin{enumerate}
\item $Var(X+b)=Var(X)$
\item $Var(aX)=a^2Var(X)$
\item $Var(X)\geq 0$
\end{enumerate}
\item Covarianza (entre dos variables aleatorias $X_i$, $X_j$): $\sigma_{i,j} = Cov(X_i,X_j) = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)$

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item Cov(X,X)= Var(X)
\item $Cov(X,Y)=Cov(Y,X)$
\end{enumerate}

\end{itemize}

\subsection{Esperanza, varianza y covarianza de vectores aleatorios}

Un vector aleatorio es un vector de variables aleatorias.

Notación: como durante el curso vamos a trabajar con vectores aleatorios, vamos a generalizar los símbolos que iremos usando:
\begin{itemize}
\item $X = (X_1, X_2,...,X_p)'$ será un vector de p variables aleatorias. Las variables aleatorias serán $X_1, X_2,...,X_p$. La comilla simple $'$ indica que $X$ es un vector columna.
\item $\mu$ será la esperanza del vector aleatorio X: $\mathbb{E}(X)$. Las esperanzas de cada variable aleatoria serán $\mu_1, \mu_2,...,\mu_p$.
\item Si A es una matriz, A' es su traspuesta
\end{itemize}

Por tanto, dado un vector de p variables aleatorias (vector aleatorio p-dimensional), tenemos un resultado parecido.

\begin{itemize}
\item Esperanza. Será un vector columna con las esperanzas de cada variable aleatoria.
\[
\mathbb{E}(X) = \mu = (\mu_1, \mu_2,..., \mu_p)'
\]

Donde cada $\mu_i = \mathbb{E}(X_i)$.

Ejemplo p=3:
\[
\mathbb{E}(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)
\right]=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)=
\mu
\]

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$. Como en el caso de variables aleatorias.
\item $\mathbb{E}(AX) = A\mathbb{E}(X)$. Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.

Lo vemos para p=3:

\[
\mathbb{E}(AX)=
\mathbb{E}\left[
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right) \right]=
\mathbb{E}\left[ 
\left(
\begin{array}{c}
a_{1,1}X_1 - a_{1,2}X_2 - a_{1,3}X_3\\
a_{2,1}X_1 - a_{2,2}X_2 - a_{2,3}X_3\\
a_{3,1}X_1 - a_{3,2}X_2 - a_{3,3}X_3
\end{array}
\right)
\right]=
\]

\[
=\left(
\begin{array}{c}
a_{1,1}\mathbb{E}(X_1) - a_{1,2}\mathbb{E}(X_2) - a_{1,3}\mathbb{E}(X_3)\\
a_{2,1}\mathbb{E}(X_1) - a_{2,2}\mathbb{E}(X_2) - a_{2,3}\mathbb{E}(X_3)\\
a_{3,1}\mathbb{E}(X_1) - a_{3,2}\mathbb{E}(X_2) - a_{3,3}\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\]
\[
=A\mathbb{E}(X)
\]

\end{enumerate}

\item Varianza. La varianza va a ser una matriz, donde cada elemento va a ser la covarianza entre dos de las p variables aleatorias que conforman el vector. Será por tanto una matriz simétrica (ya que $\sigma_{i,j}=Cov(X_i,X_j)=Cov(X_j,X_i)=\sigma_{j,i}$). La matriz resultante será la llamada matriz de covarianzas $\Sigma$.
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]

\begin{proof}
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')= 
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

Ejemplo p=3:

\[
Var(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1-\mu_1\\
X_2-\mu_2\\
X_3-\mu_3
\end{array}
\right)
(X_1-\mu_1, X_2-\mu_2, X_3-\mu_3)\right]=
\left(
\begin{array}{ccc}
\sigma_{1,1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \sigma_{2,2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \sigma_{3,3}
\end{array}
\right)=
\]

\[
=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right) = \Sigma
\]

Donde se cumple que $\sigma_{1,2}=\sigma_{2,1}$, $\sigma_{1,3}=\sigma_{3,1}$ y $\sigma_{3,2}=\sigma_{2,3}$. Y por tanto $\Sigma$ es simétrica. 
\end{itemize}

Propiedades:
\begin{enumerate}
\item $Var(AX+b) = \mathbb{E}\left[ A(X-\mu)(X-\mu)'A' \right]=A \Sigma A'$.  Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.
\begin{proof}
\[
Var(AX+b) = \mathbb{E}\left[ (AX+b-A\mu-b)(AX+b-A\mu-b)' \right] =
\]
\[
 =\mathbb{E}\left[ (AX-A\mu)(AX-A\mu)' \right] = \mathbb{E}\left[A(X-\mu)(X-\mu)'A'\right] = A\mathbb{E}\left[(X-\mu)(X-\mu)'\right]A' = 
 \]
 \[
 =A \Sigma A'
\]

\end{proof}
\end{enumerate}

\textcolor{red}{Mirar si tiene importancia lo de $\Sigma$ semidefinida positiva y tal}

\subsection{Función característica}
La función característica de un vector aleatorio X es:
\[
\phi_X(t)=\mathbb{E}(\exp^{it'X})
\]

Siendo X y t p-dimensionales.

\prop Mecanismo de Cramer-Wold...

Esta función caracteriza la distribución de X:
\prop Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]

\textcolor{red}{Completar este apartado consultando a Elena}

\subsection{Matriz de covarianzas}
Como ya dijimos anteriormente la matriz de covarianzas $\Sigma$ define la varianza de un vector aleatorio y es simétrica. Por tanto podemos expresar $\Sigma$ de la siguiente forma:
\[
\Sigma = CDC^{-1}
\]

Siendo D una matriz diagonal.

\textcolor{red}{$C^{-1}=C'$ ya que las columnas de C son vectores ortogonales. OJO CUIDAO, que tienen que ser ortonormales...
Una matriz real A es ortogonal si y sólo si sus vectores filas o vectores columna son cada uno un conjunto ortonormal de vectores.}
Por tanto:
\[
\Sigma = CDC'  \text{ y } \Sigma^{-1} = CD^{-1}C'
\]

\textcolor{blue}{Caso particular:
\[
p=2 \text{ , } 
\mu=\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{ , }
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\]
Tenemos:
\[
(X_1, X_2)
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right) = cte
\Rightarrow
\frac{X_1^2}{\lambda_1}+\frac{X_2^2}{\lambda_2}=cte
\]
}


\subsection{Estandarización multivariante}
\begin{defn}
Sea un vector aleatorio X, es normal p-dimensional con vector de medias $\mu$ y matriz de covarianzas $\Sigma$ (notación: $X\equiv N_p(\mu, \Sigma)$) si tiene densidad dada por:

\[
f(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right) 
\]
\end{defn}

\prop Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right) 
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

También hay que considerar la exponencial de la fórmula de la densidad, ahi hacemos el cambió de variable de:

$$e^X \text{por} e^{h^{-1}(Y)}=e^{\Sigma^{1/2}Y+\mu}$$ 

Y el Jacobiano sería $e^{\Sigma^{1/2}Y}$:
 

Por tanto nos quedaría:
\[
f(X) = f(h^{-1}(Y))*\abs{Jh(x)} = \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y+\mu-\mu)'  \right) \exp\left( \Sigma^{1/2}Y \right) \Sigma^{1/2}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y)' \right) \exp\left( \Sigma^{1/2}Y \right) \abs{\Sigma}^{1/2} =
\]
\[
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'\Sigma^{-1/2}\Sigma^{1/2}Y \right) \abs{\Sigma^{1/2}} = (2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'Y) \right) 
\]
\end{proof}


\subsection{Ejercicio 1}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), Var(X_i))$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right] 
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?
???

\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$.

Procedemos de la siguiente manera:

\[
(2X_1-X_2+3X_3)=(2,-1,3)\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)\equiv
N\left( 0,  \right)
\]



\subsection{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1|X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Consideramos las particiones correspondientes de $µ$ y de $\Sigma$.

\end{prop}

\begin{proof}
Definimos $X_{2.1} = X_2 - Σ_{21}Σ_{11}^{-1}X_1$.

\[
\begin{pmatrix}
X_1\\
X_{2.1}
\end{pmatrix} = 
\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]

Como es una combinación lineal de $(X_1,X_{2.1})'$, entonces $X_{2.1}$ es normal multivariante.

Vamos a calcular la media y la matriz de covarianzas de $X_{2.1}$

$X_{2-1} = N\left( µ_2-Σ_{21}Σ_{11}^{-1}µ_1 , \begin{pmatrix} Σ_{11} &|&0\\\hline 0&|&Σ_{2.1} \end{pmatrix} \right)$

Donde las covarianzas se calculan: $AΣA'$, siendo $A$ la matriz de la combinación lineal, es decir:

\[
A=\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]



\paragraph{Conclusiones:}

\begin{itemize}
	\item $X_1$ es independiende de $X_{2.1}$
	\item $X_{2.1}$ es normal, con media y varianza calculadas anteriormente.
	\subitem $X_{2.1}|X_1$, al ser independientes, también se distribuye normalmente, con los mismos parámetros.
	\item Dado $X_1$, los vectores $X_{2.1}$ y $X_2$  difieren en el vector constante $Σ_{21}Σ_{11}^{-1}X_1 \implies X_2|X_1 = N\left( µ_{2.1}, Σ_{2.1} \right)$
\end{itemize}

\end{proof}

\begin{example}
Vamos a considerar $X_1, X_2$ como escalares, para entender la proposición. Este ejemplo le surgió a un investigador que quería predecir la estatura de los hijos en función de la de los padres (que no padres y madres, sólo padres).


\[
\begin{pmatrix}
X\\Y
\end{pmatrix} \equiv N_2\left( \begin{pmatrix} µ_x \\ µ_y \end{pmatrix}, \begin{pmatrix}
σ_x^2&σ_{xy}\\σ_{xy}&σ_y^2
\end{pmatrix} \right)
\]
\label{form::EspVarCondicionada}
Definimos $\gor{Y} = E(Y|X) = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x)$. La esperanza de la altura del hijo condicionada a la altura del padre será la media de las alturas de los hijos corregida por un factor en el que influye la diferencia de altura del padre con respecto a su media. Es de esperar que si Yao Ming tiene un hijo, sea más alto que la media.

El factor de corrección $\frac{σ_{xy}}{σ_x^2}$ es importante y no me he enerado bien de dónde sale.

Ahora vamos a calcular $V(Y|X) = σ_{y}^2 - \frac{σ_{xy}^2}{σ_x^2} = σ_y^2 \left( 1- \rho^2\right)$ donde $\rho = \frac{σ_{xy}^2}{σ_x^2σ_y^2}$, el coeficiente de correlación.

Ha dicho algo así como \textbf{La única relación que puede existir entre 2 variables normales es una relación lineal.}


Este coeficiente de correlación aparece también en la expresión de la esperanza. Vamos a verlo:

 \[\gor{Y} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x) \dimplies \frac{\gor{Y}-µ_y}{σ_y} = \frac{σ_{xy}}{σ_xσ_y}\frac{x-µ_x}{σ_x}\]

 Es decir:

 \[
\frac{\gor{Y}-µ_y}{σ_y} = \rho \frac{x-µ_x}{σ_x}
 \]

Aplicado a la estatura de los hijos respecto de los padres, se interpreta como: ``Si un padre es muy alto, su hijo será alto pero no destacará tanto como el padre''. Este fenómeno lo definió como \concept{Regresión a la mediocridad}. 

\end{example}

\begin{defn}[Homocedástico]
$Σ_{2.1}$ no depende de $X_1$.

Esto se da cuando $\begin{pmatrix}X_1,X_2\end{pmatrix}$ es normal multivariante. Si no fueran normal multivariante, serían heterocedásticas. \footnote{Un ejemplo sería $X_1$ la renta de una familia y $X_2$ los ahorros de la misma. Los datos no se distribuyen conjuntamente normal, con lo que la $Σ_{2.1}$ si depende de $X_1$. Ya veremos más adelante este concepto con mayor detalle.}
\end{defn}


\begin{example}

Ahora vamos a ver un par de ejemplos numéricos:

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 \left( \begin{pmatrix}0,0\end{pmatrix}, \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]
 
\paragraph{Distribución $Y|X$:}

\[E(Y|X) = \frac{3}{10}x\]
\[V(Y|X) = \frac{1}{10}\]

\paragraph{Distribución $X|Y$:}

\[E(X|Y) = 3y\]
\[V(X|Y) = 1\]

Ambas son normales unidimensionales ya que $(X Y)$ es normal multivariante.

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 ...\]

Sea $Z_1 = X+Y$ y $Z_2 = X-Y$.

\[
\begin{pmatrix}Z_1//Z_2\end{pmatrix} = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix} \implies \begin{pmatrix}Z_1\\Z_2\end{pmatrix} = N_2\left(\begin{pmatrix}2\\0\end{pmatrix},\begin{pmatrix}7&1\\1&3\end{pmatrix}\right)
\]

Ahora vamos a calcular lo que nos piden: $E(Z_1|Z_2=1)$.

\[E(Z_1|Z_2=1) = 2 + \frac{1}{3}(1-0) = \frac{7}{3}\]

Es importante destacar que la distribución no depende del valor concreto por ser homocedásticas .

\end{example}

\appendix
\chapter{Ejercicios}
\input{tex/EstII_ejercicios.tex}

\chapter{Prácticas}
\includepdf[pages={1-5}]{pdf/_practica1E2.pdf}

\end{document}