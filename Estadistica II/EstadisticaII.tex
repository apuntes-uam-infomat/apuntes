\documentclass[nochap]{apuntes}

\usepackage{hyperref}

\usepackage{tikztools}
\usepackage{fastbuild}
\usepackage{tikz-3dplot}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{latexsym, amsfonts, amsmath, amssymb, amscd, epsfig,amsthm}
\input xy
\xyoption{all} %%!!
\usetikzlibrary{calc, intersections}
\author{Alberto Parramón}
\date{2014/2015 2º cuatrimestre}

\renewcommand*{\arraystretch}{1.5}
\title{Estadística II}
\precompileTikz

\begin{document}

\pagestyle{plain}
\maketitle

\tableofcontents
\newpage

\chapter{Distribución normal multivariante}

\section{Esperanza, varianza y covarianza de variables aleatorias}

Dada una variable aleatoria definimos:
\begin{itemize}
\item Esperanza: $\mu = \mathbb{E}(X) = \int_{-\infty}^{\infty}x\cdot f_P(x) dx$

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(aX) = a\mathbb{E}(X)$
\item $\mathbb{E}(X+Y) = \mathbb{E}(X)+\mathbb{E}(Y)$
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$ (La esperanza de una constante es la propia constante)
\end{enumerate}
\item Varianza: $Var(X) = \mathbb{E}((X-\mathbb{E}(X))^2) =\mathbb{E}((X-\mu)^2) = \mathbb{E}(X^2)-\mu^2$

Propiedades:
\begin{enumerate}
\item $Var(X+b)=Var(X)$
\item $Var(aX)=a^2Var(X)$
\item $Var(X)\geq 0$
\end{enumerate}
\item Covarianza (entre dos variables aleatorias $X_i$, $X_j$): $\sigma_{i,j} = Cov(X_i,X_j) = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)$

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item Cov(X,X)= Var(X)
\item $Cov(X,Y)=Cov(Y,X)$
\end{enumerate}

\end{itemize}

\section{Esperanza, varianza y covarianza de vectores aleatorios}

Un vector aleatorio es un vector de variables aleatorias.

Notación: como durante el curso vamos a trabajar con vectores aleatorios, vamos a generalizar los símbolos que iremos usando:
\begin{itemize}
\item $X = (X_1, X_2,...,X_p)'$ será un vector de p variables aleatorias. Las variables aleatorias serán $X_1, X_2,...,X_p$. La comilla simple $'$ indica que $X$ es un vector columna.
\item $\mu$ será la esperanza del vector aleatorio X: $\mathbb{E}(X)$. Las esperanzas de cada variable aleatoria serán $\mu_1, \mu_2,...,\mu_p$.
\item Si A es una matriz, A' es su traspuesta
\end{itemize}

Por tanto, dado un vector de p variables aleatorias (vector aleatorio p-dimensional), tenemos un resultado parecido.

\begin{itemize}
\item Esperanza. Será un vector columna con las esperanzas de cada variable aleatoria.
\[
\mathbb{E}(X) = \mu = (\mu_1, \mu_2,..., \mu_p)'
\]

Donde cada $\mu_i = \mathbb{E}(X_i)$.

Ejemplo p=3:
\[
\mathbb{E}(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)
\right]=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)=
\mu
\]

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$. Como en el caso de variables aleatorias.
\item $\mathbb{E}(AX) = A\mathbb{E}(X)$. Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.

Lo vemos para p=3:

\[
\mathbb{E}(AX)=
\mathbb{E}\left[
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right) \right]=
\mathbb{E}\left[
\left(
\begin{array}{c}
a_{1,1}X_1 - a_{1,2}X_2 - a_{1,3}X_3\\
a_{2,1}X_1 - a_{2,2}X_2 - a_{2,3}X_3\\
a_{3,1}X_1 - a_{3,2}X_2 - a_{3,3}X_3
\end{array}
\right)
\right]=
\]

\[
=\left(
\begin{array}{c}
a_{1,1}\mathbb{E}(X_1) - a_{1,2}\mathbb{E}(X_2) - a_{1,3}\mathbb{E}(X_3)\\
a_{2,1}\mathbb{E}(X_1) - a_{2,2}\mathbb{E}(X_2) - a_{2,3}\mathbb{E}(X_3)\\
a_{3,1}\mathbb{E}(X_1) - a_{3,2}\mathbb{E}(X_2) - a_{3,3}\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\]
\[
=A\mathbb{E}(X)
\]

\end{enumerate}

\item Varianza. La varianza va a ser una matriz, donde cada elemento va a ser la covarianza entre dos de las p variables aleatorias que conforman el vector. Será por tanto una matriz simétrica (ya que $\sigma_{i,j}=Cov(X_i,X_j)=Cov(X_j,X_i)=\sigma_{j,i}$). La matriz resultante será la llamada matriz de covarianzas $\Sigma$.
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]

\begin{proof}
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')=
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

Ejemplo p=3:

\[
Var(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1-\mu_1\\
X_2-\mu_2\\
X_3-\mu_3
\end{array}
\right)
(X_1-\mu_1, X_2-\mu_2, X_3-\mu_3)\right]=
\left(
\begin{array}{ccc}
\sigma_{1,1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \sigma_{2,2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \sigma_{3,3}
\end{array}
\right)=
\]

\[
=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right) = \Sigma
\]

Donde se cumple que $\sigma_{1,2}=\sigma_{2,1}$, $\sigma_{1,3}=\sigma_{3,1}$ y $\sigma_{3,2}=\sigma_{2,3}$. Y por tanto $\Sigma$ es simétrica.
\end{itemize}

Propiedades:
\begin{enumerate}
\item $Var(AX+b) = \mathbb{E}\left[ A(X-\mu)(X-\mu)'A' \right]=A \Sigma A'$.  Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.
\begin{proof}
\[
Var(AX+b) = \mathbb{E}\left[ (AX+b-A\mu-b)(AX+b-A\mu-b)' \right] =
\]
\[
 =\mathbb{E}\left[ (AX-A\mu)(AX-A\mu)' \right] = \mathbb{E}\left[A(X-\mu)(X-\mu)'A'\right] = A\mathbb{E}\left[(X-\mu)(X-\mu)'\right]A' =
 \]
 \[
 =A \Sigma A'
\]

\end{proof}
\end{enumerate}

\textcolor{red}{Mirar si tiene importancia lo de $\Sigma$ semidefinida positiva y tal}

\section{Función característica}
La función característica de un vector aleatorio X es:
\[
\phi_X(t)=\mathbb{E}(\exp^{it'X})
\]

Siendo X y t p-dimensionales.

\begin{prop} Mecanismo de Cramer-Wold... \end{prop}

Esta función caracteriza la distribución de X:
\begin{prop} Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]

\textcolor{red}{Completar este apartado consultando a Elena}
\end{prop}
\section{Matriz de covarianzas}
Como ya dijimos anteriormente la matriz de covarianzas $\Sigma$ define la varianza de un vector aleatorio y es simétrica. Por tanto podemos expresar $\Sigma$ de la siguiente forma:
\[
\Sigma = CDC^{-1}
\]

Siendo D una matriz diagonal.

\textcolor{red}{$C^{-1}=C'$ ya que las columnas de C son vectores ortogonales. OJO CUIDAO, que tienen que ser ortonormales...
Una matriz real A es ortogonal si y sólo si sus vectores filas o vectores columna son cada uno un conjunto ortonormal de vectores.}
Por tanto:
\[
\Sigma = CDC'  \text{ y } \Sigma^{-1} = CD^{-1}C'
\]

\textcolor{blue}{Caso particular:
\[
p=2 \text{ , }
\mu=\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{ , }
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\]
Tenemos:
\[
(X_1, X_2)
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right) = cte
\Rightarrow
\frac{X_1^2}{\lambda_1}+\frac{X_2^2}{\lambda_2}=cte
\]
}


\section{Estandarización multivariante}
\begin{defn}
Sea un vector aleatorio X, es normal p-dimensional con vector de medias $\mu$ y matriz de covarianzas $\Sigma$ (notación: $X\equiv N_p(\mu, \Sigma)$) si tiene densidad dada por:

\[
f(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]
\end{defn}

\begin{prop} Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).\end{prop}

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

También hay que considerar la exponencial de la fórmula de la densidad, ahi hacemos el cambió de variable de:

$$e^X \text{por} e^{h^{-1}(Y)}=e^{\Sigma^{1/2}Y+\mu}$$

Y el Jacobiano sería $e^{\Sigma^{1/2}Y}$:


Por tanto nos quedaría:
\[
f(X) = f(h^{-1}(Y))*\abs{Jh(x)} = \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y+\mu-\mu)'  \right) \exp\left( \Sigma^{1/2}Y \right) \Sigma^{1/2}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y)' \right) \exp\left( \Sigma^{1/2}Y \right) \abs{\Sigma}^{1/2} =
\]
\[
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'\Sigma^{-1/2}\Sigma^{1/2}Y \right) \abs{\Sigma^{1/2}} = (2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'Y) \right)
\]
\end{proof}


\section{Ejercicio 1}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), Var(X_i))$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right]
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?
???

\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$.

Procedemos de la siguiente manera:

\[
(2X_1-X_2+3X_3)=(2,-1,3)\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)\equiv
N\left( 0,  \right)
\]



\section{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1|X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Consideramos las particiones correspondientes de $µ$ y de $\Sigma$.

\end{prop}

\begin{proof}
Definimos $X_{2.1} = X_2 - Σ_{21}Σ_{11}^{-1}X_1$.

\[
\begin{pmatrix}
X_1\\
X_{2.1}
\end{pmatrix} =
\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]

Como es una combinación lineal de $(X_1,X_{2.1})'$, entonces $X_{2.1}$ es normal multivariante.

Vamos a calcular la media y la matriz de covarianzas de $X_{2.1}$

$X_{2-1} = N\left( µ_2-Σ_{21}Σ_{11}^{-1}µ_1 , \begin{pmatrix} Σ_{11} &|&0\\\hline 0&|&Σ_{2.1} \end{pmatrix} \right)$

Donde las covarianzas se calculan: $AΣA'$, siendo $A$ la matriz de la combinación lineal, es decir:

\[
A=\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]



\paragraph{Conclusiones:}

\begin{itemize}
	\item $X_1$ es independiende de $X_{2.1}$
	\item $X_{2.1}$ es normal, con media y varianza calculadas anteriormente.
	\subitem $X_{2.1}|X_1$, al ser independientes, también se distribuye normalmente, con los mismos parámetros.
	\item Dado $X_1$, los vectores $X_{2.1}$ y $X_2$  difieren en el vector constante $Σ_{21}Σ_{11}^{-1}X_1 \implies X_2|X_1 = N\left( µ_{2.1}, Σ_{2.1} \right)$
\end{itemize}

\end{proof}

\begin{example}
Vamos a considerar $X_1, X_2$ como escalares, para entender la proposición. Este ejemplo le surgió a un investigador que quería predecir la estatura de los hijos en función de la de los padres (que no padres y madres, sólo padres).


\[
\begin{pmatrix}
X\\Y
\end{pmatrix} \equiv N_2\left( \begin{pmatrix} µ_x \\ µ_y \end{pmatrix}, \begin{pmatrix}
σ_x^2&σ_{xy}\\σ_{xy}&σ_y^2
\end{pmatrix} \right)
\]
\label{form::EspVarCondicionada}
Definimos $\gor{Y} = E(Y|X) = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x)$. La esperanza de la altura del hijo condicionada a la altura del padre será la media de las alturas de los hijos corregida por un factor en el que influye la diferencia de altura del padre con respecto a su media. Es de esperar que si Yao Ming tiene un hijo, sea más alto que la media.

El factor de corrección $\frac{σ_{xy}}{σ_x^2}$ es importante y no me he enerado bien de dónde sale.

Ahora vamos a calcular $V(Y|X) = σ_{y}^2 - \frac{σ_{xy}^2}{σ_x^2} = σ_y^2 \left( 1- \rho^2\right)$ donde $\rho = \frac{σ_{xy}^2}{σ_x^2σ_y^2}$, el coeficiente de correlación.

Ha dicho algo así como \textbf{La única relación que puede existir entre 2 variables normales es una relación lineal.}


Este coeficiente de correlación aparece también en la expresión de la esperanza. Vamos a verlo:

 \[\gor{Y} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x) \dimplies \frac{\gor{Y}-µ_y}{σ_y} = \frac{σ_{xy}}{σ_xσ_y}\frac{x-µ_x}{σ_x}\]

 Es decir:

 \[
\frac{\gor{Y}-µ_y}{σ_y} = \rho \frac{x-µ_x}{σ_x}
 \]

Aplicado a la estatura de los hijos respecto de los padres, se interpreta como: ``Si un padre es muy alto, su hijo será alto pero no destacará tanto como el padre''. Este fenómeno lo definió como \concept{Regresión a la mediocridad}.

\end{example}

\begin{defn}[Homocedástico]
$Σ_{2.1}$ no depende de $X_1$.

Esto se da cuando $\begin{pmatrix}X_1,X_2\end{pmatrix}$ es normal multivariante. Si no fueran normal multivariante, serían heterocedásticas. \footnote{Un ejemplo sería $X_1$ la renta de una familia y $X_2$ los ahorros de la misma. Los datos no se distribuyen conjuntamente normal, con lo que la $Σ_{2.1}$ si depende de $X_1$. Ya veremos más adelante este concepto con mayor detalle.}
\end{defn}


\begin{example}

Ahora vamos a ver un par de ejemplos numéricos:

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 \left( \begin{pmatrix}0,0\end{pmatrix}, \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]

\paragraph{Distribución $Y|X$:}

\[E(Y|X) = \frac{3}{10}x\]
\[V(Y|X) = \frac{1}{10}\]

\paragraph{Distribución $X|Y$:}

\[E(X|Y) = 3y\]
\[V(X|Y) = 1\]

Ambas son normales unidimensionales ya que $(X Y)$ es normal multivariante.

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 ...\]

Sea $Z_1 = X+Y$ y $Z_2 = X-Y$.

\[
\begin{pmatrix}Z_1//Z_2\end{pmatrix} = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix} \implies \begin{pmatrix}Z_1\\Z_2\end{pmatrix} = N_2\left(\begin{pmatrix}2\\0\end{pmatrix},\begin{pmatrix}7&1\\1&3\end{pmatrix}\right)
\]

Ahora vamos a calcular lo que nos piden: $E(Z_1|Z_2=1)$.

\[E(Z_1|Z_2=1) = 2 + \frac{1}{3}(1-0) = \frac{7}{3}\]

Es importante destacar que la distribución no depende del valor concreto por ser homocedásticas .

\end{example}

\chapter{Contrastes no paramétricos}
Hipótesis no paramétrica: hipótesis que no se formula en términos de un número finito de parámetros.

\begin{enumerate}
\item \textbf{Bondad de ajuste}: A partir de una muestra $X_1,...,X_n \stackrel{iid}{\sim} F$ de observaciones ({\textcolor{red}{Parra: son muestras o variables aleatorias o es simple notación?}}{\textcolor{blue}{Jorge: son muestras que provienen de v.a. $X_i$ con distribución F}}) ($\stackrel{iid}{\sim}$ significa que son muestras aleatorias independientes idénticamente distribuidas que siguen una distribución F en este caso), contrastar:
\begin{itemize}
\item $H_0: F=F_0$ donde $F_0$ es una distribución prefijada.
\item $H_0: F \in \{F_{\theta} : \theta\in H\}$ H es el espacio paramétrico.
\end{itemize}
\item \textbf{ Homogeneidad}: Dados $X_1,...,X_n \stackrel{iid}{\sim} F$ y $Y_1,...,Y_n \stackrel{iid}{\sim} G$ de observaciones. Contrastar $H_0: F=G$.

(Por ejemplo para ver si el salario de los hombres $F$ tiene la misma distribución que el de las mujeres $G$).

\item \textbf{Hipótesis de independencia}: Dada $(X_1,Y_1),...,(X_n,Y_n) \stackrel{iid}{\sim} F$ de observaciones. Contrastar $H_0: X$ e $Y$ son independientes.

(Por ejemplo para $X$ salario e $Y$ sexo, querríamos ver si el salario es independiente del sexo).
\end{enumerate}

Antes de explicar los contrastes en detalle, vamos a definir y tratar de entender bien algunos conceptos. (quien ya lo entienda que pase de este apartado, que el profesor no lo ha explicado):

\begin{defn}{\textbf{$H_0$ = Hipótesis nula. }}
Más que una definición, es una interpretación: La hipótesis nula es lo que queremos rechazar cuando hacemos el contraste de hipótesis.

\begin{expla}
Es decir, nosotros lo que hacemos es obtener una muestra empírica de unos datos, y lo que vamos a hacer es mirar si podemos decir que NO siguen una distribución en concreto, o por el contrario, no podemos decir nada. Por tanto, el objetivo del contraste es ver si podemos rechazar que los datos siguen esa distribución definida por la hipótesis nula. Pero cuidado, el que no la rechacemos no significa que los datos sigan la distribución, sino que no tenemos suficiente evidencia estadística para afirmar que NO la siguen....
\end{expla}
\end{defn}


\begin{defn}{\textbf{$\alpha$ = nivel de significación. }}
Es la probabilidad máxima que queremos tener de equivocarnos si rechazamos la hipótesis nula. No depende de nada, lo asignamos nosotros en cada problema que queramos resolver.

\begin{expla}
Es decir, si hacemos un contraste de hipótesis con un nivel de significación $\alpha =0.05$, quiere decir, que si finalmente rechazamos la hipótesis nula, asumimos que lo estamos haciendo con un máximo de un $5\%$ de probabilidades de equivocarnos.
\end{expla}
\end{defn}

\begin{defn}{\textbf{p-valor. }}
valor de $\alpha$ mínimo con el que se empieza a rechazar la hipótesis nula. Depende de los datos de partida y de la hiṕotesis nula.
\begin{expla}
Interpretación del p-valor: El p-valor es un número entre 0 y 1, y representa la probabilidad que tenemos de equivocarnos si rechazamos la hipótesis nula. Dicho de otra forma,  el p-valor nos muestra la probabilidad de haber obtenido el resultado que hemos obtenido si suponemos que la hipótesis nula es cierta.

Razonémoslo con un ejemplo:
Supongamos que el p-valor sale 0.40. Esto quiere decir que si rechazamos la hipótesis nula, tenemos un $40\%$ de posibilidades de equivocarnos, por tanto, lo mejor es no rechazarla. Esto cuadra con la teoría, ya que si el p-valor es 0.40 (bastante alto), su valor estadístico asociado (T), es muy pequeño, y por tanto la región de rechazo ($R=\{T>c\}$) es bastante pequeña.

Según la otra interpretación tenemos que el resultado que hemos obtenido tendría un $40\%$ de posibilidades de obtenerse si consideramos que las variables aleatorias siguen la distribución que indica la hipótesis nula (en lugar de la que nos sale empíricamente). Esto también es razonable, ya que un $40\%$ es una probabilidad bastante alta como para rechazarla, por tanto, lo que hacemos es no rechazar la hipótesis nula.

Sin embargo, si nos sale un p-valor igual a 0.01, quiere decir que si rechazamos la hipótesis nula tenemos un $1\%$ de posibilidades de equivocarnos, que es bastante poco, por tanto, tenderemos a rechazarla.
\end{expla}
\end{defn}

Ahora vamos a ver la relación entre el p-valor y el nivel de significación:
\begin{expla}
Supongamos que queremos hacer el contraste de hipótesis con $\alpha=0.05$ y con una hipótesis nula $H_0$ cualquiera. Estudiamos los datos, y obtenemos un p-valor de 0.40. Por tanto, esto significa que si rechazamos la hipótesis nula tendríamos un $40\%$ de posibilidades de equivocarnos. Como nuestro $\alpha=0.05$ significa que solo estamos dispuestos a rechazar la hipótesis nula si tuviéramos un $5\%$ de probabilidades de equivocarnos, pero hemos visto que tenemos un $40\%$, por tanto, no rechazamos.

Supongamos ahora que queremos hacer el contraste de hipótesis con $\alpha=0.05$ pero obtenemos un p-valor de $0.02$. Por tanto, esto significa que si rechazamos la hipótesis nula tendríamos un $2\%$ de equivocarnos. Como hemos decidido que estamos dispuestos a rechazar la hipótesis nula con hasta un $5\%$ de probabilidades de equivocarnos, rechazamos. Si por el contrario imponemos $\alpha=0.01$, no rechazaríamos ya que sólo estaríamos dispuestos a equivocarnos como máximo un $1\%$ de las veces, y el p-valor solo nos asegura un $2\%$.
\end{expla}

\begin{defn}[T = valor Estadístico]
El estadístico es un valor que depende, al igual que el p-valor, de los datos de partida y de $H_0$. P-valor y estadístico están totalmente relacionados, si cambia uno, cambia el otro. Lo utilizamos para construir la región de rechazo.
\end{defn}

\section{Contraste $\chi^2$ de bondad de ajuste}
Consideramos una distribución totalmente especificada bajo $F_0$. Y consideramos una muestra empírica $X_1,...,X_n \stackrel{iid}{\sim} F$.

$H_0: F=F_0$ es la hipótesis nula y queremos ver que F, que es la distribución obtenida con los datos verdaderos (las muestras $X_i$ obtenidas empíricamente) es igual a $F_0$ que es la distribución teórica.

\textbf{Notación: }$P_A(B)$ es la probabilidad de B condicionada a A.

Vamos a definir los pasos que tenemos que seguir para comprobar si $H_0$ es cierta:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$.

\item Se calculan las frecuencias esperadas para cada clase si $H_0$ fuese cierta. A este dato lo llamaremos $\mathbb{E}_i$ o $\mathbb{E}_{H_0}(O_i)$: 
\[
\mathbb{E}_{H_0}(O_i) = np_i
\]

\obs Las $O_i$ son variables aleatorias que se distribuyen como una binomial $B(n, p_i=P_{H_0}(A_i))$. Siendo $n$ el número de intentos y $p_i$ la probabilidad de que una muestra pertenezca a la clase $A_i$ bajo la hipótesis nula. 

La notación puede resultar liosa, a grades rasgos:
\begin{itemize}
\item $O_i$ tendrá un valor que será la frecuencia observada de una clase i. Es decir, el número de observaciones que caen en una la clase i.
\item $\mathbb{E}_{H_0}(O_i)$ será el valor esperado de $O_i$ considerando la hipótesis nula como cierta. La esperanza de una $B(n,p)$ es igual a $np$.
\end{itemize}

\item Se comparan las frecuencias observadas y esperadas mediante el:

\begin{defn}[estadístico de Pearson]
\[T = \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i}\]
\end{defn}

Se divide entre $E_i$ para darle más importancia a la diferencia si el valor es pequeño, Por ejemplo, si E=100 y O=101, no es lo mismo que si E=1 y O=2. Sin embargo, si no dividiéramos por $E_i$ nos daría el mismo resultado.

\item Se rechaza $H_0$ en la región crítica $R=\{T > c\}$ donde c es tal que $\alpha=P_{H_0}(T>c)$. Es decir, $\alpha$ (también llamado 'nivel de significación') es la probabilidad de rechazar la hipótesis nula siendo esta cierta. O dicho de otra forma, la probabilidad de entrar en la región de rechazo $'T>c'$ considerando que $H_0$ es cierta.
\end{enumerate}


Ahora vamos a ver qué podemos decir del estadístico de Pearson 'T':
\[
O_i=B(n,p_i) \simeq N(np_i, np_i(1-p_i))
\]

Imaginémonos por un momento que podríamos despreciar el término $np_i^2$ de la varianza de la normal. Nos quedaría:
\[
\simeq N(np_i, np_i(1-p_i)) \simeq N(np_i, np_i) \simeq N(E_i, E_i)
\]

Que por el Teorema Central del Límite (\url{https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central}) nos queda:
\[
\frac{O_i-E_i}{\sqrt{E_i}} \simeq N(0,1)
\]

Y como deberíamos saber, una distribución $\chi^2_k$  no es más que una distribución de probabilidad continua con un parámetro k que representa los grados de libertad de la variable aleatoria $X = Z_1^2 + \cdots + Z_k^2$, donde $Z_i$ son variables aleatorias normales independientes de media cero y varianza uno. Por tanto:

\[
\frac{(O_i-E_i)^2}{E_i} \simeq \chi^2_1
\]

Por tanto, como tenemos k clases, podríamos tener $T\simeq \chi^2_k$. pero por otro lado sabemos que $O_1+O_2+…+O_k=n$, esta restricción hace que no haya una independencia entre todos los sumandos $O_i$, por tanto nos queda: $T\simeq \chi^2_{k-1}$.

Finalmente nos queda que la región de rechazo, dado un nivel de significación $\alpha$, se alcanza cuando el estadístico de Pearson T, obtenido a partir de los datos muestrales, vale más que $\chi^2_{k-1,α}$. $\chi^2_{k-1,α}$ es, dada la función de densidad de una $\chi^2_{k-1}$, el valor del eje de abscisas que hace que se quede un $\alpha*100\%$ del área encerrada bajo la curva a la derecha de ese valor:

%DIBUJOOOO

\obs
\begin{enumerate}
\item  Tal y como lo hemos definido tenemos que $\sum_{i=1}^k O_i = n$ y que $\sum_{i=1}^k E_i = \sum_{i=1}^k np_i= n(p_1+p_2+...+p_k) = n$, por tanto tenemos:
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \sum_{i=1}^k \frac{O_i^2}{E_i}-n
\]

\item Por deefinición de $\chi^2_{k-1}$, su esperanza es:
\[
\mathbb{E}_{H_0}(T) \simeq k-1
\]

\item Por definición de $\chi^2_{k-1}$, su varianza es:
\[
\mathrm{Var}_{H_0}(T) \simeq 2(k-1)
\]




\end{enumerate}

\begin{example}
Tiramos un dado 100 veces y obtenemos:

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Resultados & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
Frecuencia & 10 & 20 & 20 & 10 & 15 & 25\\
\hline
\end{tabular}

Y consideramos $H_0: p_i=1/6 \text{ } \forall i=1,...,6$. Es decir que el dado no está trucado y cada cara tiene la misma probabilidad ($p_i$) de salir.

Por otro lado consideramos $H_1: \exists i$ tal que $p_i\neq 1/6$. Es decir, que el dado está trucado y hay caras que salen mas que otras.

Seguimos los pasos:
\begin{enumerate}
\item En este caso cada clase será la cara del dado que sale, habrá por tanto 6 clases: k=6.
\item Se cuentan cuantos datos caen en cada clase: $O_1=10$, $O_2=20$, $O_3=20$, $O_4=10$, $O_5=15$, $O_6=25$
\item Se calculan las frecuencias esperadas si $H_0$ fuese cierta. En este caso $n=10+20+20+10+15+25=100$ y $p_i=1/6 \text{ } \forall i=1,...,6$. Nos queda: $\mathbb{E}_i=np_i = 100 \frac{1}{6}=100/6$.
\item Se obtiene el estadístico de Pearson:
\[
T=\sum_{i=1}^n \frac{O_i^2}{E_i}-n = \frac{6}{100}(10^2+20^2+20^2+10^2+15^2+25^2)-100=11
\]
\item Rechazamos $H_0$ si $T>c$. En este caso, consideramos un nivel de significación $\alpha = 0.05$. Sabemos que $\alpha = P_{H_0}(T>c)$ Como tenemos 6 clases, el estadístico de Pearson tendrá una distribución $\chi^2_5$. Buscamos en la tabla (mirar apéndice) y obtenemos que $\chi^2_5, 0.05 = 11.07$. Este será nuestro valor de c.

Puesto que nuestra región de rechazo es $R=(T>c)$, y tenemos que $11>11.07$, no podemos rechazar la hipótesis nula, y por tanto, no podemos concluir que el dado esta trucado. El p-valor en este caso será mayor que $0.05$.

Si consideramos un valor de significación $\alpha = 0.06$ si hubiéramos rechazado la hipótesis nula (aunque con un 6\% de opciones de equivocarnos) y hubiéramos concluido que el dado esta trucado. 

Lo vemos en el siguiente dibujo, que representa la gráfica de una $\chi^2_5$:

\includegraphics[scale=0.75]{img/ejemploEstadistica.png}

La raya vertical se sitúa en $x=11.07$, que es el valor que sale si se mira la tabla de la distribución $\chi^2$ con 5 grados de libertad (ver apéndice) y nivel de significación, por tanto, la zona sombreada es la región de rechazo de la hipótesis nula. Como ha salido T=11, no entramos dentro de esa región de rechazo (por poco) y no podemos rechazar la hipótesis nula.

\end{enumerate}
\end{example}

\begin{theorem}
Bajo $H_0$:
\[
\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} \stackrel{d}{\rightarrow} \chi^2_{k-1} \text{ , si } n \rightarrow \infty
\]
\end{theorem}
\begin{proof}
\textcolor{blue}{Esta demostración es un poco liosa, si no la entendéis, a otra cosa (great pareado).}

Definimos los vectores aleatorios $\xi_1,...,\xi_n$ de la siguiente forma: $\xi_i=(0,...,\overbrace{1}^{(j)},...,0)' \in \mathbb{R}^k \Leftrightarrow x_i \in A_j$. Es decir cada $\xi_i$ va a ser un vector de 0's, salvo porque van a tener un 1 en una posición j. Esta posición j les identificará con la clase $A_j$. Tenemos que:
\[
\xi_1+...+\xi_n=(O_1,...,O_k)'
\]
Es decir, que su suma nos da un vector con las frecuencias de aparición de cada clase (Recordemos que 'k' es el número de clases). Por ejemplo, en el ejemplo del dado tendríamos que $\xi_1+...+\xi_6=(10,20,20,10,15,25)=(O_1,...,O_6)$

\textbf{Notación: } $p\equiv (p_1,...,p_k)'$. $np=(E_1,...,E_k)'$, entonces:
\[
(O_1-E_1,...,O_k-E_k)'=\left\{\sum_{i=1}^n(\xi_i)\right\}-np=n(\overline{\xi}-p)
\]

Definimos la matriz $\mathbb{P}$, que tiene rango k, se define con las probabilidades $p_i$ en la diagonal y 0 el resto de elementos:
\[
\mathbb{P}=\left(
\begin{array}{cccc}
p_1    & 0      & \cdots & 0 \\
0      & p_2    & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0      & 0      & \cdots & p_k \\
\end{array}
\right)
\]

Y cogiendo la raíz del estadístico de Pearson y sabiendo que $\sqrt{E_i}=\sqrt{np_i}$ nos queda:
\[
\left(\frac{O_1-E_1}{\sqrt{(E_1)}},...,\frac{O_k-E_k}{\sqrt{(E_k)}}  \right) = \mathbb{P}^{-1/2}\sqrt{n} (\overline{\xi}-\mathbb{P})
\]

Por otro lado: 
\[
v=(v_1,...,v_k) \rightarrow v'(\frac{v_1}{\lambda_1},...,\frac{v_k}{\lambda_k})
\]

Así, tomamos $\xi_1,...,\xi_n$ independientes y distribuidas como un vector $\xi$ tal que:
\[
\mathbb{E}(\xi)=p
\]
\[
V(\xi)=\mathbb{E}(\xi \xi') -pp' = \mathbb{P}-pp' \equiv \Sigma 
\]

$\mathbb{E}(\xi\xi')=\mathbb{P}$ ya que tenemos:
\[	ξ_r ξ_l =
	\begin{cases}
		0, & r≠l \\
		ξ_r^2 = ξ_r, & r=l \text{ ,pues } ξ_r \text{ es una Bernoulli}
	\end{cases}
\]

Por otra parte: 
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \norm{p^{-1/2}\sqrt{n}(\xi-p)}^2
\]

Por el TCL:
\[
\sqrt{n}(\overline{\xi}-p) \stackrel{d}{\rightarrow} N_k(0, \Sigma) \implies \mathbb{P}^{-1/2}\sqrt{n}(\overline{\xi}-p) \stackrel{d}{\rightarrow} N_k(0, \mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2}) 
\]
\[
\Rightarrow \norm{P^{-1/2} \sqrt{n} (\xi-p)}^2 \stackrel{d}{\rightarrow} \norm{Y}^2 \text{ con } Y \equiv N_k(0,\mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2})
\]

Queda claro que $\mathbb{P}^{-1/2} Σ  \mathbb{P}^{-1/2}$ es simétrica, veamos que es idempotente:
\[\mathbb{P}^{-\frac{1}{2}} (\mathbb{P} - pp') \mathbb{P}^{-\frac{1}{2}} = I-\sqrt{p}\sqrt{p}'\]
donde $\sqrt{p}=(\sqrt{p_1}, …, \sqrt{p_2})'$.

\[
(I-\sqrt{p}\sqrt{p}')(I-\sqrt{p}\sqrt{p}') = I-2\sqrt{p}\sqrt{p}'+\sqrt{p}\underbrace{\sqrt{p}'\sqrt{p}}_{\sum p_i=1}\sqrt{p}'=I-\sqrt{p}\sqrt{p}'
\]

De el ejercicio 9 de la hoja 1 sabemos que una normal multivariante de media 0 y cuya matriz de covarianzas es simétrica e idempotente, cumple que su norma al cuadrado se distribuye como:
\[
\norm{Y}^2 \equiv \chi^2_{k-1}
\]
Los grados de libertad vienen de la traza de $\Sigma$, y de que $traza(I)=k$ y $traza(\sqrt{p}\sqrt{p'})=1$: 
\[
traza(\Sigma)=traza(I-\sqrt{p}\sqrt{p}') = traza(I) - traza(\sqrt{p}\sqrt{p}')=k-1
\]
\end{proof}

\section{Contraste de bondad de ajuste $\chi^2$ para hipótesis nula compuesta}

Problema: $X_1,...,X_n \stackrel{iid}{\sim} F$. Suponemos como hipótesis nula:
\[
H_0 : F \in \{F_{\theta}: \theta \in H \subset \mathbb{R}^r\}
\]

\textcolor{red}{La diferencia es que ahora la hipótesis nula que consideramos es que los datos van a seguir una distribución teórica $F_0$ que no está totalmente especificada, ya que va a depender de un parámetro. Por ello, decimos con palabras que:}

\textcolor{red}{La hipótesis nula es que los datos muestrales van a tener una función de distribución $F$, que va a ser igual a $F_{\theta}$, siendo $\theta$ el parámetro del que dependerá, el cual pertenece a un espacio paramétrico $H$}

Pasos:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$. Hasta aquí todo igual que antes.

\item Para estimar/calcular las frecuencias esperadas se sigue un método ligeramente diferente:

Se estima $\theta$ por el método de máximo verosimilutd. Sea $\hat{\theta}$ el EMV.

\textcolor{red}{explicar bien esto}

\item Se calculan las frecuencias esperadas estimadas bajo $H_0$: $\hat{E}_i=n\hat{p}_i$ con $i=1,...,k$ donde $\hat{p}_i = p_{\hat{\theta}}(A_i)$.

\item Calculamos el estadístico $\chi^2$ de Pearson:
\[
T=\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i}
\]

Ahora puedo elegir de todas las posibles distribuciones, aquella que más se parece. De modo que cabe esperar que T tienda a tomar valores menores que en el caso simple.

Además, al estimar r (\textcolor{red}{¿De dónde sale r? Es la dimensión del parámetro estimado??}) parámetros se introducen r nuevas restricciones sobre el vector $O_1,O_2,...,O_r$.

Se puede probar bajo condiciones de regularidad:
\[
\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i} \stackrel{d}{\rightarrow} \chi^2_{k-1-r} \text{ bajo } H_0 \text{ si } n \rightarrow \infty
\]

\item Se rechaza $H_0$ en la región crítica: $R=\{T>\chi^2_{k-1-r;\alpha}  \}$

Tal y como se ha hecho en el caso anterior.

\end{enumerate}

\begin{example}
Los bombardeos de Londres. El problema trata de estudiar los bombardeos que sufrío Londres entre 1944 y 1945. Se quería saber si los impactos sobre la ciudad de Londres eran en lugares aleatorios o estaban dirigidos a lugares concretos.

La fórmula de Poisson se ajusta bastante a un modelo de distribución aleatoria de impactos. Por tanto, tendríamos que estimar el parámetro $\lambda$ de la distribución de Poisson, que tiene por función de densidad:

$$ f(k,\lambda)=\frac{e^{-\lambda}\lambda^k}{k!} $$

Donde:
\begin{itemize}
\item k es el número de ocurrencias del evento o fenómeno (la función nos da la probabilidad de que el evento suceda precisamente k veces).

\item λ es un parámetro positivo que representa el número de veces que se espera que ocurra el fenómeno durante un intervalo dado. Por ejemplo, si el suceso estudiado tiene lugar en promedio 4 veces por minuto y estamos interesados en la probabilidad de que ocurra k veces dentro de un intervalo de 10 minutos, usaremos un modelo de distribución de Poisson con λ = 10×4 = 40.
\end{itemize} 
Dicho esto, vamos a seguir los pasos anteriormente detallados:



\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. En nuestro caso, las clases van a ser el número de impactos que ha habido en un cuadrado. Por tanto los cuadrados que pertenezcan a $A_1$ serán aquellos que han sufrido un único impacto.

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$. En nuestro caso tenemos: $O_0=229$, $O_1=211$, $O_2=93$, $O_3=35$, $O_4=8$ ($O_4$ es 4 o más impactos).


\item Para estimar/calcular las frecuencias esperadas se estima $\theta$ por el método de máximo verosimilutd. Sea $\hat{\theta}$ el EMV. En este caso, nuestro $\theta$ sera $\lambda$ y nuestro $\hat{\theta}$ será $\hat{\lambda}$, que será el parámetro de la distribución de Poisson:

$$ \hat{\lambda} = \frac{0\cdot229 + 1\cdot211+2\cdot93+3\cdot35+4\cdot7+5\cdot1}{576}=0.9323$$

\textcolor{red}{explicar por qué esto es el EMV, ya que en estadistica 1 hacíamos u lio increible pa sacarlo}

\item Se calculan las frecuencias esperadas $\hat{E}_i=n\hat{p}_i$ con $i=1,...,k$ donde $\hat{p}_i = p_{\hat{\theta}}(A_i)$. En nuestro caso:

$$\hat{E}_k = n\hat{p}_k = 576\cdot e^{-\hat{\lambda}\frac{\hat{\lambda}^k}{k!}}$$

Sustituimos $\lambda = 0.9323$ y $k=0,...,5$ y nos queda: $\hat{E}_0 = 226.74 $, $\hat{E}_1 = 211.34 $, $\hat{E}_2 = 98.54$, $\hat{E}_3 = 30.62$, $\hat{E}_4 = 8.71$.

\item Calculamos el estadístico $\chi^2$ de Pearson:
\[
T=\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i} = 1.0176
\]

Bajo $H_0$ tenemos que $T \equiv \chi^2_3$. El 3 sale de k=5 clases menos 1 parámetro estimado menos 1 como hacíamos antes.

\item Se rechaza $H_0$ en la región crítica: $R=\{T>\chi^2_{k-1-r;\alpha}  \}$

En nuestro caso, tomando $\alpha = 0.05$, tenemos: $$R=\{T>\chi^2_{3;\alpha} \} \rightarrow \{1.0176>7.815 \} \rightarrow \text{No se puede rechazar } H_0$$

Podemos calcular el p-valor mirando:
$$P\{\chi^2_3 > 1.0176\} = 0.797$$

Efectivamente, si miramos la tabla de la $\chi^2_3$, con $\alpha =0.797$, T valdría aproximadamente 1.
\end{enumerate}

\end{example}

\begin{example}
Ejemplo con R de los bombardeos:

Tenemos el siguiente comando para contrastes de bondad de ajuste de $\chi^2$:
\begin{verbatim}
chisq.test(datos,p=...)
\end{verbatim}

\begin{itemize}
\item datos: La muestra de la que disponemos.
\item p: Es el vector de probabilidades esperadas.
\item Por defecto, se contraste la hipótesis de que los datos siguen una distribución uniforme.
\item Se supone que bajo $H_0$ la distribución está completamente especificada (k-1 grados de libertad)
\end{itemize}
\textcolor{red}{Tengo anotado que R sólo funciona con hipótesis simples, y no compuestas, donde tenemos en cuenta eso?}

Exponemos el código a ejecutar y explicamos a continuación lo que hace:

\begin{verbatim}
res = c(seq(0,4),7)
obs = c(229,211,93,35,7,1)
n = sum(obs)
lambda = sum(res*obs)/n
prob = dpois(res,lambda)
esp = n*prob
\end{verbatim}

\begin{enumerate}
\item Guarda en \verb|res| un vector con las clases. Es decir, el número de impactos que ha habido en un cuadrado. Se obtiene:

\verb|res = 0 1 2 3 4 7|

\item Guarda en \verb|obs| un vector con el número de cuadrados de cada clase. Se obtiene:

\verb|obs = 229 211  93  35   7   1|

\item Guarda en \verb|n| el tamaño de la muestra, que es la suma de los elementos del vector \verb|obs|. Se obtiene \verb|n = 576|
\item Guarda en lambda el parámetro de la distribución de Poisson. Se obtiene \verb|lambda = 0.9322917|. Y sale de esta fórmula:
$$ \hat{\lambda} = \frac{0\cdot229 + 1\cdot211+2\cdot93+3\cdot35+4\cdot7+5\cdot1}{576}=0.9323$$
\item Guarda en \verb|prob| un vector con las probabilidades de aparición de cada clase, como la Poisson es una función de distribución discreta, que depende de dos parámetros, lo único que hacemos es sustituir en esta fórmula con $\lambda$ = \verb|lambda| y los valores de k = \verb|res|:
$$e^{-\hat{\lambda}\frac{\hat{\lambda}^k}{k!}}$$

Se obtiene:

\verb|prob = 3.9365e-01 3.6699e-01 1.7107e-01 5.3163e-02 1.2391e-02 4.7812e-05|

\item Guarda en \verb|esp| un vector con las esperanzas de cada clase. Se obtiene:

\verb|esp = 226.74272 211.39035  98.53873  30.62227   7.13722   0.02754|
\end{enumerate}

Continuamos agrupando las clases 4 y 5 es una sola clase, es decir, obteniendo una sola clase que serán los cuadrados con 4 o más impactos:

\begin{verbatim}
obs = c(obs[1:4], sum(obs[5:6]))
prob = c(prob[1:4], 1-sum(prob[1:4]))
esp = c(esp[1:4], n-sum(esp[1:4]))
\end{verbatim}

\begin{enumerate}
\item Obtenemos: \verb|obs = 229 211  93  35   8|
\item Obtenemos: \verb|prob = 0.393650 0.366997 0.171074 0.053163 0.015114|
\item Obtenemos: \verb|esp = 226.7427 211.3903  98.5387  30.6222   8.7059|
\end{enumerate}

Ahora vamos a dibujar el gráfico de barras:

\includegraphics[scale=0.5]{img/contrasteba.png}

\begin{verbatim}
matriz = rbind(prob, obs/n)
rownames(matriz) = c('Frecuencias', 'Poisson')
barplot(matriz, beside=TRUE, names.arg=c(0:4),
 legend.text=TRUE, col=c('lightgreen','orange'))
\end{verbatim}

\begin{enumerate}
\item Guardamos en \verb|matriz| una matriz de dos filas, la primera son las probabilidades teóricas esperadas, la segunda las muestrales:
\begin{verbatim}
          [,1]      [,2]      [,3]       [,4]       [,5]
prob 0.3936506 0.3669971 0.1710742 0.05316368 0.01511444
     0.3975694 0.3663194 0.1614583 0.06076389 0.01388889
\end{verbatim}
\item Asignamos a la primera fila el nombre de 'Frecuencias' y a la segunda 'Poisson'.
\item Pintamos las barras con \verb|barplot|, con leyenda, y como nombre de cada par de barras ponemos 0,1,2,3 y 4, identificando las clases.
\end{enumerate}

Por último calculamos los valores importantes en un contraste, que son el p-valor, que es el mínimo valor de $\alpha$ a partir del cual podemos rechazar la hipótesis nula.

\begin{verbatim}
t = chisq.test(obs,p=prob)$statistic
pvalor = 1-pchisq(t,3)
\end{verbatim}

Obtenemos \verb|t =  1.017589  | y \verb|pvalor = 0.7969959|. El pvalor es muy alto, por tanto no podemos rechazar la hipótesis nula, es decir, no podemos rechazar que los datos proceden de una distribución de Poisson.  El nivel habitual de rechazo sería con $\alpha = 0.05$ que implica que si lo rechazamos tenemos un $5\%$ de posibilidades de equivocarnos. Si quisiéramos rechazar con un $\alpha = 0.79$, tendríamos una probabilidades del $79\%$ de equivocarnos.



\end{example}



\section{Contraste de bondad de ajuste de Kolmogorov-Smirnov}
Sea $X_1,...,X_n \stackrel{iid}{\sim} F$. Definimos la función de distribución empírica, correspondiente a $X_1,...,X_n$ como:

$$ F_n(x)=\frac{1}{n}\#\{i: X_i \leq x\} $$

Es una función de distribución constante a trozos, y con saltos de magnitud $\frac{1}{n}$ en cada valor muestral de $X_i$. Aunque ponga $F_n$, solo hay una para la muestra entera (ya que las variables aleatorias están idénticamente distribuidas), sólo se pone $F_n$ porque depende directamente del número de elementos de la muestra.

Consideramos como hipótesis nula $H_0: F=F_0$. Siendo $F_0$ una distribución previamente especificada

Así, $F_n$ es un estimador de la verdadera distribución F. Que como toda distribución se define como $F(X)=P(X\leq x)$.

\begin{example}
Consideramos una muestra con 3 elementos: $X_1=1, X_2=4, X_3=6$. Ahora, para que sea más fácil construir la función de distribución ordenamos la muestra y nos queda:
$$ X_{(1)}=1 \text{, }X_{(2)}=4 \text{, }X_{(3)}=6 \rightarrow \text{Estos son los estadísticos de orden}$$

Por tanto, la función de distribución queda:

\includegraphics[scale=0.5]{img/contrasteks.png}

Y es bastante razonable. Por ejemplo $P(X=1) = F(1^+)-F(1^-)=\frac{1}{3}-0=\frac{1}{3}$. Algo similar ocurre con $P(X=4) = P(X=6) = \frac{1}{3}$. Lo cual es razonable si nos limitamos únicamente a observar la muestra. Además, para el resto de valores de X, la probabilidad es 0: $P(X=2)=F(2^+)-F(2^-) = \frac{1}{3}-\frac{1}{3}=0$
\end{example}

\begin{obs}
\begin{enumerate}
\item Esta observación sale de sustituir en las fórmulas con las definiciones que hemos dado. Sabiendo que la esperanza de una binomial es $\mathbb{E}\Big(B(n,p)\Big)=np$ $$nF_n(x) = \#\{i:X_i\leq x \} \equiv B(n, F(x))  \Rightarrow \mathbb{E}\Big[F_n(X)\Big] = \frac{1}{n} n F(x) = F(x)$$
\item Con el mismo razonamiento, pero sabiendo que si $X \sim B(n,p)$, entonces $Var(X)=np(1-p)$:
$$ Var(F_n(X))=\frac{1}{n^2}nF(x)(1-F(x)) \stackrel{n \rightarrow \infty}{\rightarrow} 0$$
\item Como consecuencia:
$$ F_n(X) \stackrel{P}{\rightarrow} F(X)$$

Convergencia en probabilidad o en medida: Si $\forall \epsilon >0$, $\lim_{n \rightarrow \infty}P(\abs{X-X_n}\geq \epsilon)=0$.
\end{enumerate}
\end{obs}
De hecho, se cumple que (lema de Glivenko-Cantelli):
$$\norm{F_n-F}_{\infty} = \sup\abs{(F_n(X)-F(x)} \stackrel{c.s.}{\rightarrow} 0$$

Si $H_0: F=F_0$ fuese cierta, se espera que $D_n = \norm{F_n-F_0}_{\textcolor{red}{\infty}}$ sea pequeño ($D_n$ es el estadístico de Kolmogorov-Smirnov). La idea es rechazar en la región $R=\{D_n > C\}$, para un valor c tal que $P_{H_0}(D_n > c)=\alpha$, donde $\alpha$ es el nivel de significación.

\textcolor{red}{\textbf{Importante:}la distribución bajo $H_0$ de $D_n$ es la misma para cualquier distribución continua $F_0$. El valor de c en la región crítica es el mismo para cualquier distribución continua $F_0$ y esta tabulado. $F_0$ es la distribución teórica a la que queremos ver si pertenecen los datos. Mientras que $F=F_n$ que es la empírica.}

\begin{prop}
Si una v.a. X tiene distribución continua (\textcolor{red}{Continua por la derecha en todo caso}) $F_0$, entonces la v.a. $F_0(X) \sim U(0,1)$ (Uniforme en (0,1)).
\end{prop}
\begin{proof}
Queremos ver que $P(F_0(X)\leq u)=u$ $\forall u \in [0,1]$ (que es lo que ocurriría si $F_0$ siguiera una distribución uniforme entre 0 y 1).

Así, sea $F_0$ continua, entonces existe un x tal que $F_0(x)=u$. Y tendríamos que:
$$ P(F_0(X)\leq u) = P(F_0(X)\leq F_0(x)) $$

Ahora sabiendo que la función de distribución es monótona creciente (m.c.), del primer miembro nos quitamos $F_0(X)\leq F_0(X)$ y del segundo, el menor o igual, ya que si $X>x$ solo puede ser que $F_0(X)= F_0(X)$:
$$\{ F_0(X)\leq F_0(x) \} = \{ F_0(X)\leq F_0(x), X \leq x\} \cup \{ F_0(X)\leq F_0(x), X > x\} = $$
$$ =  \{ X \leq x\} \cup \{ F_0(X) \underbrace{=}_{m.c.} F_0(x), X > x\}$$

Y, basándonos en que $F_0(X)=P(X \leq x)=u$ y en que la probabilidad es 0 en un trozo donde la función de distribución es constante, nos qued: 
$$P(F_0(X)\leq F_0(x)) = P(X \leq x) + P(F_0(X) = F_0(x), X \geq x) = F_0(X) + 0 = u$$
\end{proof}

\begin{obs}
Existe un recíproco de la proposición: Si $U \sim U(0,1) \Rightarrow F^{-1}(U) \sim F$. \textcolor{red}{Explicar mejor este recíproco}
\end{obs}

La $D_n$ de la que estábamos hablando antes de meternos en la proposición se conoce como:

\begin{defn}[Estadístico Kolmogorov-Smirnov]
	\[D_n = \max \left\{ 0, \max_{i=1,…,n} \left( \frac{i}{n} - F_0(x_{(i)}) \right), \max_{i=1,…,n} \left(F_0(x_{(i)}) - \frac{i-1}{n})\right) \right\}\]
\end{defn}

Y a continuación vamos a demostrar por qué tiene la expresión que aparece en la definición.
\begin{proof}

$$ D_n = \max \left\{ \sup_{x\in\mathbb{R}} \Big( F_n(x)-F_0(x)\Big), \sup_{x\in\mathbb{R}} \left( F_0(x)-F_n(x)\right) \right\} $$

Si representamos los estadísticos de orden de la muestra en una recta, y llamamos a $X_{(0)}=-\infty$ y $X_{(n+1)}=\infty$:

\includegraphics[scale=0.5]{img/contrasteks2.png}

Nos queda que si x está entre $X_{(i)}$ y $X_{(i+1)}$, entonces $F_n(x)=\frac{i}{n}$.

Desarrollando el primer término de $D_n$ nos queda:
$$ \sup_{x\in\mathbb{R}} \Big( F_n(x)-F_0(x)\Big) = \max_{i=0,...,n} \Bigg(\sup_{x\in(X_{(i)},X_{(i+1)})} \Big( F_n(x)-F_0(x)\Big) \Bigg) = $$

$$ = \max_{i=0,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big) = \max\Big\{ 0, \max_{i=1,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big) \Big\}$$

\textcolor{red}{Explicar por qué es $\frac{i}{n}$} % Jorge: Es por definición de la función de distrib. empírica.

Desarrollando el segundo término nos queda:
\[\sup_{x∈ℝ}\left( F_0(x) - F_n(x) \right) = \max_{j=0,…,n} \left( \sup_{x∈(x_{(j)},x_{(j+1)})} \left( F_0(x) - F_n(x) \right) \right) =\]

\[= \max_{j=0,…,n} \left( F_0(x_{(j+1)}) - \frac{j}{n} \right) \underbrace{=}_{i=j+1} \max_{i=1,…,n+1} \left( F_0(x_{(i)}) - \frac{i-1}{n} \right) =  \]
$$ = \left\{ 0, \max_{i=1,...,n} \left( F_0(X_{(i)}) - \frac{i-1}{n} \right) \right\} $$

\textcolor{red}{Explicar por qué es $\frac{i-1}{n}$}

Por tanto, finalmente nos queda:
$$ D_n =  \norm{F_n-F_0}_{\infty} = \max\Big\{ 0, \max_{i=1,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big) , \max_{i=1,...,n} \Big( F_0(X_{(i)}) - \frac{i-1}{n} \Big)$$

Por tanto concluimos que $D_n$ depende de $F_0$ a través de los valores de $F_0(X_{(1)}), F_0(X_{(2)}),...,F_0(X_{(n)})$. Si tengo una muestra de $X_1,...,X_n \stackrel{iid}{\sim}F_0$, entonces $F_0(X_1),...,F_0(X_1) \stackrel{iid}{\sim} U(0,1)$. Ordenándolos los elementos: $X_{(1)}\leq...\leq X_{(n)}$, entonces $F_0(X_{(1)})\leq...\leq F_0(X_\textbf{{(n)}}) \stackrel{iid}{\sim} U(0,1)$. Que son los estadísticos de orden de una muestra de tamaño n, de variables aleatorias iid, que siempre van a seguir una distribución de una $U(0,1)$ para toda $F_0$ continua.

\textbf{Notación}
$$D_n^+=\max_{i=0,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big)$$
$$D_n^-= \max_{i=0,...,n} \Big( F_0(X_{(i)}) - \frac{i-1}{n} \Big)$$

\end{proof}

\begin{example}
Ejemplo con R:

Tenemos el siguiente comando para contrastes de bondad de ajuste de Kolmogorov-Smirnov:
\begin{verbatim}
ks.test(datos,distribucion,parametros)
\end{verbatim}

\begin{itemize}
\item \verb|datos|: La muestra de la que disponemos.
\item \verb|distribucion|: Distribución bajo $H_0$. Es la distribución que creemos teórica de los datos, la que hemos llamado $F$. (Por ejemplo, $pnorm$).
\item \verb|parametros|: Parámetros de la distribución $F$. 
\end{itemize}
Vamos a probar a usar los datos 'Kevlar'. Corresponden al tiempo hasta el fallo (en horas) de 101 barras de un material utilizado en los transbordadores espaciales.

Obtenemos los datos de \url{http://www.uam.es/personal_pdi/ciencias/acuevas/docencia/estI/Datos-kevlar.txt}. Los metemos en un archivo de texto $kevlar.txt$.

Ejecutamos:

\begin{verbatim}
kev = scan('kevlar.txt')

boxplot(kev)

hist(kev)

plot(ecdf(kev), verticals=TRUE, do.points=FALSE)
curve(pexp(x), add=TRUE, col='red')
\end{verbatim}
 
Y obtenemos estas tres figuras:

\includegraphics[scale=0.8]{img/contrasteks3.png}
\includegraphics[scale=0.5]{img/contrasteks4.png}

En esta última observamos perfectamente la función $F_n$ constante a trozos con valores $\frac{1}{n}$. Hemos contrastado la muestra con la hipótesis nula de que los datos siguen una distribución exponencial de parámetro $\lambda = 1$. (esta es la recta roja que sale con $pexp(x)$).

Por último ejecutamos:
\begin{verbatim}
ks.test(kev,pexp)
\end{verbatim}

Y obtenemos:
\begin{verbatim}
data:  kev
D = 0.087038, p-value = 0.4286
alternative hypothesis: two-sided
\end{verbatim}

Si ejecutamos \verb|ks.test(kev, pnorm)$statistic|, obtenemos solo el valor del estadístico: \verb|0.08703787| 

\end{example}

\begin{example}
Contrastar a nivel $\alpha=0.01$ si la muestra $X_1=16, X_2=8, X_3=10, X_4=12, X_5=6$ procede de una distribución exponencial de media $11.5$.

Sea X una v.a con distribución exponencial, tiene función de distribución:
$$F_0(X) = 1-e^{-\lambda x} \text{ si x } \geq 0$$

Y sabemos que $\mathbb{E}(X)=\frac{1}{\lambda}$. De esto, sacamos que en nuestro caso $\lambda = \frac{1}{11.5}$ 

\begin{tabular}{|c|c|c|c|c|}
\hline
 $X_{(i)}$ & i/n & $F_0(X_{(i)})$ & $D_n^+$ & $D_n^-$ \\
\hline
6 & 0.2 & 0.41 & -0.21 & 0.41 \\
\hline
8 & 0.4 & 0.5 & -0.1 & 0.3 \\
\hline
10 & 0.6 & 0.58 & 0.02 & 0.18 \\
\hline
12 & 0.8 & 0.65 & 0.15 & 0.05 \\
\hline
16 & 1 & 0.75 & 0.25 & -0.05 \\
\hline
\end{tabular}

Así, nos queda que $D_n = 0.41$.

Y mirando en la tabla de la exponencial con nivel de significación $\alpha = 0.01$, tenemos que c= \textcolor{red}{terminar ejercicio}

\end{example}

\section{Gráficos de probabilidad}
Sean $X_1,...,X_n \stackrel{iid}{\sim} F \Rightarrow F(X_1),...,F(X_n) \stackrel{iid}{\sim} U(0,1)$. Si ordenamos las F nos quedan los estadísticos de orden de una $U(0,1)$: $F(X_{(1)}),...,F(X_{(n)})$.

Por tanto, si tengo una muestra de tamaño 2, entonces la media sería que F del dato más pequeño $F(X_{(1)})$ sea $\frac{1}{3}$ y que el dato más grande $F(X_{(2)})$ sea $\frac{2}{3}$. Ya que hemos estimado que F sigue una distribución uniforme en $[0,1]$.

De la misma forma, si hay n datos, la media sería que el dato mínimo se encuentre en $\frac{1}{n+1}$ y el dato máximo en $\frac{n}{n+1}$.

En definitiva, tenemos que la media del valor de F del dato i-ésimo es:
$$\mathbb{E}(F(X_{(i)}) \approx \frac{i}{n+1}$$

Es decir, tendríamos que $F(X_{(i)}) \approx \frac{i}{n+1}$ y por tanto, debería ocurrir que $X_{(i)} \approx F^{-1}(\frac{i}{n+1})$. Si esto ocurre, tendríamos una gráfica que representaría la recta y=x, en el eje de ordenadas tendríamos $F^{-1}(\frac{i}{n+1})$ y en el eje de abscisas tendríamos $X_{(i)}$. La idea es que si esto ocurre los datos vienen de una normal, es decir: $X_1,...,X_n \stackrel{iid}{\sim} F=N(\mu, \sigma)$.

Además, sea $\Phi \sim N(0,1)$. Sea $F(X) = \Phi(\frac{x-\mu}{\sigma})$ entonces:
$$ X_{(i)} = F^{-1}\Big(\frac{i}{n+1}\Big) = \sigma \Phi^{-1}\Big(\frac{i}{n+1}\Big)+ \mu $$

Se representa la gráfica:
$$ \bigg( X_{(i)}, \Phi^{-1}\Big(\frac{i}{n+1}\Big) \bigg)$$

Si la gráfica es una recta, no necesariamente de pendiente 1, quiere decir que los datos son normales.

Aquí tenemos 12 ejemplos:

\includegraphics[scale=0.6]{img/graficos1.png}

\includegraphics[scale=0.6]{img/graficos2.png}

\section{Contraste $\chi^2$ de homogeneidad}
Sean un conjunto de muestras tomados de diferentes lugares o contextos $M_1,...,M_p$. Cada conjunto de muestras seguirá una distribución $F_1,..,F_p$.

\begin{align*}
M_1 \equiv & X_{11} .... X_{1\cap 1} \stackrel{iid}{\sim} F_1 \\
& \vdots \\
M_p \equiv & X_{p1} .... X_{p\cap p} \stackrel{iid}{\sim} F_p
\end{align*}

\textcolor{red}{lo de la intersección en $X_{p\cap p}$ que es??}

Consideraremos como hipótesis nula: $H_0: F_1=...=F_p$. Es decir, queremos ver si las muestras tomadas de diferentes lugares siguen la misma distribución.

Dividimos los datos de cada conjunto de muestras en clases $A_1,...,A_k$, todos los conjuntos $M_1,...,M_p$ tendrán los mismos tipos de clases. Y consideramos las frecuencias observadas: $O_{ij}=$ nº de datos de $M_j$ en $A_i$.

Llamamos tabla de contingencia a la siguiente tabla:

\begin{tabular}{cccc}
& $M_1$ & ... & $M_p$ \\
$A_1$ & $O_{11}$ &  & $O_{1p}$ \\
\vdots &  & $O_{ij}$ &  \\
$A_k$ & $O_{k1}$ &  & $O_{kp}$ \\
\end{tabular}

Cada elemento $O_{ij}$ de la tabla es el número de muestras de una clase para cada conjunto de muestras. Queremos estimar este valor mediante una binomial (\textcolor{red}{por que?}). Así:
$$ O_{ij,H_0} \equiv B(n_j, p_i)  \text{ con } p_i=P_{H_0}(A_i)$$

Pero desconocemos este valor $P_{H_0}(A_i)$, por lo que lo tenemos que estimar.

Llamamos $E_ij =n_j p_i$ frecuencia esperada bajo $H_0$.

Realizamos la siguiente operación \textcolor{red}{El por qué es aún un misterio}:
$$ \sum_j \underbrace{\sum_i \frac{(O_{ij}-E_ij)^2}{E_{ij}}}_{\chi^2_{k-1}} \stackrel{d}{\rightarrow} \underbrace{\chi^2_{p(k-1)}}_{p\chi^2_{k-1}} $$

Queremos estimar $p_1,...,p_k$:
$$\hat{p}_i=\frac{\sum_{j=1}^p O_{ij}}{n}$$

Con $n=n_1+...+n_p$, como tenemos homogeneidad, es como si tuviéramos $n_1+n_2+...+n_p$ datos en total. 

Ahora podemos calcular la esperanza estimada:
$$\hat{E}_{ij} = n_j \hat{p}_i=n_j \cdot \frac{\sum_j O_{ij}}{n}$$

\textbf{Notación:} 
\begin{itemize}
\item $\sum_j O_{ij} = O_{i\cdot}$
\item $\sum_j O_{ij} = n_j =  O_{\cdot j}$
\end{itemize}
Por tanto:
$$\hat{E}_{ij} = \frac{O_{i\cdot} O_{\cdot j}}{n}$$

Y ahora hacemos una tabla parecida a la anterior pero con las esperanzas estimadas:

\begin{tabular}{cccc}
& $M_1$ & ... & $M_p$ \\
$A_1$ & $\hat{E}_{11}$ &  & $\hat{E}_{1p}$ \\
\vdots &  & $\hat{E}_{ij}$ &  \\
$A_k$ & $\hat{E}_{k1}$ &  & $\hat{E}_{kp}$ \\
\end{tabular}

Ahora con los estimadores obtenidos:
$$ \sum_i \sum_j \frac{(O_{ij}-\hat{E}_{ij})^{\textcolor{red}{2}}}{\hat{E}_{ij}} \stackrel{d}{\rightarrow} \chi^2_{(p-1)(k-1)} $$

Como podemos observar, la $\chi^2$ no es de $p(k-1)$ como antes sino que es de $p(k-1)-(k-1)$, los últimos $k-1$ son el nº de parámetros estimados. Ya que estimamos $\hat{p}_1,...,\hat{p}_k$, pero con la condición $\hat{p}_1+...+\hat{p}_k  1$.

La región de rechazo quedaría:
$$ R=\{T > \chi^2_{(k-1)(p-1),\alpha}\}$$

\begin{obs}
Se puede comprobar que:
$$ T = \sum_{i=1}^k \sum_{j=1}^p \frac{O_{ij}}{E_{ij}}-n$$

\textcolor{red}{Y antes dónde habíamos definido T??}
\end{obs}

\begin{example}
Tenemos 3 muestras, una de España, otra de Italia y otra de Francia, todas de tamaño $n=100$.Las clases son 'no fumadores' (NF), 'fumadores ocasionales' (FO) y 'fumadores habituales' (FH).

Tenemos la siguiente tabla de contingencia:

\begin{tabular}{ccccc}
& $M_1=$España & $M_2=$Italia & $M_3=$Francia & \\
$A_1=$NF & $O_{11}=30$ & $O_{12}=15$ & $O_{13}=20$ & $O_{1\cdot}=65$ \\
$A_2=$FO & $O_{21}=50$ & $O_{22}=40$ & $O_{23}=50$ & $O_{2\cdot}=140$ \\
$A_3=$FH & $O_{31}=20$ & $O_{32}=45$ & $O_{33}=30$ & $O_{3\cdot}=95$ \\
& $O_{\cdot1}=n_1=100$ & $O_{\cdot2}=n_2=100$ & $O_{\cdot3}=n_3=100$ & 300 \\
\end{tabular}

Recordamos la fórmula de la esperanza estimada:
$$\hat{E}_{ij} = n_j \cdot \frac{\sum_j O_{ij}}{n} = \frac{O_{i\cdot} O_{\cdot j}}{n}$$

Vamos a calcular la esperanza estimada $\hat{E}_{12}$ es decir, la de Italia y no fumadores:

$$\hat{E}_{12} = n_2 \hat{p}_1 = n_2 \cdot \frac{\sum_{j=1}^3 O_{1j}}{n} = 100 \cdot \frac{30+15+20}{300} = 21,\stackrel{\frown}{6}$$

Así, la tabla de esperanzas quedaría:

\begin{tabular}{ccccc}
& España & Italia & Francia & \\
NF & $\hat{E}_{11}=21,\stackrel{\frown}{6}$ & $\hat{E}_{12}=21,\stackrel{\frown}{6}$ & $\hat{E}_{13}=21,\stackrel{\frown}{6}$ & 65 \\
FO & $\hat{E}_{21}=46,\stackrel{\frown}{6}$ & $\hat{E}_{22}=46,\stackrel{\frown}{6}$ & $\hat{E}_{23}=46,\stackrel{\frown}{6}$ & 140 \\
FH & $\hat{E}_{31}=31,\stackrel{\frown}{6}$ & $\hat{E}_{32}=31,\stackrel{\frown}{6}$ & $3\hat{E}_{33}=31,\stackrel{\frown}{6}$ & 95 \\
& 100 & 100 & 100 & 300 \\
\end{tabular}

Ahora calculamos el estadístico T:
$$ T = \sum_{i=1}^k \sum_{j=1}^p \frac{O_{ij}}{E_{ij}}-n = $$
$$\frac{30}{21,\stackrel{\frown}{6}} + \frac{15}{21,\stackrel{\frown}{6}} + \frac{20}{21,\stackrel{\frown}{6}} + \frac{50}{46,\stackrel{\frown}{6}} + \frac{40}{46,\stackrel{\frown}{6}} +
\frac{50}{46,\stackrel{\frown}{6}} +
\frac{20}{31,\stackrel{\frown}{6}} +
\frac{45}{31,\stackrel{\frown}{6}} +
\frac{30}{31,\stackrel{\frown}{6}} = 9$$

\textcolor{red}{No sale lo esperado, revisar y terminar}

La región de rechazo es:
$$ R=\{T > \chi^2_{(k-1)(p-1),\alpha}\}$$

En nuestro caso, suponiendo un nivel de significación $\alpha=0.05$:
$$ R=\{T > \chi^2_{(3-1)(3-1),0.05}\} \Rightarrow R=\{T > \chi^2_{4,0.05}\} \Rightarrow  R=\{T > 9.488\}$$
\end{example}

\section{Contraste Kolmogorov-Smirnov de homogeneidad}
Este contraste sólo es válido para dos muestras, y para distribuciones continuas. Al igual que antes queremos ver que las dos muestras tienen la misma distribución.

Así, tenemos $X_1,...,X_n \stackrel{iid}{\sim} F$ y $Y_1,...,Y_n \stackrel{iid}{\sim} G$, con F y G continuas. La hipótesis nula será $H_0: F=G$, es decir, los datos de la primera muestra están distribuidos con la misma función de distribución que los datos de la segunda muestra.

Para ello calculamos el estadístico K-S para dos muestras:

$$D_{n,m} = \norm{F_n -G_m}_{\infty} = \sup_{x \in \mathbb{R}} \abs{F_n(x)-G_m(x)}  $$

Bajo $H_0$ la distribución $D_{n,m}$ no depende de F=G y está tabulada.
$$ R=\left\{D_{n,m} > C_{\alpha}\right\} $$

\section{Contraste $\chi^2$ de independencia}
Sea $(X_1, Y_1),...,(X_n, Y_n) \stackrel{iid}{\sim} F$. Y sea la hipótesis nula $H_0 :$ X e Y son independientes.

\textcolor{red}{terminar esto en otro momento que no lo veo muy claro}

\appendix
\chapter{Ejercicios}
\input{tex/EstII_ejercicios.tex}

\chapter{Distribuciones, tablas}
\includepdf[pages={1}]{pdf/_chicuadrado.pdf}


\chapter{Prácticas}
Se incluyen las soluciones de las prácticas:
\includepdf[pages={1-6}]{pdf/_p1E2.pdf}
\includepdf[pages={1-8}]{pdf/_p2E2.pdf}



\end{document}
