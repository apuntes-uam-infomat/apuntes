\documentclass{apuntes}
\author{Guillermo Julián Moreno}
\date{13/14 C1}
\title{Estadística I}

\begin{document}

\pagestyle{plain}
\maketitle

\tableofcontents
\newpage

\section{Estadística descriptiva de datos univariantes}

La estadística descriptiva es el conjunto de técnicas para resumir la información proporcionada por una gran masa de datos. El primer objetivo natural es resumir la información que proporcionan esos datos.

\subsection{Estadísticos de tendencia central}
 
\begin{defn}[Media]

\[ \avg{x} = \frac{\sum_{i=1}^n x_i}{n} \]

Es la medida de tendencia central más utilizada. Es bastante sensible a los valores atípicos (\textit{outliers}), observaciones anormalmente grandes que aparecen en el conjunto de datos por errores de transcripción o medición.

\end{defn}

\begin{defn}[Mediana]
Es el valor que divide a los datos en dos mitades, de tal forma que la mitad son menores y la otra mitad mayores que la mediana. 

La mediana se calcula de la siguiente forma: dado un conjunto de datos $\{x_1,\dotsc, x_n\}$, la mediana es $x_{\frac{n+1}{2}}$ si $n$ es impar y  el promedio entre $x_{\frac{n}{2}}$ y $x_{\frac{n}{2} + 1}$.
\end{defn} 

\subsection{Estadísticos de dispersión}

\begin{defn}[Varianza]
\[ \sigma^2 = \frac{1}{n} \sum_{i=1}^n \left(x_i - \avg{x}\right)^2 = \frac{1}{n} \sum_{i=1}^n x_i^2 - \avg{x}^2 \]
\end{defn}

\begin{defn}[Desviación\IS típica]
\[\sigma = \sqrt{\sigma^2} \]

La desviación típica es la raíz de la varianza.
\end{defn}

\begin{defn}[Cuantil]
Para $p\in (0, 1)$ se llama cuantil $p$ o $q_p$ al valor que deja el $100p \%$ de los datos a la izquierda.
\end{defn}

\begin{defn}[Cuartil]
Los cuartiles son los tres datos que dejan a la izquierda el 25, 50 y 75 por ciento de los datos respectivamente. Es decir:

\begin{itemize}
\item $Q_1 = q_{0.25}$
\item $Q_2 = q_{0.5}$. El cuartil dos es la mediana.
\item $Q_3 = q_{0.75}$
\end{itemize}
\end{defn}

Hay varios métodos para el cálculo de cuantiles. Para hacerlo a mano, podemos usar el siguiente método.

Si el dato en la posición $p(n+1)$ no es un número entero, entonces se interpola entre las observaciones ordenadas que están en la posición $\floor{p(n+1)}$ y $\floor{p(n+1)} + 1$ de la siguiente forma: sea $j$ la parte entera de $p(n+1)$ y $m$ la parte decimal. Entonces, \[ q_p = (1-m)x_j + m x_{j+1} \]


\begin{defn}[Coeficiente\IS de asimetría]
\index{Skewness}
El tercer momento con respecto a la media se define como \[ \frac{1}{n}\sum_{i=1}^n\left(x_i-\avg{x}\right)^3 \] que, en su versión adimensional dividimos por $\sigma^3$.
\end{defn}

Al ser una función cúbica, los valores que se alejen mucho de la media tendrán un valor muy alto en valor absoluto (positivo o negativo según se aleje por la derecha o izquierda, respectivamente). Si la distribución de datos es muy asimétrica, los valores más altos no se cancelan con los valores altos del otro lado (porque no hay) y saldrá un valor más alejado de cero.\footnote{Está explicado como el p. culo, ya.}

\subsection{Representación gráfica de datos}

\begin{defn}[Box-plot]
El diagrama de caja o \textit{box-plot}  (imagen \ref{imgCaja}) nos permite visualizar las medidas de dispersión respecto a la mediana. Hay que añadir una nueva medida, el \textbf{rango intercuartílico}\index{Rango!intercuartílico}, la diferencia entre el primer y el tercer cuartil: \[RI = Q_3 - Q_1 \]

\easyimg{DiagramaCaja.png}{Diagrama de caja}{imgCaja}
\end{defn}

\begin{defn}[Histograma]
El histograma se trata de una aproximación discreta a la función de densidad continua $f(t)$ de la variable que estamos midiendo. Es un diagrama de frecuencias que \textit{mantiene la forma} de esa función de densidad. 

Definimos una serie, las marcas de intervalos $a^n_1, \dotsc, a^n_n$, donde $n$ es el número de intervalos y la longitud de cada intervalo  es $h_n = a^n_{j+1} - a^n_j$. Sea el conjunto $\{x_i\}_{i=0,\dotsc,m}$ los datos de nuestra muestra. Entonces, el estimador, la función $\hat{f}_n$, se define de la siguiente forma:

\[ \hat{f}^n(t) = \frac{\card{i \tq x_i \in \left( a_j^n, a_{j+1}^n \right]}}{n h_n} = \frac{\sum_{i=1}^m \ind_{(a_j^n, a_{j+1}^n]} (x_i)}{n h_n} \]

Recordemos que \[ \ind_A (n) = \begin{cases} 1 & n \in A \\ 0 & n \notin A\end{cases}\]

A grandes rasgos, lo que hace en una función es definir un número de intervalos fijos de ancho $h_n$. Al evaluar $\hat{f}^n(t)$ buscamos en qué intervalo cae $t$ y contamos cuántas de nuestras mediciones caen también en ese intervalo.

\easyimg{DensidadAHistograma.png}{El histograma es una aproximación de la función de densidad real en base a la muestra que hemos obtenido.}{lblDensidad}

\end{defn}

\subsubsection{Estimadores núcleo o kernel}

\begin{defn}[Método de ventana móvil][Ventana móvil]
El método de ventana móvil nos da una estimación de la función de densidad en un punto $t$ midiendo los $x_i$ que están en el intervalo de radio $h_n$ centrado en $t$. Matemáticamente:

\[ \hat{f}_n(t) = \frac{1}{n2h_n}\sum_{i=1}^n \ind_{[t-h_n, t+h_n]}(x_i) = \frac{1}{n2h_n}\sum_{i=1}^n \ind_{[-1,1]}\left(\frac{t-x_i}{h_n}\right) \]
\end{defn}

Podemos reemplazar la función $\frac{1}{2}\ind_{[-1, 1]}$ por otra, llamada la función de densidad $K$, kernel o núcleo:

\begin{defn}[Estimador\IS núcleo]
Dada una función de densidad $K$ simétrica, no necesariamente positiva, definimos el estimador kernel como:

\[ \hat{f}_n(t) = \frac{1}{n}\sum_{i=1}^n K_h (t - x_i)  = \frac{1}{nh_n} \sum_{i=1}^n K\left(\frac{t-x_i}{h_n}\right) \]

con $K_h(x) = \frac{1}{h}K(\frac{x}{h})$.
\end{defn}

La elección del núcleo $K$ no afecta especialmente a lo bien aproximada que esté la función de densidad. Sin embargo, sí que influye la selección de la ventana $h_n$ (figura \ref{lblSuavizado}), también llamada \textit{bandwith} en inglés.  Si escogemos una ventana muy pequeña, damos demasiado peso a los datos de nuestra muestra. Si elegimos una ventana muy grande, nuestra muestra pierde importancia y podemos perder información importante.

La elección del $h_n$ más habitual es el que minimiza la distancia $L^2$ entre $\hat{f}$ y $f$, es decir, el parámetro que minimice $\displaystyle\int\left(\hat{f}_h-f\right)^2$. Sin embargo, hay un problema: no sabemos qué es $f$. Hay trucos que imagino que veremos más tarde.

\easyimgw{Suavizado.png}{Los efectos que causa elegir una ventana más grande o más pequeña en el estimador}{lblSuavizado}{1}

Las funciones kernel más usadas son la uniforme, $\frac{1}{2}\ind_{[-1, 1]}$, la gaussiana $\frac{1}{\sqrt{2 \pi}}e^{-\frac{t^2}{2}}$ y la de Epanechnikov, que matemáticamente es la que mejor aproxima $f$.

El estimador kernel $\hat{f}_n(t)$ es la función de densidad de una medida de probabilidad que es la convolución \footnote{Ya aprenderemos en al algún momento de nuestra vida qué narices es una convolución} de dos medidas de probabilidad: una, $K_h(x)$ (el kernel reescalado) y otra que da probabilidad $\frac{1}{n}$ a cada punto de la muestra $\{x_i\}$ (distribución o medida empírica).

\paragraph{Generación de datos del estimador kernel} Supongamos que $K$ es el núcelo gaussiano. Podemos generar datos artificiales de la densidad así:

\[ x_i^0 = x_i^* + h_n Z_i,\; i=1,\dotsc, k \]

donde $x_i^*$ es una observación elegida al azar entre los datos originales y $Z_i$ una observación aleatoria con probabilidad $N(0,1)$. Es decir, lo que hacemos es añadir un dato aleatorio de la muestra y sumamos una pequeña perturbación aleatoria.

\section{Estadística descriptiva de datos bivariantes}

En esta sección estudiaremos dos variables $(X, Y)$ para explorar la relación entre ambas y tratar de inferir si existe una relación funcional para predecir los valores de una variable en función de los de la otra.

\subsection{Representación gráfica}

\begin{defn}[Diagrama\IS de dispersión]
El diagrama de dispersión representa cada variable en función de la otra para que podamos ver la posible relación entre ambas. Ver figura \ref{lblDispersion}.

\easyimg{Dispersion.png}{Diagrama de dispersión}{lblDispersion}
\end{defn} 

\subsection{Regresión}

\begin{defn}[Recta de regresión]

La recta de regresión de $y$ sobre $x$ es la recta de forma $\hat{y} = \hat{a} + \hat{b}x$ que más se aproxima a los datos, minimizando los cuadrados de la distancia: \[ (\hat{a},\hat{b}) =\argmin_{a, b} \sum_{i=1}^n\left(y_i - a - bx_i)\right)^2 \]
\end{defn}

La recta de regresión se calcula obteniendo primero $\hat{b}$:

\[ \hat{b} = \frac{\sigma_{x,y}}{\sigma^2_x} \]

donde \[ \sigma_{x,y} = \frac{1}{n} \left( \sum_{i=1}^n x_i y_i\right)  - \avg{x}\avg{y} \] y después, sabiendo que la recta pasa por el punto $(\avg{x}, \avg{y})$, obtenemos $\hat{a}$ \[ \hat{a} = \avg{y} - \hat{b}\avg{x} \]

El valor $b$ se denomina \textbf{coeficiente de regresión lineal}\index{Regresión lineal!coeficiente de} o parámetro de la regresión. Cada valor $e_i= y_i - \hat{y}_i$ se denomina \textbf{residuo}\index{Residuo}. Hay que notar que

\begin{gather*}
 \sum_{i=1}^n e_i = \sum_{i=1}^n \left(y_i - \hat{a} -\hat{b}x_i \right)= \sum_{i=1}^n\left( y_i - (\avg{y} - \hat{b}\avg{x}) - \hat{b}x_i \right) = \\
 = \sum_{i=1}^n  \left(y_i - \hat{b}x_i\right) - n\avg{y}  + n\hat{b}\avg{x} = n\avg{y} - n \hat{b}\avg{x}- n\avg{y} + n\hat{b}\avg{x} = 0 \end{gather*}

Esta ecuación ($\sum_{i=1}^n e_i = 0$) junto con \[ \sum_{i=1}^n x_i e_1 = 0 \] son las dos restricciones entre los residuos que nos dan la recta.

\begin{defn}[Varianza\IS residual]
La varianza residual $s_R^2$ o $\hat{\sigma}_e^2$ mide, aproximadamente el \textit{error cuadrático} cometido en la aproximación dada por la recta de regresión:

\[ s_R^2 = \hat{\sigma}_e^2 = \frac{1}{n}\sum_{i=1}^n e_i^2 \]
\end{defn}

\begin{defn}[Coeficiente\IS de correlación lineal]
\index{Coeficiente!de Pearson}
El coeficiente de correlación lineal o coeficiente de Pearson

\[ r = \frac{\hat{\sigma}_{x,y}}{\hat{\sigma}_x \hat{\sigma}_y} \] que cumple las siguientes condiciones:

\begin{gather*}
0 ≤ r^2 ≤ 1 \\
\hat{\sigma}_e^2 = \hat{\sigma}_y^2(1-r^2) \\
r = \hat{b}\frac{\hat{\sigma}_x}{\hat{\sigma}_y} 
\end{gather*}

nos indica el grado de ajuste lineal entre las dos variables. Un valor absoluto más cercano a 1 indica una correlación más fuerte. Un valor absoluto cercano a cero indica una correlación débil. El signo, positivo o negativo, indica si la correlación es creciente o decreciente.
\end{defn}


\section{Muestreo aleatorio}

La muestra aleatoria de una cierta v.a. $X$ se denomina como la \textbf{muestra aleatoria} o simplemente \textbf{muestra}.\index{Muestra}

Durante este tema, usaremos conceptos de Probabilidad, que repasaré aquí brevemente porque no me apetece escribir demasiado.

\subsection{Conceptos de probabilidad}

\begin{defn}[Distribución de una v.a.][Distribución]
\[ P_X(B) = P(X \in B) \]
\end{defn}

\begin{defn}[Función\IS de distribución]
\[F(t) = P(X ≤ t) \]
\end{defn}

\begin{defn}[Media\IS de una distribución] \index{Esperanza} También llamada esperanza de X:
\[ E(X) = \int_{-\infty}^\infty F(t)\,dt \]
\end{defn}

\begin{theorem}[Teorema\IS de cambio de espacio de integración] Sea $g$ una función real medible tal que $E(g(X))$ es finita, entonces 

\[ E(g(X)) = \int_\real g(x) \, dF(x) = \int_\real g(x)\, dP(x) \]. 

En particular \[ µ =\int_\real x\, dF(x)  \] y \[ \sigma^2 = \int_\real \left(x - µ\right)^2 \, dF(x) \]
\end{theorem}

\begin{defn}[Momento] El momento $µ_k$ es la esperanza de X elevado a una potencia de orden $k$. Es el valor esperado de la distancia de orden $k$ con respecto a la media

\[ µ_k = E\left((X-µ)^k\right) \]
\end{defn}

\subsubsection{Distribuciones aleatorias}

Ver apéndice \ref{secDistr} (página \pageref{secDistr}).

\subsubsection{Criterios de convergencia}

Queremos buscar convergencias entre variables aleatorias.

\begin{defn}[Convergencia\IS en distribución]\index{Convergencia!débil} Se dice que $X_n$ converge débilmente o en distribución a $X$ si la función de distribución de $X_n$ $F_n(x)$ tiende a $F(x)$ para todo $x$ punto de continuidad de $F$, donde $F$ y $F_n$ son las funciones de distribución de $X$ y $X_n$ respectivamente.

Esto es equivalente a decir que  \[ \lim_{n\to\infty} P(X_n\in (-\infty, x]) = P(X\in (-\infty, x]) \]

Notación:
\[ X_n  \convdist X \text{ ó }  X_n \convs[w] X \] 
\end{defn}

\begin{defn}[Convergencia\IS en probabilidad] 
Se dice que $X_n$ converge en probabilidad a $X$ si $\forall \epsilon > 0$ se tiene que \[ P(\abs{X_n-X} > \epsilon) \convs 0 \]. Es decir, que para cualquier error que tomemos el error cometido en la aproximación va a tender a cero siempre que tome un $X_n$ suficientemente grande.

Notación: \[ X_n \convprob X \]
\end{defn}

\begin{defn}[Convergencia\IS casi segura] También denotada c.s o a.s en inglés, convergencia en casi todo punto (c.t.p) o convergencia con probabilidad 1. Se dice que $X_n$ converge a $X$ casi seguro si el conjunto de puntos que no son convergentes tiende a ser vacío. Es decir \[ \prob{X_n \convs X} = 1\]

Más estrictamente, la condición se expresa como \[\prob{\omega \in \Omega\tq X_n(\omega) \convs X(\omega)} = 1\]

Notación \[ X_n\convcs X \]
\end{defn}


\begin{theorem}Se puede probar que si $\{X_n\}$ es una sucesión de variables aleatorias y $X$ es variable aleatoria, 

\[ X_n\convcs X \implies X_n \convprob X \implies X_n \convdist X \]
Al contrario no tiene por qué darse.
\end{theorem}


\begin{theorem}[Teorema\IS de Slutsky] Sean $\{X_n\}$, $\{Y_n\}$ sucesiones de variables aleatorias tales que $X_n\convdist X$, $Y_n\convprob c$ con $c\in\real$ constante. Entonces

\begin{enumerate}
\item $X_n + Y_n \convdist X + c$
\item $X_n \cdot Y_n \convdist X \cdot c$
\item $\dfrac{X_n}{Y_n}\convdist \dfrac{X}{c}$ si $c≠0$.
\end{enumerate}
\end{theorem}

\subsubsection{Desigualdades básicas}

\begin{theorem}[Desigualdad\IS de Markov] Sea $X$ v.a. Entonces, $\forall \epsilon > 0$, \[ \prob{\abs{X} > \epsilon} ≤ \frac{\esp{X}}{\epsilon} \]
\end{theorem}

\begin{theorem}[Desigualdad\IS de Chebichev] En las mismas condiciones del teorema anterior, se cumple que  \[ \prob{\abs{X - \esp{X}} > \epsilon} ≤ \frac{V(X)}{\epsilon^2} \]
\end{theorem}
\subsection{Problema de inferencia}
\subsubsection{Interpretación estadística de la ley de los grandes números}

\begin{theorem}[Ley\IS de los grandes números] Sea $\{x_k\}$ una sucesión de v.a.i.i.d con media finita $µ$. Se verifica entonces que 
\label{thmGrandes}
\[ \avg{X} = \frac{\sum_{i=1}^n x_i}{n} \convcs µ \]

\end{theorem}

\subsubsection{Función de distribución empírica}

\begin{defn}[Función\IS de distribución empírica] La función de distribución empírica asociada a la muestra $\{x_n\}$ se define mediante

\[ \prob{X ≤ t} =  \fd_n(t) = \frac{1}{n}\sum_{i=1}^n \ind_{(-\infty, t]} (x_i) \]

Es decir, $\fd_n(t)$ es la proporción de puntos de la muestra que caen en el intervalo $(-\infty, t]$.
\end{defn}

Sin embargo, surge una duda: ¿converge la función de distribución empírica a la función de distribución original?

Intuitivamente, podemos pensar que cuantos más puntos cojamos más se aproximará a la función de distribución original. De hecho, eso es lo que demuestra el siguiente teorema:

\begin{theorem}[Teorema\IS de Glivenko-Cantelli] Sean $\{x_n\}$ v.a.i.i.d con función de distribución $F$. Se verifica que
\label{thmGlivenko}
\[ \md{\fd_n - F}_\infty=\sup_{t\in\real} \abs{\fd_n(t) - F(t)} \convcs 0 \]

donde $\md{\fd_n - F}_\infty$ es el \index{Estadístico! de Kolmogorov-Smirnov} \textbf{estadístico de Kolmogorov-Smirnov}.

\end{theorem}

\begin{proof}
Empezamos demostrando la convergencia de los términos intermedios. Es decir, queremos demostrar que 

\begin{equation}\label{eqConvCsGC}
\fd_n(t) \convcs F(t)
\end{equation} 

Tenemos que \[ \fd_n(t) = \frac{1}{n}\sum_{i=1}^n \ind_{(-\infty, t]} (x_i) \]

A cada uno de los términos de los términos de la suma $\ind_{(-\infty, t]}(x_i)$ los podemos llamar $y_i$. Estos valores son una muestra de la distribución \[ Y = \ind_{(-\infty, t]}(X) \]. Por lo tanto y por la LGN (\ref{thmGrandes}) \[ \fd_n(t) = \frac{1}{n}\sum_{i=1}^n Y_i = \avg{Y} \convcs \esp{Y} \]

pero

\[ \esp{Y} = \esp{\ind_{(-\infty, t]}(X)} = \prob{X\in (-\infty, t]} = F(t) \] por lo tanto hemos demostrado (\ref{eqConvCsGC}).

Ahora tenemos que demostrar que el límite por la izquierda converge. Es decir, hay que demostrar que \begin{equation}
 \fd_n(t^-) \convcs F(t^-)  \label{eqConvIzq}
\end{equation}. Esa convergencia se da si y sólo si en un conjunto de probabilidad $1$ se tiene que $ \fd_n(t^-) \convs F(t^-) $. Según la definición de límite, esto se da si y sólo si \begin{equation}
 \forall \epsilon > 0\; \exists N \tq n ≥ N \implies \abs{\fd_n(t^-) - F(t^-) } < \epsilon \label{eqLim1} \end{equation}

Sabemos que 
\begin{equation}
	\exists\epsilon >0\tq \fd_n (t^-) = \fd_n (x)\; \forall x \in (t-\delta, t+\delta) \label{eqLim2}
\end{equation}. Seguimos:

\begin{equation}
 F(t^-) = \lim_{x\to t^-} F(x) \dimplies \forall \epsilon > 0 \; \exists \delta > 0 \tq x \in (t - \delta, t) \implies \abs{F(x) - F(t^-)} < \frac{\epsilon}{2}\label{eqLim3} 
\end{equation}

Tomamos $x\in(t-\delta, t)$ con un delta que cumpla tanto la condición en (\ref{eqLim2}) como en (\ref{eqLim3}). Entonces

\[ \abs{\fd_n(t^-) - F(t^-)} =  \abs{\fd_n(x) - F(x) + F(x) - F(t^-)} ≤ \underbrace{\abs{\fd_n(x) - F(x)}}_{(a)} + \underbrace{\abs{F(x) - F(t^-)}}_{(b)} \]

Sabemos que $(a)$ es menor que $\frac{\epsilon}{2}$ por (\ref{eqLim1}) y que $(b)$ también es menor que $\frac{\epsilon}{2}$ por (\ref{eqLim3}), por lo tanto 

\[ \abs{\fd_n(t^-) - F(t^-)}  < \epsilon \]

Buscamos ahora una partición finita de $\real$ dada por $t_0 = -\infty ≤ t_1 ≤ \dotsb ≤ t_k = \infty$ tal que para todo $\epsilon > 0$ se cumpla que $\abs{F(t_i^-) - F(t_{i-1})} ≤ \epsilon$. Lo construimos de forma recursiva: dado $t_{i-1}$ tomamos

\[ t_i =\sup_{z\in\real} \{ F(z) ≤ F(t_{i-1} + \epsilon \} \]

El siguiente paso: para todo $t_{i-1} ≤ t ≤ t_i$ se tiene que 

\[ \fd_n(t) - F(t) ≤ \fd_n(t_i^-) - F(t_i^-) + \epsilon \]

Como $\fd_n$ es no decreciente (es una función de distribución), tenemos también que 

\[ \fd_n(t) - F(t) ≥ \fd_n(t_{i-1}) - F(t_{i-1}) - \epsilon \]

Con estas dos últimas ecuaciones, llegamos a que 

\[ \sup_{t\in\real} \abs{\fd_n(t) - F(t)} ≤ \max\left\lbrace \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i) - F(t_i)},\; \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i^-) - F(t_i^n)} \right\rbrace + \epsilon \]

Por (\ref{eqConvCsGC}), sabemos que $\abs{\fd_n(t_i) - F(t_i)} \convcs 0$, y por lo tanto \[ \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i) - F(t_i)} \convcs 0 \].

De la misma forma, usando (\ref{eqConvIzq}) tenemos que \[ \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i^-) - F(t_i^n)} \convcs 0 \]. Por lo tanto, todo ese máximo enorme vale 0, de tal forma que 

\[ \lim_{n\to\infty} \sup_{t\in\real} \abs{\fd_n(t) - F(t)}  =  \lim_{n\to\infty} \md{\fd_n - F}_\infty ≤ \epsilon \]

para cualquier $\epsilon > 0$ arbitrario que cojamos. Es decir, que \[ \md{\fd_n - F}_\infty=\sup_{t\in\real} \abs{\fd_n(t) - F(t)} \convcs 0 \]
\end{proof}

\subsection{Estadísticos}

Cuando extraemos una muestra $\{x_n\}$ de $X$ se pueden calcular algunas \textit{medidas resumen}. Cualquiera de ellas se puede expresar matemáticamente como una función $T(x_1,\dotsc,x_n)$ de la muestra. 

\begin{defn}[Estadístico]
Sea $T(x_1,\dotsc,x_n)$ una función cuyo dominio incluye el espacio muestral del vector aleatorio $(X_1, \dotsc, X_n)$. Entonces la variable aleatoria $T$ se denomina \textbf{estadístico}. La única restricción es que un estadístico no puede ser función de un parámetro.
\end{defn}

Como la distribución de $T$ se calcula a partir de la distribución de las variables $X_i$ que constituyen la muestra, la denominaremos distribución de $T$ en el muestreo (\textit{sampling distribution).}

\begin{defn}[Error\IS típico]\index{Error!estándar}
El error estándar o error típico $\sigma$ de un estadístico $T$ es la desviación típica de su distribución en el muestreo. Como en ocasiones depende de alguna cantidad desconocida, también se denomina error típico a una estimación de ese valor.
\end{defn}

En ocasiones, se cumple que $\dfrac{T}{\sigma}$ sigue una distribución t de Student, lo que nos permitirá definir intervalos de confianza.

\subsubsection{Media muestral y poblacional}

\begin{defn}[Media\IS muestral] La media muestral \[ \avg{X} = \frac{\sum_{i=1}^n X_i}{n} \] se puede expresar de la siguiente forma

\[ \avg{X} = \int_\real x\,d\fd_n(x) \]
\end{defn}

\index{Media!poblacional}
La definición es análoga con la de la \textbf{media poblacional}

\[ µ = \int_\real x \,dF(x) \]

Esto nos da una clave de la estadística: sustituir todo lo que desconozco de la población con su análogo muestral (en este caso, pasamos de la función de distribución teórica a la función de distribución empírica). Sólo quedaría ver si los estimadores que resultan son adecuados.

La media muestral tiene otras relaciones muy importantes con $µ$:

\begin{enumerate}
\item $\avg{X}$ es \index{Estimador!insesgado}\index{Estimador!centrado} \textbf{estimador insesgado o centrado} de µ: $ \esp{\avg{X}} = µ$
\item $\var{\avg{X}} = \dfrac{\sigma^2}{n}$. Como es inversamente proporcional, está claro que cuantos más datos haya mejor nos aproximaremos a lo que queremos estimar.
\end{enumerate}

\begin{theorem}[Teorema\IS central del límite] Suponemos que $\{X_n\}$ son v.a.i.i.d. con media $µ$ y desviación típica $\sigma$ finitas. Entonces
\label{thmCentral}
\[ \sqrt{n}\frac{\avg{X}-µ}{\sigma} \convdist N(0,1) \]

Si denotamos la función de distribución de la normal como \[ \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}\] entonces

\[ \forall t\in\real\quad \prob{ \sqrt{n}\frac{\avg{X}-µ}{\sigma} ≤ t } \convs \Phi(t) \]

Por tanto, para $n$ grande se tiene

\[ \prob{\sqrt{n} \left(\avg{X} - µ\right) ≤ x} ≈ \Phi(\frac{x}{\sigma}) \]

\textbf{aunque las $X_i$ no tengan distribución normal.}

\end{theorem}

\subsubsection{Varianza muestral y poblacional}

Una medida importante de dispersión de una variable aleatoria es la varianza \begin{equation}
 \mathbb{V}(X)=\sigma^2 = \int_\real(x-µ)^2 \,dF(x) \label{eqVarianza}
\end{equation}

\begin{defn}[Varianza\IS muestral]El análogo muestral de $\sigma^2$ es la \textbf{varianza muestral.}. Utilizando el criterio \textit{plugin} en (\ref{eqVarianza})

\[\hat{\sigma}^2_n = \int_\real (x-\avg{X})^2\,d\fd_n(x) = \frac{1}{n}\sum_{i=1}^n(X_i - \avg{X})^2 \]
\end{defn}

\begin{theorem} La varianza muestral cumple lo siguiente

\begin{gather*}
\esp{\hat{\sigma}^2_n} = \frac{n-1}{n}\sigma^2\\
\hat{\sigma}^2_n \convcs \sigma^2
\end{gather*}
\end{theorem}

Por lo tanto, la varianza muestral es un estimador sesgado. No es un problema grande ya que cuando $n\to\infty$ acaba convergiendo a $\sigma^2$ y el sesgo 

\[ \esp{\hat{\sigma}^2_n} - \sigma^2= \frac{n-1}{n}\sigma^2 - \sigma^2 = \frac{-1}{n}\sigma^2 \]

también tiende a cero. Es decir, es \index{Asintóticamente!insesgado} \textbf{asintóticamente insesgado}.

\begin{defn}[Cuasivarianza\IS muestral] En lugar de usar $\hat{\sigma}^2_n$ usamos la cuasivarianza muestral, definida como

\[ S^2 = \frac{n}{n-1}\hat{\sigma}^2_n \] de tal forma que se tiene

\begin{gather*}
\esp{S^2} = \sigma^2 \\
S^2 \convcs \sigma^2 
\end{gather*}
\end{defn}

\subsubsection{Estadísticos de orden}

\begin{defn}[Estadístico\IS de orden]Dada una muestra $\{X_n\}$, se denotan como \[ X_{(1)} ≤ \dotsb ≤ X_{(n)} \] las observaciones de la muestra ordenadas de menor a mayor, llamados \textbf{estadísticos de orden}. Cuando la distribución de las v.a. es continua, la probabilidad de coincidencia en valores es $0$ y con probabilidad $1$ se tiene que \[ X_{(1)} < \dotsb < X_{(n)} \]
\end{defn}

Los estadísticos de orden pueden utilizarse para definir la mediana o los cuartiles. Sin embargo, podemos usar la función cuantílica para definir mejor estos conceptos.

\begin{defn}[Función\IS cuantílica] La función cuantílica en $p$ es el punto que deja una probabilidad $p$ a la izquierda, de tal forma que una proporción $p$ de los individuos de la población $X$ sería menor que el cuantil poblacional de orden $p$.

La función cuantílica correspondiente a la función de distribución $F$ como la función 

\begin{gather*}
\appl{\inv{F}}{\real}{(0,1)} \\
\inv{F}(p) = \inf \left\lbrace x \tq F(x) ≥ p \right\rbrace 
\end{gather*}
\end{defn}

La función cuantílica nos permite obtener los \textbf{cuantiles poblacionales de orden $p$} \index{Cuantil!poblacional} al valor $\inv{F}(p)$. El análogo es el \textbf{cuantil muestral de orden $p$}, \index{Cuantil!muestral} se define a partir de la función de distribución empírica como $\inv{\fd_n}(p)$.

\section{Estimación puntual paramétrica}

En este tema, supondremos que la muestra, absolutamente continua o discreta, con función de densidad o probabilidad $f(·;\theta)$ que es totalmente conocida salvo el valor de un parámetro $\theta$ del cuál sólo se conoce su rango de posibles valores $\Theta$, al que se llama el \textbf{espacio paramétrico.}\index{Espacio!paramétrico}

\subsection{Estimadores}

\begin{defn}[Estimador] Sean $\{X_n\}$ v.a.i.i.d. con distribución común caracterizada por la función de densidad/masa $f(\cdot;\theta)$, con $\theta$ un parámetro desconocido del que sólo se sabe que pertenece al espacio paramétrico $\Theta \subset \real$.

El \textbf{estimador} es una función medible $\hat{\theta}_n = T_n(X_1,\dotsc, X_n)$ que se utiliza para estimar o aproximar el valor de $\theta$.
\end{defn}

Cuando tenemos una muestra aleatoria $\{X_n\}$, cada $T_n(X_1, \dotsc, X_n)$ es un estimador de $\theta$, una variable aleatoria. Si por el contrario tenemos una serie de observaciones de una muestra $\{x_n\}$ entonces $T_n(x_1,\dotsc,x_n)$ es una \textbf{estimación} de $\theta$.

Podemos evaluar la calidad de un estimador con el \textbf{error cuadrático medio} (ECM):

\[ ECM(T_n) = \esp{(T_n - \theta)^2}\]

Si sumamos y restamos $\esp{T_n}$, nos queda que 

\[ ECM(T_n) = \var{T_n} + (\text{sesgo}\, T_n)^2 \]

que nos describe el error cuadrático medio en función de la varianza y del sesgo de $T_n$.

\subsubsection{Propiedades interesantes de los estimadores}
Buscaremos varias propiedades interesantes de los estimadores:

\index{Estimador!insesgado}
\paragraph{Ausencia de sesgo} Se dice que un estimador $T_n$ es \textbf{insesgado} \index{Estimador!insesgado} si, siempre que $X_i \sim f(\cdot;\theta)$ se tiene que \[\esp{T_n} = \theta\; \forall \theta \in \Theta \]

\appendix
\section{Distribuciones notables}
\label{secDistr}
\includepdf[pages={2-last}, nup=1x3]{_Distribuciones.pdf}

\section{Ejercicios}
\input{EI_Ejercicios.tex}

\newpage
\printindex
\end{document}