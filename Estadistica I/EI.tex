\documentclass{apuntes}

\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{calc, intersections}
\author{Guillermo Julián Moreno}
\date{13/14 C1}

\renewcommand*{\arraystretch}{1.5}

\title{Estad\'{i}stica I}

\begin{document}

\pagestyle{plain}
\maketitle

\tableofcontents
\newpage
\chapter{Estadística descriptiva}
\section{Estadística descriptiva de datos univariantes}

La estadística descriptiva es el conjunto de técnicas para resumir la información proporcionada por una gran masa de datos. El primer objetivo natural es resumir la información que proporcionan esos datos.

\subsection{Estadísticos de tendencia central}

\begin{defn}[Media]

\[ \avg{x} = \frac{\sum_{i=1}^n x_i}{n} \]

Es la medida de tendencia central más utilizada. Es bastante sensible a los valores atípicos (\textit{outliers}), observaciones anormalmente grandes que aparecen en el conjunto de datos por errores de transcripción o medición.
\end{defn}

\begin{defn}[Mediana]
Es el valor que divide a los datos en dos mitades, de tal forma que la mitad son menores y la otra mitad mayores que la mediana. 

La mediana se calcula de la siguiente forma: dado un conjunto de datos $\{x_1,\dotsc, x_n\}$, la mediana es $x_{\frac{n+1}{2}}$ si $n$ es impar y  el promedio entre $x_{\frac{n}{2}}$ y $x_{\frac{n}{2} + 1}$.
\end{defn} 

\subsection{Estadísticos de dispersión}

\begin{defn}[Varianza]
\[ \sigma^2 = \frac{1}{n} \sum_{i=1}^n \left(x_i - \avg{x}\right)^2 = \frac{1}{n} \sum_{i=1}^n x_i^2 - \avg{x}^2 \]
\end{defn}

\begin{defn}[Desviación\IS típica]
\[\sigma = \sqrt{\sigma^2} \]

La desviación típica es la raíz de la varianza.
\end{defn}

\begin{defn}[Cuantil]
Para $p\in (0, 1)$ se llama cuantil $p$ o $q_p$ al valor que deja el $100p \%$ de los datos a la izquierda.
\end{defn}

\begin{defn}[Cuartil]
Los cuartiles son los tres datos que dejan a la izquierda el 25, 50 y 75 por ciento de los datos respectivamente. Es decir:

\begin{itemize}
\item $Q_1 = q_{0.25}$
\item $Q_2 = q_{0.5}$. El cuartil dos es la mediana.
\item $Q_3 = q_{0.75}$
\end{itemize}
\end{defn}

Hay varios métodos para el cálculo de cuantiles. Para hacerlo a mano, podemos usar el siguiente método.

Si el dato en la posición $p(n+1)$ no es un número entero, entonces se interpola entre las observaciones ordenadas que están en la posición $\floor{p(n+1)}$ y $\floor{p(n+1)} + 1$ de la siguiente forma: sea $j$ la parte entera de $p(n+1)$ y $m$ la parte decimal. Entonces, \[ q_p = (1-m)x_j + m x_{j+1} \]


\begin{defn}[Coeficiente\IS de asimetría]
\index{Skewness}
El tercer momento con respecto a la media se define como \[ \frac{1}{n}\sum_{i=1}^n\left(x_i-\avg{x}\right)^3 \] que, en su versión adimensional dividimos por $\sigma^3$.
\end{defn}

Al ser una función cúbica, los valores que se alejen mucho de la media tendrán un valor muy alto en valor absoluto (positivo o negativo según se aleje por la derecha o izquierda, respectivamente). Si la distribución de datos es muy asimétrica, los valores más altos no se cancelan con los valores altos del otro lado (porque no hay) y saldrá un valor más alejado de cero.\footnote{Está explicado como el p. culo, ya.}

\subsection{Representación gráfica de datos}

\begin{defn}[Box-plot]
El diagrama de caja o \textit{box-plot}  (imagen \ref{imgCaja}) nos permite visualizar las medidas de dispersión respecto a la mediana. Hay que añadir una nueva medida, el \textbf{rango intercuartílico}\index{Rango!intercuartílico}, la diferencia entre el primer y el tercer cuartil: \[RI = Q_3 - Q_1 \]

\easyimg{DiagramaCaja.png}{Diagrama de caja}{imgCaja}
\end{defn}

\begin{defn}[Histograma]
El histograma se trata de una aproximación discreta a la función de densidad continua $f(t)$ de la variable que estamos midiendo. Es un diagrama de frecuencias que \textit{mantiene la forma} de esa función de densidad. 

Definimos una serie, las marcas de intervalos $a^n_1, \dotsc, a^n_n$, donde $n$ es el número de intervalos y la longitud de cada intervalo  es $h_n = a^n_{j+1} - a^n_j$. Sea el conjunto $\{x_i\}_{i=0,\dotsc,m}$ los datos de nuestra muestra. Entonces, el estimador, la función $\hat{f}_n$, se define de la siguiente forma:

\[ \hat{f}^n(t) = \frac{\card{i \tq x_i \in \left( a_j^n, a_{j+1}^n \right]}}{n h_n} = \frac{\sum_{i=1}^m \ind_{(a_j^n, a_{j+1}^n]} (x_i)}{n h_n} \]

Recordemos que \[ \ind_A (n) = \begin{cases} 1 & n \in A \\ 0 & n \notin A\end{cases}\]

A grandes rasgos, lo que hace en una función es definir un número de intervalos fijos de ancho $h_n$. Al evaluar $\hat{f}^n(t)$ buscamos en qué intervalo cae $t$ y contamos cuántas de nuestras mediciones caen también en ese intervalo.

\easyimg{DensidadAHistograma.png}{El histograma es una aproximación de la función de densidad real en base a la muestra que hemos obtenido.}{lblDensidad}

\end{defn}

\subsubsection{Estimadores núcleo o kernel}
\label{secEst}
\begin{defn}[Método de ventana móvil][Ventana móvil]
El método de ventana móvil nos da una estimación de la función de densidad en un punto $t$ midiendo los $x_i$ que están en el intervalo de radio $h_n$ centrado en $t$. Matemáticamente:

\[ \hat{f}_n(t) = \frac{1}{n2h_n}\sum_{i=1}^n \ind_{[t-h_n, t+h_n]}(x_i) = \frac{1}{n2h_n}\sum_{i=1}^n \ind_{[-1,1]}\left(\frac{t-x_i}{h_n}\right) \]
\end{defn}

Podemos reemplazar la función $\frac{1}{2}\ind_{[-1, 1]}$ por otra, llamada la función de densidad $K$, kernel o núcleo:

\begin{defn}[Estimador\IS núcleo]
Dada una función de densidad $K$ simétrica, no necesariamente positiva, definimos el estimador kernel como:

\[ \hat{f}_n(t) = \frac{1}{n}\sum_{i=1}^n K_h (t - x_i)  = \frac{1}{nh_n} \sum_{i=1}^n K\left(\frac{t-x_i}{h_n}\right) \]

con $K_h(x) = \frac{1}{h}K(\frac{x}{h})$.
\end{defn}

La elección del núcleo $K$ no afecta especialmente a lo bien aproximada que esté la función de densidad. Sin embargo, sí que influye la selección de la ventana $h_n$ (figura \ref{lblSuavizado}), también llamada \textit{bandwith} en inglés.  Si escogemos una ventana muy pequeña, damos demasiado peso a los datos de nuestra muestra. Si elegimos una ventana muy grande, nuestra muestra pierde importancia y podemos perder información importante.

La elección del $h_n$ más habitual es el que minimiza la distancia $L^2$ entre $\hat{f}$ y $f$, es decir, el parámetro que minimice $\displaystyle\int\left(\hat{f}_h-f\right)^2$. Sin embargo, hay un problema: no sabemos qué es $f$. Hay trucos que imagino que veremos más tarde.

\easyimgw{Suavizado.png}{Los efectos que causa elegir una ventana más grande o más pequeña en el estimador}{lblSuavizado}{1}

Las funciones kernel más usadas son la uniforme, $\frac{1}{2}\ind_{[-1, 1]}$, la gaussiana $\frac{1}{\sqrt{2 \pi}}e^{-\frac{t^2}{2}}$ y la de Epanechnikov, que matemáticamente es la que mejor aproxima $f$.

El estimador kernel $\hat{f}_n(t)$ es la función de densidad de una medida de probabilidad que es la convolución \footnote{Ya aprenderemos en al algún momento de nuestra vida qué narices es una convolución} de dos medidas de probabilidad: una, $K_h(x)$ (el kernel reescalado) y otra que da probabilidad $\frac{1}{n}$ a cada punto de la muestra $\{x_i\}$ (distribución o medida empírica).

\paragraph{Generación de datos del estimador kernel} Supongamos que $K$ es el núcelo gaussiano. Podemos generar datos artificiales de la densidad así:

\[ x_i^0 = x_i^* + h_n Z_i,\; i=1,\dotsc, k \]

donde $x_i^*$ es una observación elegida al azar entre los datos originales y $Z_i$ una observación aleatoria con probabilidad $N(0,1)$. Es decir, lo que hacemos es añadir un dato aleatorio de la muestra y sumamos una pequeña perturbación aleatoria.

\section{Estadística descriptiva de datos bivariantes}

En esta sección estudiaremos dos variables $(X, Y)$ para explorar la relación entre ambas y tratar de inferir si existe una relación funcional para predecir los valores de una variable en función de los de la otra.

\subsection{Representación gráfica}

\begin{defn}[Diagrama\IS de dispersión]
El diagrama de dispersión representa cada variable en función de la otra para que podamos ver la posible relación entre ambas. Ver figura \ref{lblDispersion}.

\easyimg{Dispersion.png}{Diagrama de dispersión}{lblDispersion}
\end{defn} 

\subsection{Regresión}

\begin{defn}[Recta de regresión]

La recta de regresión de $y$ sobre $x$ es la recta de forma $\hat{y} = \hat{a} + \hat{b}x$ que más se aproxima a los datos, minimizando los cuadrados de la distancia: \[ (\hat{a},\hat{b}) =\argmin_{a, b} \sum_{i=1}^n\left(y_i - a - bx_i)\right)^2 \]
\end{defn}

La recta de regresión se calcula obteniendo primero $\hat{b}$:

\[ \hat{b} = \frac{\sigma_{x,y}}{\sigma^2_x} \]

donde \[ \sigma_{x,y} = \frac{1}{n} \left( \sum_{i=1}^n x_i y_i\right)  - \avg{x}\avg{y} \] y después, sabiendo que la recta pasa por el punto $(\avg{x}, \avg{y})$, obtenemos $\hat{a}$ \[ \hat{a} = \avg{y} - \hat{b}\avg{x} \]

El valor $b$ se denomina \textbf{coeficiente de regresión lineal}\index{Regresión lineal!coeficiente de} o parámetro de la regresión. Cada valor $e_i= y_i - \hat{y}_i$ se denomina \textbf{residuo}\index{Residuo}. Hay que notar que

\begin{gather*}
 \sum_{i=1}^n e_i = \sum_{i=1}^n \left(y_i - \hat{a} -\hat{b}x_i \right)= \sum_{i=1}^n\left( y_i - (\avg{y} - \hat{b}\avg{x}) - \hat{b}x_i \right) = \\
 = \sum_{i=1}^n  \left(y_i - \hat{b}x_i\right) - n\avg{y}  + n\hat{b}\avg{x} = n\avg{y} - n \hat{b}\avg{x}- n\avg{y} + n\hat{b}\avg{x} = 0 \end{gather*}

Esta ecuación ($\sum_{i=1}^n e_i = 0$) junto con \[ \sum_{i=1}^n x_i e_1 = 0 \] son las dos restricciones entre los residuos que nos dan la recta.

\begin{defn}[Varianza\IS residual]
La varianza residual $s_R^2$ o $\hat{\sigma}_e^2$ mide, aproximadamente el \textit{error cuadrático} cometido en la aproximación dada por la recta de regresión:

\[ s_R^2 = \hat{\sigma}_e^2 = \frac{1}{n}\sum_{i=1}^n e_i^2 \]
\end{defn}

\begin{defn}[Coeficiente\IS de correlación lineal]
\index{Coeficiente!de Pearson}
El coeficiente de correlación lineal o coeficiente de Pearson

\[ r = \frac{\hat{\sigma}_{x,y}}{\hat{\sigma}_x \hat{\sigma}_y} \] que cumple las siguientes condiciones:

\begin{gather*}
0 ≤ r^2 ≤ 1 \\
\hat{\sigma}_e^2 = \hat{\sigma}_y^2(1-r^2) \\
r = \hat{b}\frac{\hat{\sigma}_x}{\hat{\sigma}_y} 
\end{gather*}

nos indica el grado de ajuste lineal entre las dos variables. Un valor absoluto más cercano a 1 indica una correlación más fuerte. Un valor absoluto cercano a cero indica una correlación débil. El signo, positivo o negativo, indica si la correlación es creciente o decreciente.
\end{defn}


\chapter{Muestreo aleatorio}

La muestra aleatoria de una cierta v.a. $X$ se denomina como la \textbf{muestra aleatoria} o simplemente \textbf{muestra}.\index{Muestra}

Durante este tema, usaremos conceptos de Probabilidad, que repasaré aquí brevemente porque no me apetece escribir demasiado.

\section{Conceptos de probabilidad}

\begin{defn}[Distribución de una v.a.][Distribución]
\[ \prob[X]{B} = \prob{X \in B} \]
\end{defn}

\begin{defn}[Función\IS de distribución]
\[F(t) = \prob{X ≤ t} \]
\end{defn}

\begin{defn}[Media\IS de una distribución] \index{Esperanza} También llamada esperanza de X:
\[ \esp X  = \int_{-\infty}^\infty F(t)\,dt \]
\end{defn}

\begin{theorem}[Teorema\IS de cambio de espacio de integración] Sea $g$ una función real medible tal que $\esp{g(X)}$ es finita, entonces 

\[ \esp{g(X))} = \int_\real g(x) \, dF(x) = \int_\real g(x)\, dP(x) \]. 

En particular \[ µ =\int_\real x\, dF(x)  \] y \[ \sigma^2 = \int_\real \left(x - µ\right)^2 \, dF(x) \]
\end{theorem}

\begin{defn}[Momento] El momento $µ_k$ es la esperanza de X elevado a una potencia de orden $k$. Es el valor esperado de la distancia de orden $k$ con respecto a la media

\[ µ_k = \esp{(X-µ)^k} \]
\end{defn}

\subsection{Distribuciones aleatorias}

Ver apéndice \ref{secDistr} (página \pageref{secDistr}).

\subsubsection{Criterios de convergencia}

Queremos buscar convergencias entre variables aleatorias.

\begin{defn}[Convergencia\IS en distribución]\index{Convergencia!débil}

Se dice que $X_n$ converge débilmente o en distribución a $X$ si la función de distribución de $X_n$ $F_n(x)$ tiende a $F(x)$ para todo $x$ punto de continuidad de $F$, donde $F$ y $F_n$ son las funciones de distribución de $X$ y $X_n$ respectivamente.

Esto es equivalente a decir que  

\[\lim_{n\to\infty} \prob{X_n\in (-\infty, x]} = \prob{X\in (-\infty, x]} \]
Notación:
\[ X_n  \convdist X \text{ ó }  X_n \convs[w] X \] 
\end{defn}

\begin{defn}[Convergencia\IS en probabilidad] 
Se dice que $X_n$ converge en probabilidad a $X$ si $\forall \epsilon > 0$ se tiene que 

\[\prob{\abs{X_n-X} > \epsilon} \convs 0 \].

 Es decir, que para cualquier error que tomemos el error cometido en la aproximación va a tender a cero siempre que tome un $X_n$ suficientemente grande.

Notación: \[ X_n \convprob X \]
\end{defn}

\begin{defn}[Convergencia\IS casi segura] También denotada c.s o a.s en inglés, convergencia en casi todo punto (c.t.p) o convergencia con probabilidad 1. Se dice que $X_n$ converge a $X$ casi seguro si el conjunto de puntos que no son convergentes tiende a ser vacío. Es decir \[ \prob{X_n \convs X} = 1\]

Más estrictamente, la condición se expresa como \[\prob{\omega \in \Omega\tq X_n(\omega) \convs X(\omega)} = 1\]

Notación \[ X_n\convcs X \]
\end{defn}


\begin{theorem}Se puede probar que si $\{X_n\}$ es una sucesión de variables aleatorias y $X$ es variable aleatoria, 

\[ X_n\convcs X \implies X_n \convprob X \implies X_n \convdist X \]
Al contrario no tiene por qué darse.
\end{theorem}


\begin{theorem}[Teorema\IS de Slutsky]\label{thmSlutsky} Sean $\{X_n\}$, $\{Y_n\}$ sucesiones de variables aleatorias tales que $X_n\convdist X$, $Y_n\convprob c$ con $c\in\real$ constante. Entonces

\begin{enumerate}
\item $X_n + Y_n \convdist X + c$
\item $X_n \cdot Y_n \convdist X \cdot c$
\item $\dfrac{X_n}{Y_n}\convdist \dfrac{X}{c}$ si $c≠0$.
\end{enumerate}
\end{theorem}

\subsubsection{Desigualdades básicas}

\begin{theorem}[Desigualdad\IS de Markov]\label{desMarkov} Sea $X$ v.a. Entonces, $\forall \epsilon > 0$, \[ \prob{\abs{X} > \epsilon} ≤ \frac{\esp{X}}{\epsilon} \]
\end{theorem}

\begin{theorem}[Desigualdad\IS de Chebichev] En las mismas condiciones del teorema anterior, se cumple que  \[ \prob{\abs{X - \esp{X}} > \epsilon} ≤ \frac{\var {X}}{\epsilon^2} \]
\end{theorem}

\section{Problema de inferencia}
\subsection{Interpretación estadística de la ley de los grandes números}

\begin{theorem}[Ley\IS de los grandes números] Sea $\{x_k\}$ una sucesión de v.a.i.i.d con media finita $µ$. Se verifica entonces que 
\label{thmGrandes}
\[ \avg{X} = \frac{\sum_{i=1}^n x_i}{n} \convcs µ \]

\end{theorem}

\subsection{Función de distribución empírica}

\begin{defn}[Función\IS de distribución empírica] La función de distribución empírica asociada a la muestra $\{x_n\}$ se define mediante

\[ \prob{X ≤ t} =  \fd_n(t) = \frac{1}{n}\sum_{i=1}^n \ind_{(-\infty, t]} (x_i) \]

Es decir, $\fd_n(t)$ es la proporción de puntos de la muestra que caen en el intervalo $(-\infty, t]$.
\end{defn}

Sin embargo, surge una duda: ¿converge la función de distribución empírica a la función de distribución original?

Intuitivamente, podemos pensar que cuantos más puntos cojamos más se aproximará a la función de distribución original. De hecho, eso es lo que demuestra el siguiente teorema:

\begin{theorem}[Teorema\IS de Glivenko-Cantelli] Sean $\{x_n\}$ v.a.i.i.d con función de distribución $F$. Se verifica que
\label{thmGlivenko}
\[ \md{\fd_n - F}_\infty=\sup_{t\in\real} \abs{\fd_n(t) - F(t)} \convcs 0 \]

donde $\md{\fd_n - F}_\infty$ es el \index{Estadístico! de Kolmogorov-Smirnov} \textbf{estadístico de Kolmogorov-Smirnov}.

\end{theorem}

\begin{proof}
Empezamos demostrando la convergencia de los términos intermedios. Es decir, queremos demostrar que 

\begin{equation}\label{eqConvCsGC}
\fd_n(t) \convcs F(t)
\end{equation} 

Tenemos que \[ \fd_n(t) = \frac{1}{n}\sum_{i=1}^n \ind_{(-\infty, t]} (x_i) \]

A cada uno de los términos de los términos de la suma $\ind_{(-\infty, t]}(x_i)$ los podemos llamar $y_i$. Estos valores son una muestra de la distribución \[ Y = \ind_{(-\infty, t]}(X) \]. Por lo tanto y por la LGN (\ref{thmGrandes}) \[ \fd_n(t) = \frac{1}{n}\sum_{i=1}^n Y_i = \avg{Y} \convcs \esp{Y} \]

pero

\[ \esp{Y} = \esp{\ind_{(-\infty, t]}(X)} = \prob{X\in (-\infty, t]} = F(t) \] por lo tanto hemos demostrado (\ref{eqConvCsGC}).

Ahora tenemos que demostrar que el límite por la izquierda converge. Es decir, hay que demostrar que \begin{equation}
 \fd_n(t^-) \convcs F(t^-)  \label{eqConvIzq}
\end{equation}. Esa convergencia se da si y sólo si en un conjunto de probabilidad $1$ se tiene que $ \fd_n(t^-) \convs F(t^-) $. Según la definición de límite, esto se da si y sólo si \begin{equation}
 \forall \epsilon > 0\; \exists N \tq n ≥ N \implies \abs{\fd_n(t^-) - F(t^-) } < \epsilon \label{eqLim1} \end{equation}

Sabemos que 
\begin{equation}
	\exists\epsilon >0\tq \fd_n (t^-) = \fd_n (x)\; \forall x \in (t-\delta, t+\delta) \label{eqLim2}
\end{equation}

Seguimos:

\begin{equation}
 F(t^-) = \lim_{x\to t^-} F(x) \dimplies \forall \epsilon > 0 \; \exists \delta > 0 \tq x \in (t - \delta, t) \implies \abs{F(x) - F(t^-)} < \frac{\epsilon}{2}\label{eqLim3} 
\end{equation}

Tomamos $x\in(t-\delta, t)$ con un delta que cumpla tanto la condición en (\ref{eqLim2}) como en (\ref{eqLim3}). Entonces

\[ \abs{\fd_n(t^-) - F(t^-)} =  \abs{\fd_n(x) - F(x) + F(x) - F(t^-)} ≤ \underbrace{\abs{\fd_n(x) - F(x)}}_{(a)} + \underbrace{\abs{F(x) - F(t^-)}}_{(b)} \]

Sabemos que $(a)$ es menor que $\frac{\epsilon}{2}$ por (\ref{eqLim1}) y que $(b)$ también es menor que $\frac{\epsilon}{2}$ por (\ref{eqLim3}), por lo tanto 

\[ \abs{\fd_n(t^-) - F(t^-)}  < \epsilon \]

Buscamos ahora una partición finita de $\real$ dada por $t_0 = -\infty ≤ t_1 ≤ \dotsb ≤ t_k = \infty$ tal que para todo $\epsilon > 0$ se cumpla que $\abs{F(t_i^-) - F(t_{i-1})} ≤ \epsilon$. Lo construimos de forma recursiva: dado $t_{i-1}$ tomamos

\[ t_i =\sup_{z\in\real} \{ F(z) ≤ F(t_{i-1} + \epsilon \} \]

El siguiente paso: para todo $t_{i-1} ≤ t ≤ t_i$ se tiene que 

\[ \fd_n(t) - F(t) ≤ \fd_n(t_i^-) - F(t_i^-) + \epsilon \]

Como $\fd_n$ es no decreciente (es una función de distribución), tenemos también que 

\[ \fd_n(t) - F(t) ≥ \fd_n(t_{i-1}) - F(t_{i-1}) - \epsilon \]

Con estas dos últimas ecuaciones, llegamos a que 

\[ \sup_{t\in\real} \abs{\fd_n(t) - F(t)} ≤ \max\left\lbrace \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i) - F(t_i)},\; \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i^-) - F(t_i^n)} \right\rbrace + \epsilon \]

Por (\ref{eqConvCsGC}), sabemos que $\abs{\fd_n(t_i) - F(t_i)} \convcs 0$, y por lo tanto \[ \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i) - F(t_i)} \convcs 0 \].

De la misma forma, usando (\ref{eqConvIzq}) tenemos que \[ \max_{i=1,\dotsc ,k} \abs{\fd_n(t_i^-) - F(t_i^n)} \convcs 0 \]. Por lo tanto, todo ese máximo enorme vale 0, de tal forma que 

\[ \lim_{n\to\infty} \sup_{t\in\real} \abs{\fd_n(t) - F(t)}  =  \lim_{n\to\infty} \md{\fd_n - F}_\infty ≤ \epsilon \]

para cualquier $\epsilon > 0$ arbitrario que cojamos. Es decir, que \[ \md{\fd_n - F}_\infty=\sup_{t\in\real} \abs{\fd_n(t) - F(t)} \convcs 0 \]
\end{proof}

\section{Estadísticos}

Cuando extraemos una muestra $\{x_n\}$ de $X$ se pueden calcular algunas \textit{medidas resumen}. Cualquiera de ellas se puede expresar matemáticamente como una función $T(x_1,\dotsc,x_n)$ de la muestra. 

\begin{defn}[Estadístico]
Sea $T(x_1,\dotsc,x_n)$ una función cuyo dominio incluye el espacio muestral del vector aleatorio $(X_1, \dotsc, X_n)$. Entonces la variable aleatoria $T$ se denomina \textbf{estadístico}. La única restricción es que un estadístico no puede ser función de un parámetro.
\end{defn}

Como la distribución de $T$ se calcula a partir de la distribución de las variables $X_i$ que constituyen la muestra, la denominaremos distribución de $T$ en el muestreo (\textit{sampling distribution).}

\begin{defn}[Error\IS típico]\index{Error!estándar}
El error estándar o error típico $\sigma$ de un estadístico $T$ es la desviación típica de su distribución en el muestreo. Como en ocasiones depende de alguna cantidad desconocida, también se denomina error típico a una estimación de ese valor.
\end{defn}

En ocasiones, se cumple que $\dfrac{T}{\sigma}$ sigue una distribución t de Student, lo que nos permitirá definir intervalos de confianza.

\subsection{Media muestral y poblacional}

\begin{defn}[Media\IS muestral] La media muestral \[ \avg{X} = \frac{\sum_{i=1}^n X_i}{n} \] se puede expresar de la siguiente forma

\[ \avg{X} = \int_\real x\,d\fd_n(x) \]
\end{defn}

\index{Media!poblacional}
La definición es análoga con la de la \textbf{media poblacional}

\[ µ = \int_\real x \,dF(x) \]

Esto nos da una clave de la estadística: sustituir todo lo que desconozco de la población con su análogo muestral (en este caso, pasamos de la función de distribución teórica a la función de distribución empírica). Sólo quedaría ver si los estimadores que resultan son adecuados.

La media muestral tiene otras relaciones muy importantes con $µ$:

\begin{enumerate}
\item $\avg{X}$ es \index{Estimador!insesgado}\index{Estimador!centrado} \textbf{estimador insesgado o centrado} de µ: $ \esp{\avg{X}} = µ$
\item $\var{\avg{X}} = \dfrac{\sigma^2}{n}$. Como es inversamente proporcional, está claro que cuantos más datos haya mejor nos aproximaremos a lo que queremos estimar.
\end{enumerate}

\begin{theorem}[Teorema\IS central del límite] Suponemos que $\{X_n\}$ son v.a.i.i.d. con media $µ$ y desviación típica $\sigma$ finitas. Entonces
\label{thmCentral}
\[ \sqrt{n}\frac{\avg{X}-µ}{\sigma} \convdist N(0,1) \]

Si denotamos la función de distribución de la normal como \[ \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}\] entonces

\[ \forall t\in\real\quad \prob{ \sqrt{n}\frac{\avg{X}-µ}{\sigma} ≤ t } \convs \Phi(t) \]

Por tanto, para $n$ grande se tiene

\[ \prob{\sqrt{n} \left(\avg{X} - µ\right) ≤ x} ≈ \Phi(\frac{x}{\sigma}) \]

\textbf{aunque las $X_i$ no tengan distribución normal.}

\end{theorem}

\subsection{Varianza muestral y poblacional}

Una medida importante de dispersión de una variable aleatoria es la varianza \begin{equation}
 \mathbb{V}(X)=\sigma^2 = \int_\real(x-µ)^2 \,dF(x) \label{eqVarianza}
\end{equation}

\begin{defn}[Varianza\IS muestral]El análogo muestral de $\sigma^2$ es la \textbf{varianza muestral.}. Utilizando el criterio \textit{plugin} en (\ref{eqVarianza})

\[\hat{\sigma}^2_n = \int_\real (x-\avg{X})^2\,d\fd_n(x) = \frac{1}{n}\sum_{i=1}^n(X_i - \avg{X})^2 \]
\end{defn}

\begin{theorem} La varianza muestral cumple lo siguiente

\begin{gather*}
\esp{\hat{\sigma}^2_n} = \frac{n-1}{n}\sigma^2\\
\hat{\sigma}^2_n \convcs \sigma^2
\end{gather*}
\end{theorem}

Por lo tanto, la varianza muestral es un estimador sesgado. No es un problema grande ya que cuando $n\to\infty$ acaba convergiendo a $\sigma^2$ y el sesgo 

\[ \esp{\hat{\sigma}^2_n} - \sigma^2= \frac{n-1}{n}\sigma^2 - \sigma^2 = \frac{-1}{n}\sigma^2 \]

también tiende a cero. Es decir, es \index{Asintóticamente!insesgado} \textbf{asintóticamente insesgado}.

\begin{defn}[Cuasivarianza\IS muestral] En lugar de usar $\hat{\sigma}^2_n$ usamos la cuasivarianza muestral, definida como

\[ S^2 = \frac{n}{n-1}\hat{\sigma}^2_n \] de tal forma que se tiene

\begin{gather*}
\esp{S^2} = \sigma^2 \\
S^2 \convcs \sigma^2 
\end{gather*}
\end{defn}

\subsection{Estadísticos de orden}

\begin{defn}[Estadístico\IS de orden]Dada una muestra $\{X_n\}$, se denotan como \[ X_{(1)} ≤ \dotsb ≤ X_{(n)} \] las observaciones de la muestra ordenadas de menor a mayor, llamados \textbf{estadísticos de orden}. Cuando la distribución de las v.a. es continua, la probabilidad de coincidencia en valores es $0$ y con probabilidad $1$ se tiene que \[ X_{(1)} < \dotsb < X_{(n)} \]
\end{defn}

Los estadísticos de orden pueden utilizarse para definir la mediana o los cuartiles. Sin embargo, podemos usar la función cuantílica para definir mejor estos conceptos.

\begin{defn}[Función\IS cuantílica] La función cuantílica en $p$ es el punto que deja una probabilidad $p$ a la izquierda, de tal forma que una proporción $p$ de los individuos de la población $X$ sería menor que el cuantil poblacional de orden $p$.

La función cuantílica correspondiente a la función de distribución $F$ como la función 



\begin{gather*}
\appl{\inv{F}}{\real}{(0,1)} \\
\inv{F}(p) = \inf \left\lbrace x \tq F(x) ≥ p \right\rbrace 
\end{gather*}
\end{defn}

La función cuantílica nos permite obtener los \textbf{cuantiles poblacionales de orden $p$} \index{Cuantil!poblacional} al valor $\inv{F}(p)$. El análogo es el \textbf{cuantil muestral de orden $p$}, \index{Cuantil!muestral} se define a partir de la función de distribución empírica como $\inv{\fd_n}(p)$.

\chapter{Estimación paramétrica}

En este tema, supondremos que la muestra, absolutamente continua o discreta, con función de densidad o probabilidad $f(.;\theta)$ que es totalmente conocida salvo el valor de un parámetro $\theta$ del cuál sólo se conoce su rango de posibles valores $\Theta$, al que se llama el \textbf{espacio paramétrico.}\index{Espacio!paramétrico}

\section{Estimadores}

\begin{defn}[Estimador] Sean $\{X_n\}$ v.a.i.i.d. con distribución común caracterizada por la función de densidad/masa $f(\cdot;\theta)$, con $\theta$ un parámetro desconocido del que sólo se sabe que pertenece al espacio paramétrico $\Theta \subset \real$.

El \textbf{estimador} es una función medible $\hat{\theta}_n = T_n(X_1,\dotsc, X_n)$ que se utiliza para estimar o aproximar el valor de $\theta$.
\end{defn}

Cuando tenemos una muestra aleatoria $\{X_n\}$, cada $T_n(X_1, \dotsc, X_n)$ es un estimador de $\theta$, una variable aleatoria. Si por el contrario tenemos una serie de observaciones de una muestra $\{x_n\}$ entonces $T_n(x_1,\dotsc,x_n)$ es una \textbf{estimación} de $\theta$.

Podemos evaluar la calidad de un estimador con el \textbf{error cuadrático medio} (ECM):

\[ \ECM (T_n) = \esp{(T_n - \theta)^2}\]

Si sumamos y restamos $\esp{T_n}$, nos queda que 

\[ \ECM (T_n) = \var{T_n} + \sesgo^2 T_n \]

que nos describe el error cuadrático medio en función de la varianza y del sesgo de $T_n$.

\subsection{Propiedades interesantes de los estimadores}
Buscaremos varias propiedades interesantes de los estimadores:

\index{Estimador!insesgado}
\subsubsection{Ausencia de sesgo} Se dice que un estimador $T_n$ es \textbf{insesgado} \index{Estimador!insesgado} si, siempre que $X_i \sim f(\cdot;\theta)$ se tiene que \[\esp{T_n} = \theta\; \forall \theta \in \Theta \]

\subsubsection{Consistencia}\index{Consistencia!en probabilidad} Se dice que $\{T_n\} = \{ T_n(X_1, \dotsc, X_n) \}$ es consistente en probabilidad si, siempre que $X_i \sim f(.;\theta)$ se tiene que $T_n \convprob \theta \; \forall \theta \in \Theta$.

Si reemplazamos la consistencia en probabilidad por la convergencia casi segura, se obtiene la \textbf{consistencia fuerte} o casi segura. \index{Consistencia!casi segura}\index{Consistencia!fuerte}

Para probar la consistencia fuerte, usaremos el siguiente teorema:

\begin{theorem}[Teorema\IS de la aplicación continua] \label{thmApContinua} Sea $\appl{g}{\real}{\real}$ continua en todo punto de un conjunto $C$ tal que $\prob{X\in C} = 1$, entonces

\begin{itemize}
\item Si $X_n\convdist X$ entonces $g(X_n)\convdist  g(X)$.
\item Si $X_n\convprob X$ entonces $g(X_n)\convprob g(X)$.
\item Si $X_n\convcs X$ entonces $g(X_n)\convcs g(X)$.
\end{itemize}

\end{theorem}
 
Otra forma de probarlo sería usar la desigualdad de Markov (\ref{desMarkov}). Buscamos probar que 

\[ \prob{\abs{T_n - \theta} > \epsilon} \convs 0 \]

entonces

\[ \prob{\abs{T_n - \theta} > \epsilon}   = \prob{(T_n - \theta)^2 > \epsilon^2} \]

que por Markov tenemos que

\[ \prob{(T_n - \theta)^2 > \epsilon^2} ≤ \frac{\esp{T_n-\theta}^2}{\epsilon^2} \]

y entonces sólo nos quedaría probar que $\esp{T_n-\theta}^2 \convs 0$.

También podemos usar condiciones suficientes

\begin{theorem}[Condición\IS de Borel-Cantelli] Si se cumple que 

\[ \sum_{n=1}^\infty \prob{\abs{T_n-\theta} > \epsilon} < \infty\;\forall\epsilon > 0 \]

entonces $T_n \convcs \theta$.
\end{theorem}

Con esta condición, bastaría ver que la probabilidad o la esperanza convergen y automáticamente se cumpliría la condición.

\begin{example} Sean $\{X_n\}$ v.a.i.i.d con distribución uniforme en el intervalo $[0,\theta]$ con $\theta > 0$. Estudiar la consistencia de los siguientes estimadores de $\theta$

\paragraph{a)}

\[ T_n = 2\avg{X} \]

Este estimador se basa en que $\esp{X} = \frac{\theta}{2}$. Esto se estima mediante la media muestral $\avg{X}$, y por lo tanto un estimador razonable sería duplicar esa media muestral: $T_n = 2 \avg{X}$.

Como $T_n$ se expresa como una función continua de la media muestral, por la LFGN y el teorema de la aplicación continua 

\[ T_n = g(\avg{X}) \convcs g(µ) = 2µ = 2\esp{X} = \theta \]

y por lo tanto tiene consistencia fuerte.

\paragraph{b)}

\[ T_n=X_{(n)} = \max \{ X_1,\dotsc,X_n\} \]

Aquí usaremos la segunda herramienta: estudiar la probabilidad que el estimador no se aleja del valor esperado en más de $\epsilon$:

\[ \prob{\abs{T_n - \theta} > \epsilon} = \prob{\abs{X_{(n)} - \theta} > \epsilon} = \prob{\theta - X_{(n)} > \epsilon} = \prob{X_{(n)} < \theta - \epsilon} \]

Si pedimos que el máximo sea mayor que $\theta - \epsilon$, es lo mismo que pedir que lo sean todas las observaciones:

\[ \prob{X_{(n)} < \theta - \epsilon} = \prob{X_1 < \theta - \epsilon, \dotsc , X_n < \theta - \epsilon} \]

Y con esto logramos quitarnos los estadísticos de orden, que nos causan problemas al tratar de seguir con la demostración. Como las variables de la muestra son independientes, podemos expresarlo todo como producto

\[ \prod_{i=1}^n \prob{X_i < \theta - \epsilon} = \left(\frac{\theta - \epsilon}{\theta}\right)^n \]

Esta probabilidad está contenida en el intervalo $(0, 1)$ y por lo tanto converge a cero cuando $n\to \infty$. Entonces, $T_n$ es un estimador de $\theta$ consistente en probabilidad.

Para examinar si se cumple la condición de Borel-Cantelli, examinamos la serie 

\[ \sum_{n=1}^\infty \prob{\abs{T_n - \theta} > \epsilon} = \sum_{n=1}^\infty \left(\frac{\theta - \epsilon}{\theta}\right)^n < \infty \]

se cumple la condición y es un estimador consistente casi seguro.

Si quisiésemos explorar cuál de los dos estimadores es mejor, usaríamos el error cuadrático medio.

\end{example}

\subsubsection{Normalidad asintótica}\index{Asintóticamente!normal}\index{Normalidad!asintótica} Se dice que una sucesión de estimadores $\{T_n\}$ del parámetro $\theta$ es \textbf{asintóticamente normal} con tasa $\sqrt{n}$ si

\[ \sqrt{n} (T_n - \theta) \convdist N(0,\sigma) \]

¿Cómo se puede probar la normalidad asintótica? La herramienta se llama el \textbf{método delta}\label{defMetDelta} \index{Método!delta} y es consecuencia casi inmediata del teorema del valor medio y de las propiedades de la convergencia en distribución: intentaremos expresar el estimador que se propone como una función $C^1$ de la media muestral y aplicar entonces el Teorema Central del Límite (\ref{thmCentral}).

Si llamamos $T_n = g(\avg{X})$ con $g\in C^1$ entonces podemos expresar, con un $µ^\ast$ entre $\avg{X}$ y $µ$

\[ \sqrt{n} (g(\avg{X}) - g(µ)) \underset{TVM}{=} g'(µ^\ast) \sqrt{n}(\avg{X} -µ) \]

Como $\avg{X}\convcs µ$ entonces $µ^\ast \convcs µ$ y por lo tanto y usando el Thm. de la aplicación continua (\ref{thmApContinua}) $g'(µ^\ast)\convcs g'(µ)$. Al final 

\[ g'(µ^\ast) \sqrt{n}(\avg{X} -µ) \convdist N(0, \abs{g'(µ)}\sigma) \]

En general, se habla de normalidad asintótica con tasa $a_n$ si se cumple que \[ a_n (T_n - \theta) \convdist N(0,\sigma) \], con $a_n$ sucesión creciente y mayor que cero.

\subsection{Estimador de máxima verosimilitud (EMV)}

En lo que sigue vamos a suponer que $\{X_n\}$ es una muestra formada por v.a.i.i.d. cuya distribución tiene una función de densidad o de masa $f(.;\theta_0)$ perteneciente a una familia de funciones $\{f(.;\theta) \tq \theta \in \Theta\}$. $\theta_0$ nos indica el valor real, y $\theta$ es un parámetro genérico.

Intuitivamente, lo que pensamos con este método es que la función de masa mide lo verosímil que es que salga un cierto parámetro. 

\begin{defn}[Función\IS de verosimilitud] También llamada \textit{likelihood function}. Dada una muestra fija $\{x_n\}$, se define como

\[ L_n(\theta;x_1,\dotsc,x_n) = L_n(\theta) = \prod_{i=1}^n f(x_i;\theta) \]
\end{defn}

\begin{defn}[Estimador\IS de máxima verosimilitud] También llamado EMV o MLE (\textit{maximum likelihood estimator}) es el argumento que maximiza la función de verosimilitud:

\[ \hat{\theta}_n = \hat{\theta}_n(x,\dotsc,x_n) = \argmax_{\theta\in\Theta} L_n(\theta;x_1,\dotsc,x_n) \]

cuando ese máximo está bien definido.
\end{defn}

Para evitar usar derivadas en un producto potencialmente muy largo, podemos maximizar el logaritmo de la verosimilitud, que es creciente y está bien definido porque la densidad es siempre mayor que cero, y los casos en los que sea cero no los estudiamos porque no ocurren (probabilidad 0).

\subsubsection{Cálculo efectivo}

El valor del estimador se obtiene como solución de la \index{Ecuación!de verosimilitud} \textbf{ecuación de verosimilitud}.

\[ \dpa{}{\theta}\log L_n = \dpa{}{\theta}\sum_{i=1}^n\log f(\theta;x_i) = 0 \]

\paragraph{Ejemplos}

\subparagraph{Distribución de Poisson de parámetro $\lambda$}. Suponemos que $X\sim \text{Poisson}\,(\lambda)$ con $\lambda > 0$, de tal forma que

\[ \prob{X = x} = e ^{-\lambda}\frac{\lambda^x}{x!}; \; x\in \ent^+ \]

Dada una muestra $\{x_n\}$ de $X$. Entonces

\[ L_n(\lambda) = \prod_{i=1}^n f(x_i;\lambda) = \prod_{i=1}^n \prob{X=x} = \prod_{i=1}^n e ^{-\lambda}\frac{\lambda^x}{x!} = e^{-n\lambda}\frac{\lambda^{\sum_{i=1}^nx_1}}{x_1!\dotsb x_n!} \]

Tomamos logaritmos:

\[ \log  L_n(\lambda) = -n\lambda + \log\lambda \sum_{i=1}^n x_i- \log\left(x_1!\dotsb x_n!\right) \]

y derivando

\[ \dpa{}{\lambda}\log  L_n(\lambda)  = -n +\frac{1}{\lambda} \sum_{i=1}^n x_i \]

de tal forma que nos queda \[ \hat{\lambda} = \frac{\sum_{i=1}^n x_i}{n} = \avg{x} \].

En la imagen (\ref{imgPoisson}) vemos cómo las diferentes funciones se aproximan a $\lambda = 1$.

\easyimg{VerosimilitudPoisson.png}{Diferentes funciones de verosimilitud para diferentes muestras de la distribución de Poisson}{imgPoisson}

\subparagraph{Distribución normal de parámetros $µ,\sigma$} Tenemos \[f(x;µ,\sigma) = \frac{1}{\sqrt{2\pi}\sqrt{\sigma^2}}e^{\frac{-1}{2}\frac{(x-µ)^2}{\sigma^2}} \]

La función de verosimilitud es

\[ L_n = \prod_{i=1}^n f(x_i;µ,\sigma) = \frac{1}{(2\pi)^{n/2} (\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-µ)^2} \]

Tomamos logaritmos:

\[ \log L_n = - \frac{n}{2}\log 2\pi - \frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^n(x_i-µ)^2 \]

Derivamos con respecto de $µ$

\[ \dpa{\log L_n}{µ} = - \frac{1}{\sigma^2} \sum_{i=1}^n(x_i-µ)(-1) = - \frac{1}{\sigma^2}\left( \sum_{i=1}^nx_i - nµ\right) \]

de tal forma que $\hat{µ} = \avg{x}$.

Hacemos lo mismo con $\sigma$

\[ \dpa{\log L_n}{\sigma} = - \frac{n}{2}\frac{1}{\sigma^2} - \frac{1}{2}\sum_{i=1}^n(x_i-µ)^2 \frac{-1}{\sigma^4} = \frac{1}{2\sigma^2}\left(-n + \sum_{i=1}^n\left(x_i-µ\right)^2\frac{1}{\sigma^2}\right) = 0 \]

y por lo tanto \[\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(x_i-\avg{x})^2 = \sigma^2 \]

\subparagraph{Distribución Weibull} La función de densidad de la distribución de Weibull, que toma dos parámetros $k$ y $\theta$, es

\[ f(x; \theta, k) = \frac{k}{\theta}\left(\frac{x}{\theta}\right)^{k-1}e^{-\left(\frac{x_i}{\theta}\right)^k}\ind_{[0, \infty)}(x) \]

La función de verosimilitud para los dos parámetros es:

\begin{gather*}
 L_n(k,\theta) = \prod_{i=1}^n f(x_i, \theta, k) = \prod_{i=1}^n \frac{k}{\theta}\left(\frac{x_i}{\theta}\right)^{k-1}e^{-\left(\frac{x_i}{\theta}\right)^k} = \\
 = k^n \theta^{-n}\left(\prod_{i=1}^n x_i\right)^{k-1} \theta^{-n(k-1)} e^{-\frac{1}{\theta^2}\sum_{i=1}^nx_i^k} = k^n\theta^{nk} \left(\prod_{i=1}^n x_i\right)^{k-1} e^{-\frac{1}{\theta^k}\sum_{i=1}^nx_i^k}
 \end{gather*}

Tomamos logaritmos:

\[ \log L = n\log k - nk\log \theta + (k-1)\sum_{i=1}^n\log x_i - \frac{1}{\theta^k} \sum_{i=1}^n \left(\frac{x_i}{\theta}\right)^k\]

y derivamos con respecto de ambas variables

\[ \dpa{\log L}{\theta} = -nk \frac{1}{\theta} - (-k) \theta^{-k-1} \sum_{i=1}^nx_i^k = \frac{k}{\theta}\left(-n + \frac{1}{\theta^k}\sum_{i=1}^nx_k^k\right) = 0 \]

\[ \dpa{\log L}{k} = \frac{n}{k} -n\log\theta + \sum_{i=1}^n \log x_i - \sum_{i=1}^n\left(\frac{x_i}{\theta}^k\right) \log\frac{x_1}{\theta} = 0 \]

Con la primera ecuación, tenemos que

\[ \frac{1}{\theta^k} = \frac{\sum_{i=1}^n x_i^k}{n} \therefore \hat{\theta} = \left(\frac{n}{\sum_{i=1}^n x_i^k}\right)^\frac{1}{k} \]

De la segunda ecuación resulta prácticamente imposible despejar $k$. Sin embargo, podemos usar métodos numéricos para obtener el valor de $k$. 

\begin{theorem}[Invarianza del EMV] Si $\tau$ es una función biyectiva y $\hat{\theta}$ es el e.m.v. de $\theta$, entonces el e.m.v. de $\tau(\theta)$ es $\tau(\hat\theta)$
\end{theorem}

Por ejemplo, tomamos $X\sim N(µ, \sigma)$. Ya habíamos calculado el e.m.v. de la varianza, que era la varianza muestral. ¿Cómo calcular entonces el e.m.v. de la desviación típica? Sabiendo que $\sigma = \sqrt{\sigma^2}$, tomamos $\tau(\theta) = \sqrt{\theta}$ que es una función biyectiva en $\real^+$ y por lo tanto podemos decir que $\text{emv}\,(\sigma) = \sqrt{\hat\sigma^2}$

\subsubsection{Motivación del método}
Estudiamos la siguiente función
\[ \frac{1}{n}\log L_n(\theta) = \frac{1}{n} \log \left(\prod_{i=1}^nf(X_i,\theta)\right) = \frac{1}{n}\sum_{i=1}^n \log f(X_i;\theta) \]

que por la L.G.N. (\ref{thmGrandes}) converge a una función $\Psi(\theta)$ que es el valor esperado de esos logaritmos de las muestras:

\[ \frac{1}{n} log L_n(\theta) \convs \Psi(\theta) \]

donde \[ \Psi(\theta) = \mathbb{E}_{\theta_0} \left[\log f(X;\theta)\right] = \int\log f(x;\theta) f(x;\theta_0) \, dx \]

\begin{theorem}[Teorema\IS MV1] \label{thmMV} Sea $X\sim f(.;\theta_0)$. Supongamos que se satisfacen las siguientes condiciones:

\subparagraph{MV0) Parametrización adecuada} Las distribuciones son distintas si el parámetro $\theta$ es distinto.

\subparagraph{MV1) Soporte común} Las distribuciones $f(.;\theta)$ tienen un soporte común. Es decir, que las funciones de densidad o de masa tienen valor distinto de cero en los mismos puntos.

\subparagraph{MV2)} $\Psi(\theta)$ es finita para todo $\theta \in \Theta$.

Entonces $\theta_0$ es el único máximo de la función $\Psi(\theta)$ y además 

\[ \mathbb{P}_{\theta_0} \left\lbrace L_n(\theta_0;X_1,\dotsc,X_n) >  L_n(\theta;X_1,\dotsc,X_n) \right\rbrace \convs 1\; \forall \theta \neq \theta_0 \]

\end{theorem} 

En el teorema se habla del soporte, definámoslo formalmente:

\begin{defn}[Soporte] El soporte de una función de distribución o masa $f$ es el conjunto de puntos en el que el valor de $f$ es distinto de 0. Es decir, \[ \mathrm{soporte}\; f = \{ x \in \real \tq f(x) \neq 0 \} \]
\end{defn}

Para la demostración, primero veremos la siguiente desigualdad:

\begin{theorem}[Desigualdad\IS de Jensen] Supongamos que $X$ es una v.a. tal que $\esp{X}< \infty$ (su esperanza existe y es finita) y que $\varphi$ es una función convexa (como una parábola, más o menos) tal que $\esp{\varphi(X)} < \infty$.

Entonces \[ \esp{\varphi(X)} ≥ \varphi(\esp{X}) \] \label{desJensen}
\end{theorem}

Con esto, podemos pasar a la demostración del teorema (\ref{thmMV}):

\begin{proof} Decir que \[ L_n(\theta_0;X_1, \dotsc, X_n) > L_n(\theta; X_1,\dotsc,x_n) \] es equivalente a que 

\begin{gather*}
\log L_n(\theta_0;X_1, \dotsc, X_n) >\log L_n(\theta; X_1,\dotsc,x_n) \\
\sum_{i=1}^n \log f(X_i;\theta_0) > \sum_{i=1}^n \log f(X_i;\theta) \\
0 > \frac{1}{n}\sum_{i=1}^n \left[\log f(X_i;\theta) - \log f(X_i;\theta_0)\right] = \\
= \frac{1}{n}\sum_{i=1}^n \log \frac{f(X_i;\theta)}{f(X_i;\theta_0)}\convprob \mathbb{E}_{\theta_0}  \left(\log \frac{f(X_i;\theta)}{f(X_i;\theta_0)}\right) =  - \mathbb{E}_{\theta_0}  \left(-\log \frac{f(X_i;\theta)}{f(X_i;\theta_0)}\right) < 0
\end{gather*} 

usando la L.G.N (\ref{thmGrandes}). Aplicando ahora la desigualdad  de Jensen (\ref{desJensen})

\[ - \mathbb{E}_{\theta_0}  \left(-\log \frac{f(X_i;\theta)}{f(X_i;\theta_0)}\right) > - \log  \mathbb{E}_{\theta_0}  \frac{f(X_i;\theta)}{f(X_i;\theta_0)} \]

Entonces \[  \mathbb{E}_{\theta_0}  \frac{f(X_i;\theta)}{f(X_i;\theta_0)} = \int \frac{f(x;\theta)}{f(x;\theta_0)} f(x;\theta_0) \,dx = \int f(x;\theta)\, dx = 1 \] 

y por lo tanto  \[ - \mathbb{E}_{\theta_0}  \left(-\log \frac{f(X_i;\theta)}{f(X_i;\theta_0)}\right) = - \log 1 = 0 \]

Entonces, $\forall \epsilon > 0$ 

\begin{gather*}\prob{\abs{\frac{1}{n} \sum_{i=1}^n \log \frac{f(X_i;\theta)}{f(X_i;\theta_0)} - \mathbb{E}_{\theta_0}\left(\log \frac{f(X;\theta)}{f(X;\theta_0)}\right)} > \epsilon} \convs 0 \dimplies \\
\prob{\abs{\frac{1}{n} \sum_{i=1}^n \log \frac{f(X_i;\theta)}{f(X_i;\theta_0)} - \mathbb{E}_{\theta_0}\left(\log \frac{f(X;\theta)}{f(X;\theta_0)}\right)} ≤ \epsilon} \convs 1
\end{gather*}

Tomo $\displaystyle \epsilon = \frac{1}{2}\abs{\mathbb{E}_{\theta_0}\left(\log \frac{f(X;\theta)}{f(X;\theta_0)}\right)}$ y entonces 

\[ \prob{\frac{1}{n}\sum_{i=1}^n\log\frac{f(X_i;\theta)}{f(X_i;\theta_0)}  <  \frac{1}{2}\abs{\mathbb{E}_{\theta_0}\left(\log \frac{f(X;\theta)}{f(X;\theta_0)}\right)} < 0 } \convs 1 \]
\end{proof}

\subsubsection{Consistencia del método}

\begin{theorem}[Teorema\IS MV2]\label{thmMV2} Supongamos que se cumplen las condiciones del teorema MV1 (\ref{thmMV}) y adicionalmente

\subparagraph{MV3)} El espacio paremétrico $\Theta$ es un intervalo abierto no necesariamente finito y, para casi todo $x$, $f(x;\theta)$ es diferenciable respecto a $\theta$ con derivada continua.

Entonces, con probabilidad tendiente a 1, la ecuación

\begin{equation}
 \dpa{}{\theta} \log L_n (\theta;X_1,\dotsc,X_n) = 0 \label{eqMV2}
\end{equation}
tiene una raíz $\hat\theta_n = \hat\theta_n(x_1,\dotsc,x_n)$ que \textbf{converge en probabilidad a $\theta_0$} (el verdadero valor del parámetro). Si además suponemos que la raíz es única, entonces $\hat\theta_n$ maximiza la verosimilitud $L_n$ y por lo tanto es el estimador de máxima verosimilitud.
\end{theorem}

\begin{proof}
Sea $\epsilon > 0$. Entonces para casi todo\footnote{Casi todo: puntos con probabilidad no nula} $x$ en el intervalo $\Omega = (\theta_0 - \epsilon, \theta_0 + \epsilon)$ se tiene que $f(x;\theta)$ es diferenciable con derivada continua.

\begin{center}
\begin{tikzpicture}
\draw (-4,0) -- (4,0);
\node[fill=white, circle, draw, inner sep=2pt, label=below:$\theta_0 - \epsilon$] at (-2, 0) {};
\node[fill=white, circle, draw, inner sep=2pt, label=below:$\theta_0$] at (0, 0) {};
\node[fill=white, circle, draw, inner sep=2pt, label=below:$\theta_0 +\epsilon$] at (2, 0) {};

\node[fill=black, circle, draw, inner sep=1pt, label=above left:$L_n(\theta_0 - \epsilon)$] (A) at (-2, 0.8) {};
\node[fill=black, circle, draw, inner sep=1pt, label=above:$L_n(\theta_0)$] (B) at (0, 1.3) {};
\node[fill=black, circle, draw, inner sep=1pt, label=above right:$L_n(\theta_0 +\epsilon)$] (C) at (2, 0.6) {};
\end{tikzpicture}
\end{center}

Cogemos entonces un conjunto $S_n$ definido de la siguiente forma:

\begin{align*}
S_n = \{ (x_1,\dotsc,x_n) &\tq L_n(\theta_0;x_1, \dotsc,x_n) > L_n(\theta_0 - \epsilon; x_1, \dotsc, x_n) \\
&\y  L_n(\theta_0;x_1, \dotsc,x_n) > L_n(\theta_0 + \epsilon; x_1, \dotsc, x_n) \} 
\end{align*}

Aplicando el teorema MV1 (\ref{thmMV}), tenemos que $\mathbb{P}_{\theta_0}(S_n) \convs 1$. 

En algún punto del interior del intervalo $\Omega$ hay un máximo local. Como puede haber varios máximos locales, tomo $\hat\theta_n$ como el punto de máximo local más cercano a $\theta_0$. 

Se cumple que cada uno de esos puntos de máximo satisfacen la ecuación de verosimilitud (\ref{eqMV2}). En consecuencia $\hat\theta_n$ satisface también esa misma ecuación. Por lo tanto

\[ \prob{\abs{\hat\theta_n - \theta_0} < \epsilon} \convs 1 \dimplies \prob{\abs{\hat\theta_n - \theta_0} ≥ \epsilon} \convs 0 \]

y entonces

\[ \hat\theta_n \convprob \theta_0 \]
\end{proof}

\subsubsection{Información de Fisher}

Supongamos el conjunto de todos los estimadores de un parámetro $\theta$. Su error cuadrático medio es 

\[ \ECM \hat\theta = \var{\hat\theta} + \sesgo^2 \hat\theta \]

Si queremos buscar el \textit{mejor} estimador, buscamos los que minimicen el ECM. Por lo tanto, nos interesaremos en el subconjunto de estimadores insesgados ($\sesgo \hat\theta = 0$). Sin embargo, no tenemos una forma clara de distinguir cuál es mejor entre esos estimadores insesgados. En esta sección vamos a buscar una \textit{escala}, a la que llamaremos la \textbf{información de Fisher}, que nos dará una cota para la varianza de un estimador. 

Suponemos que en la integral $\int f(x;\theta)\,dx$ se puede derivar dos veces bajo el signo integral (esto es, que $\int \frac{∂^2}{∂\theta^2} f(x;\theta)\,dx$ existe) y que además se puede permutar la integral y la derivada parcial (vemos condiciones suficientes en el apéndice \ref{secConds}, página \pageref{secConds}). Entonces

\[ \int f(x;\theta)\, dx = 1 \implies \dpa{}{\theta} \int f(x;\theta)\,dx = 0 \]

Por tanto

\[ \int \dpa{}{\theta}(\log f(x;\theta) ) f(x;\theta) \, dx = \mathbb{E}_\theta\left(\dpa{}{\theta}\log f(X;\theta)\right) = 0 \]

Si derivamos de nuevo en la integral

\begin{gather*}
\frac{∂^2}{∂\theta^2} \int f(x;\theta)\,dx = 0  = \int \frac{∂^2}{∂\theta^2} f(x;\theta)\,dx = \\
=\int \frac{∂^2}{∂\theta^2} \log f(x;\theta) f(x;\theta)\,dx + \int\dpa{}{\theta} \log f(x;\theta) \cdot \dpa{}{\theta}f(x;\theta)\,dx = \wtf \\
=\int \frac{∂^2}{∂\theta^2}\log f(x;\theta) f(x;\theta)\, dx + \int \left(\dpa{}{\theta}\log f(x;\theta)\right)^2 f(x;\theta)\,dx = \\
= \mathbb{E}_\theta \left[\frac{∂^2}{∂\theta^2}\log f(X;\theta) \right] 
	+ \mathbb{E}_\theta \left[ \left(\dpa{}{\theta} \log f(X;\theta)\right)^2\right] = 0
\end{gather*}

El segundo valor se llama información de Fisher:
\begin{defn}[Información\IS de Fisher] Se denota por $I(\theta)$ la información de Fisher del parámetro $\theta$

\[ I(\theta) =\esp[\theta]{\left(\dpa{}{\theta} \log f(X;\theta)\right)^2} \]

Representa intuitivamente la \textit{cantidad de información} acerca del valor del parámetro $\theta$ contenida en una observación de $X$.
\end{defn}

¿En qué consiste esa cantidad de información? Tomemos, por ejemplo, una normal $N(0,\theta)$ con $\theta$ pequeña. Una observación $X$ que hagamos nos dará mucha información sobre el modelo, ya que todos los valores de la normal están muy agrupados, y por lo tanto $I(\theta)$ será grande. Si tomamos $\theta$ grande, una observación $X$ no nos dará mucha información sobre el modelo porque los valores están más dispersos, y por lo tanto tendremos un valor de $I(\theta)$ pequeño.

La información de Fisher nos da una cota inferior para la varianza.

\begin{theorem}[Cota\IS de Fréchet-Cramér-Rao] Dado $\hat\theta$ un estimador insesgado de $\theta$, entonces
\label{thmCotaFCR}
\[ \var{\hat\theta} ≥ \frac{1}{nI(\theta)} \]

donde $\frac{1}{nI(\theta)}$ se llama la \textbf{cota de Fréchet-Cramér-Rao}.
\end{theorem}

\begin{proof} Tomamos la v.a. $Z$ como la derivada del logaritmo de la verosimilitud

\[ Z = \dpa{}{\theta}\log L_n (X,\theta) = \sum_{i=1}^n \dpa{}{\theta} \log f(X_i;\theta) \]

La desigualdad de Cauchy-Schwartz establece que \footnote{Por ejemplo, porque no tengo ni idea de dónde sale esto.}

\[ \var[\theta]{T_n} ≥ \frac{\text{Cov}_\theta^2 (Z,T_n)}{\var[\theta]{Z}} \]

Veremos que el numerador vale 1 si $T_n$ es un estimador insesgado, y que $\var[\theta]{Z} = n I(\theta)$.

Primero observamos que

\[ \esp[\theta]{Z} = \sum_{i=1}^n \esp{\frac{∂}{∂\theta} \log f(X_i;\theta)} = 0 \]

Y la varianza

\[ \var[\theta]{Z} = \sum_{i=1}^n \var[\theta]{\frac{∂}{∂\theta} \log f(X_i;\theta)} = \sum_{i=1}^n \esp{\left(\dpa{}{\theta} \log f(X;\theta)\right)^2}[\theta] = n I(\theta) \]

La primera parte está demostrada.

Ahora vemos que, si $\esp[\theta]{Z} = 0$, entonces

\[ \text{Cov }(Z,T_n) = \esp[\theta]{ZT_n} - \underbrace{\esp[\theta]{Z}}_0 \esp[\theta]{T_n} = \esp[\theta]{ZT_n}  \]

Como $Z$ y $T_n$ dependen de la muestra

\[ \esp[\theta]{ZT_n} = \esp[\theta] {Z(X_1,\dotsc,X_n) \cdot T_n(X_1,\dotsc,X_n)}= \int_{\real^n} Z(x_1,\dotsc,x_n) \cdot T_n(x_1,\dotsc,x_n) \cdot f_\theta(x_1,\dotsc,x_n)\,d(x_1, \dotsc, x_n) \]

Como las $X_1,\dotsc,X_n$ son independientes, 

\[ f_\theta(x_1,\dotsc,x_n) = \prod_{i=1}^nf(x_i;\theta)\ \]

y la integral nos queda entonces como una serie de integrales iteradas

\[ \int_\real \dotsb \int_\real Z(x_1,\dotsc,x_n) \cdot T_n(x_1,\dotsc,x_n)  \prod_{i=1}^nf(x_i;\theta)\,dx_i \]

Vemos cuánto vale $Z$:

\[ Z = \dpa{}{\theta}\log f(x_i;\theta) = \sum_{i=1}^n \frac{\dpa{}{\theta}f(x_i;\theta)}{f(x_i;\theta)} \]

Pero 

\[ \sum_{i=1}^n \frac{\dpa{}{\theta}f(x_i;\theta)}{f(x_i;\theta)}  \prod_{i=1}^nf(x_i;\theta)\,dx_i  = \sum_{i=1}^n\left[ \dpa{}{\theta}f(x_i;\theta \cdot \prod_{\substack{j=1\\j\neq i}}^n f(x_j;\theta) \right] \]

que por la regla de la cadena es igual a 

\[ \dpa{}{\theta}\left[\prod_{i=1}^n f(x_i;\theta) \right] \]

y entonces nos queda que

\begin{gather*}
\cov (Z,T_n) = \esp[\theta]{ZT_n} = \int_\real \dotsc \int_\real T_n(x_1,\dotsc,x_n) \dpa{}{\theta} \prod_{i=1}^n f(x_i;\theta)\, dx_i = \\
 = \dpa{}{\theta} \int_\real \dotsc \int_\real T_n(x_1,\dotsc,x_n) \prod_{i=1}^n f(x_i;\theta)\, dx_i =\\
 = \dpa{}{\theta} \esp[\theta]{T_n} 
 \end{gather*}

Como $T_n$ es un estimador insesgado $\esp[\theta]{T_n}  = \theta$ y entonces $\text{Cov}\,(Z,T_n) = 1$. Por lo tanto, nos queda que 

\[ \var{\hat\theta} ≥ \frac{1}{nI(\theta)} \]

Además, si $T_n$ no fuese un estimador insesgado

\[ \var{\hat\theta} ≥ \frac{\left(dpa{}{\theta} \esp[\theta]{T_n} \right)^2}{n I(\theta)} \]

y por lo tanto

\[ \text{ECM}(T_n) ≥ \frac{\left(\dpa{}{\theta} \esp[\theta]{T_n}\right)^2}{n I(\theta)} + \text{Sesgo }^2 (T_n) \]

\end{proof}

\begin{defn}[Estimador\IS eficiente] Se dice que un estimador es eficiente si su varianza es igual a la cota de Fréchet-Cramér-Rao (\ref{thmCotaFCR}), es decir
\[ \var{\hat\theta} = \frac{1}{nI(\theta)} \]
\end{defn}


\subsubsection{Eficiencia asintótica}
\begin{theorem}[Teorema\IS MV3] Supongamos que se verifican las condiciones MV0 - MV3 (ver teoremas \ref{thmMV}, \ref{thmMV2}) y además:

\subparagraph{MV4)} La integral $\int f(x;\theta)\,dx$ se puede derivar dos veces bajo el signo integral.
\subparagraph{MV5)} Para cada $x$ la densidad $f(x;\theta)$ es tres veces diferenciable con respecto a $\theta$, con la tercera derivada continua en $\theta$.
\subparagraph{MV6)} La información de Fisher es estrictamente positiva y finita: $0 < I(\theta_0) < \infty$
\subparagraph{MV7)} Para cada $\theta_0\in \Theta$ existen un número $c > 0$ y una función $M(x)$, que pueden depender de $\theta_0$, tales que \[ \esp[\theta_0]{M(X)} < \infty \] y \[ \abs{\frac{∂^3\log f}{∂\theta^3}(x;\theta)} ≤ M(x)\; \forall x;\;\forall\theta \in (\theta_0 - c,\theta_0 + c) \]
\end{theorem}

\begin{proof}

\begin{gather}
 \tilde\Psi_n(\theta) = L_n  \\
 \tilde\Psi_n' = \dpa{\Psi_n(\theta)}{\theta} \label{eqT3_2} \\
 f' = \dpa{f}{\theta}f
 \end{gather}
 
 donde la función \ref{eqT3_2} se llama el \textbf{score} (quizás).
 
 Recordemos que $\hat\Psi_n(\theta)$ depende de la muestra. Para cada muestra fija se tiene
 
 \[ \tilde\Psi_n(\hat\theta_n) = \hat\Psi_n'(\theta_0) + (\hat\theta_n-\theta_0)\Psi_n''(\theta_0) + \frac{\left(\hat\theta_n-\theta_0\right)^2}{2}\tilde\Psi_n'''(\theta_n^\ast) \]

Para algún $\theta_n^\ast$ entre $\hat\theta_ n$ y $\theta_0$. Como el primer miembro es 0, resulta

\[ \sqrt{n}\left(\hat\theta_n-\theta_0\right) = \frac{\frac{1}{\sqrt{n}}\tilde\Psi_n'(\theta)^2}{-\frac{1}{n}\Psi_n''(\theta_0) - \frac{1}{2n}\left(\hat\theta_n-\theta_0\right)\tilde\Psi_n'''(\theta_n^\ast)} \]

Vamos a demostrar que esto converge en tres pasos:

\begin{itemize}
\item Numerador converge a $N(0,\sqrt{I(\theta_0)}$.
\item Primera parte converge a $I(\theta_0)$.
\item Segunda parte denom. converge a 0 en prob.
\end{itemize}

Usando además el teorema de Slutsky (\ref{thmSlutsky}), que nos dice que NO DICE NADA PORQUE LO HA BORRADO.

\paragraph{Parte 1: Numerador}

\[ \frac{1}{\sqrt{n}}\tilde\Psi_n'(\theta_0) = \frac{\sqrt{n}}{n}\sum_{i=1}^n \left[\frac{f'(X_i;\theta_0)}{f(X_i;\theta_0)} - \esp[\theta_0]{\frac{f'(X_i;\theta_0)}{f(X_i;\theta_0)}}\right] \]

Como $\esp[\theta_0]{\frac{f'(X_i;\theta_0)}{f(X_i;\theta_0)}} = 0$ (vete tú a saber por qué), la aplicación del TCL (\ref{thmCentral}) a las variables $Y_i = \frac{f'(X_i;\theta_0)}{f(X_i;\theta_0)}$ y la definición de $I(\theta_0)$ proporcionan directamente

\[ \frac{1}{\sqrt{n}}\hat\Psi_n'(\theta_0) \convdist N(0,\sqrt{\var{Y}}) \]

Calculamos ahora esa desviación típica:

\begin{gather*}
 \var[\theta]{Y} = \esp[\theta]{Y^2} - \esp[\theta]{Y}^2 = \esp[\theta]{Y^2} = \\
 = \esp[\theta]{\left(\dpa{}{\theta}\log f(X;\theta)\right)^2} = I(\theta)
\end{gather*}

Y por lo tanto nos queda que 

\[ \frac{1}{\sqrt{n}}\hat\Psi_n'(\theta_0) \convdist N\left(0,\sqrt{I(\theta_0)}\right) \]

\paragraph{Parte 2: Denominador A}

Operamos con

\[ -\frac{1}{n}\Psi_n''(\theta_0) \]

Si derivamos de nuevo $\tilde\Psi_n''$ con respecto a $\theta$ tenemos que 

\[ \tilde\Psi_n''(\theta) = \sum_{i=1}^n \frac{f''(x_i;\theta)f(x_i;\theta) - \left(f'(x_i;\theta)\right)^2}{f^2(x_i;\theta)} \]

Entonces $\frac{1}{n}\Psi_n''(\theta_0)$ es un promedio, y por la LGN (\ref{thmGrandes}) 

\begin{align*}
- \frac{1}{n}\Psi_n''(\theta_0) & \convprob - \esp[\theta_0]{\frac{f''(x_i;\theta)f(x_i;\theta) - \left(f'(x_i;\theta)\right)^2}{f^2(x_i;\theta)}} =\\ & =\underbrace{\esp[\theta_0]{\left(\frac{f'(X_i;\theta_0)}{f(X_i;\theta_0)}\right)^2}}_{I(\theta_0)}- \esp[\theta_0]{\frac{f''(X_i;\theta_0)}{f(X_i;\theta_0)}} 
\end{align*}

Operamos ahora con la segunda parte 

\begin{gather*}
\esp[\theta_0]{\frac{f''(X_i;\theta_0)}{f(X_i;\theta_0)}} = \int_\real \frac{f''(X_i;\theta_0)}{f(X_i;\theta_0)}f(X_i;\theta_0)\,dx = \\
= \int_\real \left.\frac{∂^2}{∂\theta^2}f(x;\theta)\right|_{\theta=\theta_0}\,dx 
\end{gather*}

y como según el enunciado del teorema podemos permutar la derivada con la integral dos veces, tenemos que

\[ \int_\real \left.\frac{∂^2}{∂\theta^2}f(x;\theta)\right|_{\theta=\theta_0}\,dx =  \left.\frac{∂^2}{∂\theta^2}\int_\real f(x;\theta)\,dx\right|_{\theta=\theta_0} = \left.\frac{∂^2}{∂\theta^2}0\right|_{\theta=\theta_0}  = 0 \]

Por lo tanto

\[ - \frac{1}{n}\Psi_n''(\theta_0) \convprob I(\theta_0) \]

\paragraph{Paso 3: Algo}

\[\frac{1}{2n}\left(\hat\theta_n-\theta_0\right)\tilde\Psi_n'''(\theta_n^\ast) \convprob 0 \]

Por hipótesis del teorema, $\hat\theta_n$ se considera consistente y entonces \[ \left(\hat\theta_n-\theta_0\right) \convs 0 \]. Analizaremos ahora la segunda parte de esa ecuación, $\tilde\Psi_n'''(\theta_n^\ast)$, y demostraremos que tiende a una constante.

\[ \tilde\Psi_n'''(\theta_n^\ast) = \frac{1}{n}\sum_{i=1}^n \frac{∂^3}{∂\theta^3}\log f(X_i;\theta) \]

Como $\hat\theta_n$ es consistente, $\theta_n^\ast$, que es un punto intermedio entre $\hat\theta_n$ y $\theta_0$, también tiende a $\theta_0$ en probabilidad. Entonces podemos aplicar la hipótesis MV7 del teorema y acotar la derivada parcial:

\[ \abs{\frac{∂^3}{∂\theta^3}\log f(X_i;\theta)} ≤ M(X_i) \]

y por lo tanto podemos acotar en probabilidad

\[ \abs{\tilde\Psi_n'''(\theta_n^\ast)} < \frac{1}{n}\sum_{i=1}^n M(X_i) \] 

Este término converge a una constante por lo tanto, y entonces se cumple que 

\[\frac{1}{2n}\left(\hat\theta_n-\theta_0\right)\tilde\Psi_n'''(\theta_n^\ast) \convprob 0 \]
\end{proof}


\subsection{Método de los momentos}
\index{Estimador!por el método de los momentos}
Sea $X\sim f(x;\theta)$, donde $\theta = (\theta_1,\dotsc,\theta_p)$ es un parámetro p-dimensional, con $p≥1$. 

Si los momentos $\alpha_k(\theta) = \esp[\theta_0]{X^k},\;k=1,\dotsc,p$ son funciones sencillas de los $\theta_i$, un procedimiento natural para obtener un estimador de $\theta$ es resolver en $\theta_1,\dotsc,\theta_p$ el sistema de ecuaciones

\begin{gather*}
m_1 = \alpha_1(\theta) \\
\dotsb \\
m_p = \alpha_p(\theta) 
\end{gather*}

donde cada $m_k$ es el momento muestral:

\[ m_k = \frac{\sum_{i=1}^n X_i^k}{n} \]

La idea es estimar el parámetro de tal forma que los momentos muestrales coincidan con los momentos poblacionales. Por la LGN, cuando $n\to\infty$ entonces $m_k \to \alpha_k(\theta_0)$. 

El método de los momentos se utiliza poco ya que da peores estimadores que el EMV. Sin embargo, puede resultar muy útil en casos en los que el EMV se calcula difícilmente o directamente no se puede calcular. Ahí hay que usar métodos numéricos de aproximación, y usando el método de los momentos podemos encontrar una primera aproximación que mejore la convergencia de los algoritmos numéricos de búsqueda de raíces.

\subsubsection{Ejemplos}

Si se tiene el modelo \[f(x;\theta) = \frac{1+\theta x}{2}\ind_{[-1,1](x)}\,\theta\in[-1,1] \] no es sencillo calcular el EMV pero sí obtener el estimador por el método de los momentos:

\[ \esp[\theta]{X} = \int_{-1}^1 x f(x;\theta)\,dx = \frac{\theta}{3} \]

Por tanto, la solución de $\avg{X} = \esp[\theta]{X}$ es $\tilde\theta_n = 3\avg{X}$, cuya varianza es 

\[ \var[\theta]{\tilde\theta_n} = \var[\theta]{3\avg{X}} = 9\frac{\sigma^2}{n} = \frac{3-\theta^2}{n} \]

ya que \[ \sigma^2 = \var[\theta]{X}= \esp[\theta]{X^2} - \esp[\theta]{X}^2 = \frac{1}{3}-\frac{\theta^2}{9} \]. Este estimador es consistente ya que, por la LGN, $3\avg{X} \convs 3\esp[\theta]{X}$.

Supongamos un ejemplo más complicado: $X\sim Beta(a,b)$.

\[ f(x;a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1}(1-x)^{b-1} \ind_{[0,1]}(x) \]

y

\begin{align*}
\esp[\theta]{X} &= \int_0^1\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a}(1-x)^{b-1}\,dx = \\
&= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{\Gamma(a+1)}{\Gamma(a+b+1)}\underbrace{\int_0^1\frac{\Gamma(a+b+1)}{\Gamma(a+1)\Gamma(b)} x^{a}(1-x)^{b-1}\,dx}_{=1 \text{ (f. densidad)}} = 
\end{align*}

Sabiendo que $\Gamma(p+1) = p\Gamma(p)$

\[ \frac{\Gamma(a+b)}{\Gamma(a)}\frac{\Gamma(a+1)}{\Gamma(a+b+1)} = 
	\frac{\Gamma(a+b)}{\Gamma(a)}\frac{a\Gamma(a)}{(a+b)\Gamma(a+b)} = \frac{a}{a+b} \]
	
y los estimadores quedan como
	
\begin{gather*}
\hat{a} = \avg{X}\left(\frac{\avg{X}(1-\avg{X})}{s^2}-1\right) \\
\hat{b} = (1-\avg{X})\left(\frac{\avg{X}(1-\avg{X})}{s^2}-1\right) 
\end{gather*}

\subsection{Metodología bayesiana}
\index{Distribución!a priori}

En muchos casos se tiene cierta información a priori, antes de extraer la muestra, sobre la probabilidad de los diferentes valores del parámetro $\theta$. En estos casos se sabe, o se supone, que ciertos intervalos de valores de $\theta$ son \textit{más probables que otros} y se concreta esta información en una \textbf{distribución a priori sobre $\theta$} cuya función de densidad se denota $\pi(\theta)$.

De manera formal, la estadística bayesiana considera que el parámetro es una variable aleatoria y que la información previa se puede expresar a través de la distribución a priori del parámetro.

Entonces, si antes teníamos una v.a. $X\sim f(x;\theta)$, ahora lo que diremos es que $X$ sigue una distribución condicionada por un parámetro: $X\sim f(x|\theta)$.

En este caso, la muestra $X_1,\dotsc,X_n$ contiene información de la muestra y también de nuestro parámetro. Es decir, que podemos considerar la función de distribución de la muestra como \[ \prod_{i=1}^n f(x_i|\theta) \]. Para juntar toda esta información usaremos el Teorema de Bayes:

\begin{theorem}[Teorema\IS de Bayes] Sea $A_1,A_2,\dotsc$ una partición del espacio muestral y sea $B$ un suceso cualquiera. Entonces

\[ \prob{A_i|B} = \frac{\prob{A_i\cap B}}{\prob{B}} = \frac{\prob{B|A_i}\cdot \prob{A_i}}{\sum_j \prob{B|A_j}\cdot \prob{A_j}} \]

Esta formulación se refiere a sucesos probabilísticos. Podemos reformularla con la información a priori del parámetro:

\begin{equation}
\label{eqBayes}
 \pi(\theta | x_1,\dotsc,x_n) = \frac{f(x_1,\dotsc,x_n|\theta)\pi(\theta)}{\displaystyle \int_\Theta f(x_1,\dotsc,x_n|\tau)\pi(\tau)\,d\tau} 
 \end{equation}

donde $\Theta$ es todo el espacio paramétrico. A $ \pi(\theta | x_1,\dotsc,x_n) $ se le denomina \textbf{distribución a posteriori}\index{Distribución!a posteriori}
\end{theorem}

Como $\pi$ es una función de distribución, tenemos que 

\[ \int_\Theta \pi(\theta|x_1,\dotsc,x_n)\,d\theta = 1 \]

para toda posible muestra $(x_1,\dotsc,x_n)$. Estudiaremos entonces la siguiente integral

\[ \int_\Theta \frac{f(x_1,\dotsc,x_n|\theta)\pi(\theta)}{\displaystyle \int_\Theta f(x_1,\dotsc,x_n|\tau)\pi(\tau)\,d\tau} \]

En esta, integral, el término 

\[ \int_\Theta f(x_1,\dotsc,x_n|\tau)\pi(\tau)\,d\tau \] 

es constante. Por lo tanto, lo que nos interesará será el numerador, la integral 

\[ \int_\Theta f(x_1,\dotsc,x_n|\theta)\pi(\theta)\,d\theta \] 

que nos dará la información que necesitamos.

\begin{defn}[Estimador\IS Bayes] Se define, para cada muestra dada $(x_1,\dotsc,x_n)$ como la esperanza de la distribución a posteriori:

\[ T_n(x_1,\dotsc,x_n) = \int_\Theta \theta\pi(\theta|x_1,\dotsc,x_n)\,d\theta \]
\end{defn}


\subsubsection{Ejemplos}

La estadística bayesiana se suele usar para estimar los votantes de un partido político. Por ejemplo, sea $\theta$ la proporción de votantes de un partido $P$, y sea $X$ la v.a. Bernoulli que toma valor 1 cuando un votante elige $P$ y 0 en otro caso. Es decir

\[ \begin{cases}
f(x|\theta) = \theta &\text{si } x=1 \\
f(x|\theta) = 1 - \theta &\text{si } x=0 \\
\end{cases} \]

Entonces tenemos que \[ f(x_1,\dotsc,x_n|\theta) = \prod_{i=1}^n f(x_i|\theta) = \theta^{\sum_{i=1}^n x_i} (1-\theta)^{n -\sum_{i=1}^n x_i} \]

SUponemos que la distribución a priori es una Beta(4,10):

\[ \pi(\theta) = \frac{\Gamma(14)}{\Gamma(4)\Gamma(10)}\theta^3(1-\theta)^9\ind_{[0,1]}(\theta) \]

Así pues, aplicando la fórmula de Bayes (\ref{eqBayes}) nos queda 

\begin{equation}\label{eqE1}
\theta^{\sum x_i}(1-\theta)^{n-\sum x_1} \theta^3 (1-\theta)^9 = \theta^{3+\sum x_i} (1-\theta)^{\sum 9 + \sum x_i} 
\end{equation} y entonces

\[  \pi(\theta | x_1,\dotsc,x_n)  \sim Beta(4 + \sum x_i, 10 + n - \sum x_i) \]

El estimador Bayes es, por lo tanto

\[ T_n = \frac{4 + \sum x_i}{14 + n} = \underbrace{\frac{n}{4 + 10 + n}\avg{x}}_{(A)} + \underbrace{\frac{4+10}{4+10+n}\frac{4}{4+10}}_{(B)} \]

Es decir, pondera las dos información que teníamos: la media de la distribución a priori $(B)$ y la media muestral $(A)$. Si nos fijamos en la expresión, si tenemos un tamaño muestral muy grande ($n\to\infty$) damos mucho más peso a la información de la muestra que a la distribución a priori. Sin embargo, si tenemos menos muestras nuestra distribución a priori influirá más en el resultado.

Con los datos $\sum x_i = 125$ y $n = 1000$, el estimador Bayes toma valor $0.127$, mientras que el e.m.v. valdría $0.125$. Es decir, nuestro estimador bayesiano pondera la información que teníamos previamente y considera que en nuestra distribución a priori era más probable valores más altos.

Curiosamente, en (\ref{eqE1}) hemos pasado de una distribución a priori a una distribución a posteriori fácilmente identificable con una distribución Beta. Esto tiene que ver con el concepto de familias conjugadas.

\subsubsection{Familias conjugadas}

\begin{defn}[Familia\IS conjugada] Sea $\mathcal{F}$ una familia de distribuciones paramétricas $f(\cdot | \theta),\;\theta\in\Theta$; y sea $\Pi$ una familia de distribuciones a priori $\pi(\theta)$ sobre el parámetro $\theta$. 

Diremos que $\Pi$ es la familia de dsitribuciones a priori conjugada de $\mathcal{F}$ si la distribución a posteriori $ \pi(\theta | x_1,\dotsc,x_n) $ también pertence a $\Pi$ para toda muestra $ ( x_1,\dotsc,x_n) $ y para toda a priori de $\Pi$.
\end{defn}

Tenemos varias familias conjugadas identificadas:
\begin{table}[hbtp]
\centering
\begin{tabular}{|c|c|}
\hline
$\mathcal{F}$ & $\Pi$ \\
\hline 
Binomial & Beta \\ 
\hline 
Normal & Normal \\ 
\hline
\end{tabular} 
\caption{Familias conjugadas}
\end{table}

\section{Estimación por intervalos de confianza}
\label{secConfianza}
Al igual que en el tema anterior, vamos a obtener información sobre un parámetro desconocido $θ∈Θ$ a partir de una meustra $X_1,…,X_n$. Habíamos logrado una estimación puntual, pero, ¿por qué va a ser válido sólo ese valor? ¿Podría ser válido un valor cercano al estimador?

Este tema responde a esa pregunta: ofrece un intervalo que típicamente contiene a un estimador puntual, de posibles valores para un parámetro. Veremos cómo construir ese intervalo y la información que ofrecen.

\begin{defn}[Intervalo\IS de confianza] Sea una muestra $X_1,…,X_n$ de una v.a. con una función de distribución $F(.;θ)$, con $θ∈Θ⊂ℝ$ un parámetro desconocido. Sean dos estadísticos $T_n^{(1)}(X_1,…,X_n)$ y $T_n^{(2)}(X_1,…,X_n)$ con $T_n^{(1)} < T_n^{(2)}$ y un valor $α∈(0,1)$. Supongamos que se verifica

\[ \prob[θ]{T_n^{(1)}(\sample ) < θ < T_n^{(2)}(\sample) } = 1-α\; ∀θ\]

Entonces para una realización concreta de la muestra $\sample[x]$ se dice que el intervalo $(T_n^{(1)}(\sample[x]) ,T_{n}^{(2)}(\sample[x]))$ es un intervalo de confianza para $θ$ con nivel de confianza $1-α$ y lo denotaremos como

\[ IC_{1-α}(θ) \]
\end{defn}

Probemos esta definición con una muestra $\sample$ de v.a.i.i.d. $N(μ,σ)$ donde $μ$ es un parámetro desconocido y $σ$ es conocida. Se sabe que 

\[ \avg{X} \sim N\left(μ,\frac{σ}{\sqrt{n}}\right) \]

y, tipificando,

\[ \frac{\avg{X}-μ}{\frac{σ}{\sqrt{n}}} \sim~ N(0,1) \]

Por tanto, si para cualquier $α∈(0,1)$, $z_α$ denota el cuantil $1-α$ en la normal estándar ($Φ(z_α) = 1-α$, siendo $Φ$ la función de distribución de la $N(0,1)$) tenemos

\[ \prob[μ]{-z_{α/2} < \frac{\avg{X}-μ}{\frac{σ}{\sqrt{n}}}  < z_{α/2}} = 1-α \]

y, despejando

\[ \prob[μ]{\avg{X} - z_{α/2}\frac{σ}{\sqrt{n}} < μ < \avg{X} + z_{α/2}\frac{σ}{\sqrt{n}}} = 1-α \]

Y por lo tanto, el intervalo

\[ \left(\avg{x} - z_{α/2}\frac{σ}{\sqrt{n}}, \avg{x} + z_{α/2}\frac{σ}{\sqrt{n}}\right)\]

es un \textbf{intervalo de confianza de nivel $1-α$ para $μ$}.

Intuitivamente y en términos frecuentistas, si por ejemplo $1-α=0.95$ y extraemos muchas muestras de una $N(0,1)$ aproximadamente en el 95\% de los casos el intervalo 

\subsection{Intervalos de confianza asintóticos basados en el TCL}

Si $X$ no es normal, sabemos que si $μ$ y $σ$ son finitas, encontes $\avg{X} \sim N\left(μ,\frac{σ}{\sqrt{n}}\right)$ por el TCL (\ref{thmCentral}). Entonces

\[ 1-\alpha = \simeq \prob{-z_{α/2} ≤ \frac{\avg{X} - μ}{\frac{σ}{\sqrt{n}}}≤z_{α/2}} \]

Es decir, obtenemos un intervalo de confianza aproximado si el tamaño de la muestra es grande.

\paragraph{Aplicación: Intervalo de confianza aproximado para una proporción $p$} Sean $\sample$ i.i.d. Bernoulli($p$). Por el TCL

\[ \frac{\avg{X}-p}{\sqrt{\frac{p(1-p)}{n}}} \sim N(0,1) \]

y reemplazando $p$ por su estimador natural $\hat{p} = \avg{X}$ obtenemos que el intervalo de confianza aproximado para $p$ es 

\[ \left( \avg{x} - z_{α/2} \sqrt{\frac{\avg{x}(1-\avg{x})}{n}}, \avg{x} + z_{α/2} \sqrt{\frac{\avg{x}(1-\avg{x}}{n}} \right) \]

\subsection{Método de la cantidad pivotal}
\index{Cantidad!pivotal}
Una metodología general para obtener un intervalo de confianza para $θ$ consiste en encontrar una función $Q(θ;\sample)$, llamada \textbf{cantidad pivotal} cuya distribución no dependa de $θ$ y sea conocida, al menos de modo aproximado. A partir de esta distribución, fijado un valor $α∈(0,1)$ se obtienen dos valores $q_1(α), q_2(α)$ tales que 

\[ \prob[θ]{q_1(α) < Q(θ;\sample) < q_2(α)} = 1-α \]

Despejando $θ$ se obtiene una expresión del tipo 

\[ \prob[θ]{T_n^{(1)}(\sample) < } \]

\subsection{Construcción de intervalos de confianza habituales}

\subsubsection{Distribución $χ^2$}
\index{Distribución!$χ^2$}
Estamos interesados en obtener intervalos de confianza exactos, válidos para cualquier $n$, para $σ^2$ en una normal. Para ello presentaremos una distribución auxiliar que tiene una especial importancia en estadística, la \textbf{distribución $χ_k^2$}, que en realidad es la distribución $Γ(\frac{1}{2}, \frac{k}{2})$. 
Esta distribución surge del estudio de la distribución de las formas cuadráticas $X'AX$. En particular, si $\{Z_n\}$ son vectores aleatorios, entonces
\[ \sum Z_k^2 \sim χ^2 \]

De hecho, aplicando esto a una suma de varias v.a. $\sample$ $S^2$, nos queda que

\[ \frac{(n-1)S^2}{σ^2} \sim χ^2_{n-1} \]

Este resultado proporciona directamente una cantidad pivotal y, en consecuencia, un intervalo de confianza de nivel $1-α$ para $σ^2$:

\[ \left(
	 \frac{(n-1)s^2}{χ^2_{n-1;α/2}},\; \frac{(n-1)s^2}{χ^2_{n-1;1-α/2}}
\right) \]

\subsubsection{Distribución $t$ de Student}
\index{Distribución!$t$ de Student}
Sea $Z\sim N(0,1)$ y $W\sim χ_k^2$. Supongamos que $Z$ y $W$ son independientes. Entonces la distribución de la v.a.

\[ T = \frac{Z}{\sqrt{W/k}} \]

se denomina distribución $t$ de Student con $k$ grados de libertad. Su forma se aproxima a una normal $N(0,1)$.

\todo{Lema de Fisher-Cochran}

\subsection{Intervalos de confianza bayesianos}

En un problema de inferencia con un enfoque bayesiano, el elemento fundamental para realizar la inferencia es la distribución a posteriori $π(θ|\sample[x])$. A partir de esa distribución se define una \textbf{región creíble}\index{Región!creíble} de nivel $1-α$ como un subconjunto $A⊆Θ$ tal que 

\[ \int_A π(θ|\sample[x])\,dθ=1-α \]

\chapter{Contraste de hipótesis}

\section{Conceptos básicos}

El objetivo de la teoría de contraste de hipótesis es \textbf{elegir entre dos posibilidades excluyentes}, las hipótesis nula e alternativa, relativas al valor de un parámetro poblacional a partir de la información proporcionada por los datos muestrales.

Sea $\sample$ una muestra aleatoria de una v.a. $X$ con función de distribución $F_θ$ donde $θ∈Θ$. Dada una partición del espacio paramétrico $Θ=Θ_0 \cup Θ_1$, deseamos decidir, en base a la muestra obtenida, si $θ$ está en $Θ_0$ o en $Θ_1$. En el primer caso se cumple la hipótesis nula, en el segundo la alternativa. Ambas hipótesis son excluyentes.

Para resolver el problema definiremos una región de rechazo. Esta región $R⊆ℝ^n$ nos permitirá valorar si el parámetro está en $Θ_0$ o en $Θ_0$ en base a la muestra obtenida. De esta forma, si $(\sample[x])∈R$, se rechaza la hipótesis nula.

El paso más importante del contraste de hipótesis es construir la región de rechazo $R$, y a partir de entonces los pasos son muy mecánicos. En el apéndice \ref{secRegRechazo}, página \pageref{secRegRechazo}, tenemos varias muestras de regiones de rechazo.

En el test de hipótesis podemos cometer dos tipos de fallos:

\begin{itemize}
\item \textbf{Error de tipo I} Rechazar $H_0$ cuando $H_0$ es cierta.
\item \textbf{Error de tipo II} Aceptar $H_0$ cuando $H_0$ es falsa.
\end{itemize}

Para medir la probabilidad de cometer uno de esos fallos definimos la función de potencia

\begin{defn}[Función\IS de potencia] La función de potencia de un test con región de rechazo $R$ para contrastar $H_0: θ∈Θ_0$ frente a $H_1:θ∈Θ_1$ es la función

\begin{align*}
\appl{β_n}{Θ&}{[0,1]} \\
θ&\longmapsto β_n(θ) = \prob[θ]	{(\sample) ∈ R}
\end{align*}

y nos da la probabilidad de rechazar la hipótesis $Θ_0$.\label{defFuncPotencia}
\end{defn}


\subsection{Teoría de Neyman-Pearson}
\label{secNeymanPearson}
Nos gustaría que $β_n(Θ_0) = 0$ y que $β_n(Θ_1) =1$, pero normalmente no pasará esto, sino que $β_n$ será una función continua y suave del parámetro. 

La teoría de Neyman-Pearson trata de responder a este problema con los dos siguientes pasos:

\paragraph{Acotar la máxima probabilidad de error de tipo I}

\begin{itemize}
\item Se fija un \textbf{nivel de significación}\index{Nivel!de significación} $α∈(0,1)$. Típicamente se toma $α=0.05$.
\item Se define el \textbf{tamaño de un test}\index{Tamaño!de un test} como la máxima probabilidad de error de tipo I, o como

\[ \max_{θ∈Θ_0} \prob[θ]{R} = \max_{θ∈Θ} β_n(θ) \]

\item Se busca una región de rechazo $R$ tal que \[ \max_{θ∈Θ_0} \prob[θ]{R} ≤ α \]
\end{itemize}

Tal y como hemos definido $α$, se puede considerar que el nivel de significación nos indica la probabilidad de cometer un error de tipo I, es decir, de rechazar $H_0$ cuando es cierta. Por lo tanto, cuanto menor es el nivel de significación más \textit{seguros} estamos de que no estamos rechazando $H_0$ por error.

\paragraph{Minimizar la probabilidad de error de tipo II}

Se intenta buscar una región de rechazo $R$ que maximice la función de potencia cuando $θ∈Θ_1$.

Aquí podemos ver por qué las dos hipótesis no son simétricas. Los tests de hipótesis están diseñados para controlar la probabilidad máxima de rechazar $H_0$ cuando es cierta. En consecuencia, suelen ser conservadores con la hipótesis nula: hace falta mucha evidencia muestral para rechazar $H_0$. Observemos que es posible que, con los mismos datos, $H_0$ se rechace para un nivel de significación $α=0.05$ y se acepte para $α=0.01$. 

Además de la asimetría, tenemos que pensar que al aceptar $H_0$ no significa que la hayamos demostrado, sino simplemente que no se ha encontrado suficiente evidencia empírica a nivel prefijado $α$ en contra de $H_0$. \textbf{No es una demostración matemática}.

\section{Problema de una muestra}

En una primera aproximación, los problemas de contraste de hipótesis  pueden clasificarse en problemas de una muestra o de dos, según haya sólo una población de interés o queramos comparar dos poblaciones y dispongamos de una muestra de cada una de ellas. Presentaremos las ideas básicas en el caso de los problemas de una muestra pero pueden extenderse de modo análogo a los de dos muestras. 

\paragraph{Dualidad con los intervalos de confianza}

En algunos casos de hipótesis nula simple, aparece una dualidad entre el contraste de hipótesis y los intervalos de confianza (\ref{secConfianza}). Si tenemos $H_0:\; μ=μ_0$, entonces aceptar $H_0$ significa que $μ ∈ IC_{1-α}(θ)$, es decir, que está en el intervalo de confianza. La región de rechazo sería entonces

\[ R=\{ (\sample[x])\tq θ(\sample[x]) \notin IC_{1-α}(θ) \} \]

\begin{defn}[p-valor del contraste] Se define el p-valor del contraste como el ínfimo de los niveles de significación $α$ para los que rechazo $H_0$.

De esta forma, si $α$ es menor que el p-valor, aceptaremos $H_0$ y si es mayor, la rechazaremos.
\end{defn}

¿Qué información nos va a dar el p-valor? Supongamos que tenemos, por ejemplo, un p-valor pequeño ($<0.01$). Con este valor rechazaríamos la hipótesis nula para los valores más habituales de niveles de significación ($0.01, 0.05, 0.1$). Por lo tanto, en este caso lo razonable sería rechazar $H_0$.

Por otra parte, supongamos que tenemos un p-valor grande ($>0.1$). En este caso, aceptaríamos la hipótesis nula para los valores más habituales de $α$, y entonces lo más razonable sería aceptar $H_0$.

De forma general, el p-valor de contraste nos dice la probabilidad de observar la muestra que hemos obtenido suponiendo que $H_0$ es cierta. Si es muy bajo, nos indicará que es muy poco probable que la muestra obtenida haya salido así por pura casualidad.

\subsection{Regiones de rechazo para contrastes habituales} 
\subsubsection{Contraste de la media de una distribución}

En todo caso se rechaza $H_0$ cuando $(\sample) ∈ R$. Para hallar las regiones de rechazo buscaremos los \textbf{estadísticos de contraste}\index{Estadístico!de contraste}, medidas de lo razonable que es la hipótesis nula y que depende de la muestra obtenida. Cuando la hipótesis nula sea cierta, el estadístico del contraste estará en zonas de alta probabilidad.

\paragraph{Distribución normal con varianza conocida}

Primero construiremos el estadístico del contraste $Z$, que depende de la media muestral obtenida.

\[ Z = \frac{\avg{X}-μ_0}{σ/\sqrt{n}} \]

Si $H_0:\;μ=μ_0$ es cierta entonces $Z\sim N(0,1)$. Entonces las regiones de rechazo son 

\begin{table}[hbtp]
\centering
\begin{tabular}{|c|c|}
\hline  $H_0$ & $R$  \\ 
\hline  $μ=μ_0$ & $\{ (\sample[x]) \tq \abs{Z} ≥ z_{\frac{α}{2}}\}$ \\ 
\hline  $μ≤μ_0$ & $\{ (\sample[x]) \tq Z ≥ z_{\frac{α}{2}}\}$ \\ 
\hline  $μ≥μ_0$ & $\{  (\sample[x]) \tq Z ≤ z_{\frac{α}{2}}\}$ \\ 
\hline 
\end{tabular}
\caption{Regiones de rechazo para una normal $N(μ,σ)$.} 
\end{table}

\paragraph{Distribución normal con varianza desconocida} 

Sea $\sample$ una muestra aleatoria de $X\sim N(μ,σ)$ con $σ$ desconocido. Entonces el estadístico del contraste sigue una distribución $T$ de Student de $n-1$ grados de libertad:

\[ T=\frac{\avg{X} - μ_0}{s/\sqrt{n}} \]

\begin{table}[hbtp]
\centering
\begin{tabular}{|c|c|}
\hline  $H_0$ & $R$  \\ 
\hline  $μ=μ_0$ & $\{ (\sample[x]) \tq \abs{T} ≥ t_{\frac{α}{2}}\}$ \\ 
\hline  $μ≤μ_0$ & $\{ (\sample[x]) \tq T ≥ t_{\frac{α}{2}}\}$ \\ 
\hline  $μ≥μ_0$ & $\{  (\sample[x]) \tq T ≤ t_{\frac{α}{2}}\}$ \\ 
\hline 
\end{tabular} 
\caption{Regiones de rechazo para una normal $N(μ,σ)$ con $σ$ desconocida.}
\end{table}

\paragraph{Tests de nivel aproximado $α$ (muestras grandes) para la media de cualquier distribución}

 Sea $\sample$ una muestra aleatoria de $X$ con $\esp{X} = μ < ∞$. Entonces el estadístico del contraste es 
 
 \[ Z= \frac{\avg{X}-μ_0}{S/\sqrt{n}}\stackrel{TCL}{\sim} N(0,1) \]
 
 si $H_0:\, μ=μ_0$ es cierta. Por lo tanto, nos quedamos con las siguientes regiones:

\begin{table}[hbtp]
\centering
\begin{tabular}{|c|c|}
\hline  $H_0$ & $R$  \\ 
\hline  $μ=μ_0$ & $\{ (\sample[x]) \tq \abs{Z} ≥ z_{\frac{α}{2}}\}$ \\ 
\hline  $μ≤μ_0$ & $\{ (\sample[x]) \tq Z ≥ z_{\frac{α}{2}}\}$ \\ 
\hline  $μ≥μ_0$ & $\{  (\sample[x]) \tq Z ≤ z_{\frac{α}{2}}\}$ \\ 
\hline 
\end{tabular} 
\caption{Regiones de rechazo para la media de cualquier distribución}
\end{table}

\section{Contrastes para dos muestras}

Supongamos que tenemos 2 muestras $X_1,...,X_N$ y $Y_1,...,Y_N$. Siendo $\mu_1$ la esperanza de $X$ y $\mu_2$ la esperanza de $Y$.

Podemos plantear hipótesis del tipo
\begin{itemize}
\item $H_0: \,\,\, \mu_1=\mu_2$
\item $H_0: \,\,\, \mu_1\leq\mu_2$
\item $H_0: \,\,\, \sigma_1 = \sigma_2$
\end{itemize}
Este útlimo caso (si las varianzas son iguales) suele ser un requisito previo antes de plantearte contrastes como el segundo ejemplo.


Uno de los test más usuales es el de igualdad de medias para dos poblaciones \textbf{homocedásticas} \index{Muestra!homocedástica}, es decir, con $\sigma_1=\sigma_2$.

Si \[\left.\begin{array}{cc}
X\sim N(\mu_1,\sigma)\\
Y\sim N(\mu_2,\sigma)
\end{array}\right\}\text{ Independientes } \begin{array}{cc}
\gor{X} -\mu_1 \sim N\left(0,\frac{\sigma}{n_1}\right)\\
\gor{Y} - \mu_2 \sim N\left(0,\frac{\sigma}{n_2}\right)
\end{array}\]
Entonces:
\[\frac{(\gor{X}-\mu_1) - (\gor{Y} - \mu_2)}{\sigma\sqrt{\frac{1}{n_2} + \frac{1}{n_2}}}\sim N(0,1)\]

Todo esto suponiendo que $\sigma_1=\sigma_2$, desconociendo su valor real. Nos gustaría por tanto, tener en el estadístico un estimador de $\sigma$ .

Con este razonamiento sabemos podemos deducir que la región de rechazo es:


\paragraph{Contraste de igualdad de medias}
Si \[\left.\begin{array}{cc}
X\sim N(\mu_1,\sigma)\\
Y\sim N(\mu_2,\sigma)
\end{array}\right\}\text{ Independientes } \begin{array}{c}
X_1,...,X_{n_1}\to \frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2_{(n_1-1)}\\
Y_1,...,Y_{n_1}\to \frac{(n_1-1)S_2^2}{\sigma_2^2} \sim \chi^2_{(n_2-1)}\\
\end{array}\]

Para seguir con el contraste de igualdad de medias necesitamos definir la distribución \textbf{Fisher-Snedecor} con $n_1\,y\,n_2$ grados de libertad. \index{Distribuci\'on! $F$ de Fisher}. La distribución se parece mucho a la $\chi^2$, y su función de distribución se obtiene así:

\[Q_1 \sim \chi_{n_1}^2 \,;\, Q_2 \sim \chi_{n_2}^2\]
\[F \sim\displaystyle \frac{\displaystyle Q_1/n_1}{\displaystyle Q_2/n_2}\]

Volviendo al caso donde estábamos podemos definir un estadístico de esta manera:

\[\frac{\frac{(n_1-1)S_1^2}{\sigma_1^2(n_1-1)}}{\frac{(n_2-1)S_2^2}{\sigma_2^2 (n_2-1)}} \sim F_{n_1-1,n_2-1}\]
Sigue una F de Fisher.

Simplificando y suponiendo cierta la hipótesis de homocedacidad ($\sigma_1 = \sigma_2$) tenemos que $F = \displaystyle \frac{S_1^2}{S_2^2} \sim F_{n_1-1,n_2-1}$. 

Este es el estadístico del contrasete para comparar varianzas de dos poblaciones normales. Si el valor nos queda en las colas de la distribución, rechazaremos la hipótesis de igualdad de varianzas.

Con este razonamiento podemos construir la región de rechazo es: 
(completar del formulario)
\[R = ...\]

\begin{example}
Sean $X,Y$ poblaciones de datos emparejados  tal que $\esp{X} = \mu_1$ y $\esp{Y} = \mu_2$.


¿Qué significa datos emparejados? Muestras tomadas ambas a los mismo individuos de la mezcla después de una medicina por ejemplo, siendo $X$ la medida antes e $Y$ después. Esto quiere decir que $X,Y$ no son independientes.

El \textbf{procedimiento estándar} para este tipo de casos es suponer que $D =X-Y \sim N(\mu_d,\sigma)$

Y ahora expresamos nuestra hipótesis en función de $D$, de la que sabemos que $\esp{D} = \mu_d = \mu_1-\mu_2$.

\begin{itemize}
\item Si $H_0: \mu_1=\mu_2 \dimplies H_0: \mu_d=0 $. La región de rechazo de esta hipótesis será \[R = \left\{\frac{\abs{\gor{d}}}{S_d / \sqrt{n}} > t_{n-1;\frac{\alpha}{2}}\right\}\]
\item Si $H_0: \mu_1\leq\mu_2 \dimplies H_0: \mu_d\leq0 $
\item Si $H_0: \mu_1\geq\mu_2 \dimplies H_0: \mu_d\geq0 $
\end{itemize}
\end{example}
En el apéndice encontramos un ejercicio realizado en $\mathcal{R}$ ¿¿¿¿?????

\section{Consistencia de tests. Tests insesgados y UMP}

\begin{defn}[Sucesión\IS consistente] Se dice que una sucesión de tests con un nivel prefijado $α$ es consistente cuando

\[ \lim_{n\to ∞} β_n(θ) = 1\,∀θ∈Θ_1= Θ\setminus Θ_0 \]

Es decir, que la probabilidad de rechazar la hipótesis nula cuando es falsa, dada por la función de potencia (\ref{defFuncPotencia}), tienda a uno con muestras suficientemente grandes.
\end{defn}


\begin{defn}[Test\IS insesgado] Se dice que un test es insesgado cuando

\[ β_n(θ)≤α\;∀θ∈Θ_0 \]

, es decir, cuando cumple la teoría de Neyman-Pearson (ver sección \ref{secNeymanPearson}); y además \[ β_n(θ)≥α \;∀θ∈Θ_1 \]

\end{defn}

\begin{defn}[Test\IS UMP] Se dice que un test es uniformemente más potente (UMP) dentro de una clase $\mathcal{B}_{n,α}$ de tests de nivel $α$ basados en muestras de tamaño $n$ cuando

\[ β_n(θ) ≥ \tilde{β}_n(θ),\;∀θ∈Θ_1 \]

siendo $\tilde{β}_n$ la función de potencia de cualquier otro test de la clase  $\mathcal{B}_{n,α}$.
\end{defn}

\subsection{Lema de Neyman-Pearson}

Recordemos la función de verosimilitud, que medía lo verosímil que es el valor del parámetro $θ$ a la vista de la muestra. Para comparar dos hipótesis simples $H_i:\,θ=θ_i$, calcularíamos la función de verosimilitud para esos dos valores y veríamos cuál es más probable. Extendiendo esta idea llegamos al lema de Neyman-Pearson

\begin{theorem}[Lema\IS de Neyman-Pearson]\label{thmNeymanPearson}
Se considera el problema de hipótesis simple y alternativa simple, es decir, que $H_0:\,θ=θ_0$ frente a $H_1:\,θ=θ_1$. Denotemos \[ f_n(\sample[x];θ) = \prod_{i=1}^n f(x_i;θ= \]

Dado $α∈(0,1)$, supongamos que la región de rechazo

\[ R^\ast = \left\{ (\sample[x]\tq \frac{f_n(\sample[x];θ_1)}{f_n(\sample[x];θ_0} > k \right\} \]

verifica $P_{\theta_0}(R^{\ast}) = \alpha$
Completar esto.

\end{theorem}

\begin{proof} Denotamos $\vx = (\sample[x])$ para cortar.

Tenemos que probar que $\prob[\theta_1]{R^\ast} - \prob[θ_1]{R}$ es mayor o igual que cero.

\[ \prob[θ_1](R^\ast) - \prob[θ_1]{R} = \int_{R^\ast\cap R^c} f_n(\vx;θ_1)d\vx - \int_{R^{\ast c}\cap R} f_n(\vx;θ_1)d\vx \]

Por definición de $R^\ast$

\[ \int_{R^\ast\cap R^c} f_n(\vx;θ_1)d\vx ≥ k \int_{R^\ast\cap R^c} f_n(\vx;θ_0)d\vx \]

y también 

\[ \int_{R^{\ast c}\cap R} f_n(\vx;θ_1)d\vx ≥ k \int_{R^{\ast c}\cap R} f_n(\vx;θ_0)d\vx \]

Por lo tanto, 

\[ \prob[θ_1](R^\ast) - \prob[θ_1]{R} ≥ k\left[\int_{R^\ast\cap R^c} f_n(\vx;θ_0)d\vx - \int_{R^{\ast c}\cap R} f_n(\vx;θ_0)d\vx \right] \]

Y aquí es donde me he perdido.

\end{proof}

\subsection{Familias paramétricas con cociente de verosimilitudes monótono y tests óptimos}

En la subsección anterior hemos construido tests óptimos en problemas de hipótesis simple y alternativa simple. Pasaremos ahora a definirlos en modelos nilaasjdajfklsaghdkfas


\begin{defn}[Familia\IS paramétrica CVM] Se dice que $f(\cdot|θ)$ es una familia paramétrica con cociente de verosimilitudes monótono (CVM) si existe un estadístico $T_n(\sample[x])$ tal que, para todo $θ_1,θ_2$ con $θ_1<θ_2$ la razón de verosimilitudes 

\[ \frac{f_n(\sample[x];θ_2)}{f_n(\sample[x];θ_1} \]

es una función monótona no decreciente de $T_n(\sample[x])$.
\label{defFamCVM}
\end{defn}

Podemos ver algunos ejemplos de este tipo de familias.

\paragraph{Distribución exponencial}

Tomemos $X\sim \text{exp}(θ)$ con $θ>0$ y $f(x;θ) = θe^{-θx}$ para $x>0$. El cociente de las dos funciones es

\[ \frac{θ_2^ne^{-θ_2\sum x_i}}{θ_1^ne^{-θ_1\sum x_i}} = \left(\frac{θ_2}{θ_1}\right)^ne^{(θ_1-θ_2)\sum x_i } \]

con $θ_1-θ_2 < 0$. Entonces, si consideramos

\[ T_n(\sample[x]) = -\sum x_i \] 

pasa algo con nuestras vidas y la estadística en general que no he llegado a copiar.

\begin{theorem}\label{thmNeymanPearson2}
Supongamos que $F(\cdot;\theta)$ cumpla la propiedad CVM (cociente de verosimilitudes monótono) y que $k_{\alpha}$ es tal que:
\[P_{\theta_0} \{t_n > k_{\alpha} = \alpha\}\]
Además suponemos que $P_{\theta_0} \{T_n = c\} = 0, \forall \theta,c$. 

\textbf{Entonces:} $R=\{(x,q,...,x_n): T_n(x_1,...,x_n) > k_{\alpha}\}$ es la región crítica de un test óptimo (uniformemente más potente) de nivel $\alpha$ para contrastar $H_0: \theta \leq \theta_0\,;\, H_1: \theta > \theta_0$.
\end{theorem}

Vamos a ver otro ejemplo:

\begin{example}
Ya hemos visto que la exponencial tiene CVM (cociente verosimilitudes monótono).

Por el teorema tenemos que el \textbf{test óptimo} de nivel $\alpha$ para $H_0: \theta \leq \theta_0\,;\, H_1: \theta > \theta_0$.

Podemos construir la región de rechazo 
\[R = \{(x_1,...,x_n): \frac{1}{\sum_{i=1}^n x_i} > k_{\alpha}\}\] donde \[P_{\theta_0} \left\{\sum_{i=1}^n X_i < \frac{1}{k_{\alpha}}\right\} = \alpha\]
\end{example}

\begin{example}
Sea $f(\cdot;\theta)$ una uniforme en $(0,\theta)$.
Se deja como ejercicio para el lector la comprobación de que la propiedad de CVM y la obtención del estadístico (que es el máximo de la muestra)
\end{example}

\subsection{Construcción de tests. Test de cociente de verosimilitudes}

\begin{defn}[Estadístico\IS del contraste de razón de verosimilitudes]

Sea $f(\ast;θ)$ donde $θ =(\sample[θ])∈Θ⊆ℝ^k$, siendo $Θ$ un \textit{intervalo} $ℝ^k$. Dada una muestra $x=(\sample[x])$, sea \[ f_n(x;θ) =\prod_{i=1}^{n} f(x_1;θ) \]

Consideremos el problema de contrastar $H_0: \,θ_i=c_i$ para $i=1,\dotsc, r ≤ k$ y $H_1:\, θ_1≠c_i$ para algún $i=1,\dotsc, r$. 

El estadístico del contraste de razón de verosimilitudes es

\[ Λ_n = \frac{\sup_{θ∈Θ_0}f_n(x;θ)}{\sup_{θ∈Θ}f_n(x;θ)} = \frac{\sup_{θ∈Θ_0}f_n(x;θ)}{f_n(x;\hat{θ}} \]

donde $\hat{θ}$ es el e.m.v (\ref{defEMV}) de $θ$, y 
\end{defn}

Si $H_0$ es cierta y el verdadero valor de $θ$ están en $Θ_0$ entonces $A_n\eqsim1$, porque $\hat{θ}_n$ tiende al verdadero valor del parámetro. Si $H_0$ es falsa, el e.m.v. de $θ$ tiende a un valor fuera de $Θ_0$. Entonces $Λ_n$ tomará un valor significativamente menor que 1.

De esta forma, podemos construir una región de rechazo

\[ R = \{(\sample[x]\tq Λ_n(\sample[x]) < k_α \} \]

Hallar $k_α$ según la probabilidad de error que queramos es algo complejo. Por eso nos apoyamos en el siguiente teorema:

\begin{theorem}
Supongamos que 
\begin{enumerate}
\item El e.m.v. $\hat{θ}_n$ es estimador consistente en probabilidad del parámetro $θ$.
\item Para todo $x$, la función $\log f(x;θ)$ tiene derivadas parciales terceras respecto a los componentes de $θ$ continueas.
\item En las integrales que involucran a la función $f(x;θ)$ se pueden permutar las derivadas con el signo integral.
\item La matriz de información de Fisher \[ \mathcal{I}(θ) = \left(\frac{∂^2}{∂θ_i∂θ_j} \log f(X;θ)\right)_{1≤i,j≤k} \] es invertible para cada $θ$.
\end{enumerate}

Entonces, bajo $H_0$, 

\[ -2\log Λ_n\convdist χ^2 \]
\end{theorem}

\subsubsection{Aplicación a tests de bondad de ajuste} Sea $X$ una v.a. discreta que toma los valores $a_1,\dotsc a_k$. Denotemos $p_i = \prob{X=a_i}$. Supongamos que se desea contrastar

\[ H_0:\, p_i=p_{i0}\; i=1,\dots,k \]

basado en una muestra $\sample[x]$. Obsérvese que, en este caso, con la notación del teorema, $r=k-1$ porque cuando se fijan $k-1$ probabilidades $p_i$ queda fijada la probabilidad restante. Por tanto, se rechaza al nivel $α$ cuando

\[ - 2 \log Λ_n > χ_{k-1;α}^2 \]

Consideramos $f(\sample[x];\sample[p][k])$ como la probabilidad de haber observado la muestra $\sample[x]$ con los valores de los parámetros $\sample[p][k]$. Entonces el numerador de $Λ_n$ es 

\[ \frac{n!}{O_1!\dotsb O_k!}p_{10}^{O_1}\dotsb p_{k0}^{O_k} \]

siendo $ O_j=\card{i\tq x_i = a_j}$ las \textit{frecuencias observadas} de los distintos valores de la variable. Nótese que, bajo $H_0$, $(\sample[O][k])$ tiene distribución multinomial $\mathcal{M}(n:p_{10},\dotsc , p_{k0})$. 

En el denominador tenemos que poner los e.m.v. de cada $p$, de la siguiente forma

\[ \hat{p}_k = \frac{o_k}{n} \]

y por lo tanto el denominador queda

\[ \frac{n!}{O_1!\dotsb O_k!} \left(\frac{O_1}{n}\right)^{O_1} \dotsb \left(\frac{O_k}{n}\right)^{O_k} \]

Sustituyendo en $Λ_n$ es inmediato ver que que el estadístico de contraste se puede expresar en la forma 

\[ - 2 \log Λ_n \]


\begin{example}[Experimento de Mendel] Un ejemplo clásico de este tipo de ajuste se puede ver en el experimento de Mendel, en el que se cruzaron plantas de guisantes con fenotipo rugoso-amarillo con otras de fenotipo liso-verde. En la segunda generación se podían observar cuatro fenotipos cuyas respectivas probabilidades, según la teoría de la herencia mendeliana, debían ser 

\[ p_{10} = \frac{9}{16},\,p_{20}=\frac{3}{16},\,p_{30}=\frac{3}{16},\,p_{40}=\frac{1}{16} \]

Observados $n=556$ guisantes en la segunda generación del experimento se obtuvieron los siguientes números con los fenotipos.


\[ 0_1=315,\,O_2=101,\,O_3= \]

¿Proporcionan estos resultados alguna evidencia en contra de la teoría mendeliana?

Aplicamos el test para contrastar $H_0$ tomada como los valores esperados. 

Copia página 40 diapositivas y artículo original de Fisher de la página.

\end{example}

\appendix
\chapter{Anexos}
\section{Condiciones suficientes para permutar la derivada con la integral}

\label{secConds}
Sea una función $p(x,\theta)$ con $x\in \real$ y $\theta \in \mathbb{T}$ donde $\mathbb{T}$ es un intervalo abierto de los reales. Supongamos que
\begin{enumerate}
\item $p(x,\theta)$ es integrable con respecto a $x$ para cada $\theta$ (se cumple automáticamente si $p$ es función de densidad.
\item Para casi todo punto\footnote{Para todo $x$ salvo los que tienen probabilidad 0} existe $\dfrac{∂}{∂\theta} p(x,\theta)\;\forall\theta$.
\item Existe una función integrable $\appl{g}{\real}{\real}$ tal que \[ \abs{\dfrac{∂}{∂\theta} p(x,\theta)}≤g(x)\;\forall\theta \]
\end{enumerate}

Entonces para todo $θ$ 
\[ \dpa{}{\theta}\int_\real p(x,\theta)\,dx=\int_\real \dpa{}{\theta} p(x,\theta)\,dx \]

\section{Distribuciones notables}
\label{secDistr}
\includepdf[pages={2-last}, nup=1x3]{_Distribuciones.pdf}

\section{Regiones de rechazo}
\label{secRegRechazo}
\includepdf[pages={3-4}]{_formulario.pdf}

\chapter{Ejercicios}
\input{EI_Ejercicios.tex}
\newpage
\includepdf[pages={1-3}]{_T5DatosMercurio.pdf}
\newpage

\chapter{Exámenes}
\section{Enero 2013}
\includepdf[pages=-]{_ExamenEnero.pdf}
\includepdf[pages=-]{_ExamenEnero_Sol.pdf}

\section{Junio 2013}

\includepdf[pages=-]{_ExamenJunio.pdf}
\includepdf[pages=-]{_ExamenJunio_Sol.pdf}
\printindex
\end{document}

