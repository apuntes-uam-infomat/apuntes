% -*- root: ../VariableReal.tex -*-
Habitualmente, uno llamaría a esto resumen pero así mola más. La idea es dar un esquema de cada una de las partes del curso, motivando y enlazando a los teoremas ya hechos.

\section{Teoría básica de la medida y la integración}

A estas alturas, ya deberíamos conocer perfectamente la teoría básica. En cualquier caso, sólo hace falta recordar que una medida es una función que nos asigna un número a los conjuntos medibles, que forman lo que se llama una σ-álgebra (como la \nlref{def:SAlgebraBorel}). Estudiamos las funciones medibles (\fref{def:FuncMedible}) y vemos cómo integrarlas en la \fref{sec:Integracion} (lo que nos lleva, obviamente, a la definición de una \nlref{def:FuncIntegrable}).

Una vez hecho eso, podemos saltar a algo más avanzado con el \nref{thm:FubiniTonelli}, que nos permitirá integrar en varias variables con la integral de Lebesgue; o ver la \nlref{sec:IntegPolares}. Este último tema nos da una cosa interesante, que es la posibilidad de medir la bola de cualquier dimensión con el \fref{crl:MedidaBn}.

Además, vemos dos teoremas de ``casi resultados'', el \nref{thm:Egorov} y el \nref{thm:Lusin}, que nos dan resultados para convergencia casi uniforme a partir de convergencia en casi todo punto y la casi continuidad de funciones medibles.

Después, vemos resultados de convergencia y cambio de límite con la integral en la \fref{sec:Convergencia}. Los principales son los siguientes:

\begin{itemize}
\item \nref{thm:ConvMonotona}: Con una sucesión monótona de funciones podemos intercambiar el límite y la integral.
\item \nref{thm:Fatou}: Con una sucesión de funciones no negativas, la integral del $\liminf$ es menor o igual que el $\liminf$ de la integral. La versión análoga es el \nref{thm:FatouInverso}, que funciona con el $\limsup$ y simplemente cambia el sentido de la desigualdad.
\item \nref{thm:ConvDominada}: Dada una sucesión convergente CTP dominada por una función integrable positiva, entonces podemos intercambiar el límite y la integral.
\end{itemize}

\section{Complementos de teoría de la integral y la medida}

Una vez repasado el curso de TIM, vamos con las cosas más avanzadas. La primera es ver el término que falta en el \nref{thm:Fatou}, que nos lo da el \nref{thm:LiebLoss}. Con una sucesión de funciones medibles convergentes a $f$ CTP, integrables de orden $p$ todas ellas y acotadas, se cumple que \[ \lim_{n\to ∞ } \int_X\abs{\abs{f_n}^p - \abs{f}^p - \abs{f_n - f}^p} \dif μ = 0\]

La demostración de ese teorema se hace con dos desigualdades que vendrán muy bien luego: la \nlref{prop:Young} y la \fref{prop:DesigualdadLiebLoss}, que básicamente nos dicen estas dos cosas, con $p,ε > 0$:
\begin{gather*}
 (a+b)^p ≤ C_p (a^p + b^p) \quad a,b > 0\\
\abs{\abs{a+b}^p - \abs{b}^p} ≤ ε\abs{b}^p + C_{ε,p} \abs{a}^p \quad a,b ∈ ℂ
\end{gather*}

Una vez hecho esto, la \fref{sec:ModosConvergencia} explica varios modos de convergencia de funciones, que con el nombre ya se sabe de qué van. Son, en orden, la \nlref{def:ConvLp}, \nlref{def:ConvUniforme}, \nlref{def:ConvEssUnif}, \nlref{def:ConvCasiUnif}, \nlref{def:ConvPuntual}, \nlref{def:ConvCTP} y la \nlref{def:ConvMedida}. Es interesante ver las funciones \eqref{eq:Conv:CE1} a \eqref{eq:Conv:CE4} para ejemplos y contraejemplos patológicos.

Esta sección nos sirve como excusa para demostrar la \nlref{prop:DesigualdadMarkov}, que nos dice que el conjunto donde una función tiene un valor muy grande es muy pequeño:
\[ μ\left(\set{x∈X \tq \abs{f} > ε }\right) ≤ \frac{1}{ε} \int_X \abs{f} \dif μ \]

\subsection{Medidas con signo}

Para trabajos más avanzados con medidas, es necesario poder manejar fácilmente una clase más general de medidas que no nos restrinjan demasiado: \nref{def:MedidaSigno} y las medidas asociadas a funciones medibles $m_f$: \[ m_f(E) = \int_E f \dif m \]

La restricción que pondremos será trabajar con \nlref{def:MedidaSFinita}, esto es, que tengamos un recubriiento del espacio medible con conjuntos de medida finita.

Las medidas asociadas a funciones nos dan un ``inverso'', que es la \nlref{def:MedidaDerivable}: $μ$ es derivable con respecto a $m$ si y sólo si $\dif μ = f \dif m$. Esta $f$ será la llamada Derivada de Radon-Nikodin. La \fref{prop:DerivadaClasicaMedida} nos da una forma de derivar en sentido clásico estas medidas.

Una vez definidas bien las medidas con signo, podemos ver dos teoremas de descomposición: el \nref{thm:DescompHahn} y el \nref{thm:DescompJordan}, que combinados nos dicen que hay una partición $X_+ ∪ X_-$ del espacio de medida y dos medidas $μ^+, μ^-$ con soporte en $X_+$ y $X_-$ respectivamente. En otras palabras, nos aseguran que las medidas con signo se pueden descomponer en una parte positiva y otra negativa.

Pero además hay otro teorema de descomposición, el \nref{thm:LebesgueRadonNikodin}, que nos da una descomposición $μ = m_f + μ_s$ con $m_f$ una medida asociada a una función integrable, y $μ_s$ una \nlref{def:MedidaSingular} a $m$ (esto es, cuyos soportes no intersecan). El \nref{thm:DescLebesgue} nos da un resultado parecido $μ = μ_{AC} + μ_S$ con $μ_AC$ \nlref{def:MedidaAbsCont} y $μ_S$ medida singular, ambas con respecto a la medida general $m$.

Además, si trabajamos con medidas continuas (medidas que a conjuntos formados por un único punto le da valor 0), la \fref{prop:DescompLebesgue} nos dice que la parte singular la podemos descomponer en una parte singular continua y en otra pure-point (suma numerable de deltas de Dirac).

\subsection{Teoría de diferenciación de Lebesgue}

El teorema fundamental del cálculo es algo muy familiar, normalmente expresado de esta forma:
\[ F(x) = \int_a^x f(t) \dif t \] con $F$ la ``antiderivada'' de $f$. Es decir, que derivando $F$ tendremos $f$: \[ \lim_{h \to 0} \frac{F(x+h) - F(x)}{h} = f \]

Podemos unir estas dos ecuaciones y cogiendo la derivada como $\frac{F(x+h) - F(x-h)}{2h}$. En ese caso, nos queda la siguiente expresión del teorema fundamental del cálculo: \[ \lim_{h \to 0} \frac{1}{2h} \int_{x-h}^{x+h} f(t) \dif t = f(x) \]

Esta definición se puede extender a funciones generales y medidas abstractas. De esto trata la \fref{sec:DifLebesgue}, y el resultado es el \nref{thm:DifLebesgue}, que sólo pide que $f$ sea integrable localmente, y nos da una igualdad para casi todo punto: \[ \lim_{r \to 0} \fint_{E_r(x)} f(y) \dif y = f(x) \], con $\set{E_r}$ una familia que se encoge bien a $x$ (ver la \fref{def:EncogerBien}). El símbolo $\fint$ es la \nlref{def:MediaLocal}, que no es más que la integral en un conjunto dividida por la medida del conjunto.

Aunque la prueba que aparece en los apuntes del teorema es más bien corta, en realidad depende de toda la teoría por detrás. Una parte muy importante viene del \fref{thm:LimiteAr}, que nos da la misma desigualdad pero para bolas y para funciones sin valor absoluto:
\[ \lim_{r\to 0} A_r f(x) = f(x) \]

La prueba de ese teorema es la más relevante, y empieza con el truco de quitarnos los valores lejanos (el teorema es local) para tener una función integrable globalmente. Con eso, introducimos una función continua $g$ muy cercana (con la medida de la integral) a $f$. En esa función continua, $\lim A_r f \convs[][r][0] g$. Eso nos permite hacer un juego de sumas y restas, lo que nos guía a la estimación de la medida de dos conjuntos ``malos''. Midiendo esos conjuntos, podemos aplicar el \nref{thm:Maximal}, que nos da una cota superior para el supremo de las medias locales (ver la \nlref{def:FuncMaximalHL}), y nos permite decir que la medida de los puntos ``malos'' se va a cero, luego efectivamente la desigualdad del teorema se cumple.

El teorema maximal recae en el \nref{lem:Recubrimiento}, que nos da una forma de recubrir (más o menos) un conjunto con bolas disjuntas y con medida aproximada.

Con esto hecho, podemos demostrar que el \nref{def:ConjuntoLebesgue} tiene la misma medida que el espacio o, en otras palabras, que el conjunto de puntos que no son de Lebesgue tiene medida cero (\fref{thm:PuntosLebesgue}).

\section{Espacios de Banach y Hilbert. Espacios $L^p$}

En el \fref{chap:EspaciosBanachHilbert} se discute cómo crear espacios de funciones y cómo darles una estructura que nos permita manejarlos fácilmente. El primer enfoque son los espacios de Banach, espacios vectoriales normados. Además, veremos cómo proveerlos de una operación, la convolución. Por último, veremos los espacios más útiles, los espacios de Hilbert, espacios vectoriales completos con un producto escalar. Estos nos permitirán generalizar los espacios euclídeos a espacios de dimensión infinita, y con ellos podremos trabajar efectivamente con funciones como en un espacio vectorial.

\subsection{Espacios de Banach - Espacios $L^p$}

La idea del \nlref{def:EspLp} es coger las clases de equivalencia de funciones $p$-integrables (las que $\int_X \abs{f}^p < ∞$), donde dos funciones son equivalentes si son iguales en casi todo punto. Así, tenemos una sencilla descomposición que nos dice que podemos ``descojonar'' una función en casi todo punto todo lo que queramos, que seguirá siendo la misma clase de equivalencia.

Además, se define la norma $L^p$ cono la raíz $p$-ésima de la función elevada a $p$ sobre todo el espacio, que le da una estructura de espacio normado a $L^p$.

Para trabajar con las funciones \espLp, desarrollamos varias desigualdades integrales. Una de las más importantes es la \nlref{prop:DesHolder}, que nos dice que dados $1≤p,q≤∞$ con $\frac{1}{p} + \frac{1}{q} = 1$ (uno es el exponente conjugado del otro), y dos funciones $f ∈ \espLp$, $g ∈ L^q$, tenemos que \[ \int_X \abs{fg} \dif μ ≤ \norm{f}_p \norm{g}_q \], por lo que $fg ∈ L^1$.

Esta desigualdad también se puede generalizar (\fref{prop:DesHolderExt}) para $\frac{1}{p} + \frac{1}{q} = \frac{1}{r}$, y entonces nos dirá que $\norm{fg}_r ≤ \norm{f}_p \norm{g}_q$. La desigualdad también nos dice que $L^p(X) ⊆ \espLloc[1][X]$ (\fref{thm:LpIntegrableLocal}).

Otra desigualdad importante es la \nlref{thm:DesigualdadJensen}, que nos dice que \[ \fint_X φ(f(x)) \dif μ ≥ φ\left(\fint_X f(x) \dif μ\right) \] para φ convexa y obviamente con $μ(X) < ∞$. Además, $[φ(x)]_- ∈ L^1$.

En esta sección, también se demouestra que $L^p$ es un espacio de Banach (\fref{thm:RieszFrechetBanach}). Igualmente, vemos el tema de operadores (el espacio dual de $L^p$), que nos lleva al \nref{thm:ReprRiesz}, que nos dice que todo operador lineal λ sobre funciones $L^p$  se corresponde con la siguiente representación única \[ λ(f) = λ_g(f) = \int_X fg \dif μ \] con $g ∈ L^\frac{p}{p-1}$.

\subsection{Convolución}

En la \fref{sec:Convolucion} desarrollamos la operación de convolución para manejar funciones integrables, construyendo así lo que se llama un álgebra de Banach para los espacios $L^p$. La convolución tiene varias propiedades interesantes: es conmutativa, asociativa y además tenemos una serie de desigualdades que nos permiten dar estimaciones para la norma de la convolución. La versión más interesante (por no ser demasiado complicada ni demasiado específica) es la del \fref{thm:DesYoungCutre}, que nos dice que dados $1 ≤ p,q,r ≤ ∞$ con $\frac{1}{p} + \frac{1}{1} = 1 + \frac{1}{r}$, se cumple que \[ \norm{f*g}_r ≤ \norm{f}_p \norm{g}_q \]

La convolución también permite hacer estimaciones de su soporte: $\essop f*g ⊆ \essop f + \essop g$ (\fref{prop:SoporteConvolucion}). Igualmente, la convolución juega bien con la derivada (\fref{prop:DerivadasConvolucion}): $∂^α(f*g) = (∂^α f*g)$ con $∂^α$ un multiíndice.

La parte mala de la convolución es que no tenemos una función que sea la identidad (la delta de Dirac sería esa identidad, pero no es una función sino una medida). Lo que si tenemos es una \nref{def:AproxIdentidad} que nos permiten aproximar esa delta de Dirac con funciones $C^∞$ de soporte compacto y norma 1.

\subsection{Espacios de Hilbert}

Como decíamos al principio de esta sección, los espacios de Hilbert nos permitirán manejar muy cómodamente los espacios de funciones, como si fuesen espacios vectoriales. La definición  de un \nref{def:EspacioHilbert} es simple: sólo queremos un espacio vectorial, equipado con un producto escalar, y completo con respecto a la norma inducida por ese producto escalar (los límites de sucesiones convergentes se quedan en el espacio).

La última restricción es la que nos da una pista de qué espacios $L^p$ son espacios Hilbert: sólo los $L^2$, donde la norma coincide con el producto escalar de una función consigo misma.

En la \fref{sec:PropsHilbert} vemos las propiedades de los espacios de Hilbert como espacios vectoriales, básicamente comprobando que todo a lo que estamos acostumbrados se cumple. Cauchy-Schwartz ($\abs{\pesc{\vx,\vy}} ≤ \norm{\vx}\norm{\vy}$), Pitágoras para vectores ortogonales, la existencia de bases $\set{\ve_α}$, la fórmula de inversión ($\vx = \sum \pesc{\vx,\ve_α} \ve_α$), la identidad de Plancharel ($\norm{\vx}^2 = \sum \abs{\pesc{\vx,\ve_α}}^2$) para la norma de un vector o la identidad del paralelogramo ($\norm{\vx + \vy}^2 + \norm{\vx - \vy}^2 = 2 \norm{\vx}^2 + 2 \norm{\vy}^2$).

También es importante el \nref{thm:DistConvexo}, que nos da una definición para la distancia entre un punto y un conjunto convexo, y nos da además la \nlref{def:DesigVariacional}.
