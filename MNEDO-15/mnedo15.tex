\documentclass{apuntes}

\title{MNEDO}
\author{Pedro Valero \& Jorge Martín}
\date{15/16 C1}
% Paquetes adicionales
\usepackage{tikztools}
\usepackage{tikz-3dplot}
\usepackage{textcomp}
\usepackage{tikz-qtree}
\usepackage{changepage}
\usepackage{colortbl}
\usepackage{fastbuild}

\bibliographystyle{plainnat}

\usetikzlibrary{arrows}
% --------------------

\precompileTikz

\begin{document}
\pagestyle{plain}
\maketitle

\tableofcontents


\chapter{Preliminares}

\section{Problemas de valor inicial. (PVI)}

A lo largo de este curso estudiaremos problemas de valor inicial, que no son más que ecuaciones diferenciales ordinarias (EDOs) junto con un dato inicial, necesario para resolver adecuadamente la ecuación. Se trata de sistemas de la forma:
\[\begin{cases}
		y'(x)=f(x,y(x)) & x∈[a,b]\\
		y(a)=y_a
\end{cases}\]
done $y(x)$ es una función del tipo
\[\appl{y}{ℝ}{ℝ^d}\]
\[y(x)=(y_1(x), …, y_d(x))\]

Por tanto $f$ quedará definida como:
\[\appl{f}{ℝ×ℝ}{ℝ^d} \]
\[f(x,y) = (x,y(x))\]

Como notación muchas veces consideraremos $y(x)=y$, del mismo modo que $y(a)=y_a$ con $a$ un valor dado.

\begin{remark}
	En el curso trabajaremos con funciones $f∈C\left( [a,b] × ℝ^d \right)$
\end{remark}


\begin{defn}[Función Lipschitz]
	Una función $f$ será Lipschitz en la segunda variable si existe una constante $L$ tal que:
	\[∀x ∈ [a,b], ∀y_1,y_2 ∈ℝ^d \ \md{f(x,y_1) - f(x,y_2)} ≤ L\md{y_1-y_2}\]
\end{defn}

Vamos a ver en qué situaciones este tipo de problemas tienen solución única:

\begin{theorem}[Teorema de Picard]
	\label{TeoremaPicard}
	Sea $Ω=[a,b]×ℝ^d$, si $f$ es continua en $Ω$ y Lipschitz en la segunda variable; entonces \textbf{el PVI tiene una única solución} $y∈C'\big([a,b] × ℝ^d\big)$
\end{theorem}

\begin{proof}
	\textbf{Vamos a demostrar primero la existencia de la solución}:
	Para ello haremos uso de las iteradas de Picard, definidas mediante:

	\[y(x)=y_a+\int_a^x f(s,y(s))ds\]

	Si definimos la aplicación $T_y = y_a + \int_a^x f(s,y(s))ds$, encontrar la solución $y$ es equivalente a encontrar un punto fijo de dicha aplicación. Vamos a ello.

	\begin{enumerate}
		\item \textbf{Construimos la serie de funciones $\{ y_n(x) \}_{n=0}^∞$}:
		\[y_{n+1}(x) = y_a + \int_a^x f(s,y_n(s)) ds\]
		\[n=0,1,… \text{\ y \ } y_0(x)=y_a\]
		Cada función $y_n$ es continua (es una constante más una función continua).

		\item \textbf{Debemos probar que $∃y ∈ C([a,b])$ tal que}:
		\[y_n \longrightarrow y \text{\ uniformemente}\]
		Que es equivalente a decir que $\lim_{n \to ∞} \md{y_n - y}_{L^∞[a,b]} = 0$. Donde $\md{f}_{L^∞[a,b]} = \max_{x ∈ [a,b]}\md{f(x)}$, denota la norma infinito de $f$.
	\end{enumerate}

	Antes de continuar vamos a recordar el Test M de Weierstrass:

	\begin{theorem}[Test M de Weierstrass]
		\label{TestMWeierstrass}
		Sea $\{f_n\}_{n=0}^∞$ una sucesión de funciones definidas en un dominio $Ω$ con valores en $ℝ$, si existe una sucesión de números $M_n$ tal que:
		\begin{enumerate}
			\item $\abs{f_n} < M_n \text{,\ } ∀x∈Ω$
			\item $\sum_{n=0}^∞ M_n < ∞$
		\end{enumerate}

		Entonces existe una función $g$ tal que la sucesión:
		\[g_n(x) = \sum_{n=1}^N f_n\]

		converge uniformemente a $g$ en $x$
	\end{theorem}

	¿Cómo vamos a usar esto para conseguir ver que $y_n \rightarrow y$?.

	Primero vamos a reescribir $y_n$ como una suma telescópica, sabiendo que tomamos $y_a=y_0$:

	\[y_{n+1} = y_a + \sum_{k=0}^n y_{k+1} - y_k\]
	Para el caso $n=1$: $y_2 = y_a + (y_1 - y_0) + (y_2 - y_1) = y_2$.

	Ahora vamos a aplicar el test M de Weierstrass:

	Las $f_k$ que aparecen en el enunciado de \ref{TestMWeierstrass} serán $(y_{k+1} - y_k)$, si demuestro que $∃{M_k}$ tales que:

	\[\abs{y_{k+1}-y_k} < M_k \text{,\ } ∀x∈[a,b]\]
	\[\text{y que\ } \sum_k M_k < ∞ \text{, entonces } ∃y \text{ tal que:}\]
	\[y_n \rightarrow y \text{ uniformemente}\]

	Vamos a encontrar dichos $M_k$:

	\begin{lemma}
		\[\abs{y_{k+1} - y_k} ≤ \frac{C(y_a)}{L}\frac{(L(x-a))^{k+1}}{(k+1)!}\]
	\end{lemma}
	\begin{proof}
		Vamos a proceder a demostrar el lema usando inducción, así que vamos a ver que es cierto para $k=0$:

		\[\abs{y_1-y_0}=\abs{\int_a^x f(s,y_a) ds} ≤ \int_a^x \abs{f(s,y_a)} ds ≤\]
		\[≤ \md{f(x,y_n)}_{L^∞[a,b]} (x-a) = C(y_n) (x-a)\]

		Ahora supondremos que el lema es cierto para $k-1$ para poder demostrarlo para $k$:

		\[\abs{y_{k+1}-y_k} = \abs{\int_a^x f(s,y_k(s)) - \int_a^x f(s,y_{k-1}(s))} ds ≤\]
		\[≤ \int_a^x \abs{f(s,y_k(s)) - f(s,y_{k-1}(s))} ds \underbrace{≤}_{\mathclap{f \text{ Lipschitz en 2ª variable}}} L \int_a^x \abs{y_k(s) - y_{k-1}(s)} ds ≤\]
		\[\underbrace{≤}_{\text{hip. inducción}} L \int_a^x \frac{C(y_a)}{L} \frac{(L(s-a))^k}{k!} ds ≤ C(y_a)\frac{L^k}{k!}\int_a^x (s-a)^k ds ≤\]
		\[≤ C(y_a) \frac{L^k}{(k+1) k!}(x-a)^{k+1}\]
	\end{proof}

	Volviendo a la demostración de \ref{TeoremaPicard} recordemos que queríamos encontrar los $M_k$ necesarios para ver la convergencia de nuestras $y_k$. Gracias al lema que acabamos de demostrar y al test M de Weierstrass \ref{TestMWeierstrass} podemos afirmar:

	\[\abs{y_{k+1}-y_k} ≤ \frac{C(y_a)}{L} ≤ \frac{C(y_a)}{L}\frac{(L(x-a))^{k+1}}{(k+1)!} = M_k\]

	El siguiente paso es verificar la segunda hipótesis de \ref{TestMWeierstrass}, es decir, que la suma infinta de las $M_k$ está acotada:

	\[M_k = \frac{C(y_a)}{L}\frac{[L(x-a)]^{k+1}}{(k+1)!} \underbrace{=}_{s=L(x-a)} \frac{C(y_a)}{L}\frac{s^{k+1}}{(k+1)!}\]

	Sabiendo que $e^x = \sum_{k=0}^∞ \frac{x^k}{k!}$:

	\[\sum_{k=0}^∞ M_k = \frac{-C(y_a)}{L} (1 - \frac{C(y_a)}{L} \sum_{k=0}^∞ \frac{s^k}{k!}) = \frac{C(y_a)}{L} (e^s - 1) < ∞\]

	Luego aplicando el test M de Weierstrass \ref{TestMWeierstrass}:
	\[∃y \text{ tal que: } y_n\rightarrow y \text{ uniformemente en} [a,b]\]

	\begin{enumerate}
		\setcounter{enumi}{3}
		\item Por tanto hemos probado que $∃y$ tal que $y_n \rightarrow y$ uniformemente, y por lo tanto $y∈C([a,c])$ (sabemos que la convergencia uniforme de una sucesión de funciones continuas hace que la \textbf{función a la que se converge sea continua}).

		\item
			\[y_{n+1} = y_a + \int_a^x f(s,y_n(s))ds \underbrace{\longrightarrow}_{n \rightarrow ∞} y = y_a + \int_a^x f(s,y(s))ds\]
			Como $f∈C([a,b])$ y $y_n$ son continuas, está claro que $y$ es continua. De modo que \textbf{hemos demostrado que existe solución} $y∈C([a,b])$.
	\end{enumerate}

	Por últimos debemos \textbf{demostrar la unicidad} de la solución. Lo haremos por reducción al absurdo.

	Supongamos que existen dos soluciones distintas $y,\tilde{y} ∈ C([a,b])$:
	\[y(x) = y_a + \int_a^x f(s,y(s))ds\]
	\[\tilde{y}(x) = y_a + \int_a^x f(s,\tilde{y}(s))ds\]
	\[\abs{y(x) - \tilde{y}(x)} = \int_a^x \abs{f(s,y(s)) - f(s,\tilde{y}(s))} ds ≤\]
	\[≤ L \int_a^x \abs{y(s) - \tilde{y}(s)} ds \]

	Definimos $g(x)=\int_a^x \abs{y(s) - \tilde{y}(s)} ds$:

	\begin{equation}
		\label{eqDemPicard}
		g'(x) = |y(x) - \tilde{y}(x)| \leq Lg(x); \ g'(x)e^{-L(x-a)} \leq Lg(x)e^{-L(x-a)}
	\end{equation}

	Seguimos:
	\[\left( g(x)e^{-L(x-a)} \right)^{'} = g'(x) e^{-L(x-a)} - Lg(x) e^{-L(x-a)}\]
	\[g'(x) e^{-L(x-a)} = \left( g(x)e^{-L(x-a)} \right)^{'} + Lg(x) e^{-L(x-a)}\]

	Usando \ref{eqDemPicard}:
	\[\left( g(x)e^{-L(x-a)} \right)^{'} + Lg(x) e^{-L(x-a)} ≤ Lg(x) e^{-L(x-a)}\]
	\[\left( g(x)e^{-L(x-a)} \right)^{'} ≤ 0\]

	Si integramos a ambos lados entre $a$ y $X$:
	\[g(X) e^{-L(x-a)} ≤ g(a) = 0 \text{ por como hemos definido } g\]

	Por tanto:
	\[g(X) = \int_a^X \abs{\tilde{y}(x) - y(x)} dx = 0 \implies \tilde{y}=y\]

	Y hemos demostrado la unicidad de la solución.
\end{proof}

Lo siguiente que cabe preguntarse es cómo varía la solución de un problema de valor inicial (PVI) cuando variamos el dato inicial. Y la respuesta es que al variarlo muy poco, la solución también se altera muy poco.

\begin{theorem}[Continuidad con respecto al dato inicial]
	Sea $y' = f(x,y(x))$ un problema de valor inicial (PVI) con dos datos iniciales distintos $y_a, \tilde{y}_a$, con $f$ Lipschitz en la segunda variable y continua en $Ω=[a,b]×ℝ^d$:
	\[\md{y(x)-\tilde{y}(x)}_{L^∞[a,b]} ≤ \abs{y_a - \tilde{y}_a} e^{L(b-a)}\]
\end{theorem}
\begin{proof}
	Tomando como soluciones las que se obtienen con la iterada de Picard:
	\label{ineq:argumento_cont_dato_inicial}
	\[y(x)-\tilde{y}(x)=y_a-\tilde{y}_a + \int_a^x (f(s,y(s)) - f(s,\tilde{y}(s)))ds\]
	\[\abs{y - \tilde{y}} ≤ \abs{y_a - \tilde{y}_a} + \int_a^x \abs{f(s,y(s)) - f(s,\tilde{y}(s))} ds ≤\]
	\[≤ \underbrace{ \abs{y_a - \tilde{y}_a} + L\int_a^x \abs{y(s)-\tilde{y}(s)} ds}_{g(x)} \]

	Por tanto $g'(x) = L\abs{y(x) - \tilde{y}(x)}$. Si sobre esta última igualdad realizamos el procedimiento del factor integrante que hemos llevado a cabo al demostrar la unicidad en el teorema de Picard \ref{TeoremaPicard} llegamos a:

	\[g(x)e^{-L(x-a)} ≤ \abs{\tilde{y}_a - y_a} \implies g(x) ≤ \abs{\tilde{y}_a - y_a}e^{L(x-a)}\]

	Por último:
	\[\abs{y_(x) - \tilde{y}(x)} ≤ g(x)\]
	\[y(x) - \tilde{y}(x) ≤ \abs{\tilde{y}_a - y_a}e^{L(x-a)}\]
	\[\md{y - \tilde{y}}_{L^∞[a,b]} ≤ \abs{y_a - \tilde{y}_a} e^{L(b-a)}\]
\end{proof}

Vamos ahora a ver ejemplos donde aplicar el teorema de Picard \ref{TeoremaPicard}:

\begin{example}
	Para el problema de valor inicial (PVI) $y'(x)=\sqrt{x}$, $y(0)=0$; tenemos $f(x,y(x))=\sqrt(y)$. Esta $f$ no es Lipschitz y por tanto no cumple las hipótesis del teorema de Picard \ref{TeoremaPicard}, de hecho existen dos soluciones distintas:
	\[y_1(x) = 0\]
	\[y_2(x) = x^2\]
\end{example}


\section{Algunos ejemplos de métodos numéricos para PVI.}
Recordemos que el objetivo de este curso es encontrar soluciones a problemas de valor inicial (PVI) del tipo:

\[y'(x) = f(x,y(x)) \text{, } x∈[a,b]\]
\[y(a)=y_a\]

Lo que queremos ver es cómo se comporta y(x) sin saber encontrar su solución. Para ello, como ya hemos visto, podemos usar las iteradas de Picard:

\[y_{n-1}(x) = y_a + \int_a^x f(s,y_n(s)) ds\]
\[y_0(x) = y_a\]

\begin{remark}
Durante este curso, al emplear los diferentes métodos numéricos usaremos versiones discretizadas de las funciones, es decir, el conjunto de valores que tomará una función $y(x)$ será sobre una serie de valores ${x_n}_{n=0}^N$ dentro del intervalo $[a,b]$, donde $x_0=a$ y $x_N=b$.
\end{remark}
En este curso las sucesivas muestras serán equidistantes, siendo $x_{n+1}-x_n=h=\frac{b-a}{N}$.

A continuación mencionaremos algunos métodos numéricos que estudiaremos más adelante:
\begin{itemize}
	\item \textbf{Método de Euler}
	\[y'(x) = f(x,y(x))\]
	\[y'(x_n) = f(x_n,y(x_n))\]
	Pero dado que $x$ toma valores discretos:
	\[\frac{y(x_{n+1}) - y(x_n)}{h} = \frac{y_{n+1} - y_n}{h} = y'(x_n) = f(x_n,y(x))\]
	Así la fórmula de recurrencia del método de Euler queda como:
	\[y_{n+1} = y_n + h·f(x,y_n)\]
	\[y_0=y_a\]

	\item \textbf{Desarrollo en serie de Taylor}
	\[y(x_{n+1}) = y(x_n+h) = y(x_n) + h·y'(x_n) + R_n =\]
	\[= y(x_n) + h·f(x_n,y(x_n)) + R_n\]

	\item \textbf{Método leap-frog}
	\[y(x_{n+2}) - y(x_n) = \int_{x_n}^{x_{n+2}} y'(x) = \int_{x_n}^{x_{n+2}} f(x,y(x))\dif x \approx (x_{n+2}-x_n)f(x_{n+1},y(x_{n+1}))\]
	donde la aproximación se ha realizado empleando la regla del punto medio.

	Finalmente nos queda:
	\[y(x_{n+2}) - y(x_n) = 2h f(x_{n+1},y(x_{n+1}))\]
\end{itemize}

Antes de continuar, veamos un par de conceptos que serán necesarios a lo largo de este curso.

\begin{defn}[Método numérico de $k$ pasos]
Se dice que un método numérico es de $k$ pasos si para conocer $y_{n+k}$ necesitamos conocer los $k$ valores: $y_{n}, y_{n+1},... y_{n+k-1}$
\end{defn}

\begin{defn}[Método numérico convergente]
Se dice que un método numérico es convergente si para todo problema de valor inicial se tiene que:
\[\lim_{N \to \infty} \max_{0 \leq n \leq k-1}\norm{y(x_n)-y_n}= 0\implies \max_{k \leq n \leq N} \norm{y(x_n)-y_n}=0\]

$k$ será el número de pasos del método numérico, es decir, para el método de Euler tendremos $k=1$, mientras que para el método leap-frog tendremos $k=2$.

Evidentemente no nos referimos a cualquier problema imaginable sino a todos aquellos problemas de valor inicial en los que la función $f$ sea continua y Lipschitz en la segunda variable, pues esos son lso únicos problemas con los que hemos trabajado hasta ahora.
\end{defn}

\chapter{Algunos Métodos Numéricos para EDOs}
\section{Método de Euler}

Dado un problema de valor inicial del tipo:
\[y'(x) = f(x,y(x))\]
\[y'(x_n) = f(x_n,y(x_n))\]
el método de Euler nos permite aproximarnos a la solución a través de la siguiente relación de recurrencia:
\[y_{n+1} = y_n + h·f(x,y_n)\]
\[y_0=y_a\]

\begin{theorem}[El método de Euler es convergente]\label{theorem:EulerEsConvergente}

Sea $y(x)$ la solución de un problema de valor inicial y sean $\{y_n\}$ los valores que vamos aproximando en cada iteración del método de Euler, se cumple que $\{y_n\}$ converge a $y(x)$
\end{theorem}
\begin{proof}
Al trabajar con este método tenemos la relación de recurrencia:
\[y_{n+1} = y_n + h f(x_n, y_n)\]

Para empezar vamos a definir el residuo que no es más que la diferencia entre el valor aproximado $y_n$ obtenido al aplicar el método en cuestión y el valor real $y(x_n)$. Así, podemos escribir:
\[y(x_{n+1})=y(x_n)+hf(x_n,y(x_n))+R_n \text{ siendo } R_n \text{ el residuo }\]

Despejando podemos ver que
\[R_n = \underbrace{y(x_{n+1})-y(x_n)}_{hy'}-h\underbrace{f(x_n,y(x_n))}_{y'} = \frac{h^2}{2}y''(ε)\]


Según la definición de convergencia, para demostrar que la sucesión converge basta con ver que:
\[|y(x_n)-y_n | = 0 \ \implies \lim_{N \to \infty} \max_n |y(x_n)-y_n| = 0\]


\begin{itemize}
\item Basándonos en la condición de Lipschitz:
\[|y(x_{n+1})-y_{n+1}| \leq (1+Lh)|y(x_n)-y_n|+K \text{ siendo } e_{n+1} \leq (1+Lh)e_n + K \text{ con } K = \max_n |R_n|\]

\begin{proof}
\[e_n = |y(x_n)-y_n| \implies e_{n+1} \leq (1+Lh)e_n+K \implies e_n \leq (1+Lh)^n e_0 + K \frac{1-(1+hL)^n}{1-(1+kL)}\]
que simplificando nos queda:
\[e_n \leq (1+Lh)^n e_0 + \frac{K}{h}\frac{(1+hL)^n-1}{L}\]

Esta fórmula puede ser deducida de forma sencilla estudiando como se comporta el error en cada iteración, con lo que veremos que no es más que una sucesión geométrica. Otra opción posible es hacer la demostración de la misma por inducción.

Ambos procedimientos son muy sencillos y se dejan como ejercicio para el lector.
\end{proof}

Gracias a esto tenemos demostrada la \textbf{estabilidad} pues
\[|y(x_n)-y_n| \leq (1+hL)^n |y(x_0)-y_0| + \frac{(1+hL)^{n+1}-1}{L}\cdot \max_n \frac{|R_n|}{h}\]
%
%Por otro lado tenemos que:
%\[\lim_{h \to 0} \frac{|R_n|}{h} = 0\]
%con lo que vemos que el residuo lo tenemos controlado


%\item Ahora vamos a demostrar que
%\[|y(x_n) - y_n| \leq (1+Lh)^N |y(x_n)-y_0|+K\frac{(1+Lh)^N-1}{Lh}\]
%donde $N$ es el número total de $y_n$ que estamos tomando al aplicar el método.
%
%Para ello basta con ver que
%\[\max_n |y(x_n)-y_n| \leq (1+Lh)^N|y(x_n)-y_0| + K\frac{(1+Lh)^N-1}{Lh}\]
%
%Por otro lado tenemos que
%\[h = \frac{b-a}{N}\]
%con lo que podemos escribir la parte derecha de la desigualdad que queremos probar %como:%
%\[ \l%eft( 1+\frac{L(b-a)}{N}\right)^N|y(x_n)-y_0| + K \frac{\left(1+\frac{L(b-a)}{N%} %\righ%t)^N-1}{Lh}\]%
%
%Además sabemos que
%\[\lim_{N \to \infty} \max_n |y(x_n)-y_0| \leq e^{L(b-a)} |y(x_n)-y_0| + \frac{e^{L(b-a)}-1}{L}\lim_{h \to 0} \frac{K}{h}\]

\item \textbf{Control del residuo}

El residuo puede escribirse como:
\[R_n = y(x_{n+1})-y(x_n)-hy'(x_n) = \frac{h^2}{2}y''(ε)\]

Si logramos probar que
\[\max_n \frac{|R_n|}{h} \leq C h\]
tendríamos garantizado que el residuo es 0.

Sin embargo esto no es tan sencillo, puesto que para poder acotar el máximo como hemos queremos nos apoyamos en que la derivada segunda, $y''$, está acotada módulo infinito por una constance $C$. Pero nosotros no sabemos nada acerca de nuestra solución y, por tanto, no podemos realizar este tipo de asunciones.

Pero esto puede arreglarse fácilmente. Sabemos que
\[\frac{R_n}{h} = \frac{y(x_{n+1})-y(x_n)}{h} = y'(x_n) = y'(ε)-y'(x_n) \text{ siendo } x_n \leq ε \leq x_{n+1}\]
A partir de aquí vemos fácilmente que
\[\lim_{h\to 0} \frac{|R_n|}{h} = 0\]

Cuando $h \rightarrow 0$ estrechamos las distancias entre los $x_n$ (es decir $x_{n+1} \rightarrow x_n$). De esta forma se acaba consiguiendo que $ε$ (que inicialmente se encontraba en el intervalo $[x_n,x_{n+1}]$) pase a ser $x_n$, y así $y'(ε)-y'(x_n) = y'(x_n)-y'(x_n) = 0$.

\end{itemize}
\end{proof}

\obs En la prueba hemos evitado apoyarnos en información acerca de la derivada segunda de la solución $y$, pero en realidad sí que podríamos haberlo hecho:

\[
	\text{PVI} :
		\begin{cases}
			y'(x) = f(x,y(x)) \\
			f ∈ C^1([a,b]×ℝ^d)
		\end{cases}
\]

Si aplicamos el teorema de Picard \ref{TeoremaPicard} obtenemos que la solución única $y$ es $C^1$, por tanto $f$ compuesta con $y$ (nuestra $f(x,y(x))$) será también $C^1$; y por tanto $y' ∈ C^1$. Pero si tenemos que $y'$ es $C^1$, $y$ será $C^2$.

De modo que podemos obtener acotaciones para $y''$:
\[y''(x)=f_x(x,y(x)) + f_y(x,y(x))·y'(x) = f_x(x,y(x)) + f_y(x,y(x))·f(x,y(x))\]
\[\implies \md{y''}_{L^∞[a,b]} ≤ C_f\]

Con $C_f$ una constante que depende de la norma $C^1 \left( [a,b]×ℝ^d \right)$ de $f$.

\subsection{Orden del método de Euler}
\begin{defn}[Método numérico de orden p]
	Se dice que un método numérico es convergente de orden $p$ si $p$ es el mayor de los enteros tales que $f \in C^p$ y
	\[\max_{0 \leq n \leq k-1}\norm{y(x_n)-y_n)}= O(h^p)\implies \max_{k \leq n \leq N} \norm{y(x_n)-y_n}=O(h^p)\]
\end{defn}

Recordemos las definiciones de `O' y `o'

\begin{defn}[O grande]
Decimos que $f= O(g)$ en $x \to x_0$ si
\[\exists k,r > 0 \tq ∀x,\ 0 < |x-x_0|< r \text{ se tiene } |f(x)| \leq k |g(x)|\]
\end{defn}

\begin{defn}[O pequeña]
Decimos que $f=o(g)$ en $x \to x_0$ si

\[\exists r > 0 \tq ∀x,\ 0 < |x-x_0|<r \text{ tal que, donde } g \text{ no se anule, tenemos} \lim_{x\to x_0} \frac{f(x)}{g(x)}=0\]

\end{defn}

%\obs En la práctica $\max_{k≤n≤N} \md{y(x_n) - y(x_n)} = O(h^p)$ lo tomaremos como:
%\[ \max_{k≤n≤N} \md{y(x_n) - y(x_n)} ≤ C·h^p \]

\obs En la práctica $\max_{k≤n≤N} \md{y(x_n) - y(x_n)} = o(h^p)$ lo tomaremos como:
\[ \max_{k≤n≤N} \md{y(x_n) - y(x_n)} ≤ c·h^s,\ s>p,\ s∈ℝ \]

Una vez vistas estas definiciones, supongamos que tenemos $f \in C [\real \times \real^d]$. En este caso ya hemos visto que el orden de convergencia del método de Euler es al menos 1.

Para poder confirmar que el método es de orden 1, sólo nos queda comprobar que el orden no puede ser 2. Para comprobarlo nos basta con ver un ejemplo en el que no se cumpla:
\begin{example}
Tomamos el caso $y'(x)=x, \ x \in [0,1]$ con valor inicial $y(0)=0$. En este caso sabemos que la solución es $y(x)=\frac{1}{2}x^2$

Vamos a ver que ocurre si aplicamos el método de Euler. En este caso, tendríamos la relación de recurrencia:
\[y_{n+1} = y_n + h x_n\]
con lo que obtendríamos la sucesión de funciones:
\[y_1 = 0 \; ; \; y_2 = 0 + hx_1 =h^2 \; ; \; y_3 = h^2+2h^2=h^2(1+2+3) \; ; \; ...\]

De donde podemos deducir la fórmula general para $y_n$:

\[y_n = h^2\sum_{i=1}^{n-1} i = h^2\frac{n(n-1)}{2} \underbrace{=}_{x_n=n·h} =\frac{x_n^2-x_n·h}{2}\]


Así podemos ver que:
\[|y(x_n)-y_n| = \left|\frac{x_n^2}{2}-\frac{x_n^2}{2}+\frac{x_nh}{2} \right| = \frac{x_nh}{2} \implies \frac{1}{h}|y(x_n)-y_n|  = \frac{x_n}{2}\]

Por tanto el método de Euler no es convergente de orden 2, pues:
\[\max_n \md{y(x_n)-x_n} = \frac{1}{2}·h \overbrace{\implies}^{\text{por def.}}\text{es convergente de orden 1}\]
\end{example}



\textbf{Esperemos que hayáis disfrutado hasta aquí porque nos disponemos a entrar en la parte aburrida del curso.}


\chapter{Algunos Métodos Numéricos para Ecuaciones Diferenciales Ordinarias}
De aquí en adelante tomaremos $d=1$, es decir:
\[\appl{y}{ℝ}{ℝ}\]
\[\appl{f}{ℝ×ℝ}{ℝ}\]

\section{Métodos de Taylor}
Como siempre, tenemos que $y'(x)=f(x,y(x))$ y debemos buscar una relación entre $y(x_{n+1})$ y los $y(x_m)$ con $m < n+1$.

La idea del método de Taylor es emplear la relación:
\[y(x_{n+1})=y(x_n)+hf(x_n,y(x_n))+\frac{h^2}{2}y''(x_n)+R_n\]

Pero tenemos el problema de que no conocemos la derivada segudna de $y$ puesto que no conocemos $y$ (es precisamente eso lo que queremos ser capaces de estimar). Pero lo que podemos hacer es definir $y''$ apoyándonos en la regla de la cadena, con lo que obtenemos:
\[y''(x)=(\partial x f)(x,y(x))+(\partial y f)(x,y(x))y'(x)=f_x(x,y(x))+f_y(x,y(x))f(x,y(x)\]

con lo que el método de Taylor nos queda, finalmente:
\[y_{n+1} = y_n + hf(x_n,y_n)+\frac{h^2}{2}\left(f_x(x_n,y_n)+f(x_n, y_n)f_y(x_n,y_n)\right)\]

\begin{remark}
En la práctica estos métodos no se utiliza nunca puesto que no son demasido eficaces.
\end{remark}

Al hablar de estos métodos en plural hacemos referencia a las diferentes estimaciones posibles, pues podemos usar un desarrollo de Taylor de grado superior, lo que añade complejidad a la fórmula pero aproxima mejor el resultado, pues hace el residuo cada vez más pequeño.


\section{Método del trapecio}
En esta ocasión escribimos:
\[y(x_{n+1})=y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n)+ h\cdot \frac{f(x_n,y(x_n))+f(x_{n+1},y(x_{n+1}))}{2}  \]

Este método presenta una difernecia radical respecto a los demás, pues el objetivo del método es calcular $y_{x+1}$ en lugar de $y_n$.

Una forma de ver este método es definiendo el operador $T$ de la siguiente forma:
\[T(y) = y_n + \frac{h}{2} \left(f(x_n,y_n)+f(x_{n+1}, y) \right)\]
con lo que el problema a resolver se reduce a encontrar un punto fijo de este operador, es decir, una función $y$ tal que
\[T(y)=y\]

Podemos ver bajo qué condiciones la función es Lipchitz y, y por tanto tiene solución. Para ello vemos que
\[|T(y) - T(\tilde{y})| \leq \frac{h}{2} \left( |f(x_{n+1}, y) - f(x_{n+1},\tilde{y})|\right) \leq \frac{Lh}{2}\left( |y-\tilde{y}|\right)\]

Por tanto, la recurrencia tendrá solución siempre que $\frac{Lh}{2} \leq 1$

\section{Métodos basados en cuadraturas}
Ya hemos visto un método de este tipo, \textbf{el método de Euler}, en el que se define:
\[y(x_{n+1}) -y(x_n) = \int_{x_n}^{x_{n+1}} y'(x)\dif x = \int_{x_n}^{x_{n+1}} f(x_n,y(x_n)) \dif x \approx f(x_n,y(x_n))h \implies \]
\[\implies y(x_{n+1}) =y(x_n)+f(x_n,y(x_n))h\]

\subsection{Método Predictor Corrector Euler-trapecio}
En este método partimos de la fórmula del trapecio
\[y_{n+1} = y_n + \frac{h}{2} \left(f(x_n,y_n)+f(x_{n+1}, y^*_{n+1}) \right)\]

Lo que vamos a hacer ahora es aproximar el $y^*_{n+1}$, empleado a la derecha de la fórmula, por el método de Euler:
\[y^*_{n+1} = y_n + h f(x_n,y_n)\]

Una vez hemos empleado el método de Euler para hacer una predicción de $y^*_{n+1}$ empleamos el método del trapecio para corregir el valor.

\subsection{Método de Euler modificado}
Este método emplea la relación:
\[y(x_{n+1}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n) + h f\left(x+\frac{h}{2},y\left(x+\frac{h}{2}\right)\right)\]


El problema que tenemos ahora es que necesitamos aproximar $y(x+\frac{h}{2})$ (ya que $x+\frac{h}{2}$ no pertenece a nuestro mallado $\{x_n\}$), cosa que haremos mediante la ecuación:
\[y(x+\frac{h}{2}) = y(x_n)+\frac{h}{2}f(x_n, y(x_n))\]

Este método es realmente importante pues es el padre de una serie de métodos denominados \textbf{Métodos de Runge-Kutta}, que veremos a continuación

\section{Métodos de Runge-Kutta}
Estos métodos se caracterizan porque vamos a empezar calculando una serie de valores $K_i$ como sigue:
\[K_i = f\left(x_n+c_ih, y_n+h \sum_{j=1}^ia_{ij}K_j\right)\]
\[y_{n+1} = y_n+ h \sum_{i=1}^s b_i K_i\]

La idea de este método es que
\[y(x_{n+1}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n)+h \sum_{j}^{s} b_j f(x_n+hc_j, y(x_n+hc_j))\]

Las diferentes formas de aproximar la función $y(x_n+hc_j)$ nos dan las diferentes $K_i$ empleadas en la definición del método.

De entre los diferentes métodos Runge-Kutta el más empleado, por ser uno de los más eficientes es el método Runge-Kutta de cuarto orden.

\subsection{Método Runge-Kutta de cuarto orden}
\[y_{n+1} = y_n +\frac{h}{6}(K_1+2K_2+2K_3+K_4)\]
siendo:
\[K_1 = f(x_n,y_n)\]
\[K_2 = f(x_n+\frac{h}{2}, y_n+\frac{h}{2}K_1)\]
\[K_3 = f(x_n+\frac{h}{2}, y_n + \frac{h}{2}K_2)\]
\[K_4 = f(x_n+h, y_n + h K_3)\]

\textbf{Ejercicio punto extra: ¿De dónde salen estos números del Runge Kutta de cuarto orden?}
\textit{Pista: Considerar la $y$ como una función escalar y apoyarnos en los métodos de Taylor}

\section{Método leap-frog}
En este caso empelamos la relación:
\[y(x_{n+2}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x)) \dif x \approx y(x_n) + 2h f(x_{n+1},y_{n+1})\]

La diferencia entre este método y los anteriores es que este método consta de dos pasos, pues debemos aproximas $y_1$ ya que para calcular cada $y_n$ necesitamos conocer $y_{n-1}$ e $y_{n-2}$

\section{Método de Adams-Bashforth}

Se trata de un método basado en cuadratura que emplea la relación
\[y(x_{n+2}) - y(x_{n+1}) = \int_{x_{n+1}}^{x_{n+2}} f(x,y(x)) \dif x\]

\begin{remark}
Esta relación es sencilla de entender puesto que, por definición del problema original, $f(x,y(x))=y'$. Así, lo única que estamos haciendo es aplicar el Teorema Fundamental del Cálculo, es decir, estamos resolviendo la integral.

El problema, como siempre, es que no se puede hacer esta integral directamente pues la $f$ depende de $y(x)$ que no es conocida.
\end{remark}

donde $f(x,y(x))$ se aproxima por medio de la recta interpolante entre los puntos $(x_n,y(x_n))$ y $(x_{n+1},y(x_{n+1}))$, es decir:
\[f(x,y(x)) \approx f(x_n, y(x_n)) \frac{x-x_{n+1}}{x_n-x_{n+1}}+f(x_{n+1},y(x_{n+1}))\frac{x-x_n}{x_{n+1}-x_n}\]

Finalmente, el método queda expresado como:
\[y_{n+2} = y_{n+1}+\frac{h}{2}\left( 3f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]

Como podemos ver, se trata de un método de dos pasos, pues necesitamos conocer dos puntos antes de empezar a aplicar el método, y es explícito, pues directamente nos da la función que queremos conocer.

\section{Métodos de Adams-Moulton}
Se trata de un método muy similar al anterior en el que la función se interpola de otra forma distinta. De nuevo tenemos la relación:
\[y(x_{n+2}) - y(x_{n+1}) = \int_{x_{n+1}}^{x_{n+2}} f(x,y(x)) \dif x\]

pero en esta ocasión la función $f$ la aproximamos mediante una interpolación con polinomios de orden 2 en lugar de rectas:
\[f(x,y(x)) \approx f(x_n,y_n) \frac{(x-x_{n+1})(x-x_{n+2})}{(x_n-x_{n+1})(x_{n+1}-x_{n+2})} + f(x_{n+1},y_{n+1})\frac{(x-x_{n+2})(x-x_{n+2})}{(x_{n+1}-x_n)(x_{n+1}-x_{n+2})} \atop+f(x_{n+2},y_{n+2})\frac{(x-x_{n})(x-x_{n+1})}{(x_{n+2}-x_n)(x_{n+2}-x_{n+1})}\]

Ahora introducimos esta aproximación en la integral e integramos, con lo que obtenemos la siguiente fórmula de recurrencia:
\[y_{n+2} = y_{n+1} + \frac{h}{12}\left( 5f(x_{n+2},y_{n+2})+8f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]

En esta ocasión se trata de un método implícito puesto que la misma $y_{n+2}$ que queremos calcular aparece en la propia fórmula.

\section{Método predictor-corrector de Adams-Bashforth / Adamas-Moulton}

El método anterior tiene el problema de ser implícito, es decir, vamos a tener que resolver una complicada ecuación de recurrencia.

Para solucionar esto, la idea es aproximar $y_{n+2}$, calcular $y_{n+2}$ empleando la fórmula y basándonos en la aproximación y posteriormente ir corrigiendo las aproximaciones en cada paso.

Así, tendremos la fórmula de recurrencia:
\[y_{n+2} = y_{n+1} + \frac{h}{12}\left( 5f(x_{n+2},y_{n+2}^*)+8f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]

La predicción que haremos para $y^*$ será:
\[y_{n+2}^* = y_{n+1} + \frac{h}{2} \left( 3f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]
es decir, combinamos los dos métodos vistos anteriormente.

\section{Ejemplos}

\begin{problem}[1]
Escribir la siguiente ecuación diferencial de grado $n$ como un sistema de ecuaciones diferenciales de primer orden.
\[y^{n)} + \sum_{i=1}^n a_{n-i}y^{n-i)} = 0\]

\solution
La idea de este ejercicio es que, a priori, uno puede pensar que todo lo que llevamos visto durante el curso es inútil ante un problema como el aquí mostrado.

Para convertir esta ecuación en un sistema, vamos a definir una serie de nuevas ecuaciones de forma que
\[y^{i} = y_i \text{ para } i=1...n-1\]

Así el sistema de ecuaciones nos queda de la forma:
\[\begin{array}{l}
y' = y_1\\
y_1' = y_2\\
y_2' = y_3 \\
...\\
y_{n-1}' = -\left( a_{n-1}y_{n-1} + a_{n-2}y_{n-2}+...+a_1y_1 + a_0 y\right)
\end{array} \]

Como podemos comprobar, en nuestro sistema, todas las ecuaciones son de primer orden.

\end{problem}

\begin{problem}[2]
Convertir el siguiente sistema de ecuaciones diferenciales en otro donde todas las ecuaciones tengan orden 1.
\[\begin{array}{l}
u''' = u''+v'\\
v''' = u^2+e^xu'v'+\sin(v)
\end{array} \]
con $u'(0)=u''(0)=v'(0)=0 \ , \ v(0)=1$
\solution

Renombrando las variables de la forma: $u' = u_1$, $u''=u_2$, $v'=v_1$, podemos reescribir el sistema como sigue.

\[\begin{array}{l}
u' = u_1\\
u_1'=u_2\\
v'=v_1\\
u_2' = u_2+v_1\\
v_1' = u^2+e^xu_1v_1 + \sin(v)
\end{array} \]
con $u_1(0)=u_2(0)=v_1(0)=0 \ , \ v(0)=1$

\end{problem}

\begin{problem}[3]
Comprobar que las siguientes funciones satisfacen la condición de Lipschitz

\ppart
\[f(x,y)=2yx^{-4} \text{ con } x \in [1,\infty)\]

\ppart
\[f(x,y)=e^{-x^2}\arctg(y) \text{ con } x \in [1, \infty)\]

\ppart
\[\appl{f}{\real \times \real^2}{\real} \text{ tal que }f(x,y)=\left(x+\sin(y_2), \frac{x^2}{2}+\cos(y_1)\right)\]
\solution

\spart
\[|f(x,y)-f(x,\tilde{y})| = 2x^{-4}|y-\tilde{y}| < 2 |y-\tilde{y}|\]

\spart
\[|f(x,y)-f(x,\tilde{y})| = e^{-x^2} | \arctg(y)-\arctg(\tilde{y})| = e^{-x^2}\int_{\tilde{y}}^y \frac{\partial}{\partial x}\arctg{x}\dif x = e^{-x^2}\int_{\tilde{y}}^y \frac{1}{|+x^2} \dif x \leq \]
\[\leq e^{-x^2} \sup \frac{1}{1+x^2}|y-\tilde{y}| \leq \frac{e^{-1}}{2}|y-\tilde{y}|\]

\spart
Según en qué norma estemos trabajando, el módulo de $y$ se calculará de una forma diferente.

En esta ocasión, por que así lo ha indicado el profesor para este ejercicio, vamos a trabajar con la norma $L1$ de forma que
\[\norm{y}_{L_1} = |y_1| + |y_2|\]

Con esta medida, nos queda:
\[\norm{f(x,y)-f(x,\tilde{y})}_{L_1} = |\sin(y_2)-\sin(\tilde{y}_2)| + |\cos(y_1)-\cos(\tilde{y}_2)| \leq |y_2 - \tilde{y}_2|+ |y_1-\tilde{y}_1| \leq \norm{y - \tilde{y}}_{L_1}\]

\end{problem}


\begin{problem}[4]
Dado:
\[
	\begin{cases}
	y'=f(x,y), & y(0)=(0,1) \\
	\tilde{y'}=f(x,\tilde{y}), & \tilde{y}(0)=(0,1)
	\end{cases}
\]
Acotar $\max_{x∈[0,1]}{\md{y - \tilde{y}}} = \norm{y - \tilde{y}}_{L^∞}$

\solution
Tomaremos (recordando \hyperref[ineq:argumento_cont_dato_inicial]{el argumento de la demostración de continuidad respecto del dato inicial}):
\[ \abs{y - \tilde{y}} ≤ \underbrace{\abs{y(0) - \tilde{y}(0)} + L \int_0^x \abs{y - \tilde{y}}(s)\ ds}_{g(x)} \]

Derivando $g$ se obtiene $g'(x)=L\abs{y-\tilde{y}}$. Se ve que:
\[g'(x)≤Lg(x)\]
Usando el factor integrante llegamos a:
\[g'(x)e^{-Lx} ≤ Lg(x)e^{-Lx}\]
Que si nos fijamos no es más que:
\[\left( ge^{-Lx} \right)' ≤ 0\]
Que si además lo integramos en ambas partes por $\int_0^x$:
\[g(x) ≤ g(0) e^{Lx}\]
Así llegamos a:
\[\abs{y - \tilde{y}} ≤ g(x) ≤ g(0)e^{Lx} = \abs{\tilde{y}(0) - y(0)}e^{Lx} = \sqrt{2}e^{Lx}\]
De modo que tendremos la cota:
\[\max_{x∈[0,1]} \abs{y - \tilde{y}} ≤ \sqrt{2}e^{Lx}\]

\end{problem}

\begin{problem}[5]
Dada la función $\appl{f}{\real}{\real}$ tal que $f(x)=\abs{x}^α$

\ppart Demostrar que $f$ no es Lipschitz en ninún intervalo que contenga al origne

\ppart Resolver el PVI:
\[\left\{ y'(x)=f(y(x)) \atop y(0)=0\right.\]

\solution

\spart

Podemos ver que
\[\abs{f(x)-f(0)} = \abs{\abs{x}^α - 0} = \frac{\abs{x}^α}{\abs{x}}\abs{x} = \frac{1}{\abs{x}^{1-α}}\abs{x}\]

Pero en cualquier entorno del origen, el denominador podía dispararse de forma que es imposible que $\frac{1}{\abs{x}^{1-α}}$ esté acotdado por alguna $L$.

\spart

Tenemos que $y'(x)=\abs{y(x)}^α$.

Por un, de la propia definición de $y'(x)$ podemos ver que:
\[\int_0^t \frac{y'(x)}{\abs{y(x)}^α}\dif x = \int_0^t 1 \dif x = t\]

Por otro lado, sabemos calcular la integral de la izquierda con lo que obtenemos:

\[\frac{1}{1-α}\left( y(t)^{1-α}-y(0)^{1-α}\right) = \frac{1}{1-α}y(t)^{1-α} = t \implies y(t) = (t (1-α))^{\frac{1}{1-α}}\]

Como podemos ver, estamos obteniendo dos soluciones, puesto que tenemos una raíz. Esto podría contradecir el Teorema de Picard, que nos garantiza unicidad de la solución. No obstante no se ningún problema puesto que la función no es Lipschitz por lo que el teorema no aplica.
\end{problem}

\begin{problem}[6]
Sea $y(x) \in C([a,b])$ solución de $y'(x)=f(x,y(x))$ y sea $\tilde{y}(x) \in C([a,b])$ con $\tilde{y}'(x)=f(x,\tilde{y}(x))+r(x,\tilde{y}(x))$ siendo $||r||_{L^∞×L^∞} \leq M$

Demostrar que
\[\max_{x \in (a,b)} |y(x)-\tilde{y}(x)| \leq |y(a)-\tilde{y}(a)|e^{L(b-a)}+\frac{M}{L}\left( e^{L(b-a)}-1\right)\]

\solution
Tomando la resta de $y'$ y $\tilde{y}'$, y sirviéndonos de la iterada de Picard:
\[\int_a^x y'(t) - \tilde{y}'(t) = \int_a^xf(t,y(t)) - f(t,\tilde{y}(t)) - \int_a^xr(t,\tilde{t}) + \left( y(a) - \tilde{y}(a) \right)\]

Tomando valores absolutos:
\[\abs{y(x) - \tilde{y}(x)} \overbrace{≤}^{\label{ineq:dif_picard}1} \int_a^x\abs{f(t,y(t)) - f(t,\tilde{y}(t))} dt + \int_a^x \abs{r(t, \tilde{y}(t))} dt + \abs{y(a) - \tilde{y}(a)} ≤\]

\[≤ L\abs{y(t) - \tilde{y}(t)} + \int_0^xM dt + \abs{y(a) - \tilde{y}(a)}\]

Tomaremos $g'(x) = L\abs{y(x) - \tilde{y}(x)} + M$. Y obtenemos:

\[\frac{g'(x) - M}{L} = \abs{y(x) - \tilde{y}(x)} \overbrace{≤}^{\label{ineq:acotar} 2} g(x)\]

\[g'(x) - Lg(x) ≤ M\]

Tirando de factor integrante:
\[\underbrace{g'(x)e^{-L(x-a)} - Lg(x)e^{-L(x-a)}}_{\left( g(x) e^{-L(x-a)} \right)'} ≤ Me^{-L(x-a)}\]

Integrando entre $a$ y $t$ a ambos lados de la inecuación:
\[\int_a^b \left(g(x) e^{-L(x-a)} \right)'dx ≤ \int_a^t Me^{-L(x-a)} dx\]
\[g(t)e^{-L(t-a)} - g(a) ≤ \frac{M}{L} \left(1 - e^{-L(t-a)} \right)\]
\[g(t) ≤ e^{L(t-a)} \left\{ g(a) + \frac{M}{L} \left(1 - e^{-L(t-a)} \right) \right\}\]
\[\qquad \quad g(a)e^{L(t-a)} + \frac{M}{L} \left(e^{L(t-a)}-1 \right)\]

Sirviéndonos de las inecuaciones \hyperref[ineq:dif_picard]{1} y \hyperref[ineq:acotar]{2}:

\[\abs{y(x) - \tilde{y}(x)} \overbrace{≤}^{\hyperref[ineq:dif_picard]{1}} g(x) \overbrace{≤}^{\hyperref[ineq:acotar]{2}} g(a) e^{L(x-a)} + \frac{M}{L} \left(e^{L(x-a)}-1 \right) \]

Y tomando máximos a ambos lados, demostramos la desigualdad del enunciado:
\[\max_{x∈(a,b)} \abs{y(x) -\tilde{y}(x)} ≤ \max_{x∈(a,b)} \left\{ g(a) e^{L(x-a)} + \frac{M}{L} \left(e^{L(x-a)}-1 \right) \right\}\]
\[\qquad \qquad  \qquad  \qquad= \abs{y(a) - \tilde{y}(a)} e^{L(b-a)} + \frac{M}{L}\left(e^{L(b-a)}-1 \right) \]
\end{problem}

\chapter{Consistencia y estabilidad}
\section{La función incremento}
En este capítulo explicaremos conceptos como la consistencia, la estabilidad o el residuo, útiles a la hora de valorar la calidad de un método numérico.

A partir de ahora vamos a trabajar con métodos numéricos que se pueden escribir de la forma:
\[\sum_{j=0}^k α_jy_{n+j} = h \phi_f(x_n,y_n,y_{n+1},y_{n+k-1},h) \text{ con } n=0,...,N - k\]
donde $\phi_f$ se denomina \concept{función incremento}.

\obs Todos los métodos que hemos visto hasta ahora pueden escribirse de esta forma. Veamos algunos ejemplos,

\begin{example}
\begin{enumerate}
\item \textbf{Euler}
\[y_{n+1} = y_n+hf(x_n,y_n)\]

En este caso podemos pasar $y_n$ restando a la izquierda para que, junto con $y_{n+1}$, nos de el sumatorio, quedando a la derecha del igual $hf()$ con lo que tendríamos que $\phi_f=f$.

\item \textbf{Trapecio}

Con los métodos explícitos (como el de Euler que acabamos de ver) es muy sencilla la comprobación de que el método puede escribirse de la forma comentada anterormente.

No obstante, en métodos implícitos como el del trapecio es algo más complicado de comprobar.

En esta ocasión tenemos
\[y_{n+1}-y_n = \frac{h}{2}\left(f(x_n,y_n)+f(x_{n+1},y_{n+1}) \right)\]

Así tendríamos:
\[\phi_f (x_n,y_n) = \frac{h}{2}\left(f(x_n,y_n)+f(x_{n+1},y_{n+1}) \right) = \frac{1}{2}\left( f(x_n,y_n) + f(x_{n+1},y_n+h\phi_f(x_n,y_n)\right)\]

Donde el $\phi_f$ que buscamos sería la solución de esta ecuación.

La pregunta que debería surgir ahora en el lector es si esta $\phi_f$ está bien definida. Podemos ver que calcular $\phi_f$ es equivalente a encontrar un punto fijo de la aplicación:

\[F(\phi) = \frac{1}{2} \left(f(x_n,y_n)+f(x_{n+1},y_n+h\phi)\right)\]

que, para $h$ suficientemente pequeño cumple:
\begin{equation}
	\label{eq:F_contractiva}
	\left(\norm{F(\phi)-F(\tilde{\phi})} \leq \frac{Lh}{2}\norm{\phi- \tilde{\phi}}\right)
\end{equation}

es decir, la aplicación será contractiva y, por tanto, el problema del punto fijo tendrá solución.

\item \textbf{Método de Taylor}

\[y_{n+1}=y_n+hf(x_n,y_n)+\frac{h^2}{2}\left(f_x(x_n,y_n)+f_y(x_n,y_n)f(x_n,y_n)\right)\]

donde obtendríamos:
\[\phi_f = hf(x_n,y_n)+\frac{h^2}{2}\left(f_x(x_n,y_n)+f_y(x_n,y_n)f(x_n,y_n)\right)\]
\end{enumerate}
\end{example}

En general supondremos que $α_k=1$, eliminando así la arbitrariedad que surge del hecho de que podemos multiplicar a ambos lados por un mismo número distinto de 0 sin cambiar el método. Además, siempre tendremos que $α_0 \neq 0$ ó $\phi_f$ depende de forma no trivial\footnote{Es decir, aparece $y_n$ como argumento de $f$} de $y_n$.

La última propiedad nos permite excluir métodos como:
\[y_{n+2}-y_{n+1}=hf(x_{n+1},y_{n+1})\]
que es esencialmente de 1 paso y no de 2, y que puede escribirse como:
\[y_{n+1}-y_n = f(x_n,y_n)\]

Es decir, forzamos que nuestro método juegue con los subíndices más simples posibles de modo que siempre tendrá que haber algún $y_n$ en la ecuación.

\begin{prop}[Hipótesis sobre Métodos Numéricos]
Existen una serie de hipótesis que tomaremos de forma general sobre los métodos numéricos con los que trabajemos. Estas son:

\begin{enumerate}
\item $\phi_f$ es continua.

\item Existen constantes $h_0,L$ que pueden depender de $f$ tales que:
\[\norm{\phi_f(x_n,y_n,...,y_{n+k-1},h) - \phi_f(x_n,\tilde{y}_n,...,\tilde{y}_{n+k-1},h)}\leq L \sum_{j=0}^k \norm{y_{n+j}-\tilde{y_{n+j}}} \text{ si } 0 < h < h_0\]

\item $f= 0 \implies \phi_f=0$


\end{enumerate}
\end{prop}

\begin{example}
	Veamos a continuación que la \textbf{primera hipótesis} ($\phi_f$ continua) se cumple para el \textbf{método del trapecio}:

	Definimos la aplicación global contractiva (tal y como hemos visto en \ref{eq:F_contractiva}):
	\[F(\phi) = \frac{1}{2} \left[ f(x_n,y_n) + f(x_{n+1}, y_n+h\phi) \right]\]

	Y tomemos $\phi_f = \lim_{j\to∞} \phi_f^{[j]}$, donde $\phi_f^{[j]}$ viene dada por la sucesión $\phi_f^{[j]} = F\left(\phi_f^{[j-1]}\right)$ y $\phi^0 = 0$.

	\obs El método del trapecio es de 1 paso ($k=1$) y por tanto la función incremento solo depende de $x_n$, $y_n$ y $h$, es decir $\phi_f(x_n,y_n;h)$.

	Dado que $f$ es una función continua, tendremos que:
	\[\phi_f^{[1]}(x_n,y_n;h) = \frac{1}{2} \left[ f(x_n,y_n) + f(x_{n+1}, y_{n+1} + 0) \right]\]

	será continua. Como la composición de funciones continuas da como resultado otra función continua, tendremos que $\phi_f^{[2]}=F\left( \phi_f^{[1]} \right)$ es continua, y por tanto todas las $\phi_f^{[k]}(x_n,y_n;h)$ son continuas.

	Si logramos que la convergencia de las $\phi_f^{[j]}$ sea uniforme, tendremos que la sucesión convergería a una función $\phi_f$ continua (que es lo que dice la hipótesis).

	Expresamos $\phi_f^{[k]}$ como una suma telescópica:
	\begin{equation}
		\label{eq:suma_telesc_pto_fijo}
		\phi_f^{[k]} = \sum_{j=1}^k \phi_f^{[j]} - \phi_f^{[j-1]} + \phi_f^{[0]},\quad \phi_f^{[0]}=0
	\end{equation}

	Tomando la norma $L^∞$:
	\begin{equation*}
		\begin{multlined}[0.7\textwidth]
			\md{\phi_f^{[j]} - \phi_f^{[j-1]} }_{L^∞} = \md{F\left[\phi_f^{[j-1]}\right] - F\left[\phi_f^{[j-2]}\right] }_{L^∞}\\[1em]
			\shoveleft{\qquad \qquad \qquad \underbrace{≤}_{F\text{ contractiva}} \frac{Lh}{2} \md{\phi_f^{[j-1]} - \phi_f^{[j-2]}}_{L^∞}}\\
		\end{multlined}
	\end{equation*}

	Aplicando la desigualdad sucesivamente:
	\[\md{\phi_f^{[j]} - \phi_f^{[j-1]}}_{L^∞} ≤ \left(\frac{Lh}{2}\right)^{j-1} \md{\phi^{[1]}}_{L^∞}\]

	Puesto que $K = [a,b]×ℝ×(0,∞)$ es compacto:
	\[\md{\phi_f^{[j]} - \phi_f^{[j-1]}}_{L^∞} ≤ M_k\left(\frac{Lh}{2}\right)^{j-1}\]

	De modo que aplicando el test $M$ de Weierstrass, la convergencia de \ref{eq:suma_telesc_pto_fijo} es uniforme, y por tanto $\phi_f$ es continua.
\end{example}

\begin{example}
Veamos ahora que la \textbf{segunda hipótesis} se cumple para el \textbf{método del trapecio}.

Esta segunda hipótesis nos dice que existen constantes $h_0,L$ que pueden depender de $f$ tales que:
\[\norm{\phi_f(x_n,y_n,...,y_{n+k-1},h) - \phi_f(x_n,\tilde{y}_n,...,\tilde{y}_{n+k-1},h)}\leq L \sum_{j=0}^k \norm{y_{n+j}-\tilde{y_{n+j}}} \text{ si } 0 < h < h_0\]

Por definición, para el método del trapecio tenemos que $\phi_f$ es la solución de
\[\phi_f (x_n,y_n) = \frac{h}{2}\left(f(x_n,y_n)+f(x_{n+1},y_{n+1}) \right) = \frac{1}{2}\left( f(x_n,y_n) + f(x_{n+1},y_n+h\phi_f(x_n,y_n)\right)\]

Con la definición de $\phi_f$ podemos escribir
\[\norm{\phi_f(x_n,y_n,h)-\phi_f(x_n,\tilde{y}_n,h)}  \leq \]
\[\leq \frac{1}{2}\norm{f(x_n,y_n)-f(x_n,\tilde{y}_n)}+\frac{1}{2}\norm{f(x_{n+1},y_n+h\phi_f(x_n,y_n))-f(x_{n+1},\tilde{y}_n+h\phi_f(x_n,\tilde{y}_n))} \leq\footnote{por ser $f$ Lipschitz}\]
\[\leq L\norm{y_n-\tilde{y}_n}+\frac{Lh}{2}\norm{\phi_f(x_n,y_n,h)-\phi_f(x_n,\tilde{y}_n,h)}\]

Reordenando los términos de la inecuación podemos escribir:
\[\left(1-\frac{Lh}{2} \right)\norm{\phi_f(x_n,y_n,h)-\phi_f(x_n,\tilde{y}_n,h)} \leq L\norm{y_n-\tilde{y}_n}\]
\end{example}

\section{Consistencia}
Ahora que nos hemos familiarizado con el concepto de la función incremento, podemos ver la \textbf{definición formal de residuo}:

\begin{defn}[Residuo]
Dada la solución $y(x)$ del P.V.I.
\[\left\{ \begin{array}{l} y'=f(x,y) \\ y(a)=y_a \end{array}\right.\]
llamamos residuo del método numérico a:
\[R_n = \sum_{j=0}^Rα_jy(x_{n+1})-h\phi_f(x_n,y(x_n),h)\]
\end{defn}

Una vez que tenemos una definición formal del residuo, podemos definir la consistencia de un método:

\begin{defn}[Consistencia]
Decimos que un método numérico es \textbf{consistente} si
\[\lim_{h \to 0} \max_{0 \leq n \leq N-k} \frac{\norm{R_n}}{h}=0\]
\end{defn}

\begin{defn}[Orden de consistencia]
Decimos que un método numérico es \textbf{consistente de orden p} si para $f \in C^p$ se tiene que
\[\lim_{h \to 0} \max_{0 \leq n \leq N-k} \frac{\norm{R_n}}{h} = O(h^p)\]
\end{defn}

\begin{example}
\textbf{Consistencia del método del trapecio}

Para estudiar la consistencia lo primero que tenemos que hacer es calcular el residuo;

Hasta ahora habíamos considerado el residuo como:
\[Q_n = y(x_{n+1})-y(x_n)-\frac{h}{2}\left(  f(x_n,y(x_n)) +  f(x_{n+1},y(x_{n+1}))\right)\]
y con esto comprobábamos que $Q_n = O(h^3)$ con lo que tendríamos consistencia de orden 2.

Sin embargo, con la definición formal de residuo que acabamos de ver, tenemos:
\[R_n = y(x_{n+1})-y(x_n)-h\phi_f(x_n,y_n,h)\text{ donde } \phi_f(x_n,y_n,h) \text{ satisface }\]
\[\phi_f(x_n,y_n,h) = \frac{1}{2}\left( f(x_n,y(x_n))+f(x_{n+1},y(x_n)+h\phi_f(x_n,y(x_n))\right)\]

Vamos a comprobar que ambas definiciones son ``equivalentes'' en tanto en cuanto nos dan el mismo orden de consistencia. Para ello estudiaremos la diferencia entre ambas definiciones.

\[Q_n-R_n = h\left(\phi_f(x_n,y_n,h) -\frac{1}{2}\left(  f(x_n,y(x_n)) +  f(x_{n+1},y(x_{n+1}))\right) \right) =\]
sustituyendo $\phi_f(x_n,y_n,h)$ por la ecuación que lo define y simplificando nos queda:
\[=\frac{h}{2} \left(f(x_{n+1},y(x_n)+h\phi_f(x_n,y_n,h))-f(x_{n+1},y(x_{n+1})) \right)\]

Tomando valores absolutos y recordando que $f$ es Lipschitz llegamos a:
\[|Q_n-R_n| \leq \frac{h}{2}L|y(x_{n+1})-y(x_n)-h\phi_f(x_n,y_n,h)| = \frac{hL}{2}|R_n|\]
de donde podemos deducir que $Q_n$ y $R_n$ tienen el mismo orden de convergencia.
\end{example}

\section{Estabilidad}
\begin{defn}[0-estabilidad]
Un método numérico es 0-estable si para cada PVI existe una constante $C>0$ tal que para cada dos sucesiones $u_n,v_n$ que satisfacen\footnote{Es decir, que no satisfacen la descripción del método como función incremento pero casi}
\[\sum_{j=0}^kα_ju_{n+j} = h\phi_f(x_n,u_n,...,u_{n+k-1},h)+hδ_n\]
\[\sum_{j=0}^kα_jv_{n+j} = h\phi_f(x_n,v_n,...,v_{n+k-1},h)+hγ_n\]

con $0≤n≤N-k$, se verifica que:
\[\max_{k \leq n \leq N}\norm{u_n - v_n} \leq C\left( \max_{0 \leq n \leq k-1}\norm{v_n - u_n}+\max_{0 \leq n \leq N-k}\norm{δ_n - γ_n}\right)\]

O lo que es lo mismo, la diferencia está acotada por la suma de la diferencia entre los valores de arranque y los errores.
\end{defn}

\begin{prop}
Todos los métodos numéricos de un solo paso con $α_0=-1$ son 0-estables
\end{prop}
\begin{proof}
Si tenemos dos sucesiones:
\[u_{n+1}=u_n+h\phi_f(x_,y_n,h)+hδ_n\]
\[v_{n+1}=v_n+h\phi_f(x_,y_n,h)+hγ_n\]

Podemos restarlas y tomar valores absolutos obteniendo:
\[|u_{n+1}-v_{n+1}|\leq |u_n-v_n| + h |\phi_f(x_n,u_n,h)-\phi_f(x_n,v_n,h)|+h|δ_n-γ_n|\]

Y apoyándonos en que la función $\phi_f$ es Lipschitz en la segunda variable tenemos:
\[\underbrace{|u_{n+1}-v_{n+1}|}_{e_{n+1}}\leq \underbrace{|u_n-v_n| + hL|u_n-v_n|}_{(1+Lh)e_n}+hK \text{ siendo } K=\max_{0 \leq n \leq N-1}|δ_n-γ_n|\]

Ahora debemos acotar la recurrencia sobre las $e_n$ que acabamos de obtener. Podemos ver fácilmente que, de forma general:
\[e_n \leq (1+Lh)^ne_0+Kh\left( (1+Lh)^{n-1}+(1+Lh)^{n-2}+...+1\right)=\atop (1+Lh)^ne_0+Kh\frac{(1+Lh)^n-1}{Lh}=(1+Lh)^ne_0+\frac{K}{L}((1+Lh)^n-1)\]

\obs Hasta aquí, esta demostración no es del todo formal puesto que hemos sacado ``a ojo'' el término general de la recurrencia. La forma de demostrarlo correctamente pasa por aplicar inducción.

Nuetro objetivo, recordemos, era acotar el máximo de los $e_n$ para comprobar si se cumple la condición de 0-estabilidad. Vamos a ello:
\[\max_{1\leq k \leq N}e_n \leq (1+Lh)^{\frac{(b-a)}{h}}e_0 + K(b-a)e^{L(b-a)}\]

Con esto podemos concluir la demostación puesto que $e_n=|u_n-v_n|$ es el término general de la diferencia de las sucesiones, $e_0=|u_0-v_0|$ es la diferencia entre los valores de arranque en un método de un sólo paso, y la constante $C$ sería el máximo de $C_1=(b-a)e^{L(b-a)}$ y $C_2=e^{L(b-a)}$\footnote{Para poder deducir esta $C$ tenemos en cuenta que $(1+Lh)^{(b-a)h}$ es de la forma $e^{L(b-a)}$}
\end{proof}

\section{Relaciones entre consistencia, estabilidad y convergencia}
\begin{theorem}\label{theorem:consist_y_0estable_converge}
Si un método numérico es 0-estable y consistente entonces es convergente.
\end{theorem}
\begin{proof}
Escribimos el método de la forma:
\[\sum_{j=0}^kα_jy_{n+j} = h \phi_f(x_n,y_n,...,y_{n+k-1},h)\]

Por otro lado podemos escribir:
\[\sum_{j=0}^kα_jy(x_{n+j}) = h \phi_f(x_n,y(x_n),...,y(x_{n+k-1}),h) + R_n\]

Ahora, con las dos sucesiones que tenemos, $\left\{y_n\right\}_{n=0}^∞, \left\{y(x_n)\right\}_{n=0}^N$, sabiendo que el método es 0-estable y tomando $R_n=hδ_n$, podemos escribir:
\[\max_{k\leq n \leq N}\norm{y_n-y(x_n)} \leq C\left( \max_{0 \leq n \leq k-1} \norm{y_n-y(x_n)} + \max_{0 \leq n \leq N-k} \frac{\norm{R_n}}{h}\right)\]

Para demostrar la convergencia tenemos que ver que 
\[\lim_{h \to 0} \max_{0 \leq n \leq k-1} \norm{y_n-y(x_n)}= 0 \implies \lim_{h\to 0} \max_{k\leq n \leq N}\norm{(y_n-y(x_n)}=0 \]
que queda trivialmente demostrada apoyándonos en la desigualdad deducida a partir de la 0-estabilidad.
\end{proof}

\begin{theorem}
Si un método numérico es 0-estable y el residuo es de orden $p$, entonces el método numérico es convergente a orden $p$.

Es decir, consistencia a orden $p$ y 0-estabilidad garantizan convergencia a orden $p$
\end{theorem}

\begin{theorem}[Criterio de consistencia]\label{theorem:consist_iif_relaciones}
Un método numérico es consistente si y sólo si $\sum_{j=0}^kα_j=0$ y, además,
\[\phi_f(x,y(x),...,y(x),0)=\left( \sum_{j=0}^kjα_j\right)f(x,y(x))\]
\end{theorem}
\begin{proof}
Llevaremos a cabo la demostración viendo los dos sentidos de la implicación:
\begin{itemize}
\item $\implies$
Supongamos que el método es consistente. Consideramos el PVI trivial $y(x)=1$.

Puesto que $y'=f=0$, la tercera hipótesis de los métodos numéricos nos garantiza que $\phi_f=0$ por lo que podemos escribir el residuo como:
\[R_n = \sum_{j=0}^kα_jy(x_{n+j})\]
Puesto que $y(x)=1$ tenemos
\[ \frac{R_n}{h}=\frac{1}{h}\sum_{j=0}^kα_j\]

Pero, puesto que el método es consistente sabemos que el límite de $\frac{R_n}{h}$ tiende a 0, por tanto, tenemos que 
\[\lim_{h\to 0} \frac{1}{h}\sum_{j=0}^kα_j=0 \implies \sum_{j=0}^kα_j=0\]

\textbf{Demostremos ahora la segunda afirmación} del lado derecho de la equivalencia del teorema:

Consideramos ahora un método numérico general, nos olvidamos del caso concreto anterior. Ahora tendríamos que el residuo queda:
\[\frac{R_n}{h}=\frac{1}{h}\sum_{j=0}^kα_jy(x_{n+j})-\phi_f(x_n,y(x_n),…,y(x_{n+k-1}),h)\]

Por el teorema del valor medio tenemos que con $x_n ≤ ε_{n,j} ≤ x_{n+j}$:
\[y(x_{n+j})-y(x_n)=y'(ε_{n,j})(x_{n+j}-x_n) = y'(ε_{n,j})jh\]

Sustituyendo esto en la ecuación anterior tenemos:
\[\frac{R_n}{h}=\frac{1}{h}\sum_{j=0}^kα_j(y(x_n)+jhy'(ε_{j,n}))-\phi_f(x_n,y(x_n),…,y(x_{n+k-1}),h) = \]
\[ = \sum_{j=0}^k jα_jy'(ε_{n,j}) - \phi_f\left(x_n,y(x_n), y(x_n+h) , …, y(x_n + h(k-1)), h\right) = \]
\[ = \sum_{j=0}^k jα_jf(ε_{n,j}, y(ε_{n,j})) - \phi_f\left(x_n,y(x_n), y(x_n+h) , …, y(x_n + h(k-1)), h\right)\]

Tomando \underline{$n=0$}:

\[\frac{R_0}{h} = \sum_{j=0}^k jα_jf(ε_{0,j}, y(ε_{0,j})) - \phi_f\left(x_0,y(x_0), y(x_0+h) , …, y(x_0 + h(k-1)), h\right)\]

Usando la hipótesis de que nuestro MN es consistente:
\[\lim_{h\to0} \frac{R_0}{h} = 0 = f(x_0, y(x_0)) \sum_{j=0}^k jα_j - \phi_f\left(x_0,y(x_0), y(x_0) , …, y(x_0), 0\right) \]

Acabamos de demostrar que la segunda afirmación del teorema es cierta para $x=x_0$. Tomemos ahora $n=N-k$:
\[\sum_{j=0}^k jα_jf(ε_{N-k,j}, y(ε_{N-k,j})) - \phi_f\left(x_{N-k},y(x_{N-k}), y(x_{N-k}+h) , …, y(x_{N-k} + h(k-1)), h\right)\]

Procediendo igual que antes:
\[\lim_{h\to0} \frac{R_{N-k}}{h} = 0 = f(x_N, y(x_N)) \sum_{j=0}^k jα_j - \phi_f\left(x_N,y(x_N), y(x_N) , …, y(x_N), 0\right)\]

Como hemos probado que la igualdad es cierta para $N$ podemos alcanzar todo $x$, y por tanto la segunda afirmación del teorema quedaría demostrada.

\item $\Longleftarrow$

Por definición de residuo tenemos que
\[R_n=\sum_{j=0}^kα_jy(x_{n+j})-\phi_f(x_n,y(x_n),…,y(x_{n+k-1}),h)\]

Por el teorema del valor medio tenemos que con $x_n ≤ ε_{n,j} ≤ x_{n+j}$:
\[y(x_{n+j})-y(x_n)=y'(ε_{n,j})(x_{n+j}-x_n) = y'(ε_{n,j})jh\]

Sustituyendo el valor de $y(x_{n+j})$ en la fórmula del residuo tenemos:
\[R_n=\sum_{j=0}^k α_jy(x_n) + \sum_{j=0}^k jα_jhy'(ε_{j,n}) -h\phi_f(x_n,y(x_n),…,y(x_{n+k-1}),h) = \]
sabiendo que $\sum_{j=0}^kα_j=0$
\[ = \sum_{j=0}^k jα_jhy'(ε_{n,j}) - h\phi_f\left(x_n,y(x_n), y(x_{n+h}) , …, y(x_{n + h(k-1)}), h\right) \]

Finalmente, podemos escribir
\[\frac{R_n}{h} = \sum_{j=0}^k jα_jf(ε_{n,j}, y(ε_{n,j})) - \phi_f\left(x_n,y(x_n), y(x_{n+h}) , …, y(x_{n + h(k-1)}), h\right)\]

Ahora, despejamos la función $f$ de la segunda propiedad:
\[f(ε_{n,j},y(ε_{n,j})) = \frac{1}{\sum_{j=0}^kjα_j}\phi_f(ε_{n,j},y(ε_{n,j}),...,y(ε_{n,j}),h)\]

Si sustituimos en la fórmula del residuo el valor que $f$ que acabamos de calcular llegamos a:
\[\frac{R_n}{h} = \sum_{j=0}^k jα_j\left(\frac{1}{\sum_{j=0}^kjα_j}\phi_f(ε_{n,j},y(ε_{n,j}),...,y(ε_{n,j}),h)\right) - \atop \phi_f\left(x_n,y(x_n), y(x_{n+h}) , …, y(x_{n + h(k-1)}), h\right) =\]
\[ = \sum_{j=0}^k jα_j\left(\frac{\phi_f(ε_{n,j},y(ε_{n,j}),...,y(ε_{n,j}),h) - \phi_f\left(x_n,y(x_n), y(x_{n+h}) , …, y(x_{n + h(k-1)}), h\right)}{\sum_{j=0}^kjα_j}\right)\]

A partir de esta ecuación, podemos ver que si fijamos $n$ se cumple:
\[\lim_{h \to 0} \frac{R_n}{h} = 0 \text{ por continuidad de }\phi_f\]
por tanto, queda claro que el método es consistente.
\end{itemize}
\end{proof}

\begin{example}
	Veamos si el \textbf{método del punto medio es consistente} utilizando \ref{theorem:consist_iif_relaciones}:

	\[y_{n+2} = y_n + 2h f(x_{n+1},y_{n+1})\]
	\[α_0=-1, α_1=0, α_2=1 \implies \sum_{j=0}^2 α_j = 0\]
	Acabamos de ver que cumple la primera afirmación del lado derecho del teorema.
	\[\sum_{j=0}^2 jα_j = 2\]
	\[\phi_f(x_n,y_n,y_{n+1},h) = 2f(x_{n+1},y_{n+1}) = 2f(x_n+h, y_{n+1})\]
	Por tanto:
	\[\phi_f\left( x, y(x), y(x), h \right) = 2f(x+h,y(x)) \underset{h\to0}{\longrightarrow} \overbrace{2}^{\sum_{j=0}^2 jα_j}f(x,y(x))\]
	el método cumple con la segunda afirmación del lado derecho del teorema, y podemos afirmar que el método del punto medio es consistente.

	También podríamos haber resuelto el problema directamente ``a pelo'' escribiendo
	\[\frac{R_n}{h} = \frac{y(x_{n+2})-y(x_n)}{h}-2f(x_{n+1},y(x_{n+1})) = 2y'(ε_n)-2y'(x_{n+1})\]

	Y puesto que $y'=f$ y sabemos que $f$ es continua, y tanto $x_{n+1}$ como $ε_n$ tienden a $x_n$ cuando $h$ tiende a 0 por lo que es claro que
	\[\lim_{h \o 0} \frac{R_n}{h} = 0\]
\end{example}

\begin{obs}
\begin{itemize}
\item ¿Qué regularidad estamos pidiendo a $f(x,y(x))$?

Simplemente exigimos que $f\in \algb{C}$ y sea Lipschitz en la segunda variable

\item ¿Cuál es el orden de consistencia?

El criterio de consistencia que no nos ayuda en este caso. Si consideramos que $f\in \algb{C}^2$ entonces
\[y(x_{n+2})=y(x_n)+y'(x_n)2h + \frac{y''(x_n)}{2}(2h)^2 + O(h^3)\]
Por otro lado,
\[f(x_{n+1},y(x_{n+1}))=y'(x_{n+1}) = y'(x_n) + y''(x_n)2h + O(h^2)\]
multiplicando por $2h$ a ambos lados en la última ecuación y restándoselo a la primera tenemos:
\[y(x_{n+2})-y(x_n)-2hf(x_{n+1},y_{n+1})=O(h^3) \implies \frac{R_n}{h}=O(h^2)\]

Con lo que llegamos a que el método será constante, al menos, de orden 2
\end{itemize}
\end{obs}


Decimos que un método numérico satisface el \concept{criterio de la raíz} si todas las raíces del primer poliniomio característico:
\[ρ(ζ) = \sum_{j=0}^kα_jζ^j\]
tienen módulo menor o igual que 1 y las de módulo 1 son simples.

\begin{theorem}\label{theorem:criterio_raiz}
Un método numérico es 0-estable si y sólo si el método numérico satisface el criterio de la raíz.
\end{theorem}

\begin{example}
Consideremos el siguiente método numérico:
\[y_{n+2}+y_{n+1}-2y_n = h\left( 5f(x_{n+1},y_{n+1}) -2f(x_n,y_n)\right)\]

Vamos a estudiar su consistencia, para lo que comprobaremos que se satisfacen las dos propiedades del teorema \ref{theorem:consist_iif_relaciones}:
\begin{enumerate}
\item 
\[\sum_{j=0}^2α_j = 1+1-2 = 0\]

\item 
\[\phi_f(x_n,y_n,y_{n+1},h) = 5f(x_{n+1},y_{n+1}) -2f(x_n,y_n)\]
\[\phi_f(x,y(x),y(x),0) = 3f(x,y(x)) = \underbrace{\sum_{j=0}^2jα_j}_3f(x,y(x))\]
\end{enumerate}

Por tanto, queda claro que el método es \textbf{consistente}.

Consideramos ahora el problema 
\[y'(x)=0 \ (f=0)\]

Recordemos que un método numérico es 0-estable si para cada PVI existe una constante $C>0$ tal que para cada dos sucesiones $u_n,v_n$ que satisfacen:
\[\sum_{j=0}^kα_ju_{n+j} = h\phi_f(x_n,u_n,...,u_{n+k-1},h)+hδ_n\]
\[\sum_{j=0}^kα_jv_{n+j} = h\phi_f(x_n,v_n,...,v_{n+k-1},h)+hγ_n\]

con $0≤n≤N-k$, se verifica que:
\[\max_{k \leq n \leq N}\norm{u_n - v_n} \leq C\left( \max_{0 \leq n \leq k-1}\norm{v_n - u_n}+\max_{0 \leq n \leq N-k}\norm{δ_n - γ_n}\right)\]

Vamos a estudiar el criterio de la raíz sobre sobre este PVI.

El primer polinomio característico nos queda:
\[ρ(ζ) = ζ^2+ζ-2 \implies ζ_+=1 \y ζ_-=-2\]

También observar que, puesto que nuestro problema tiene $f=0$, se cumple:
\[\sum_{j=0}^2α_jy_{n+j} = y_{n+2}+y_{n+1}-2y_n = 0\]

Consideramos ahora la sucesión $u_n = ζ_-^n$ con lo que obtenemos:
\[\sum_{j=0}^2α_ju_{n+j}=ζ_-^n\sum_{j=0}^2α_jζ_-^j=0\]

Y definimos la sucesión $v_n=0$. 

Si volvemos a la definición de 0-estabilidad con estas dos sucesiones, vemos que las sucesiones $δ_n$ y $γ_n$ han de ser 0 (pues $u_n$ y $v_n$ satisfacen la relación de recurrencia). Por tanto, si queremos que se cumpla la condición de 0-estabilidad necesitamos que:
\[\max_{k \leq n \leq N}\norm{u_n - v_n} \leq C\left( \max_{0 \leq n \leq k-1}\norm{v_n - u_n}\right)\]
Pero, puesto que $\norm{u_n,v_n} = 2^n$ tenemos que $\max_{k \leq n \leq N}\norm{u_n - v_n} = 2^N $ y, además $\max_{0 \leq n \leq k-1}\norm{v_n - u_n} = 2$. 

Sustituyendo estos datos, tenemos que para que el método sea 0-estable tiene que cumplirse que:
\[2^N \leq 2C\]
Podemos observar que no hay ninguna $C$ que satisfaga esta desigualdad, por lo que podemos concluir que el método \textbf{no es 0-estable}.
\end{example}

A raíz de este ejemplo podemos extraer dos conclusiones:

\obs Las raíces del primer polinomio característico son importantes a la hora de estudiar la 0-estabilidad

\obs Existen métodos numéricos consistentes que no son 0-estables.

Una vez visto este ejemplo, que nos muestra la necesidad del teorema \ref{theorem:criterio_raiz}, procedemos a demostrarlo.
\begin{proof}
Como siempre, vamos a demostrar los dos sentidos de la implicación:
\begin{itemize}
\item $\implies$

Vamos a probar esta implicación comprobando que si no se satisface el criterio de la raíz, entonces no podemos tener 0-estabilidad ($a \implies b \equiv \neg b \implies \neg a$)

Consideramos el problema con $y'(x)=0$, lo que nos da $\sum_{j=0}^kα_jy_{n+j}$ = 0.

Tomamos ahora el primer polinomio característico:
\[ρ(ζ) = \sum_{j=0}^kα_jζ^j\]

Vamos a suponer que no se cumple el criterio de la raíz, lo cual puede ocurrir de dos formas diferentes, y ver que en esos casos no se satisface la condición de 0-estabilidad.
\begin{enumerate}
\item Supongamos que existe una solución del polinomio $ε∈ℂ$ tal que $|ε| >1$.

Tomamos ahora las sucesiones:
\[\left\{ \begin{array}{l}u_n = ε^n \implies \sum_{j=0}^kα_ju_{n+j} = 0 \implies δ_n = 0 \\
v_n = 0 \implies \sum_{j=1}^kα_jv_{n+j}=0 \implies γ_n=0 \end{array}\right.\]

Si el método fuese 0-estable, debería cumplirse que:
\[|ε|^N \leq C |ε|^{k-1}\]
Pero no existe ninguna $C$ que satisfaga la desigualdad, por lo que el método no es 0-estable.

\item Supongamos que existe una solución doble del polinomio $ε$ tal que $|ε|=1$. 

Por ser raíz doble tendremos que $p'(ε)=0$.

Tomamos ahora las sucesiones:
\[\left\{ \begin{array}{l}u_n = nε^n \implies \sum_{j=0}^kα_ju_{n+j} = nε^n\sum_{j=0}^kα_jε^j + ε^n\overbrace{\sum_{j=1}^kα_jjε^j}^{ρ'(ε)=0} = 0 \implies δ_n = 0 \\
v_n = 0 \implies \sum_{j=1}^kα_jv_{n+j}=0 \implies γ_n=0 \end{array}\right.\]


Nuevamente, vemos que desigualdad tiene que cumplirse para que el método se considere 0-estable:
\[N \leq C(k-1)\]
y vemos que, de igual forma que en el apartado anterior, el método no puede ser 0-estable.
\end{enumerate}

Así queda probado que si el método no satisface el criterio de la raíz, entonces no será 0-estable. Es decir, \textbf{si el método es 0-estable entonces satisface el criterio de la raíz}

\item $\Longleftarrow$

Vamos a demostrar ahora que si se satisface el criterio de la raíz entonces el método será 0-estable.

Para no complicar la notación, vamos a considerar problemas del tipo $y'(x)=f(x,y(x))$ con $\appl{y}{\real}{\real}$ y $\appl{f}{\real\times\real}{\real}$, aunque la demostración puede ser ``fácilmente'' extendida a problemas con $\appl{y}{\real}{\real^d}$

Con el problema que estamos considerando, tenemos:
\[\sum_{j=0}^k α_jy_{n+j} = h\phi_f(x_n,y_x,...,y_{n+k-1},h)\]
Y vamos a considerar el vector columna $Y_n=(y_{n+k-1},...,y_n)$ y escribimos:
\[Y_{n+1} = AY_n + h\cdot F(x_n,Y_n)\]

\[ A = \left(
	\begin{array}{c c c c c}
		-α_{k-1} & -α_{k-2} & … & α_1 & α_0\\
		1        & 0        & … & …   & …  \\
		0        & 1        & … & …   & …  \\
		\vdots   & \vdots   & \vdots & \vdots   & \vdots  \\
		…        & …        & … & 1   & 0  \\
	\end{array}
\right),
F(X_n,Y_n) = \left(
	\begin{array}{c}
		\phi_f(x_n,y_x,...,y_{n+k-1},h)\\
		0\\
		0\\
		\vdots\\
		0
	\end{array}
\right)
\]


Podemos comprobar fácilmente que la ecuación matricial que hemos escrito es equivalente al sistema de ecuaciones:
\[\left\{\begin{array}{l}y_{n+k}=\sum_{j=0}^k α_jy_{n+j} + h\phi_f(x_n,y_x,...,y_{n+k-1},h)\\
y_{n+k-2} = y_{n+k-2}\\
...\\
...\\
y_n = y_n\end{array}\right.\]

Una vez tenemos el sistema podemos escribir:
\[Y_n = A^nY_0 + h\sum_{l=0}^{n-1}A^{n-1-l}F(X_l,Y_l)\]
\[Y_{n+1} = A^{n+1}Y_0 + h\sum_{l=0}^nA^{n-l}F(x_l,Y_l) = A·A^nY_0+hF(x_n,Y_n) +h\sum_{l=0}^{n-1}A^{n-l}F(x_l,Y_l)=\]
\[=A\left(A^nY_0+h\sum_{l=0}^{n-1}A^{n-l-1}F(x_l,Y_l)\right)+hF(x_n,Y_n)=AY_n+hF(x_n,Y_n)\]

Consieramos ahora las sucesiones:
\[U_{n+1}=AU_n + hF(x_n,U_n)+hδ_n\]
\[V_{n+1}=AV_n + hF(x_n,V_n)+hγ_n\]

Estudiando su diferencia obtenemos:
\[U_n-V_n = A^n(U_0-V_0)+h\sum_{l=0}^{n-1}A^{n-1-l}\left(F(x_n,U_l)-F(x_n,V_l)\right) + h\sum_{l=0}^{n-1}A^{n-1-l}(δ_n-γ_n)\]

Supongamos que $|A^n|\leq M$. Entonces podremos escribir:
\[|U_n-V_n| \leq M|U_0-V_0| + hLM\sum_{l=0}^{n-1}|U_l-V_l| + \underbrace{h(n-1)}_{\leq (b-a)}\max_n\norm{δ_n-γ_n} \implies\]
\[\implies |U_n-V_n| \leq K + hLN \sum_{l=0}^{n-1} |U_l-V_l|\]

Sea $\Psi_n=K + hLN \sum_{l=0}^{n-1} |U_l-V_l|$ podemos reescribir la desigualdad anterior como
\[|U_n-V_n| \leq \Psi_n\]

y podemos observar que
\[\Psi_{n+1} - \Psi_n = hLM|U_n-V_n| \leq hLM\Psi_n \implies \Psi_{n+1} \leq (1+hLM)\Psi_n\implies \]
\[\implies \Psi_{n+1} \leq (1+hLM)^n\Psi_0\]

Finalmente podemos ver que:
\[|\Psi_n| \leq e^{(b-a)LM}\Psi_0\]
\[|U_n-V_n| \leq e^{(b-a)LM}\Psi_0\]
con lo que queda probada la 0-estabilidad.

\obs En la demostración hemos supuesto que $|A^n|\leq M$ sin haberlo demostrado por lo que, hasta que no lo probemos, no estará completada la demostración.

Vamos a probarlo.
\begin{proof}
El primer polinomio característico coincide con el polinomio característico de la matriz $A$ y, por tanto, las soluciones de $ρ(ζ)$ son los autovalores de $A$.

Por definición, si los autovalores de $A$ tienen módulo menor que 1 y el que tiene módulo 1 es simple, entonces $|A^n| \leq M \ \forall n$

\textbf{Se trata de un resultado algebraico que sale de los contenidos de la asignatura por lo que tampoco le daremos más importancia.}
\end{proof}
\end{itemize}
\end{proof}

\begin{theorem}[Teorema más importante del curso]
\[\text{ Consistencia } + \text{ 0-estabilidad } \iff \text{ Convergencia }\]
\end{theorem}
\begin{proof}
Como tantas otras veces, habría que demostrar los dos lados de la implicación.
\begin{itemize}
\item $\implies$

Ya está probado en el teorema \ref{theorem:consist_y_0estable_converge}

\item $\Longleftarrow$

Como puede resultar un poco lioso trataremos de recalcar en todo momento qué es lo que se está demostrando.

Queremos probar que:

\textcolor{blue}{\[\text{ Convergencia } \implies \text{ Consistencia } + \text{ 0-estabilidad } \]}

Vamos a hacer esta demostración por pasos. 

\begin{enumerate}
\item Primero demostraremos que 

\[\text{Convergencia} \implies \text{ criterio de la raíz }\implies \text{0-estabilidad}\]

Vamos a demostrar que si no se cumple el criterio de la raiz entonces no tendríamos convergencia, es decir,
\textcolor{blue}{\[\text{no criterio de la raiz} \implies \text{no convergencia}\]}
Como ya hemos visto anteriormente, la negación del criterio de la raíz se divide en dos partes:

\begin{enumerate}
\item Supongamos que existe una solución del polinomio $ε$ tal que $|ε| >1$.

Consideramos el PVI $y'=0$ con $y(0)=0$, con lo que la solución será $y(x)=0$.

Con este PVI tenemos las sucesiones:
\[\left\{ \begin{array}{l}y_n = hε^n \\
y(x_n) = 0\end{array}\right.\]
a partir de las cuales podemos escribir:

\[\max_{0\leq n \leq k-1} \norm{y(x_n)-y_n} = h |ε|^{k-1}\]
\[\max_{k \leq n \leq N} \norm{y(x_n)-y_n} = h|ε|^N=\frac{b-a}{N}|ε|^N\]

Recordemos que para tener convergencia necesitamos 
\[\lim_{h \to 0} \max_{0 \leq n \leq k-1} \norm{y_n-y(x_n)}= 0 \implies \lim_{h\to 0} \max_{k\leq n \leq N}\norm{(y_n-y(x_n))} = 0\]

Pero en nuestro caso, tenemos la igualdad de la izquierda pero no se cumple la igualdad asociada al límite de la derecha, puesto que $|ε|^N$ crece más rápido que el denominador $N$.

Por tanto, al no satisfacerse el criterio de la raíz, no tenemos convergencia.

\item Supongamos que existe una solución doble del polinomio $ε$ tal que $|ε|=1$. 

Por ser raíz doble tendremos que $ρ'(ε)=0$.

Si seguimos trabajando con el mismo PVI del apartado anterior, recordando la definición de función incremento, tenemos que
\[\sum_{j=0}^kα_jy_{n+j}=\phi_f = 0\]
de donde podemos obtener que $y_n = \sqrt{h}nε^n$ es raíz.

Para comprobar que es raíz debemos recordar que $ε$ es raíz del polinomio característico y de su derivada.

Teniendo en cuenta que $y(x_n)=0$ podemos ver que:
\[\max_{0\leq n \leq k-1} \norm{y(x_n)-y_n} = \sqrt{h}(k-1)\]
\[\max_{k \leq n \leq N} \norm{y(x_n)-y_n} = \sqrt{h}N\]

Recordemos que para tener convergencia necesitamos 
\[\lim_{h \to 0} \max_{0 \leq n \leq k-1} \norm{y_n-y(x_n)}= 0 \implies \lim_{h\to 0} \max_{k\leq n \leq N}\norm{(y_n-y(x_n))} = 0\]

Es sencillo de comprobar que la parte de la iqzuiqerda se cumple pero en la parte derecha tenemos:
\[\lim_{h\to 0} \max_{k\leq n \leq N}\norm{(y_n-y(x_n))} = \lim_{h\to 0} \frac{\sqrt{b-a}}{\sqrt{N}} N = \lim_{h\to 0} \sqrt{b-a}\sqrt{N} = \infty\]

Por lo que queda claro que el método no es convergente.
\end{enumerate}

Hasta aquí ya hemos probado que si el método no satisface el criterio de la raíz entonces tampoco será convergente. Es decir, tenemos que

\textcolor{ForestGreen}{\[\text{convergencia}\implies \text{criterio de la raíz}\]}

Pero el teorema \ref{theorem:criterio_raiz} nos dice que si satisface el criterio de la raíz, entonces será 0-estable, con lo que ya tenemos:
\textcolor{ForestGreen}{\[\text{convergencia}\implies \text{0-estabilidad}\]}

\item Ahora vamos a demostrar que
\textcolor{blue}{\[\text{Convergencia} \implies \text{consistencia}\]}

Para llevar a cabo esta demostración lo que haremos será probar las dos condiciones que nos garantizan la consistencia según el teorema \ref{theorem:consist_iif_relaciones}:
\begin{enumerate}
\item Probamos que
\[\sum_{j=0}^kα_j = 0\]

Para ello consideramos el PVI $y'(x)=0$ con dato inicial $y(0)=1$ del que sabemos la solución es $y(x)=1$.

Por definición sabemos que:
\[\sum_{j=0}^kα_jy_{n+k}=0\]
si tomo como valores de arranque $y_n=1$ para $n=0,...,k-1$ tenemos
\[\lim_{h \to 0} \max_{0 \leq n \leq k-1} \norm{y_n-y(x_n)} = 0\]
y por convergencia esto nos garantiza que 
\[\lim_{h \to 0} \max_{k \leq n \leq N} \norm{y_n-y(x_n)} = 0 \implies \lim_{h\to 0}y_n = 1\]

Por tanto tendríamos:
\[0 = \lim_{h \to 0} \sum_{j=0}^kα_jy_{n+j} = \sum_{j=0}^k α_j\lim_{h\to 0} y_{n+j} = \sum_{j=0}^kα_j\]

\item Ahora nos queda probar:
\[\phi_f(x,y(x),...,y(x),0)=\left( \sum_{j=0}^kjα_j\right)f(x,y(x))\]
prueba que realizaremos por reducción al absurdo.

Consideramos ahora un PVI general y supongamos que el método numérico no satisface la condición que queremos probar. Entonces
\[\phi_f(x,y(x),...,y(x),0)-\left( \sum_{j=0}^kjα_j\right)f(x,y(x)) = g(x)\]
donde $g(x)\neq 0$

Definamos ahora 
\[\tilde{f}(x,y) = f(x,y)+\frac{g(x)}{\sum_{j=0}^kjα_j}\]
\obs Sabemos que esta función está bien definida puesto que el denominador no puede ser 0 por el criterio de la raíz.

Ahora definimos:
\[\tilde{\phi}_{\tilde{f}}(x_n,y_n,...y_{n+k-1},h)=\phi_f(x_n,y_n,...,y_{n+k-1},h)\]

Con lo que podemos escribir:
\[\sum_{j=0}^kα_jy_{n+k} = h \tilde{\phi}_{\tilde{f}}(x_n,y_n,...y_{n+k-1},h)\]
que no es más que una ``copia'' de nuestro método numérico inicial:
\[\sum_{j=0}^kα_jy_{n+k} = h \phi_{f}(x_n,y_n,...y_{n+k-1},h)\]
del que sabemos es convergente y, por lo probado en el apartado anterior, 0-estable.

A partir de estas ecuaciones podemos escribir:
\[\tilde{\phi}_{\tilde{f}}(x_n,y(x),...y(x),0) =  \phi_{f}(x_n,y(x),...y(x),0) = g(x)+\sum_{j=0}^kα_{j}jf(x,y(x))=\]
donde la última igualdad se deriva de la suposición inicial de que el método no es convergente.
\[=\left( \sum_{j=0}^kjα_j\right)\tilde{f}(x,y(x))\]

Es decir, acabamos de comprobar que el método 
\[\sum_{j=0}^kα_jy_{n+k} = h \tilde{\phi}_{\tilde{f}}(x_n,y_n,...y_{n+k-1},h)\]
es convergente.

Puesto que el método es convergente, entonces $y_n$ debe converger a la solución de 
\[\tilde{y}'=\tilde{f}(x,\tilde{y}(x))\]

Por otro lado, también converge a la solución de
\[y'(x)=f(x,y(x))\]
pero $y(x)\neq \tilde{y}(x)$ puesto que las funciones son distintas, lo que supone una contradicción puesto que una sucesión no puede tener dos límites distintos de manera simultánea.

Por tanto tenemos que $g(x)=0$, es decir, el método ha de satisfacer la segunda condición necesaria para la consistentia.
\end{enumerate}
Finalmente tenemos que
\textcolor{ForestGreen}{\[\text{Convergencia} \implies \text{consistencia}\]}
\end{enumerate}
\end{itemize}

\end{proof}

\chapter{Laboratorio}
\input{tex/LaboratorioMnedo.tex}
\appendix

\chapter{Ejercicios}
\input{tex/EjerciciosMnedo.tex}
\printindex
\end{document}








