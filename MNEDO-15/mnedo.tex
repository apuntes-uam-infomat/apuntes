\documentclass{apuntes}

\title{MNEDO}
\author{Pedro Valero \& Jorge Martín}
\date{15/15 C1}
% Paquetes adicionales
\usepackage{tikztools}
\usepackage{tikz-3dplot}
\usepackage{textcomp}
\usepackage{tikz-qtree}
\usepackage{changepage}
\usepackage{colortbl}

\bibliographystyle{plainnat}

\usetikzlibrary{arrows}
% --------------------

\precompileTikz

\begin{document}
\pagestyle{plain}
\maketitle

\tableofcontents


\chapter{Preliminares}

\section{Problemas de valor inicial. (PVI)}

A lo largo de este curso estudiaremos problemas de valor inicial, que no son más que ecuaciones diferenciales ordinarias (EDOs) junto con un dato inicial, necesario para resolver adecuadamente la ecuación. Se trata de sistemas de la forma:
\[\begin{cases}
		y'(x)=f(x,y(x)) & x∈[a,b]\\
		y(a)=y_a
\end{cases}\]
done $y(x)$ es una función del tipo
\[\appl{y}{ℝ}{ℝ^d}\]
\[y(x)=(y_1(x), …, y_d(x))\]

Por tanto $f$ quedará definida como:
\[\appl{f}{ℝ×ℝ}{ℝ^d} \]
\[f(x,y) = (x,y(x))\]

Como notación muchas veces consideraremos $y(x)=y$, del mismo modo que $y(a)=y_a$ con $a$ un valor dado.

\begin{remark}
	En el curso trabajaremos con funciones $f∈C\left( [a,b] × ℝ^d \right)$
\end{remark}


\begin{defn}[Función Lipschitz]
	Una función $f$ será Lipschitz en la segunda variable si existe una constante $L$ tal que:
	\[∀x ∈ [a,b], ∀y_1,y_2 ∈ℝ^d \ \md{f(x,y_1) - f(x,y_2)} ≤ L\md{y_1-y_2}\]
\end{defn}

Vamos a ver en qué situaciones este tipo de problemas tienen solución única:

\begin{theorem}[Teorema de Picard]
	\label{TeoremaPicard}
	Sea $Ω=[a,b]×ℝ^d$, si $f$ es continua en $Ω$ y Lipschitz en la segunda variable; entonces \textbf{el PVI tiene una única solución} $y∈C'\big([a,b] × ℝ^d\big)$
\end{theorem}

\begin{proof}
	\textbf{Vamos a demostrar primero la existencia de la solución}:
	Para ello haremos uso de las iteradas de Picard, definidas mediante:

	\[y(x)=y_a+\int_a^x f(s,y(s))ds\]

	Si definimos la aplicación $T_y = y_a + \int_a^x f(s,y(s))ds$, encontrar la solución $y$ es equivalente a encontrar un punto fijo de dicha aplicación. Vamos a ello.

	\begin{enumerate}
		\item \textbf{Construimos la serie de funciones $\{ y_n(x) \}_{n=0}^∞$}:
		\[y_{n+1}(x) = y_a + \int_a^x f(s,y_n(s)) ds\]
		\[n=0,1,… \text{\ y \ } y_0(x)=y_a\]
		Cada función $y_n$ es continua (es una constante más una función continua).

		\item \textbf{Debemos probar que $∃y ∈ C([a,b])$ tal que}:
		\[y_n \longrightarrow y \text{\ uniformemente}\]
		Que es equivalente a decir que $\lim_{n \to ∞} \md{y_n - y}_{L^∞[a,b]} = 0$. Donde $\md{f}_{L^∞[a,b]} = \max_{x ∈ [a,b]}\md{f(x)}$, denota la norma infinito de $f$.
	\end{enumerate}

	Antes de continuar vamos a recordar el Test M de Weierstrass:

	\begin{theorem}[Test M de Weierstrass]
		\label{TestMWeierstrass}
		Sea $\{f_n\}_{n=0}^∞$ una sucesión de funciones definidas en un dominio $Ω$ con valores en $ℝ$, si existe una sucesión de números $M_n$ tal que:
		\begin{enumerate}
			\item $\abs{f_n} < M_n \text{,\ } ∀x∈Ω$
			\item $\sum_{n=0}^∞ M_n < ∞$
		\end{enumerate}

		Entonces existe una función $g$ tal que la sucesión:
		\[g_n(x) = \sum_{n=1}^N f_n\]

		converge uniformemente a $g$ en $x$
	\end{theorem}

	¿Cómo vamos a usar esto para conseguir ver que $y_n \rightarrow y$?.

	Primero vamos a reescribir $y_n$ como una suma telescópica, sabiendo que tomamos $y_a=y_0$:

	\[y_{n+1} = y_a + \sum_{k=0}^n y_{k+1} - y_k\]
	Para el caso $n=1$: $y_2 = y_a + (y_1 - y_0) + (y_2 - y_1) = y_2$.

	Ahora vamos a aplicar el test M de Weierstrass:

	Las $f_k$ que aparecen en el enunciado de \ref{TestMWeierstrass} serán $(y_{k+1} - y_k)$, si demuestro que $∃{M_k}$ tales que:

	\[\abs{y_{k+1}-y_k} < M_k \text{,\ } ∀x∈[a,b]\]
	\[\text{y que\ } \sum_k M_k < ∞ \text{, entonces } ∃y \text{ tal que:}\]
	\[y_n \rightarrow y \text{ uniformemente}\]

	Vamos a encontrar dichos $M_k$:

	\begin{lemma}
		\[\abs{y_{k+1} - y_k} ≤ \frac{C(y_a)}{L}\frac{(L(x-a))^{k+1}}{(k+1)!}\]
	\end{lemma}
	\begin{proof}
		Vamos a proceder a demostrar el lema usando inducción, así que vamos a ver que es cierto para $k=0$:

		\[\abs{y_1-y_0}=\abs{\int_a^x f(s,y_a) ds} ≤ \int_a^x \abs{f(s,y_a)} ds ≤\]
		\[≤ \md{f(x,y_n)}_{L^∞[a,b]} (x-a) = C(y_n) (x-a)\]

		Ahora supondremos que el lema es cierto para $k-1$ para poder demostrarlo para $k$:

		\[\abs{y_{k+1}-y_k} = \abs{\int_a^x f(s,y_k(s)) - \int_a^x f(s,y_{k-1}(s))} ds ≤\]
		\[≤ \int_a^x \abs{f(s,y_k(s)) - f(s,y_{k-1}(s))} ds \underbrace{≤}_{\mathclap{f \text{ Lipschitz en 2ª variable}}} L \int_a^x \abs{y_k(s) - y_{k-1}(s)} ds ≤\]
		\[\underbrace{≤}_{\text{hip. inducción}} L \int_a^x \frac{C(y_a)}{L} \frac{(L(s-a))^k}{k!} ds ≤ C(y_a)\frac{L^k}{k!}\int_a^x (s-a)^k ds ≤\]
		\[≤ C(y_a) \frac{L^k}{(k+1) k!}(x-a)^{k+1}\]
	\end{proof}

	Volviendo a la demostración de \ref{TeoremaPicard} recordemos que queríamos encontrar los $M_k$ necesarios para ver la convergencia de nuestras $y_k$. Gracias al lema que acabamos de demostrar y al test M de Weierstrass \ref{TestMWeierstrass} podemos afirmar:

	\[\abs{y_{k+1}-y_k} ≤ \frac{C(y_a)}{L} ≤ \frac{C(y_a)}{L}\frac{(L(x-a))^{k+1}}{(k+1)!} = M_k\]

	El siguiente paso es verificar la segunda hipótesis de \ref{TestMWeierstrass}, es decir, que la suma infinta de las $M_k$ está acotada:

	\[M_k = \frac{C(y_a)}{L}\frac{[L(x-a)]^{k+1}}{(k+1)!} \underbrace{=}_{s=L(x-a)} \frac{C(y_a)}{L}\frac{s^{k+1}}{(k+1)!}\]

	Sabiendo que $e^x = \sum_{k=0}^∞ \frac{x^k}{k!}$:

	\[\sum_{k=0}^∞ M_k = \frac{-C(y_a)}{L} (1 - \frac{C(y_a)}{L} \sum_{k=0}^∞ \frac{s^k}{k!}) = \frac{C(y_a)}{L} (e^s - 1) < ∞\]

	Luego aplicando el test M de Weierstrass \ref{TestMWeierstrass}:
	\[∃y \text{ tal que: } y_n\rightarrow y \text{ uniformemente en} [a,b]\]

	\begin{enumerate}
		\setcounter{enumi}{3}
		\item Por tanto hemos probado que $∃y$ tal que $y_n \rightarrow y$ uniformemente, y por lo tanto $y∈C([a,c])$ (sabemos que la convergencia uniforme de una sucesión de funciones continuas hace que la \textbf{función a la que se converge sea continua}).

		\item
			\[y_{n+1} = y_a + \int_a^x f(s,y_n(s))ds \underbrace{\longrightarrow}_{n \rightarrow ∞} y = y_a + \int_a^x f(s,y(s))ds\]
			Como $f∈C([a,b])$ y $y_n$ son continuas, está claro que $y$ es continua. De modo que \textbf{hemos demostrado que existe solución} $y∈C([a,b])$.
	\end{enumerate}

	Por últimos debemos \textbf{demostrar la unicidad} de la solución. Lo haremos por reducción al absurdo.

	Supongamos que existen dos soluciones distintas $y,\tilde{y} ∈ C([a,b])$:
	\[y(x) = y_a + \int_a^x f(s,y(s))ds\]
	\[\tilde{y}(x) = y_a + \int_a^x f(s,\tilde{y}(s))ds\]
	\[\abs{y(x) - \tilde{y}(x)} = \int_a^x \abs{f(s,y(s)) - f(s,\tilde{y}(s))} ds ≤\]
	\[≤ L \int_a^x \abs{y(s) - \tilde{y}(s)} ds \]

	Definimos $g(x)=\int_a^x \abs{y(s) - \tilde{y}(s)} ds$:

	\begin{equation}
		\label{eqDemPicard}
		g'(x) = |y(x) - \tilde{y}(x)| \leq Lg(x); \ g'(x)e^{-L(x-a)} \leq Lg(x)e^{-L(x-a)}
	\end{equation}

	Seguimos:
	\[\left( g(x)e^{-L(x-a)} \right)^{'} = g'(x) e^{-L(x-a)} - Lg(x) e^{-L(x-a)}\]
	\[g'(x) e^{-L(x-a)} = \left( g(x)e^{-L(x-a)} \right)^{'} + Lg(x) e^{-L(x-a)}\]

	Usando \ref{eqDemPicard}:
	\[\left( g(x)e^{-L(x-a)} \right)^{'} + Lg(x) e^{-L(x-a)} ≤ Lg(x) e^{-L(x-a)}\]
	\[\left( g(x)e^{-L(x-a)} \right)^{'} ≤ 0\]

	Si integramos a ambos lados entre $a$ y $X$:
	\[g(X) e^{-L(x-a)} ≤ g(a) = 0 \text{ por como hemos definido } g\]

	Por tanto:
	\[g(X) = \int_a^X \abs{\tilde{y}(x) - y(x)} dx = 0 \implies \tilde{y}=y\]

	Y hemos demostrado la unicidad de la solución.
\end{proof}

Lo siguiente que cabe preguntarse es cómo varía la solución de un problema de valor inicial (PVI) cuando variamos el dato inicial. Y la respuesta es que al variarlo muy poco, la solución también se altera muy poco.

\begin{theorem}[Continuidad con respecto al dato inicial]
	Sea $y' = f(x,y(x))$ un problema de valor inicial (PVI) con dos datos iniciales distintos $y_a, \tilde{y}_a$, con $f$ Lipschitz en la segunda variable y continua en $Ω=[a,b]×ℝ^d$:
	\[\md{y(x)-\tilde{y}(x)}_{L^∞[a,b]} ≤ \abs{y_a - \tilde{y}_a} e^{L(b-a)}\]
\end{theorem}
\begin{proof}
	Tomando como soluciones las que se obtienen con la iterada de Picard:
	\label{ineq:argumento_cont_dato_inicial}
	\[y(x)-\tilde{y}(x)=y_a-\tilde{y}_a + \int_a^x (f(s,y(s)) - f(s,\tilde{y}(s)))ds\]
	\[\abs{y - \tilde{y}} ≤ \abs{y_a - \tilde{y}_a} + \int_a^x \abs{f(s,y(s)) - f(s,\tilde{y}(s))} ds ≤\]
	\[≤ \underbrace{ \abs{y_a - \tilde{y}_a} + L\int_a^x \abs{y(s)-\tilde{y}(s)} ds}_{g(x)} \]

	Por tanto $g'(x) = L\abs{y(x) - \tilde{y}(x)}$. Si sobre esta última igualdad realizamos el procedimiento del factor integrante que hemos llevado a cabo al demostrar la unicidad en el teorema de Picard \ref{TeoremaPicard} llegamos a:

	\[g(x)e^{-L(x-a)} ≤ \abs{\tilde{y}_a - y_a} \implies g(x) ≤ \abs{\tilde{y}_a - y_a}e^{L(x-a)}\]

	Por último:
	\[\abs{y_(x) - \tilde{y}(x)} ≤ g(x)\]
	\[y(x) - \tilde{y}(x) ≤ \abs{\tilde{y}_a - y_a}e^{L(x-a)}\]
	\[\md{y - \tilde{y}}_{L^∞[a,b]} ≤ \abs{y_a - \tilde{y}_a} e^{L(b-a)}\]
\end{proof}

Vamos ahora a ver ejemplos donde aplicar el teorema de Picard \ref{TeoremaPicard}:

\begin{example}
	Para el problema de valor inicial (PVI) $y'(x)=\sqrt{x}$, $y(0)=0$; tenemos $f(x,y(x))=\sqrt(y)$. Esta $f$ no es Lipschitz y por tanto no cumple las hipótesis del teorema de Picard \ref{TeoremaPicard}, de hecho existen dos soluciones distintas:
	\[y_1(x) = 0\]
	\[y_2(x) = x^2\]
\end{example}


\section{Algunos ejemplos de métodos numéricos para PVI.}
Recordemos que el objetivo de este curso es encontrar soluciones a problemas de valor inicial (PVI) del tipo:

\[y'(x) = f(x,y(x)) \text{, } x∈[a,b]\]
\[y(a)=y_a\]

Lo que queremos ver es cómo se comporta y(x) sin saber encontrar su solución. Para ello, como ya hemos visto, podemos usar las iteradas de Picard:

\[y_{n-1}(x) = y_a + \int_a^x f(s,y_n(s)) ds\]
\[y_0(x) = y_a\]

\begin{remark}
Durante este curso, al emplear los diferentes métodos numéricos usaremos versiones discretizadas de las funciones, es decir, el conjunto de valores que tomará una función $y(x)$ será sobre una serie de valores ${x_n}_{n=0}^N$ dentro del intervalo $[a,b]$, donde $x_0=a$ y $x_N=b$.
\end{remark}
En este curso las sucesivas muestras serán equidistantes, siendo $x_{n+1}-x_n=h=\frac{b-a}{N}$.

A continuación mencionaremos algunos métodos numéricos que estudiaremos más adelante:
\begin{itemize}
	\item \textbf{Método de Euler}
	\[y'(x) = f(x,y(x))\]
	\[y'(x_n) = f(x_n,y(x_n))\]
	Pero dado que $x$ toma valores discretos:
	\[\frac{y(x_{n+1}) - y(x_n)}{h} = \frac{y_{n+1} - y_n}{h} = y'(x_n) = f(x_n,y(x))\]
	Así la fórmula de recurrencia del método de Euler queda como:
	\[y_{n+1} = y_n + h·f(x,y_n)\]
	\[y_0=y_a\]

	\item \textbf{Desarrollo en serie de Taylor}
	\[y(x_{n+1}) = y(x_n+h) = y(x_n) + h·y'(x_n) + R_n =\]
	\[= y(x_n) + h·f(x_n,y(x_n)) + R_n\]

	\item \textbf{Método leap-frog}
	\[y(x_{n+2}) - y(x_n) = \int_{x_n}^{x_{n+2}} y'(x) = \int_{x_n}^{x_{n+2}} f(x,y(x))\dif x \approx (x_{n+2}-x_n)f(x_{n+1},y(x_{n+1}))\]
	donde la aproximación se ha realizado empleando la regla del punto medio.

	Finalmente nos queda:
	\[y(x_{n+2}) - y(x_n) = 2h f(x_{n+1},y(x_{n+1}))\]
\end{itemize}

Antes de continuar, veamos un par de conceptos que serán necesarios a lo largo de este curso.

\begin{defn}[Método numérico de $k$ pasos]
Se dice que un método numérico es de $k$ pasos si para conocer $y_{n+k}$ necesitamos conocer los $k$ valores: $y_{n}, y_{n+1},... y_{n+k-1}$
\end{defn}

\begin{defn}[Método numérico convergente]
Se dice que un método numérico es convergente si para todo problema de valor inicial se tiene que:
\[\lim_{N \to \infty} \max_{0 \leq n \leq k-1}\norm{y(x_n)-y_n}= 0\implies \max_{k \leq n \leq N} \norm{y(x_n)-y_n}=0\]

$k$ será el número de pasos del método numérico, es decir, para el método de Euler tendremos $k=1$, mientras que para el método leap-frog tendremos $k=2$.

Evidentemente no nos referimos a cualquier problema imaginable sino a todos aquellos problemas de valor inicial en los que la función $f$ sea continua y Lipschitz en la segunda variable, pues esos son lso únicos problemas con los que hemos trabajado hasta ahora.
\end{defn}

\chapter{Algunos Métodos Numéricos para EDOs}
\section{Método de Euler}

Dado un problema de valor inicial del tipo:
\[y'(x) = f(x,y(x))\]
\[y'(x_n) = f(x_n,y(x_n))\]
el método de Euler nos permite aproximarnos a la solución a través de la siguiente relación de recurrencia:
\[y_{n+1} = y_n + h·f(x,y_n)\]
\[y_0=y_a\]

\begin{theorem}[El método de Euler es convergente]

Sea $y(x)$ la solución de un problema de valor inicial y sean $\{y_n\}$ los valores que vamos aproximando en cada iteración del método de Euler, se cumple que $\{y_n\}$ converge a $y(x)$
\end{theorem}
\begin{proof}
Al trabajar con este método tenemos la relación de recurrencia:
\[y_{n+1} = y_n + h f(x_n, y_n)\]

Para empezar vamos a definir el residuo que no es más que la diferencia entre el valor aproximado $y_n$ obtenido al aplicar el método en cuestión y el valor real $y(x_n)$. Así, podemos escribir:
\[y(x_{n+1})=y(x_n)+hf(x_n,y(x_n))+R_n \text{ siendo } R_n \text{ el residuo }\]

Despejando podemos ver que
\[R_n = \underbrace{y(x_{n+1})-y(x_n)}_{hy'}-h\underbrace{f(x_n,y(x_n))}_{y'} = \frac{h^2}{2}y''(ε)\]


Según la definición de convergencia, para demostrar que la sucesión converge basta con ver que:
\[|y(x_n)-y_n | = 0 \ \implies \lim_{N \to \infty} \max_n |y(x_n)-y_n| = 0\]


\begin{itemize}
\item Basándonos en la condición de Lipschitz:
\[|y(x_{n+1})-y_{n+1}| \leq (1+Lh)|y(x_n)-y_n|+K \text{ siendo } e_{n+1} \leq (1+Lh)e_n + K \text{ con } K = \max_n |R_n|\]

\begin{proof}
\[e_n = |y(x_n)-y_n| \implies e_{n+1} \leq (1+Lh)e_n+K \implies e_n \leq (1+Lh)^n e_0 + K \frac{1-(1+hL)^n}{1-(1+kL)}\]
que simplificando nos queda:
\[e_n \leq (1+Lh)^n e_0 + \frac{K}{h}\frac{(1+hL)^n-1}{L}\]

Esta fórmula puede ser deducida de forma sencilla estudiando como se comporta el error en cada iteración, con lo que veremos que no es más que una sucesión geométrica. Otra opción posible es hacer la demostración de la misma por inducción.

Ambos procedimientos son muy sencillos y se dejan como ejercicio para el lector.
\end{proof}

Gracias a esto tenemos demostrada la \textbf{estabilidad} pues
\[|y(x_n)-y_n| \leq (1+hL)^n |y(x_0)-y_0| + \frac{(1+hL)^{n+1}-1}{L}\cdot \max_n \frac{|R_n|}{h}\]
%
%Por otro lado tenemos que:
%\[\lim_{h \to 0} \frac{|R_n|}{h} = 0\]
%con lo que vemos que el residuo lo tenemos controlado


%\item Ahora vamos a demostrar que
%\[|y(x_n) - y_n| \leq (1+Lh)^N |y(x_n)-y_0|+K\frac{(1+Lh)^N-1}{Lh}\]
%donde $N$ es el número total de $y_n$ que estamos tomando al aplicar el método.
%
%Para ello basta con ver que
%\[\max_n |y(x_n)-y_n| \leq (1+Lh)^N|y(x_n)-y_0| + K\frac{(1+Lh)^N-1}{Lh}\]
%
%Por otro lado tenemos que
%\[h = \frac{b-a}{N}\]
%con lo que podemos escribir la parte derecha de la desigualdad que queremos probar %como:%
%\[ \l%eft( 1+\frac{L(b-a)}{N}\right)^N|y(x_n)-y_0| + K \frac{\left(1+\frac{L(b-a)}{N%} %\righ%t)^N-1}{Lh}\]%
%
%Además sabemos que
%\[\lim_{N \to \infty} \max_n |y(x_n)-y_0| \leq e^{L(b-a)} |y(x_n)-y_0| + \frac{e^{L(b-a)}-1}{L}\lim_{h \to 0} \frac{K}{h}\]

\item \textbf{Control del residuo}

El residuo puede escribirse como:
\[R_n = y(x_{n+1})-y(x_n)-hy'(x_n) = \frac{h^2}{2}y''(ε)\]

Si logramos probar que
\[\max_n \frac{|R_n|}{h} \leq C h\]
tendríamos garantizado que el residuo es 0.

Sin embargo esto no es tan sencillo, puesto que para poder acotar el máximo como hemos queremos nos apoyamos en que la derivada segunda, $y''$, está acotada módulo infinito por una constance $C$. Pero nosotros no sabemos nada acerca de nuestra solución y, por tanto, no podemos realizar este tipo de asunciones.

Pero esto puede arreglarse fácilmente. Sabemos que
\[\frac{R_n}{h} = \frac{y(x_{n+1})-y(x_n)}{h} = y'(x_n) = y'(ε)-y'(x_n) \text{ siendo } x_n \leq ε \leq x_{n+1}\]
A partir de aquí vemos fácilmente que
\[\lim_{h\to 0} \frac{|R_n|}{h} = 0\]

Cuando $h \rightarrow 0$ estrechamos las distancias entre los $x_n$ (es decir $x_{n+1} \rightarrow x_n$). De esta forma se acaba consiguiendo que $ε$ (que inicialmente se encontraba en el intervalo $[x_n,x_{n+1}]$) pase a ser $x_n$, y así $y'(ε)-y'(x_n) = y'(x_n)-y'(x_n) = 0$.

\end{itemize}
\end{proof}

\obs En la prueba hemos evitado apoyarnos en información acerca de la derivada segunda de la solución $y$, pero en realidad sí que podríamos haberlo hecho:

\[
	\text{PVI} :
		\begin{cases}
			y'(x) = f(x,y(x)) \\
			f ∈ C^1([a,b]×ℝ^d)
		\end{cases}
\]

Si aplicamos el teorema de Picard \ref{TeoremaPicard} obtenemos que la solución única $y$ es $C^1$, por tanto $f$ compuesta con $y$ (nuestra $f(x,y(x))$) será también $C^1$; y por tanto $y' ∈ C^1$. Pero si tenemos que $y'$ es $C^1$, $y$ será $C^2$.

De modo que podemos obtener acotaciones para $y''$:
\[y''(x)=f_x(x,y(x)) + f_y(x,y(x))·y'(x) = f_x(x,y(x)) + f_y(x,y(x))·f(x,y(x))\]
\[\implies \md{y''}_{L^∞[a,b]} ≤ C_f\]

Con $C_f$ una constante que depende de la norma $C^1 \left( [a,b]×ℝ^d \right)$ de $f$.

\subsection{Orden del método de Euler}
\begin{defn}[Método numérico de orden p]
	Se dice que un método numérico es convergente de orden $p$ si $p$ es el mayor de los enteros tales que $f \in C^p$ y
	\[\max_{0 \leq n \leq k-1}\norm{y(x_n)-y_n)}= O(h^p)\implies \max_{k \leq n \leq N} \norm{y(x_n)-y_n}=O(h^p)\]
\end{defn}

Recordemos las definiciones de `O' y `o'

\begin{defn}[O grande]
Decimos que $f= O(g)$ en $x \to x_0$ si
\[\exists k,r > 0 \tq ∀x,\ 0 < |x-x_0|< r \text{ se tiene } |f(x)| \leq k |g(x)|\]
\end{defn}

\begin{defn}[O pequeña]
Decimos que $f=o(g)$ en $x \to x_0$ si

\[\exists r > 0 \tq ∀x,\ 0 < |x-x_0|<r \text{ tal que, donde } g \text{ no se anule, tenemos} \lim_{x\to x_0} \frac{f(x)}{g(x)}=0\]

\end{defn}

%\obs En la práctica $\max_{k≤n≤N} \md{y(x_n) - y(x_n)} = O(h^p)$ lo tomaremos como:
%\[ \max_{k≤n≤N} \md{y(x_n) - y(x_n)} ≤ C·h^p \]

\obs En la práctica $\max_{k≤n≤N} \md{y(x_n) - y(x_n)} = o(h^p)$ lo tomaremos como:
\[ \max_{k≤n≤N} \md{y(x_n) - y(x_n)} ≤ c·h^s,\ s>p,\ s∈ℝ \]

Una vez vistas estas definiciones, supongamos que tenemos $f \in C [\real \times \real^d]$. En este caso ya hemos visto que el orden de convergencia del método de Euler es al menos 1.

Para poder confirmar que el método es de orden 1, sólo nos queda comprobar que el orden no puede ser 2. Para comprobarlo nos basta con ver un ejemplo en el que no se cumpla:
\begin{example}
Tomamos el caso $y'(x)=x, \ x \in [0,1]$ con valor inicial $y(0)=0$. En este caso sabemos que la solución es $y(x)=\frac{1}{2}x^2$

Vamos a ver que ocurre si aplicamos el método de Euler. En este caso, tendríamos la relación de recurrencia:
\[y_{n+1} = y_n + h x_n\]
con lo que obtendríamos la sucesión de funciones:
\[y_1 = 0 \; ; \; y_2 = 0 + hx_1 =h^2 \; ; \; y_3 = h^2+2h^2=h^2(1+2+3) \; ; \; ...\]

De donde podemos deducir la fórmula general para $y_n$:

\[y_n = h^2\sum_{i=1}^{n-1} i = h^2\frac{n(n-1)}{2} \underbrace{=}_{x_n=n·h} =\frac{x_n^2-x_n·h}{2}\]


Así podemos ver que:
\[|y(x_n)-y_n| = \left|\frac{x_n^2}{2}-\frac{x_n^2}{2}+\frac{x_nh}{2} \right| = \frac{x_nh}{2} \implies \frac{1}{h}|y(x_n)-y_n|  = \frac{x_n}{2}\]

Por tanto el método de Euler no es convergente de orden 2, pues:
\[\max_n \md{y(x_n)-x_n} = \frac{1}{2}·h \overbrace{\implies}^{\text{por def.}}\text{es convergente de orden 1}\]
\end{example}



\textbf{Esperemos que hayáis disfrutado hasta aquí porque nos disponemos a entrar en la parte aburrida del curso.}


\chapter{Algunos Métodos Numéricos para Ecuaciones Diferenciales Ordinarias}
De aquí en adelante tomaremos $d=1$, es decir:
\[\appl{y}{ℝ}{ℝ}\]
\[\appl{f}{ℝ×ℝ}{ℝ}\]

\section{Métodos de Taylor}
Como siempre, tenemos que $y'(x)=f(x,y(x))$ y debemos buscar una relación entre $y(x_{n+1})$ y los $y(x_m)$ con $m < n+1$.

La idea del método de Taylor es emplear la relación:
\[y(x_{n+1})=y(x_n)+hf(x_n,y(x_n))+\frac{h^2}{2}y''(x_n)+R_n\]

Pero tenemos el problema de que no conocemos la derivada segudna de $y$ puesto que no conocemos $y$ (es precisamente eso lo que queremos ser capaces de estimar). Pero lo que podemos hacer es definir $y''$ apoyándonos en la regla de la cadena, con lo que obtenemos:
\[y''(x)=(\partial x f)(x,y(x))+(\partial y f)(x,y(x))y'(x)=f_x(x,y(x))+f_y(x,y(x))f(x,y(x)\]

con lo que el método de Taylor nos queda, finalmente:
\[y_{n+1} = y_n + hf(x_n,y_n)+\frac{h^2}{2}\left(f_x(x_n,y_n)+f(x_n, y_n)f_y(x_n,y_n)\right)\]

\begin{remark}
En la práctica estos métodos no se utiliza nunca puesto que no son demasido eficaces.
\end{remark}

Al hablar de estos métodos en plural hacemos referencia a las diferentes estimaciones posibles, pues podemos usar un desarrollo de Taylor de grado superior, lo que añade complejidad a la fórmula pero aproxima mejor el resultado, pues hace el residuo cada vez más pequeño.


\section{Método del trapecio}
En esta ocasión escribimos:
\[y(x_{n+1})=y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n)+ h\cdot \frac{f(x_n,y(x_n))+f(x_{n+1},y(x_{n+1}))}{2}  \]

Este método presenta una difernecia radical respecto a los demás, pues el objetivo del método es calcular $y_{x+1}$ en lugar de $y_n$.

Una forma de ver este método es definiendo el operador $T$ de la siguiente forma:
\[T(y) = y_n + \frac{h}{2} \left(f(x_n,y_n)+f(x_{n+1}, y) \right)\]
con lo que el problema a resolver se reduce a encontrar un punto fijo de este operador, es decir, una función $y$ tal que
\[T(y)=y\]

Podemos ver bajo qué condiciones la función es Lipchitz y, y por tanto tiene solución. Para ello vemos que
\[|T(y) - T(\tilde{y})| \leq \frac{h}{2} \left( |f(x_{n+1}, y) - f(x_{n+1},\tilde{y})|\right) \leq \frac{Lh}{2}\left( |y-\tilde{y}|\right)\]

Por tanto, la recurrencia tendrá solución siempre que $\frac{Lh}{2} \leq 1$

\section{Métodos basados en cuadraturas}
Ya hemos visto un método de este tipo, \textbf{el método de Euler}, en el que se define:
\[y(x_{n+1}) -y(x_n) = \int_{x_n}^{x_{n+1}} y'(x)\dif x = \int_{x_n}^{x_{n+1}} f(x_n,y(x_n)) \dif x \approx f(x_n,y(x_n))h \implies \]
\[\implies y(x_{n+1}) =y(x_n)+f(x_n,y(x_n))h\]

\subsection{Método Predictor Corrector Euler-trapecio}
En este método partimos de la fórmula del trapecio
\[y_{n+1} = y_n + \frac{h}{2} \left(f(x_n,y_n)+f(x_{n+1}, y^*_{n+1}) \right)\]

Lo que vamos a hacer ahora es aproximar el $y^*_{n+1}$, empleado a la derecha de la fórmula, por el método de Euler:
\[y^*_{n+1} = y_n + h f(x_n,y_n)\]

Una vez hemos empleado el método de Euler para hacer una predicción de $y^*_{n+1}$ empleamos el método del trapecio para corregir el valor.

\subsection{Método de Euler modificado}
Este método emplea la relación:
\[y(x_{n+1}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n) + h f\left(x+\frac{h}{2},y\left(x+\frac{h}{2}\right)\right)\]


El problema que tenemos ahora es que necesitamos aproximar $y(x+\frac{h}{2})$ (ya que $x+\frac{h}{2}$ no pertenece a nuestro mallado $\{x_n\}$), cosa que haremos mediante la ecuación:
\[y(x+\frac{h}{2}) = y(x_n)+\frac{h}{2}f(x_n, y(x_n))\]

Este método es realmente importante pues es el padre de una serie de métodos denominados \textbf{Métodos de Runge-Kutta}, que veremos a continuación

\section{Métodos de Runge-Kutta}
Estos métodos se caracterizan porque vamos a empezar calculando una serie de valores $K_i$ como sigue:
\[K_i = f\left(x_n+c_ih, y_n+h \sum_{j=1}^ia_{ij}K_j\right)\]
\[y_{n+1} = y_n+ h \sum_{i=1}^s b_i K_i\]

La idea de este método es que
\[y(x_{n+1}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n)+h \sum_{j}^{s} b_j f(x_n+hc_j, y(x_n+hc_j))\]

Las diferentes formas de aproximar la función $y(x_n+hc_j)$ nos dan las diferentes $K_i$ empleadas en la definición del método.

De entre los diferentes métodos Runge-Kutta el más empleado, por ser uno de los más eficientes es el método Runge-Kutta de cuarto orden.

\subsection{Método Runge-Kutta de cuarto orden}
\[y_{n+1} = y_n +\frac{h}{6}(K_1+2K_2+2K_3+K_4)\]
siendo:
\[K_1 = f(x_n,y_n)\]
\[K_2 = f(x_n+\frac{h}{2}, y_n+\frac{h}{2}K_1)\]
\[K_3 = f(x_n+\frac{h}{2}, y_n + \frac{h}{2}K_2)\]
\[K_4 = f(x_n+h, y_n + h K_3)\]

\textbf{Ejercicio punto extra: ¿De dónde salen estos números del Runge Kutta de cuarto orden?}
\textit{Pista: Considerar la $y$ como una función escalar y apoyarnos en los métodos de Taylor}

\section{Método leap-frog}
En este caso empelamos la relación:
\[y(x_{n+2}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x)) \dif x \approx y(x_n) + 2h f(x_{n+1},y_{n+1})\]

La diferencia entre este método y los anteriores es que este método consta de dos pasos, pues debemos aproximas $y_1$ ya que para calcular cada $y_n$ necesitamos conocer $y_{n-1}$ e $y_{n-2}$

\section{Método de Adams-Bashforth}

Se trata de un método basado en cuadratura que emplea la relación
\[y(x_{n+2}) - y(x_{n+1}) = \int_{x_{n+1}}^{x_{n+2}} f(x,y(x)) \dif x\]

\begin{remark}
Esta relación es sencilla de entender puesto que, por definición del problema original, $f(x,y(x))=y'$. Así, lo única que estamos haciendo es aplicar el Teorema Fundamental del Cálculo, es decir, estamos resolviendo la integral.

El problema, como siempre, es que no se puede hacer esta integral directamente pues la $f$ depende de $y(x)$ que no es conocida.
\end{remark}

donde $f(x,y(x))$ se aproxima por medio de la recta interpolante entre los puntos $(x_n,y(x_n))$ y $(x_{n+1},y(x_{n+1}))$, es decir:
\[f(x,y(x)) \approx f(x_n, y(x_n)) \frac{x-x_{n+1}}{x_n-x_{n+1}}+f(x_{n+1},y(x_{n+1}))\frac{x-x_n}{x_{n+1}-x_n}\]

Finalmente, el método queda expresado como:
\[y_{n+2} = y_{n+1}+\frac{h}{2}\left( 3f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]

Como podemos ver, se trata de un método de dos pasos, pues necesitamos conocer dos puntos antes de empezar a aplicar el método, y es explícito, pues directamente nos da la función que queremos conocer.

\section{Métodos de Adams-Moulton}
Se trata de un método muy similar al anterior en el que la función se interpola de otra forma distinta. De nuevo tenemos la relación:
\[y(x_{n+2}) - y(x_{n+1}) = \int_{x_{n+1}}^{x_{n+2}} f(x,y(x)) \dif x\]

pero en esta ocasión la función $f$ la aproximamos mediante una interpolación con polinomios de orden 2 en lugar de rectas:
\[f(x,y(x)) \approx f(x_n,y_n) \frac{(x-x_{n+1})(x-x_{n+2})}{(x_n-x_{n+1})(x_{n+1}-x_{n+2})} + f(x_{n+1},y_{n+1})\frac{(x-x_{n+2})(x-x_{n+2})}{(x_{n+1}-x_n)(x_{n+1}-x_{n+2})} \atop+f(x_{n+2},y_{n+2})\frac{(x-x_{n})(x-x_{n+1})}{(x_{n+2}-x_n)(x_{n+2}-x_{n+1})}\]

Ahora introducimos esta aproximación en la integral e integramos, con lo que obtenemos la siguiente fórmula de recurrencia:
\[y_{n+2} = y_{n+1} + \frac{h}{12}\left( 5f(x_{n+2},y_{n+2})+8f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]

En esta ocasión se trata de un método implícito puesto que la misma $y_{n+2}$ que queremos calcular aparece en la propia fórmula.

\section{Método predictor-corrector de Adams-Bashforth / Adamas-Moulton}

El método anterior tiene el problema de ser implícito, es decir, vamos a tener que resolver una complicada ecuación de recurrencia.

Para solucionar esto, la idea es aproximar $y_{n+2}$, calcular $y_{n+2}$ empleando la fórmula y basándonos en la aproximación y posteriormente ir corrigiendo las aproximaciones en cada paso.

Así, tendremos la fórmula de recurrencia:
\[y_{n+2} = y_{n+1} + \frac{h}{12}\left( 5f(x_{n+2},y_{n+2}^*)+8f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]

La predicción que haremos para $y^*$ será:
\[y_{n+2}^* = y_{n+1} + \frac{h}{2} \left( 3f(x_{n+1},y_{n+1})-f(x_n,y_n)\right)\]
es decir, combinamos los dos métodos vistos anteriormente.

\section{Ejemplos}

\begin{problem}[1]
Escribir la siguiente ecuación diferencial de grado $n$ como un sistema de ecuaciones diferenciales de primer orden.
\[y^{n)} + \sum_{i=1}^n a_{n-i}y^{n-i)} = 0\]

\solution
La idea de este ejercicio es que, a priori, uno puede pensar que todo lo que llevamos visto durante el curso es inútil ante un problema como el aquí mostrado.

Para convertir esta ecuación en un sistema, vamos a definir una serie de nuevas ecuaciones de forma que
\[y^{i} = y_i \text{ para } i=1...n-1\]

Así el sistema de ecuaciones nos queda de la forma:
\[\begin{array}{l}
y' = y_1\\
y_1' = y_2\\
y_2' = y_3 \\
...\\
y_{n-1}' = -\left( a_{n-1}y_{n-1} + a_{n-2}y_{n-2}+...+a_1y_1 + a_0 y\right)
\end{array} \]

Como podemos comprobar, en nuestro sistema, todas las ecuaciones son de primer orden.

\end{problem}

\begin{problem}[2]
Convertir el siguiente sistema de ecuaciones diferenciales en otro donde todas las ecuaciones tengan orden 1.
\[\begin{array}{l}
u''' = u''+v'\\
v''' = u^2+e^xu'v'+\sin(v)
\end{array} \]
con $u'(0)=u''(0)=v'(0)=0 \ , \ v(0)=1$
\solution

Renombrando las variables de la forma: $u' = u_1$, $u''=u_2$, $v'=v_1$, podemos reescribir el sistema como sigue.

\[\begin{array}{l}
u' = u_1\\
u_1'=u_2\\
v'=v_1\\
u_2' = u_2+v_1\\
v_1' = u^2+e^xu_1v_1 + \sin(v)
\end{array} \]
con $u_1(0)=u_2(0)=v_1(0)=0 \ , \ v(0)=1$

\end{problem}

\begin{problem}[3]
Comprobar que las siguientes funciones satisfacen la condición de Lipschitz

\ppart
\[f(x,y)=2yx^{-4} \text{ con } x \in [1,\infty)\]

\ppart
\[f(x,y)=e^{-x^2}\arctg(y) \text{ con } x \in [1, \infty)\]

\ppart
\[\appl{f}{\real \times \real^2}{\real} \text{ tal que }f(x,y)=\left(x+\sin(y_2), \frac{x^2}{2}+\cos(y_1)\right)\]
\solution

\spart
\[|f(x,y)-f(x,\tilde{y})| = 2x^{-4}|y-\tilde{y}| < 2 |y-\tilde{y}|\]

\spart
\[|f(x,y)-f(x,\tilde{y})| = e^{-x^2} | \arctg(y)-\arctg(\tilde{y})| = e^{-x^2}\int_{\tilde{y}}^y \frac{\partial}{\partial x}\arctg{x}\dif x = e^{-x^2}\int_{\tilde{y}}^y \frac{1}{|+x^2} \dif x \leq \]
\[\leq e^{-x^2} \sup \frac{1}{1+x^2}|y-\tilde{y}| \leq \frac{e^{-1}}{2}|y-\tilde{y}|\]

\spart
Según en qué norma estemos trabajando, el módulo de $y$ se calculará de una forma diferente.

En esta ocasión, por que así lo ha indicado el profesor para este ejercicio, vamos a trabajar con la norma $L1$ de forma que
\[\norm{y}_{L_1} = |y_1| + |y_2|\]

Con esta medida, nos queda:
\[\norm{f(x,y)-f(x,\tilde{y})}_{L_1} = |\sin(y_2)-\sin(\tilde{y}_2)| + |\cos(y_1)-\cos(\tilde{y}_2)| \leq |y_2 - \tilde{y}_2|+ |y_1-\tilde{y}_1| \leq \norm{y - \tilde{y}}_{L_1}\]

\end{problem}


\begin{problem}[4]
Dado:
\[
	\begin{cases}
	y'=f(x,y), & y(0)=(0,1) \\
	\tilde{y'}=f(x,\tilde{y}), & \tilde{y}(0)=(0,1)
	\end{cases}
\]
Acotar $\max_{x∈[0,1]}{\md{y - \tilde{y}}} = \norm{y - \tilde{y}}_{L^∞}$

\solution
Tomaremos (recordando \hyperref[ineq:argumento_cont_dato_inicial]{el argumento de la demostración de continuidad respecto del dato inicial}):
\[ \abs{y - \tilde{y}} ≤ \underbrace{\abs{y(0) - \tilde{y}(0)} + L \int_0^x \abs{y - \tilde{y}}(s)\ ds}_{g(x)} \]

Derivando $g$ se obtiene $g'(x)=L\abs{y-\tilde{y}}$. Se ve que:
\[g'(x)≤Lg(x)\]
Usando el factor integrante llegamos a:
\[g'(x)e^{-Lx} ≤ Lg(x)e^{-Lx}\]
Que si nos fijamos no es más que:
\[\left( ge^{-Lx} \right)' ≤ 0\]
Que si además lo integramos en ambas partes por $\int_0^x$:
\[g(x) ≤ g(0) e^{Lx}\]
Así llegamos a:
\[\abs{y - \tilde{y}} ≤ g(x) ≤ g(0)e^{Lx} = \abs{\tilde{y}(0) - y(0)}e^{Lx} = \sqrt{2}e^{Lx}\]
De modo que tendremos la cota:
\[\max_{x∈[0,1]} \abs{y - \tilde{y}} ≤ \sqrt{2}e^{Lx}\]

\end{problem}

\begin{problem}[5]
Dada la función $\appl{f}{\real}{\real}$ tal que $f(x)=\abs{x}^α$

\ppart Demostrar que $f$ no es Lipschitz en ninún intervalo que contenga al origne

\ppart Resolver el PVI:
\[\left\{ y'(x)=f(y(x)) \atop y(0)=0\right.\]

\solution

\spart

Podemos ver que
\[\abs{f(x)-f(0)} = \abs{\abs{x}^α - 0} = \frac{\abs{x}^α}{\abs{x}}\abs{x} = \frac{1}{\abs{x}^{1-α}}\abs{x}\]

Pero en cualquier entorno del origen, el denominador podía dispararse de forma que es imposible que $\frac{1}{\abs{x}^{1-α}}$ esté acotdado por alguna $L$.

\spart

Tenemos que $y'(x)=\abs{y(x)}^α$.

Por un, de la propia definición de $y'(x)$ podemos ver que:
\[\int_0^t \frac{y'(x)}{\abs{y(x)}^α}\dif x = \int_0^t 1 \dif x = t\]

Por otro lado, sabemos calcular la integral de la izquierda con lo que obtenemos:

\[\frac{1}{1-α}\left( y(t)^{1-α}-y(0)^{1-α}\right) = \frac{1}{1-α}y(t)^{1-α} = t \implies y(t) = (t (1-α))^{\frac{1}{1-α}}\]

Como podemos ver, estamos obteniendo dos soluciones, puesto que tenemos una raíz. Esto podría contradecir el Teorema de Picard, que nos garantiza unicidad de la solución. No obstante no se ningún problema puesto que la función no es Lipschitz por lo que el teorema no aplica.
\end{problem}

\begin{problem}[6]
Sea $y(x) \in C([a,b])$ solución de $y'(x)=f(x,y(x))$ y sea $\tilde{y}(x) \in C([a,b])$ con $\tilde{y}'(x)=f(x,\tilde{y}(x))+r(x,\tilde{y}(x))$ siendo $||r||_{L^∞×L^∞} \leq M$

Demostrar que
\[\max_{x \in (a,b)} |y(x)-\tilde{y}(x)| \leq |y(a)-\tilde{y}(a)|e^{L(b-a)}+\frac{M}{L}\left( e^{L(b-a)}-1\right)\]

\solution
Tomando la resta de $y'$ y $\tilde{y}'$, y sirviéndonos de la iterada de Picard:
\[\int_a^x y'(t) - \tilde{y}'(t) = \int_a^xf(t,y(t)) - f(t,\tilde{y}(t)) - \int_a^xr(t,\tilde{t}) + \left( y(a) - \tilde{y}(a) \right)\]

Tomando valores absolutos:
\[\abs{y(x) - \tilde{y}(x)} \overbrace{≤}^{\label{ineq:dif_picard}1} \int_a^x\abs{f(t,y(t)) - f(t,\tilde{y}(t))} dt + \int_a^x \abs{r(t, \tilde{y}(t))} dt + \abs{y(a) - \tilde{y}(a)} ≤\]

\[≤ L\abs{y(t) - \tilde{y}(t)} + \int_0^xM dt + \abs{y(a) - \tilde{y}(a)}\]

Tomaremos $g'(x) = L\abs{y(x) - \tilde{y}(x)} + M$. Y obtenemos:

\[\frac{g'(x) - M}{L} = \abs{y(x) - \tilde{y}(x)} \overbrace{≤}^{\label{ineq:acotar} 2} g(x)\]

\[g'(x) - Lg(x) ≤ M\]

Tirando de factor integrante:
\[\underbrace{g'(x)e^{-L(x-a)} - Lg(x)e^{-L(x-a)}}_{\left( g(x) e^{-L(x-a)} \right)'} ≤ Me^{-L(x-a)}\]

Integrando entre $a$ y $t$ a ambos lados de la inecuación:
\[\int_a^b \left(g(x) e^{-L(x-a)} \right)'dx ≤ \int_a^t Me^{-L(x-a)} dx\]
\[g(t)e^{-L(t-a)} - g(a) ≤ \frac{M}{L} \left(1 - e^{-L(t-a)} \right)\]
\[g(t) ≤ e^{L(t-a)} \left\{ g(a) + \frac{M}{L} \left(1 - e^{-L(t-a)} \right) \right\}\]
\[\qquad \quad g(a)e^{L(t-a)} + \frac{M}{L} \left(e^{L(t-a)}-1 \right)\]

Sirviéndonos de las inecuaciones \hyperref[ineq:dif_picard]{1} y \hyperref[ineq:acotar]{2}:

\[\abs{y(x) - \tilde{y}(x)} \overbrace{≤}^{\hyperref[ineq:dif_picard]{1}} g(x) \overbrace{≤}^{\hyperref[ineq:acotar]{2}} g(a) e^{L(x-a)} + \frac{M}{L} \left(e^{L(x-a)}-1 \right) \]

Y tomando máximos a ambos lados, demostramos la desigualdad del enunciado:
\[\max_{x∈(a,b)} \abs{y(x) -\tilde{y}(x)} ≤ \max_{x∈(a,b)} \left\{ g(a) e^{L(x-a)} + \frac{M}{L} \left(e^{L(x-a)}-1 \right) \right\}\]
\[\qquad \qquad  \qquad  \qquad= \abs{y(a) - \tilde{y}(a)} e^{L(b-a)} + \frac{M}{L}\left(e^{L(b-a)}-1 \right) \]
\end{problem}

\chapter{Consistencia y estabilidad}

En este capítulo explicaremos conceptos como la consistencia, la estabilidad o el residuo, útiles a la hora de valorar la calidad de un método numérico.

A partir de ahora vamos a trabajar con métodos numéricos que se pueden escribir de la forma:
\[\sum_{j=0}^k α_jy_{n+j} = h \phi_f(x_n,y_n,y_{n+1},y_{n+k-1},h) \text{ con } n=0,...,N - k\]
donde $\phi_f$ se denomina \concept{función incremento}.

\obs Todos los métodos que hemos visto hasta ahora pueden escribirse de esta forma. Veamos algunos ejemplos,

\begin{example}
\begin{enumerate}
\item \textbf{Euler}
\[y_{n+1} = y_n+hf(x_n,y_n)\]

En este caso podemos pasar $y_n$ restando a la izquierda para que, junto con $y_{n+1}$, nos de el sumatorio, quedando a la derecha del igual $hf()$ con lo que tendríamos que $\phi_f=f$.

\item \textbf{Trapecio}

Con los métodos explícitos (como el de Euler que acabamos de ver) es muy sencilla la comprobación de que el método puede escribirse de la forma comentada anterormente.

No obstante, en métodos implícitos como el del trapecio es algo más complicado de comprobar.

En esta ocasión tenemos
\[y_{n+1}-y_n = \frac{h}{2}\left(f(x_n,y_n)+f(x_{n+1},y_{n+1}) \right)\]

Así tendríamos:
\[\phi_n (x_n,y_n) = \frac{h}{2}\left(f(x_n,y_n)+f(x_{n+1},y_{n+1}) \right) = \frac{1}{2}\left( f(x_n,y_n) + f(x_{n+1},y_n+h\phi_f(x_n,y_n)\right)\]

Donde el $\phi_f$ que buscamos sería la solución de esta ecuación.

La pregunta que debería surgir ahora en el lector es si esta $\phi_f$ está bien definida. Podemos ver que calcular $\phi_f$ es equivalente a encontrar un punto fijo de la aplicación:

\[F(\phi) = \frac{1}{2} \left(f(x_n,y_n)+f(x_{n+1},y_n+h\phi)\right)\]

que, para $h$ suficientemente pequeño cumple:
\begin{equation}
	\label{eq:F_contractiva}
	\left(\norm{F(\phi)-F(\tilde{\phi})} \leq \frac{Lh}{2}\norm{\phi- \tilde{\phi}}\right)
\end{equation}

es decir, la aplicación será contractiva y, por tanto, el problema del punto fijo tendrá solución.

\item \textbf{Método de Taylor}

\[y_{n+1}=y_n+hf(x_n,y_n)+\frac{h^2}{2}\left(f_x(x_n,y_n)+f_y(x_n,y_n)f(x_n,y_n)\right)\]

donde obtendríamos:
\[\phi_f = hf(x_n,y_n)+\frac{h^2}{2}\left(f_x(x_n,y_n)+f_y(x_n,y_n)f(x_n,y_n)\right)\]
\end{enumerate}
\end{example}

En general supondremos que  $α_k$ será igual a 1 puesto que siempre podremos dividir por una constante si lo deseamos.

Además, siempre tendremos que $α_0 \neq 0$ ó $\phi_f$ depende de forma no trivial de $y_n$.

La idea de estos convenios es que no consideremos como métodos distintos algunos que trivialmente son iguales.

\begin{prop}[Hipótesis sobre Métodos Numéricos]
Existen una serie de hipótesis que tomaremos de forma general sobre los métodos numéricos con los que trabajemos. Estas son:

\begin{enumerate}
\item $\phi_f$ es continua.

\item Existen constantes $h_0,L$ que pueden depender de $f$ tales que:
\[\norm{\phi_f(x_n,y_n,...,y_{n+k-1},h) - \phi_f(x_n,\tilde{y}_n,...,\tilde{y}_{n+k-1},h)}\leq L \sum_{j=0}^k \norm{y_{n+j}-\tilde{y_{n+j}}} \text{ si } 0 < h < h_0\]

\item $f= 0 \implies \phi_f=0$


\begin{example}
	Veamos a continuación que la \textbf{primera hipótesis} ($\phi_f$ continua) se cumple para el \textbf{método del trapecio}:

	Definimos la aplicación global contractiva (tal y como hemos visto en \ref{eq:F_contractiva}):
	\[F(\phi) = \frac{1}{2} \left[ f(x_n,y_n) + f(x_{n+1}, y_n+h\phi) \right]\]

	Y tomemos $\phi_f = \lim_{j\to∞} \phi_f^{[j]}$, donde $\phi_f^{[j]}$ viene dada por la sucesión $\phi_f^{[j]} = F\left(\phi_f^{[j-1]}\right)$ y $\phi^0 = 0$.

	\obs El método del trapecio es de 1 paso ($k=1$) y por tanto la función incremento solo depende de $x_n$, $y_n$ y $h$, es decir $\phi_f(x_n,y_n;h)$.

	Dado que $f$ es una función continua, tendremos que:
	\[\phi_f^{[1]}(x_n,y_n;h) = \frac{1}{2} \left[ f(x_n,y_n) + f(x_{n+1}, y_{n+1} + 0) \right]\]

	será continua. Como la composición de funciones continuas da como resultado otra función continua, tendremos que $\phi_f^{[2]}=F\left( \phi_f^{[1]} \right)$ es continua, y por tanto todas las $\phi_f^{[k]}(x_n,y_n;h)$ son continuas.

	Si logramos que la convergencia de las $\phi_f^{[j]}$ sea uniforme, tendremos que la sucesión convergería a una función $\phi_f$ continua (que es lo que dice la hipótesis).

	Expresamos $\phi_f^{[k]}$ como una suma telescópica:
	\begin{equation}
		\label{eq:suma_telesc_pto_fijo}
		\phi_f^{[k]} = \sum_{j=1}^k \phi_f^{[j]} - \phi_f^{[j-1]} + \phi_f^{[0]},\quad \phi_f^{[0]}=0
	\end{equation}

	Tomando la norma $L^∞$:
	\begin{equation*}
		\begin{multlined}[0.7\textwidth]
			\md{\phi_f^{[j]} - \phi_f^{[j-1]} }_{L^∞} = \md{F\left[\phi_f^{[j-1]}\right] - F\left[\phi_f^{[j-2]}\right] }_{L^∞}\\[1em]
			\shoveleft{\qquad \qquad \qquad \underbrace{≤}_{F\text{ contractiva}} \frac{Lh}{2} \md{\phi_f^{[j-1]} - \phi_f^{[j-2]}}_{L^∞}}\\
		\end{multlined}
	\end{equation*}

	Aplicando la desigualdad sucesivamente:
	\[\md{\phi_f^{[j]} - \phi_f^{[j-1]}}_{L^∞} ≤ \left(\frac{Lh}{2}\right)^{j-1} \md{\phi^{[1]}}_{L^∞}\]

	Puesto que $K = [a,b]×ℝ×(0,∞)$ es compacto:
	\[\md{\phi_f^{[j]} - \phi_f^{[j-1]}}_{L^∞} ≤ M_k\left(\frac{Lh}{2}\right)^{j-1}\]

	De modo que aplicando el test $M$ de Weierstrass, la convergencia de \ref{eq:suma_telesc_pto_fijo} es uniforme, y por tanto $\phi_f$ es continua.
\end{example}


\end{enumerate}
\end{prop}

\chapter{Laboratorio}
\input{tex/LaboratorioMnedo.tex}
\appendix

\chapter{Ejercicios}
\input{tex/EjerciciosMnedo.tex}
\printindex
\end{document}



