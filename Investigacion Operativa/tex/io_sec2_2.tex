\section{Convexidad}

En esta sección, vamos a repasar conceptos y algunas aplicaciones porque de aquí en adelante trabajaremos con estos conceptos.


\begin{itemize}
	\item Gradiente.
	\item Matriz Hessiana.
	\item Desarrollo de Taylor en varias variables:
		\[
			f(\vx) = f(\gor{\vx}) + \grad f(\gor{\vx})^\top (\vx-\gor{\vx}) + (\vx-\gor{\vx})\Hf{\gor{\vx}}(\vx-\gor{\vx}) + \norm{\vx-\gor{\vx}}R(\gor{\vx},\vx-\gor{\vx})
		\]
		con $\displaystyle \lim R(\vx,\gor{\vx}) = 0$ ($R$ es el resto).
\end{itemize}

\subsection{Condiciones para óptimos locales}

\paragraph{Condición necesaria de primer orden}
Si $\vx$ es un máximo o mínimo local, entonces $\grad f(\vx) = 0$.

Para ver que la condición es necesaria pero no suficiente, podemos pensar en $f(x_1,x_2) = x_1^2 - x_2^2$, cuyo gradiente se anula en $(0,0)$, pero no es un mínimo ni un máximo. Estamos ante un punto de silla (punto de inflexión).


\paragraph{Condiciones necesarias de segundo orden} Sea $f$ 2 veces diferenciable en $\gor{\vx}$. Si $\gor{\vx}$ es un mínimo local de $f$, encones:
\begin{itemize}
	\item $\grad f(\gor{\vx}) = 0$
	\item $\Hf{\gor{\vx}}$ semidefinida positiva.
\end{itemize}

Con el mismo ejemplo anterior podemos ver que es condición necesaria.

\paragraph{Condiciones suficientes} Sea $f$ 2 veces diferenciable en $\gor{\vx}$. Si $\gor{\vx}$ es un mínimo local de $f$, encones:
\begin{itemize}
	\item $\grad f(\gor{\vx}) = 0$
	\item $\Hf{\gor{\vx}}$ definida positiva.
\end{itemize}
\begin{proof}
	Si $\vx$ no es un mínimo local estrico, entonces existe una sucesión $\vx_k \to \gor{\vx}$ con $\gor{\vx_k} ≠ \vx$ tal que $f(\vx_k) ≤ f(\gor{\vx})$.

	Sea \[d_k = \frac{\vx_k - \gor{\vx}}{\norm{\vx_k - \gor{\vx}}}\]

	Las normas están acotadas, y estamos en $\real^n$ y algún argumento de compacidad, podemos tomar una subsucesión convergente de $d_k\to d$, que vamos a llamar $d_k$

	Tomamos el desarrollo de Taylor de $f(\vx_k)$:

	\[
		f(\vx_k) = f(\gor{\vx}) = f(\gor{\vx}) + \frac{1}{2}\Hf{f(\gor{\vx})}(\vx_k - \gor{\vx}) + \norm{\vx_k - \gvx}^2R(\gx,\gx_k)
	\]

	\[
		0≥\frac{f(\vx_k - f(\gx)}{\norm{\vx_k - \gvx}^2} = \frac{1}{2}d_k^t\Hf{f(\gvx)}d_k + R(\gvx,\vx_k)
	\]
	Hemos llegado a que $\Hf{f(\gvx)}$ tiene que ser semidefinida negativa.
\end{proof}

Estas condiciones están bien, pero no son del todo interesantes para este curso, ya que estamos buscando soluciones globales a problemas y todas estas condiciones son locales.
Vamos a necesitar alguna restricción más sobre $f$ para poder buscar soluciones globales.

\subsection{Funciones convexas}

\begin{defn}[Función\IS convexa]
Una función $\appl{f}{D}{ℝ}$ es convexa si su dominio $D\subset\real ℝ^n$ es covexa y  $∀\vx,\vy\in D$, $∀λ∈[0,1]$ se verifica:

\[
	f(λ\vx + (1-λ)\vy ≤ λf(\vx) + (1-λ)f(\vy)
\]
\end{defn}

\obs
\begin{itemize}
	\item \concept{Convexidad\IS estricta} requiere $<$ en lugar de $≤$.
	\item Una función es (estrictamente) cóncava si $-f$ es (estrictamente) convexa.
	\item $f$ es convexa $\dimplies ∀x,\vec{u}, g(t) = f(x+tu)$ es convexa (en su dominio, es decir $\{t : x+tu\in D\}$)
	\begin{proof}
		\[g(λt_1+(1-λ)t_2) = f(x+(λt_1+(1-λt_2)\vu) = ...
		\]
	\end{proof}
\end{itemize}

\begin{example}
\begin{itemize}
	\item $f(x) = x^2$ es convexa.

	Como todavía no hemos visto la definición de convexidad en términos de la segunda derivada, vamos a recurrir a la definición.
	\begin{proof}
		\[
			\left(λx_1 + (1-λ)x_2\right)^2 ≤ λx_1^2 +(1-λ)x_2^2)
		\]
	\end{proof}
	\item $f(\vx) = \max\{x_1,...,x_n\}$ es convexa.
	\begin{proof}
		\[f(λ\vx + (1-λ) \vy) = \max\set{λx_1 + (1-λ)y_1,...,λx_n + (1-λ)y_n} ≤ λ\max\{x_k\} + (1-λ)\max\{y_k\} = λf(\vx) + (1-λ)f(\vy)\]
	\end{proof}
	\item Las funciones afines $f(x) = A\vx + b$ son cóncavas y convexas.
	\item Cualquier norma $f(\vx) = \norm{\vx}$ es una función convexa.
	\item Si $S$ es convexo, la distancia a $S$:
	\[
		f(x) = d(x,S) = \inf_{s\in S}\norm{\vx-s}
	\]
	es una función convexa.

\end{itemize}

\begin{theorem}[Desigualdad\IS de Jensen]
Si $\appl{f}{D}{ℝ}$ es convexa, entonces $∀x_1,...,x_n\in D$ y $,λ_1,...,λ_n ≥0$ con $\sum^n λ_i = 1$ se cumple:

\[f\left(\sum^k x_iλ_iy\right) ≤ \sum^k λ_if(x_i)\]

\end{theorem}
\begin{proof}
Por inducción sobre $k$.
\end{proof}

\obs Podríamos interpretar $λ_i$ como probabilidades y tendríamos:

Si $X$ es una variable aleatoriao que toma los valores $x_i$ con probabilidad $λ_i$, entonces $f(\esp{X})≤\esp{f(X)}$

\end{example}

Vamos a ver más condiciones de convexidad de funciones, para no tener que recurrir a la definición.

\paragraph{Completar la teoría}

\begin{defn}[Epigrafo]

El epigrafo de una función $\appl{f}{D}{ℝ}$ se define como:

\[
	\epigraph{f} = \{(x,t) : x\in D, f(\vx) ≤ t\} \subset ℝ^{n+1}
\]
\end{defn}

\begin{itheorem}
Una función es convexa si y sólo si su epigrafo es un conjunto convexo.
\end{itheorem}

\begin{itheorem}
Sea $\{f_i : i\in I\}$ una familia de funiones convexas definidas sobre un conjunto convexo no vacío $S$ tal que para todo $x\in S$, el conjunto $\{ f_i(x) : i\in I\}$ está acotado superiormente.

\textbf{Entonces,} la función $f(\vx) = \sup\{f_i(\vx) : i\in I\}, x\in S$ es convexa.
\end{itheorem}

\begin{proof}

	utilizando el teorema anterior y la propiedad que tendríamos que demostrar: $\epigraph{f} = \cap \epigraph{f}$

\end{proof}

\begin{itheorem}[Funciones convexas\IS  diferenciables]
Sea $\appl{f}{D}{ℝ}$ diferenciable sobre un dominio convexo y abierto $D$. Entonces, $f$ es convexa si y sólo si:

\[f(\vx) ≥ f(\gor{\vx}) + \pesc{\grad f(\gor{\vx}),(\vx-\gor{\vx})} \;\; ∀\vx,\gvx \in D\]

\end{itheorem}
\begin{proof}
Vamos a demostrar cada implicación por separado.

\proofpart{$\implies$}

Si $\vx,\gvx\in D$ y $f$ es convexa en $D$.

\[
f (λ\vx + (1 − λ)\bar{\vx} ) ≤ λf (\vx) + (1 − λ)f (\bar{\vx}) \dimplies [f(\bar{\vx} + λ(\vx −\bar{\vx} ) − f (\bar{\vx} )]/λ ≤ f (\vx) − f (\bar{\vx}).
\]

Tomando $\lim_{λ\to 0}$



\proofpart{$\impliedby$}

Si $\vx≠\vy$ y $λ\in [0,1]$, se aplica la condición a $x$ e $y$ con $\gvx = λ\vx + (1-λ)\vy$:

\begin{equation}
f(\vx)  \geq f(\gvx) + \grad f(\gvx)(\vx-\gvx)
\end{equation}

\begin{equation}
f(y)  \geq f(\gvx) + \grad f(\gvx)(y-\gvx)
\end{equation}

Se multiplica primera la desigualdad por $λ$, la segunda por $(1-λ)$ se suman.

\end{proof}

\begin{itheorem}[Funciones convexas\IS  diferenciables dos veces]
Sea $\appl{f}{D}{ℝ}$ con 2 derivadas continuas sobre un dominio convexo y abierto $D$. Entonces,

\[
	f \text{ convexa} \dimplies \Hf{\vx} \text{ semidefinida positiva } ∀\vx \in D
\]

\end{itheorem}

\begin{proof}
Para $n=1$ está como ejercicio. 

Para $n>1$ hay que considerar $g(λ) = f(x+λd)$ y se tiene en cuenta que $g''(λ) = d^\top \Hf(x+λd)d$
\end{proof}

\begin{itemize}
	\item Una función 2 veces diferenciable es cóncava si $D$ es convexo y $\Hf(\vx)$ es semidefinida negativa.
	\item Si $\Hf{\vx}$ es definida positiva $∀x\in D$, entonces, la función es estrictamente convexa. El recíproco \textbf{no} es cierto (por ejemplo $f(x) = x^4$)
\end{itemize}

\begin{prop}[Operaciones que preservan la convexidad]

A continuación, definimos unas cuantas operaciones muy útiles para reconocer funciones convexas.

\begin{itemize}
	\item Supremo de 2 funciones convexas es una función convexa.
	\item Sumas ponderadas no negativas.
	\item Composición con una función afín.
	\item Composición de una función convexa y otra creciente y convexa.
	\item Minimización parcial
	\subitem $f$ convexa en $(\vx,\vy)$ y $C$ un conjunto convexo no vacío. \[g(\vx) = \inf_{\vy\in C} f(\vx,\vy) \text{ es convexa}\]
\end{itemize}

\end{prop}


\paragraph{Ejemplos}
\begin{itemize}
	\item $f(x) = -\log(x)$ podemos derivar 2 veces y como es positiva, convexa.
	\item $f(x) = e^{ax}$ es convexa.
	\item $f(x) = x^a$
	\subitem $a≤0, a≥1 \to$ convexa.
	\subitem $0<a<1 \to $ cóncava.
	\item $f(x) = x^\top Ax + a^\top x+ c$ donde $A$ es simétrica. Queremos ver que $f$ convexa $\dimplies$ $A$ definida positiva.
	\item La más relevante: $X$ una matrix $n\times p$ y sean $β\in\real^p$ e $y\in\real^n$, entonces:
	$f(β) = \norm{y-Xβ}$ es convexa, siendo $\norm{·}$ cualquier norma.

	Esta función es convexa porque es la composición de 2 funciones. $Xβ$ es una función afín y la norma es convexa para cualquier norma. Entonces, como $f(β)$ es una combinación de funciones convexas, es convexa.
\end{itemize}


\paragraph{Problema general de optimización convexa}

\begin{itheorem}[Teorema\IS local-global]
Todo mínimo local de un problema convexo es también mínimo global.
\end{itheorem}
\begin{proof}
Sea $\gvx$ un mínimo local. Entonce existe $R>0$ tal que $f(\gvx)≤f(\vx) ∀\vx \in S\cap B(\gvx,R)$, por definición de mínimo.

Supongamos que existe $\vy \in S$ tal que $f(\vy) < f(\vx)$.

Consideramos $\vz = (1-λ)\gvx + λ \vy$ para $λ>0$ suficientemente pequeño.

Entonces $\vz \in S\cap B(\gvx,R)$ y $f(\vz) < f(\gvx)$, con lo que no puede existir $\vz$.
\end{proof}

\begin{itheorem}[Mínimos globales bajo convexidad y diferenciabilidad]
Consideramos el problema $(1)$ en el que además $f$ es diferenciable.

Un punto $\gvx\in S$ es mínimo global si y sólo si:

\[
	\grad f(\gvx) (\vx-\gvx) ≥ 0\;\; ∀\vx\in S
\]

\end{itheorem}

\begin{proof}

\proofpart{$\impliedby$} Por la convexidad de f

\proofpart{$\implies$} Por reducción al absurdo. Supongamos que existe $\vx\in S$ tal que $\grad f(\gvx) (\vx-\gvx)$. Entonces $(\vx-\gvx)$ es una dirección de descenso local.

Existe
\end{proof}

\obs 

\begin{example}

\end{example}


\subsection{Cuasiconvexidad}

Una noción no vista en teoría, pero que se define en un ejercicio es lo que vamos a tratar hasta el final del capítulo.



\subsubsection{Utilidad}

La cuasiconvexidad es una propiedad más relajada que al convexidad pero que es condición suficiente para algunos resultados interesantes. Por ejemplo:

\begin{prop}
Mínimo local en función cuasiconvexa es un mínimo global.
\end{prop}

\begin{proof}
Se deja como ejercicio
\end{proof}
