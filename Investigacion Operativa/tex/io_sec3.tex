En este capítulo discutiremos la unicidad y optimalidad de soluciones encontradas.
%
Es necesario distinguir óptimos locales de globales y para ello es importante conocer la noción de función convexa y más elementos de convexiadad, que nos permitirán conocer cuándo la solución que tenemos es global y cuando no.
%
Las condiciones KKT (de Karouse Kunh Tacker\footnote{O algo así}) añaden condiciones necesarias y/o suficientes para que un mínimo o máximo sea global, sin necesidad de que la función sea convexa.

\input{tex/io_sec2_2}


\section{Condiciones KTT}


\paragraph{Motivación:} Puede ser que los problemas a los que nos enfrentemos no cumplan las condiciones de convexidad estudiadas en el tema anterior. En ese caso, necesitaremos nuevas herramientas, como las condiciones KTT.
Vamos a intentar relajar las condiciones de convexidad para los problemas que vamos a resolver, encontrando otras condiciones (definidas por Karoush, Kuhn y Tucker) que permiten la identificación de los mínimos de un problema. 

En este tema vamos a estudiar tanto problemas generales como problemas convexos en un dominio del problema abierto que incluye todos los puntos que satisfacen las restricciones.

Consideremos el problema:

\begin{ioprob}
\goal{$\min f(x)$}
\restrictions{$f_i(x)≤0\;i=1,...,m$}{}{}{}{}{}
\end{ioprob}


\begin{defn}[Condiciones\IS KTT]
Sean $u_i ≥0$ y el Lagrangiano:

\[
	L(x) = f(x) + \sum_i u_i f_i(x)
\] 


Se dice que el punto $\vx$ y el vector de multiplicadores $\vu \in ℝ^m$ verifica las condiciones KTT para un problema $(P)$ si

\begin{align*}
KKT(\vx) \dimplies \grad L(\vx) = 0 \dimplies \grad f(\vx) &+ \sum_i u_i\grad f_i(\vx) =0\\
f_i(\vx)  &≤ 0 \;\; i=1,...,m\\
u_i&≥0\;\; i=1,...,m\\
u_if_i(\vx) &= 0  \;\; i=1,...,m
\end{align*}

\obs Estos $u_i$ utilizados los denominaremos \concept{multiplicadores} por el \href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{teorema de los multiplicadores de Lagrange}.

\end{defn}

\paragraph{Notación:} $KKT(\vx)$ significa que para $\vx$ existe un vector de multiplicadores tales que verifican las condiciones KTT.
%
De la misma manera, $KTT(\vx,\vu)$ significa que el punto $\vx$ y el vector de multiplicadores satisfacen las condiciones KKT.

Cualquier mínimo local de un problema $(P)$, que también cumpla las cualificación de las restricciones (que definimos a continuación) tiene que cumplir las condiciones KTT. 
Esto nos puede dar una pista para encontrar mínimos locales en un problema.


\begin{defn}[Cualificación de las restricciones]
Dado un punto factible $\vx$, sea $I_{\vx} = \{i : f_i(\vx) = 0\}$ el conjunto de restricciones \textbf{activas} en $\vx$.

$\vx$ verifica la cualificación de las restricciones si los vectores $\grad f_i(\vx)$ son linealmente independientes.
\end{defn}

Vamos a ver el resultado importante que relaciona estos conceptos:

\begin{theorem}
\label{thm:cualificacionYKKT}
\[
\left.
	\begin{array}{c}
		\vx \text{ mínimo local de } (P)\\
		\vx \text{ verifica la cualificación de las restricciones}
	\end{array}
\right\} \implies KKT(\vx)
\]
\end{theorem}

\begin{proof}
Si $\vx$ es un mínimo local de $(P)$ entonces no existe ninguna dirección $d∈ℝ^n$ tal que sin salir del conjunto factible mejoremos el valor objetivo de $\vx$ en esa dirección, es decir:

\[
	\pesc{\grad f(\vx),d}< 0 \wedge \pesc{\grad f_i(\vx) < 0} ∀ i \in I
\]

Siendo $I$ el conjunto de las restricciones activas en $\vx$.

Por el teorema Gordan, existen $u_0≥0, u_i≥0, i\in I$ no todos nulos tales que $\pesc{(u_0,\vu),(\grad f, \grad f_i}$

$u_0 >0$ por la cualificación de las restricciones, ya que 
\[
	u_0 = 0 \implies \sum u_i\grad f_i (\vx) = 0 
\]
con lo que habríamos encontrado una combinación lineal y podríamos concluir que $\grad f_i$ son linealmente dependientes y por hipótesis no puede serlo.

Definimos $\gor{u_i} = \frac{u_i}{u_0}$ si $i\in I$y $\gor{u_i} = 0$ si $i\notin I$. De esta manera tenemos:

\[
	\gor{u_i} f_i = 0 \impliedby
	\left\{ 
		\begin{array}{cc} 
			i\in I &\implies f_i(\vx) = 0 \\
			i\notin I &\implies \gor{u_i} = 0
		\end{array}
	\right.
\]

Ahora sólo queda comprobar $KKT(\vx,\gor{\vu})$.

\end{proof}

Con este teorema, clasificamos \textbf{todos} los mínimos que puede tener un problema en 2 clases.
%
\textbf{O cumple la cualificación de las restricciones o KKT} (tal vez incluso las 2). 
%
Si el problema tiene solución, debe encontrarse entre los puntos hallados de alguna de estas formas.

\begin{example}
Determina los puntos que verifican KKT y resuelve el problema:

\begin{ioprob}
\goal{$\min x_1^2+x_2^2 + 2x_3^2$}
\restrictions{$2x_1+3x_2+x_3 ≥ 6 \to -2x_1 - 3x_2 - x_3 + 6 ≤0$}{}{}{}{}{}
\end{ioprob}

Tenemos $\grad f(\vx) = (2x_1,2x_2,4x_3)$, con $f_i = (-2,-3,-1)$

Construimos el "sistema":

\[
\grad f(\vx) + \sum u_i f_i(\vx) \to 
\left\{
	\begin{array}{cc}
		2x_1-2u = 0&\\
		-2x_2 - 3u = 0&\\
		6x_3 - u = 0&\\
		u≥0, 2x_1+3x_2+x_3≥6 &(Factibilidad)\\
		u[-2x_1 - 3x_2 - x_3 + 6] = 0 &(Holgura Complementaria)
	\end{array}\right.
\]

Y ahora resolvemos el sistema\footnote{No hay ningún algoritmo mejor que ir razonando.}.

\begin{equation}
\label{KTT:ejemplo1}
2x_1+3x_2+x_3 > 6 \overset{H.C.}{\to} u=0 \to x_i = 0 \to \neg F
\end{equation}
De \ref{KTT:ejemplo1} obtenemos que $2x_1+3x_2 + x_3 = 6$ y nos hemos quitado la desigualdad.

Ahora tenemos un sistema de 4 incógnitas $(x_1,x_2,x_3,u)$ y 4 ecuaciones. Resolviendo el sistema obtenemos:

\[
	\left.
		\begin{array}{r}
			x_1 - u = 0\\
			2x_2-3u = 0\\
			4x_3- u = 0\\
			2x_1+3x_2+x_3 = 6
		\end{array}
	\right\} \to (x_1,x_2,x_3,u) = \left( \rfrac{8}{9},\rfrac{4}{3},\rfrac{2}{9},\rfrac{8}{9}\right)
\]

Llamamos $\gor{\vx} = (\rfrac{8}{9},\rfrac{4}{3},\rfrac{2}{9})$. Hemos encontrado un punto que $KKT(\gor{\vx})$.

Por otro lado: como $\grad f_i(\vx) = (-2,-3,1) ∀\vx$, todos los $\grad f_i$ son linealmente independientes para todos los $\vx$, con lo que todos los puntos cumplen la cualificación de las restricciones.

Por el \fref{thm:cualificacionYKKT} como todos los puntos cumplen la cualificación de las restricciones, todos los mínimos del problema necesariamente son $KKT$. 
%
$\gor{\vx}$ es el único que cumple KKT, con lo que es el único mínimo que hay. 
%
Como sólo hay un mínimo, éste tiene que ser el mínimo global.

\end{example}

\begin{example}

Resolver gráficamente y determinar los puntos que verifican KKT en el siguiente problema:

\begin{ioprob}
\goal{$\max 3x_1+x_2$}
\restrictions{$x_2-(1-x_1)^3≤0$}{$x_1≥0$}{$x_2≥0$}{}{}{}
\end{ioprob}

Lo primero es escribirlo como problema de minimización para poder utilizar lo visto hasta ahora:

\begin{ioprob}
\goal{$\min -3x_1-x_2$}
\restrictions{$x_2-(1-x_1)^3≤0$}{$-x_1≤0$}{$-x_2≤0$}{}{}{}
\end{ioprob}

Para comprobar KKT, calculamos el Lagrangiano:

\[
	L(\vx,\vu) = -3x_1 - x_2 + u_0[x_2-(1-x_1)^3] + u_1(-x_1) + u_2(x_2)
\]

Y construimos el sistema:

\[
	\left\{
	\begin{array}{lc}
		\dpa{L}{x_1} = -3 + 3u_0(1-x_1)^2 - u_1 = 0&(I)\\
		\dpa{L}{x_2} = (-1) u_0 - u_2 = 0 & (II)\\
		\left. 
			\begin{array}{c}
				u_0[x_2-(1-x_1)^3] = 0\\
				u_1(x_i) = 0\\
				u_2(x_2) = 0
			\end{array}
			\right\} &(HC)
	\end{array}
	\right.
\]

Y se resuelve.

\end{example}


\subsection{KTT con igualdad}

Podemos extender la noción de condición KKT a un problema en el que haya restricciones con igualdad.
%
Se dice que el punto $\bar{\vx}\in\mathbb{R}^n$, junto con los vectores de multiplicadores $\bar{\vu}\in\mathbb{R}^m$ y $\bar{\vv}\in\mathbb{R}^p$, verifica las condiciones KKT para (PI) si

\begin{align*}
\nabla f(\bar{\vx}) + \sum_{i=1}^m \bar{\vu}_i \nabla f_i(\bar{\vx}) + \sum_{i=1}^p \bar{v}_i\nabla h_i(\bar{\vx}) & = 0 \\
f_i(\bar{\vx}) &\leq 0, & i = 1,\ldots,m \\
h_i(\bar{\vx}) &= 0, & i = 1,\ldots,p \\
\bar{u}_i    & \geq 0, & i = 1,\ldots,m\\
\bar{u}_i f_i(\bar{\vx}) &  = 0, & i = 1,\ldots,m\\
\end{align*}


\obs Al añadir los multiplicadores $\gv$, la notación utilizada anteriormente hay que actualizarla.
%
Hasta ahora $\KKT{\vx} \dimplies \exists \vu \tq (\vx,\vu)$ cumple las condiciones $KKT$.
%
Ahora, incluimos los multiplicadores $\gv$, es decir:

\[
	\KKT{\vx} \dimplies \exists \vu,\vv \tq (\vx,\vu,\vv) \text{ cumplen } KKT
\]

También hay una condición de cualificación de las restricciones y un teorema con el resultado análogo al anterior para problemas con restricciones con igualdad.


\begin{defn}[Cualificación de las restricciones\IS con igualdad)] 
Dado un punto factible $\gvx$, sea $I=\{i:\, f_i(\gvx)=0\}$ el conjunto de restricciones activas en $\gvx$.

Se dice que $\gvx$ verifica la cualificación de las restricciones si  los vectores $\nabla f_i(\gvx)$, $i\in I$ y $\nabla h_i(\gvx)$, $i=1,\ldots,p$ son linealmente independientes.
\end{defn}


El teorema \ref{thm:cualificacionYKKT} sigue siendo cierto también para restricciones con igualdad y la demostración sigue un esquema muy similar.


\section{Problema dual}

Podemos definir un nuevo problema semi-equivalente a partir de uno dado que siempre es convexo y tiene otras propiedades interesantes.
%
En ocasiones es más fácil de resolver y en otras aporta información muy claramente que en el problema inicial no están definidas. 


Vamos a comenzar definiendo la función que da lugar al problema dual:

\begin{defn}[Función\IS lagrangiana]
Para un problema general (no necesariamente convexo) con $m$ restricciones con desigualdad y $p$ restricciones con igualdad, definimos:

\[
	L(\vx,\vu,\vv) = f(\vx) + \sum_i^m u_if_i(\vx) + \sum_i^p v_ih_i(\vx)
\]
\end{defn}

\paragraph{Interpretación:} Vamos a interpretar las restricciones como penalizaciones. Ahora el conjunto factible será todo $\real^n$, pero los puntos que no cumplan una restricción van a ser "penalizados". Definimos la función objetivo:

\[
	f(\vx) + \sum I_{-}(f_i(\vx)) + \sum I_{0}(f_i(\vx)) \to \left\{ 
		\begin{array}{c}
		I_{-} = \left\{
			\begin{array}{ccc}
				0 & si & x≤0\\
				∞ & si & x> 0\\
			\end{array}\right.\\
		I_{0} = \left\{
			\begin{array}{ccc}
				0 & si & x = 0\\
				∞ & si & x ≠ 0\\
			\end{array}\right.
		\end{array}
		\right.
\]

De esta manera hemos agrandado el conjunto factible y tenemos un problema equivalente, pero sin restricciones. Tomando valores interesantes en vez de $I_{-}$ e $I_{0}$, llegaríamos la función lagrangiana.


\begin{defn}[Función\IS dual]
	\[
		g(\vu,\vv) = \inf_{x\in D} L(\vx,\vu,\vv)
	\]
\end{defn}

Tomando la función dual, tenemos:

\[
	g(\vu,\vv) = \inf_{\vx\in D} L(\vx,\vu,\vv) ≤ L(\vx,\vu\vv) = f(x) + \sum u_if_i(\vx) + \sum v_if_i(\vx) \overset{(1)}{≤} f(\vx)
\]

$(1)$ si $\vx$ es un punto factible.

\begin{prop}
La función dual es una función cóncava (aunque el problema inicial $(PI)$ no sea convexo).
\end{prop}

\begin{proof}
\[
	g(λu_1 + (1-λ)u_2, λu_1 + (1-λ)v_2) = \min \{L(\vx,\vu,\vv)\} = \min \{λL(\vx,u_1,v_1) + (1-λ)L(\vx,u_2,v_2)\} \]
\[≥ λ\underbrace{\min\{L(\vx,u_1,v_1)\}}_{g(u_1,v_1)} + (1-λ) \underbrace{\min\{L(\vx,u_2,v_2)\}}_{g(u_2,v_2)}
\]
\end{proof}

La función dual es una cota inferior del valor óptimo del problema inicial. Podríamos considerar resolver el problema inicial buscando la mejor cota inferior de la función dual. De esta manera, igual llegamos a resolver el mismo problema. 
Los siguientes resultados nos ayudarán a resolver estas cuestiones:

Por si no nos creemos que sea una cota inferior, vamos a verlo formalmente como teorema (aunque no lo demostremos):

\begin{theorem}
Sea $\bar{p}$ el valor óptimo de un problema de optimización cuya función dual es $g(\vu,\vv)$. \textbf{Entonces:} $∀ \vu≥0,∀\vv, g(\vu,\vv) = \bar{p}$
\end{theorem}
\begin{proof}
No entra en los contenidos de este curso.
\end{proof}


\begin{defn}[Problema\IS dual]
Sea $g(\vu,\vv)$ la función dual asociada a un problema $(PI)$.

\begin{ioprob}
\goal{$\max g(\vu,\vv)$}
\restrictions{$u≥0$}{}{}{}{}{}
\end{ioprob}
\end{defn}

\begin{itemize}
	\item Llamaremos \concept{problema\IS primal} al problema inicial.
	\item El problema dual es \textbf{siempre convexo}, independientemente de que lo sea o no el problema primal al que corresponde.
	\item Un punto $(\vu,\vv)$ con $\vu≥0$ y $g(u,v) < -∞$ se llama \concept{solución\IS factible dual}.
	\item Llamamos \concept{solución\IS factible dual óptima} al punto $(\vu,\vv)$ que resuelve el problema dual.
	\item El problema dual tiene conjunto factible convexo, función cóncava y es un problema convexo\footnote{Consultar la definición de problema convexo \ref{defn:problema_convexo}}. 
	El problema es que la función dual puede ser muy complicada.
\end{itemize}


\subsection{Problemas duales}

\paragraph{El problema dual de un problema lineal en forma estándar}

\begin{center}
\begin{tabular}{ll}
minimizar & $c^\top x$  \\
s.a. & $A\vx=b$,     \\
	 & $\vx\geq 0$.  
\end{tabular}
\end{center}

La función lagrangiana asociada es

\[
L(\vx,\vu,\vv) = -b^\top \vv + (c+A^\top \vv -\vu)^\top \vx.
\]

Y con la lagrangiana podemos construir la función dual:

\[
	g(\vu,\vv)=
		\left\{
			\begin{array}{cl} 
				-b^\top \vv & \text{ si }  c+A^\top \vv -\vu=0\\
				-\infty & \text{ en caso contrario.}
			\end{array}
		\right.
	\]


Vemos que $\min c^\top \vx$ como teníamos inicialmente es lo mismo que $\max{b^\top \vv}$.
%
Transcribiendo las restricciones en términos de los multiplicadores obtenemos el problema dual asociado:

\begin{center}
\begin{tabular}{ll}
maximizar & $b^\top v$  \\
s.a. & $A^\top v \leq c$       
\end{tabular}
\end{center}

\paragraph{El dual de un problema lineal en forma canónica}

Dado un problema en forma canónica, vamos a escribir su dual asociado.

\begin{center}
\begin{tabular}{ll}
maximizar & $c^\top x$  \\
s.a. & $Ax\leq b$,     \\
	 & $x\geq 0$.  
\end{tabular}
\end{center}

Calculando la lagrangiana y la función dual obtenemos el problema dual asociado:

\begin{center}
\begin{tabular}{ll}
minimizar & $b^\top u$  \\
s.a. & $A^\top u \geq c$,     \\
	 & $u\geq 0$.  
\end{tabular}
\end{center}



\subsubsection{Resumen}

A continuación, una tabla que resume el cálculo de problemas duales a partir de primales.
\begin{table}[hbtp]
\centering
\begin{tabular}{c|c|c}
	Primal $\equiv$ (P) &
	$\begin{array}{c} \min c'x\\Ax = b\\x≥ 0\end{array}$ & 
	$\begin{array}{c} \max c'x\\Ax ≤ b\\x≥ 0\end{array}$
\\
\hline
	Dual $\equiv$ (D) & 
	$\begin{array}{c} \max b'u\\A'u ≤ c\end{array}$ & 
	$\begin{array}{c} \min b'u\\A^\top u ≥ c\\u≥ 0\end{array}$
\\
\end{tabular}
\caption{Resumen de pasar del problema primal al dual.}
\end{table}

\obs El dual del dual es el primal.

\begin{example}
Escribe el problema dual del siguiente problema de optimización lineal.

\begin{ioprob}
\goal{$\min 6x_1 + 8x_2$}
\restrictions{$3x_1+x_2 \geq 4$}{$5x_1 + 2x_2\geq 7$}{$x_1\geq 0,\ x_2\geq 0$}{}{}{}
\end{ioprob}

\doneby{Dejuan} 


\begin{ioprob}
\goal{$\max 4u_1 + 7u_2$}
\restrictions{$3u_1+5u_2 ≥ 6$}{$u_1+2u_2 ≥ 8$}{$u_1 ≥0$}{$u_2≥0$}{}{}
\end{ioprob}

\end{example}

\begin{example}
Escribe el problema dual del siguiente problema de optimización lineal.

\begin{ioprob}
\goal{$\min 8x_1 + 3x_2 - 2x_3$}
\restrictions{$x_1-6x_2+x_3 \geq 2$}{$5x_1 + 7x_2 - 2x_3= -4$}{$x_1\geq 0,\ x_2\geq 0,\ x_3\geq 0$}{}{}{}
\end{ioprob}

\doneby{Dejuan}

Necesitamos que todas las restricciones sean del mismo tipo:

\begin{ioprob}
\goal{$\min 8x_1 + 3x_2 - 2x_3$}
\restrictions{$x_1-6x_2+x_3 \geq 2$}{$5x_1 + 7x_2 - 2x_3+x_4 ≥ -4$}{$x_1\geq 0,\ x_2\geq 0,\ x_3\geq 0,x_4≥0$}{}{}{}
\end{ioprob}

Y ahora construimos el dual, utilizando la tabla. (Aquí estamos calculando el dual del dual)

\begin{ioprob}
\goal{$\max 2u_1-4u_2$}
\restrictions{$u_1+5u_2 ≤ 8$}{$-6u_1+7u_2 ≤ 3$}{$u_1-2u_2≤-2$}{$u_1,u_2≥0$}{}{}
\end{ioprob}

\end{example}

\subsection{Dualidad fuerte y débil}


\begin{defn}[Dualidad\IS débil]
Sean $\bar{p},\bar{d}$ sendos valores óptimos del problema primal y su dual.

Entonces:\[\bar{d} ≤ \bar{p}\]
\end{defn}

De aquí podemos sacar 2 conclusiones:

\begin{itemize}
	\item Si el primal no es acotado ($\bar{p} = -∞$), entonces el dual no es factible.
	\item Si el dual no es acotado ($\bar{d} = ∞$), entonces el primal no es factible.
\end{itemize}


\begin{defn}[Dualidad\IS fuerte]
$\bar{p} = \bar{d}$
\end{defn}


\begin{theorem}[Dualidad fuerte para problemas lineales]

Consideremos un problema lineal en forma estándar $(P)$ y el correspondiente problema dual $(D)$. 

Si $(P)$ tiene solución factible óptima finita, también la tiene $(D)$ y los correspondientes valores óptimos son iguales.
\end{theorem}

\begin{proof}
\label{proof:dualidadFuerte}
\[u' \equiv c_B'B^{-1}\]

\[
\begin{array}{c}
u'b = c'_BB^{-1}b
c'x = c'_BB^{-1}n]c_N'0
\end{array}
\]


$u$ es factible para $(D)$ $\dimplies$ 

\[u'A ≤ c' = c'_BB^{-1}\left(B | N \right) ≤ (c'_B,c'_N)\dimplies \underbrace{c_B'B^{-1}N}_{z_j} ≤ c'_N\]

Demostrar que $u$ es factible es lo mismo que demostrar que $z_j ≤ c_j$ en la tabla del simplex. Como $\bar{x}$ es el óptimo, en la última fila de simplex tenemos todos $≤0$, con lo que $z_j ≤ c_j$

\paragraph{Conclusión: } Encontrar un óptimo para el primal es encontrar un factible para el dual.

\end{proof}

\obs
\[
\dpa{\bar{z}}{b_i} = u_i
\]

Es decir, el resultado de incrementar uno de los coeficientes del problema primal, corresponde al valor del multiplicador.

\paragraph{Dualidad fuerte para problemas lineas}
\begin{itemize}
	\item La condición de factibilidad de $\bar{u}$ equivale a la de optimalidad de $\gx$.
	\item Resolver el primal y el dual es equivalente a partir de la relación obtenida en la demostración \ref{proof:dualidadFuerte}:
	\[
		\bar{u}^\top =  c_B^\top B^{-1}
	\]
\end{itemize}

\subsection{Interpretación económica del problema dual}

La solución del problema dual del pastelero es $u = \left(\rfrac{1}{3},0,10\right)$, que son los valores que aparecían en el problema \ref{ejer:pastelero}. Además, vemos también que $\bar{p} = \bar{d} = 775$. Es decir, la dualidad es fuerte. 


En la solución de este problema (\ref{ejer:pastelero}), se preguntaba la modificación del valor objetivo al aumentar en 1 cada una de las cantidades y la solución obtenida se corresponde con la solución del problema dual,
como nos indicaba la observación anterior.


\subsection{Algoritmo simplex-dual}

\paragraph{Motivación:} 
Tenemos un problema de programación lineal que resolvemos con simplex normal. 
Llegamos a una solución óptima que es factible del dual. Ahora, imaginemos que cambiamos $b$, con lo que las restricciones dejan de cumplirse. El estado de la tabla del simplex se mantiene, salvo la columna de la izquierda. 
Este es un punto de partida para este algoritmo. Otro caso ventajoso de este algoritmo se da al tener ¿coeficientes negativos?





\begin{example} Resolver el problema:

\begin{ioprob}
\goal{$\min 3x_1 + 2x_2$}
\restrictions{$3x_1+x_2 \geq 3$}{$4x_1 + 3x_2 \geq 6$}{$x_1+x_2\leq 3$}{$x_1\geq 0,\ x_2\geq 0.$}{}{}
\end{ioprob}


Lo primero es pasarlo a forma estándar:


\begin{ioprob}
\goal{$\min{3x_1 + 2x_2}$}
\restrictions{$-3x_1-x_2+x_3 = -3$}{$-4x_1 - 3x_2 + x_4 = -6$}{$x_1+x_2 + x_5 = 3$}{$x_i\geq 0,\ i=1,\ldots, 5.$}{}{}
\end{ioprob}


En este caso, no podemos empezar con el algoritmo del simplex. 
%
Según el simplex normal, tenemos una solución óptima ($z_j-c_j < 0\;∀j$) y la solución sería $(0,0,-3,-6,3)$, que no es factible por la restricción: $x_i > 0$.
%
Al ser óptima en el primal, es factible en el dual y podemos aplicar el algoritmo del simplex-dual, que sirve para estos casos.
%
Vamos a ver la evolución de las tablas.



\begin{center}
\begin{tabular}{r | rrrrr}
$c$ & 3 & 2 & 0 & 0 & 0 \\ \hline
Variables & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\ \hline
$x_3 = -3$ & -3 & -1 & 1 & 0 & 0 \\
$x_4=  -6$   & -4 & \textcolor{red}{-3} & 0 & 1 & 0  \\
$x_5=3$ & 1 & 1 & 0 & 0 & 1 \\ \hline
$z_j-c_j$ & -3 & -2 & 0 & 0 & 0 
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{r | rrrrr}
$c$ & 3 & 2 & 0 & 0 & 0 \\ \hline
Variables & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\ \hline
$x_3 = -1$ & \textcolor{blue}{-$\rfrac{5}{3}$} & 0 & 1 & -$\rfrac{1}{3}$ & 0 \\
$x_2=  2$   & $\rfrac{4}{3}$ & 1 & 0 & -$\rfrac{1}{3}$ & 0  \\
$x_5=1$ & -$\rfrac{1}{3}$ & 0 & 0 & $\rfrac{1}{3}$ & 1 \\ \hline
$z_j-c_j$ & -$\rfrac{1}{3}$ & 0 & 0 & -$\rfrac{2}{3}$ & 0 
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{r | rrrrr}
$c$ & 3 & 2 & 0 & 0 & 0 \\ \hline
Variables & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\ \hline
$x_1 = \rfrac{3}{5}$ & 1 & 0 & $-\rfrac{3}{5}$ & $\rfrac{1}{5}$ & 0 \\
$x_2=  \rfrac{6}{5}$   & 0 & 1 & $\rfrac{4}{5}$ & $-\rfrac{3}{5}$ & 0  \\
$x_5=\rfrac{6}{5}$ & 0 & 0 & $-\rfrac{1}{5}$ & $\rfrac{2}{5}$ & 1 \\ \hline
$z_j-c_j$ & 0 & 0 & $-\rfrac{1}{5}$ & $-\rfrac{3}{5}$ & 0 
\end{tabular}
\end{center}

Ya hemos llegado a una solución óptima factible.


\textbf{Vamos a verlo gráficamente paso a paso}

\begin{figure}[hbtp]
\centering
\begin{tikzpicture}[scale=1.5]
\draw[thick,->] (0,0) -- (3.5,0);
\draw[thick,->] (0,0) -- (0,3.5);
\draw[thick,-] (3,0) -- (0,3);
\draw[thick,-] (1,0) -- (0,3);
\draw[thick,-] (1.5,0) -- (0,2);
\filldraw[fill=blue, opacity = 0.4] (3/5,6/5) -- (1.5,0) -- (3,0) -- (0,3) -- cycle;
\node [label={below:$(1)$},nodepoint, inner sep=2pt,color=red] at (0,0) {};
\node [label={left:$(2)$},nodepoint, inner sep=2pt,color=blue] at (0,2) {};
\node [label={below left:$(3)$},nodepoint, inner sep=2pt,color=green] at (3/5,6/5) {};
\end{tikzpicture}
\caption{Evolución del algoritmo del simplex-dual.}
\end{figure}

La solución del primal y su correspondiente valor objetivo:

\[\left(x_1,x_2\right) = \left(\frac{3}{5},\frac{6}{5}\right) \;\;\bar{z}=\frac{21}{5}\]


La solución del dual es 
\[
\bar{u}^\top = c_B^\top B^{-1} = 
	(3,2,0)
		\begin{pmatrix}
			-3/5 & 1/5 & 0 \\ 
			4/5 & -3/5 & 0 \\ 
			-1/5 & 2/5 & 1
		\end{pmatrix} = (-1/5,-3/5,0).
\]


\end{example}


\begin{prop}
Tras el pivoteo se mantiene la factibilidad dual (optimilidad primal)

\[
	\hat{z}_j - \hat{c}_j = (z_j-c_j) - \frac{y_{rj}}{y_{rk}} (z_k-c_k)
\]

\end{prop}


\begin{proof}

\[
	\underbrace{(z_j-c_j)}_{≤0} - \frac{y_{rj}}{y_{rk}}\underbrace{(z_k-c_k)}_{≤0}
\]

También sabemos que $y_{rk}<0$.  Distingamos 2 casos:

\[
	y_{rj} ≥ 0 \implies \hat{z_j}-\hat{c_j} ≤ z_j-c-j ≤ 0
\]

\[
	y_{rj} ≤ 0 \implies \hat{z_j}-\hat{c_j} ≤ 0 \dimplies \frac{z_j-c_j}{y_{rj}}≥\frac{z_l-c_l}{y_{rk}}
\]
Esta segunda condición se cumple por el criterio de entrada a la base (selección de $k$).

Hemos visto que para cualquier valor de $y_{rj}$, se mantiene la condición de optimalidad ($z_j-c_j <0$)
\end{proof}

\begin{prop}
Tras el pivoteo se mejora el objetivo del dual, es decir:

\[
	\sum_{i\in \hat{B}} c_i\gor{b_i} > \sum_{i\in B} c_i\gor{b_i}
\]

\end{prop}

% % Obtenido de las traspas.
\begin{proof}
	\begin{align*}
		\sum_{i\in \hat{B}} c_i\bar{b}_i &=  \sum_{i\in B} c_i\left(\bar{b}_i - \frac{\bar{b}_r}{y_{rk}}y_{ik} \right) + c_k\frac{\bar{b}_r}{y_{rk}}\\
		\sum_{i\in \hat{B}} c_i\bar{b}_i &= \sum_{i\in B} c_i\bar{b}_i - \frac{\bar{b}_r}{y_{rk}} ( \sum_{i\in B} c_iy_{ik} - c_k)\\
		\sum_{i\in \hat{B}} c_i\bar{b}_i &= \sum_{i\in B} c_i\bar{b}_i - \frac{\bar{b}_r}{y_{rk}} ( z_k - c_k) > \sum_{i\in B} c_i\bar{b}_i.
	\end{align*}
\end{proof}

\begin{prop}

Si $y_{tj} ≥ 0 ∀j$, el problema primal no es factible
\end{prop}

\begin{proof}
Si existiera $\vx≥0$ con $A\vx = \vb$ tendríamos:

\[
	A\vx = \vb \dimplies B^{-1}A\vx = \gor{b} \dimplies \sum_{j=1}^n y_jx_j = \gor{b}
\]

En particular, $\gor{b_r} = \sum_j y_{rj}x_j ≥ 0$, lo que no puede ser por el criterio de elección de $r$ ya que ésta provoca que $\gor{b_r} < 0$.

\end{proof} 


\section{KTT y dualidad}

Hasta ahora hemos visto las condiciones $KKT$ por un lado y el problema dual por el otro. Ahora vamos a ver qué relación tiene un concepto con el otro.

\paragraph{Holgura complementaria en problemas lineales}

Sean $\vx$ y $\vu$ las soluciones de $(P) \equiv \max c^\top x \;\;s.a.\;\; A\vx ≤ b$ y de $(D)$ su dual.

En el caso de dualidad fuerte, debido a que $c^\top \gvx = b^\top$tenemos:

\[
	\vc^\top \vx ≤ \vu^\top A \vx ≤ \vu^\top \vb = \vc^\top \vx
\]

%%%%%%%%%%%% Revisar desde aquí
\begin{prop}
Si una restricción no se activa en el primal, la variable correspondiente en el dual es 0. 
También se cumple intercambiando el papel del dual y del primal.

Si una variable del dual es positiva, la restricción correspondiente del primal está activa (por la holgura complementaria).

\end{prop}
\begin{proof}

\end{proof}

Estas condiciones caracterizan las soluciones de $(P)$ y de $(D)$ porque ... 




\begin{example}

Vamos a ver un ejemplo para entender los resultados anteriores.


\begin{ioprob}
\goal{$\max 3x_1 + 4x_2 + 6x_3$}
\restrictions{$3x_1+x_2+x_3≤2$}{$x_1+2x_2+6x_3≤1$}{$x_i≥0$}{}{}{}
\end{ioprob}

\textbf{Escribe el problema dual:}

\begin{ioprob}
\goal{$\min 2u_1 + u_2$}
\restrictions{$3u_1 + 1u_2 ≥ 3$}{$1u_1 + 2u_2 ≥ 4$}{$1u_1 + 6u_2 ≥ 6$}{$u_i ≥ 0$}{}{}
\end{ioprob}


\textbf{Resuelve el dual gráficamente:}

Gracias a pasar al dual podemos representar gráficamente un problema que inicialmente tenía 3 variables y no podríamos dibujar con tanta facilidad.
%
Resolvemos el problema como en el tema 1.



\begin{figure}[hbtp]
\centering
\begin{tikzpicture}[scale=1.5]
\filldraw[shading = axis,rectangle, left color=blue!90!white, right color=blue!30!white,shading angle=135,blue!30!white] (0,3) -- (2/5,9/5) -- (3,0.5) -- (6,0) -- (7.5,0) -- (7.5,4) -- (0,4) -- cycle;
\draw[thick,->] (0,0) -- (8,0);
\draw[thick,->] (0,0) -- (0,4.2);
\draw[thick,-] (1,0) -- (0,3);
\draw[thick,-] (4,0) -- (0,2);
\draw[thick,-] (6,0) -- (0,1);
\node [label={above right:$(Optm.)$},nodepoint, inner sep=2pt,color=red] at (2/5,9/5) {};
\draw[green] (1,0) -- (0,2);
%\node [label={left:$(2)$},nodepoint, inner sep=2pt,color=blue] at (0,2) {};
%\node [label={below left:$(3)$},nodepoint, inner sep=2pt,color=green] at (3/5,6/5) {};
\end{tikzpicture}
\caption{Resolución gráfica del ejemplo. En verde la recta $2x+y=c$. Queremos minimizar $c$, con $x,y\in S$.}
\end{figure}

Ese punto corresponde a las restricciones 1 y 2 activas, es decir:
%
\[
	\begin{array}{c}
		3u_1+u_2 = 3\\
		u_1+2u_2 = 4
	\end{array}

\]
%
Resolviendo el sistema, obtenemos: 
%
\[
	\left(\gor{u_1},\gor{u_2},\gor{d}\right) = \left(\rfrac{2}{5},\rfrac{9}{5},\rfrac{13}{5}\right)
\]



En el óptimo dual, la 3 restricción no está activa, con lo que $x_3 = 0$. Además, como los $\gor{u_1} ≠ 0$ implica que la restricción correspondiente se verifica con igualdad. Lo mismo con $\gor{u_2}≠0$.Recapitulando, tenemos:

\[
\left.\begin{array}{c}
	3x_1+x_2+x_3 = 2\\
	x_1+2x_2+6x_3 = 1
\end{array}\right\} \implies (x_3 = 0) + \text{ con igualdad } \implies \left\{\begin{array}{c}3x_1 + x_2 = 2\\x_1 + 2x_2 = 1\end{array}\right.
\]

\obs Si no supiéramos que $x_3 = 0$, por la dualidad fuerte tenemos otra ecuación que sería:

\[
	3x_1 + 4x_2 + 6x_3 = \gor{d}
\]
\end{example}

\subsection{Problemas no lineales}

\subsubsection{Holgura complementaria}.

Consideramos un problema general (no necesariamente convexo) para el que se cumple la dualidad fuerte

\begin{center}
\begin{tabular}{lll}
minimizar & $f(x)$ \\
s.a. & $f_i(x)\leq 0,$  &  $i=1,\ldots,m$ \\
	 & $h_i(x) = 0,$  &  $i=1,\ldots,p.$
\end{tabular}
\end{center}

Sean $\bar{x}$ y $(\bar{u},\bar{v})$ las soluciones óptimas del primal y el dual respectivamente.

\begin{align*}
f(\bar{x}) &= g(\bar{u},\bar{v}) = \inf_x \big(f(x) + \sum_{i=1}^m \bar{u}_i f_i(x) + \sum_{i=1}^p \bar{v}_i h_i(x) \big)\\
&\leq f(\bar{x}) + \sum_{i=1}^m \bar{u}_i f_i(\bar{x}) + \sum_{i=1}^p \bar{v}_i h_i(\bar{x}) \leq f(\bar{x})
\end{align*}


Todas las desigualdades son en realidad igualdades, es decir:

\begin{align*}
f(\bar{x}) &= g(\bar{u},\bar{v}) = \inf_x \big(f(x) + \sum_{i=1}^m \bar{u}_i f_i(x) + \sum_{i=1}^p \bar{v}_i h_i(x) \big)\\
&= f(\bar{x}) + \sum_{i=1}^m \bar{u}_i f_i(\bar{x}) + \sum_{i=1}^p \bar{v}_i h_i(\bar{x}) 
\end{align*}

De esta cadena de igualdades obtenemos las condiciones de holgura complementaria, ya que $\bar{x}$ minimiza $L(x,\bar{u},\bar{v})$ en $x$.
%
Además, se cumple $\sum_{i=1}^m \bar{u}_i f_i(\bar{x})=0$.
%
Como consecuencia:

	\[
		\bar{u_i} f_i(\bar{x}) = 0, \ \ \ i=1,\ldots,m.
	\]


\subsubsection{Dualidad y KKT}

Supongamos las siguientes condiciones:

\begin{itemize}
\item Las funciones $f,f_1,\ldots, f_m,h_1,\ldots,h_p$ son diferenciables.
\item Las soluciones óptimas de los problemas primal y dual se alcanzan en los puntos
 $\bar{x}$ y $(\bar{u},\bar{v})$.
\item Hay dualidad fuerte: $f(\bar{x})=\bar{p}=\bar{d}=g(\bar{u},\bar{v})$.
\end{itemize}


\

Entonces, los óptimos deben cumplir las condiciones KKT:

\begin{align*}
\nabla f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \nabla f_i(\bar{x}) + \sum_{i=1}^p \bar{v}_i\nabla h_i(\bar{x}) & = 0 \\
f_i(\bar{x}) &\leq 0, & i = 1,\ldots,m \\
h_i(\bar{x}) &= 0, & i = 1,\ldots,p \\
\bar{u}_i    & \geq 0, & i = 1,\ldots,m\\
\bar{u}_i f_i(\bar{x}) &  = 0, & i = 1,\ldots,m\\
\end{align*}



\begin{theorem}[convexidad y KKT]
\label{thm:convKKT}

Consideremos un problema primal convexo y con funciones diferenciables.
%
Sean $\bar{x}$ y $(\bar{u},\bar{v})$ puntos que satisfacen las condiciones KKT. 

\textbf{Entonces} $\bar{x}$ es la solución del problema primal, $(\bar{u},\bar{v})$ es la solución del dual y  $\bar{p} =  \bar{d}$.
\end{theorem}

\begin{proof}
	\todoby{Parra}
\end{proof}

En problemas convexos la dualidad fuerte no se cumple en general, pero puede probarse bajo condiciones adicionales.

\begin{defn}[Condición de Slater]

Sea un problema convexo:

\begin{tabular}{lll}
minimizar & $f(x)$ & \\
s.a. & $f_i(x)\leq 0,$  &  $i=1,\ldots,m$ \\
	 & $Ax=b,$  &  
\end{tabular}

en el que $\mbox{rango}(A)=p$, se cumple la condición de Slater si existe $\bar{x}\in D$  tal que $f_i(\bar{x})<0$, para $i=1,\ldots,m$, y $A\bar{x}=b$. 
\end{defn}

Gracias a esta condición adicional, tenemos el siguiente teorema:

\begin{theorem}
Si en un problema convexo se verifica la condición de Slater, entonces $\bar{p} =  \bar{d}$.
\end{theorem}

\begin{proof}
	\todoby{Parra}
\end{proof}


