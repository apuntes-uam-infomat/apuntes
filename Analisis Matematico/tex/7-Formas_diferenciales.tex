\chapter{Teorema de Stokes}

\section{Lenguaje de las formas diferenciales}

\paragraph{0-formas}
\index{Formas\IS 0}

Son funciones escalares definidas en un abierto de $\real^n$
\[\appl{f}{\Omega\subset\real^N}{\real}\]

Operaciones habituales:
\begin{itemize}
\item Suma: sí
\item Producto: sí
\item Composiciones: no (porque no cuadran las dimensiones)
\end{itemize}

\paragraph{1-formas}
\index{Formas\IS 1}

Sea $\mathcal{C} = \{e_1,e_2,...,e_n\}$ la base canónica en $\real^N$.

Sea $L$ una aplicación lineal
\[\appl{L}{\real^N}{\real}\]

Que recordamos que cumplen:
\[ L(\gx+\gy) = L(\gx)+L(\gy); L(\lambda\gx) = \lambda L(\gx)\]

Definimos $\gy\in\real^N \leadsto \gy = \displaystyle\sum_1^n y_i e_i$, con lo que \[L(\gy) = \sum y_i L(e_i)\]

Entonces \[\left.\begin{array}{cc}
v_i = L(e_i)\\
y_i = P_i(\gy)
\end{array}\right\} \rightarrow L(\gy) = \sum_i v_iP_i(y)\]

Siendo $P_i$ las proyecciones, una base del espacio dual.

\textbf{Notación:}

$P_i \equiv dx_i$.

$dx_i[\gy] \equiv P_i(\gy) = y_i$

Entonces, dado un $\gv$ podemos construir 
\[L \equiv \sum_i^N v_idx_i\]

\[L[\gy] = \sum_i^N v_idx_i[\gy] = \sum_i^N v_iy_i\]

\begin{defn}[1-forma]
\[\omega(\gx)= \sum_1^N F_i(\gx) dx_i\]

\begin{itemize}
\item Se evalúa en $\gx\in\real$
\item Actúa sobre $\gy\in\real^N$ 
\end{itemize}

Es decir, \[\omega(\gx)[\gy] = \left(\sum F_i(\gx)dx_i\right)[\gy] = \sum F_i(\gx)dx_i[\gy] = \sum F_i(\gx)y_i\]
\end{defn}

Indicaremos con paréntesis el punto en el que estamos evaluando, y con corchetes el punto en el que estamso actuando.

\textbf{Operaciones:}
\begin{itemize}
\item Sumar: sí (lo razonable)
\item Multiplicar: por una función escalar sí está definida.
\end{itemize}


\paragraph{Ejemplo:}

Supongamos $f$ una función escalar (una 0-forma).

\[\grad f(\gx) = \left( \dpa{f}{x_i}(\gx)\right)\, i=1,...,N\]

Nos podemos construir una 1-forma desde el gradiente

\[\dpa{f}{x_i}(\gx)dx_i \]

A esta 1-forma en particular la llamaremos $df(\gx)$.

¿Utilidad? Ya la veremos, pero es una forma de escribir el producto escalar.
\[\pesc{\grad f(\gx),\gy} = df(\gx)[\gy]\]


\paragraph{2-formas}
\index{Formas\IS 2}

Punto de partida: Aplicaciones \textbf{bilineales alternadas}

\[\appl{\Phi}{\real^N\x\real^N}{\real}\]

Que cumplen \begin{itemize}
\item $\Phi([\gu,\gv]) = - \Phi([\gv,\gu]) \implies \Phi(\gu,\gu)=0$
\item $ \Phi([\gu+\gv,\gw]) = \Phi ([\gu,\gw]) + \Phi([u,w])$
\item$\Phi([\lambda \gu,\gv]) = \lambda \Phi([\gu,\gv])$
\end{itemize}

Consecuencias:

\begin{itemize}
\item $\Phi(\gor{r}, \gor{s}+\gor{t}) = \Phi(\gor{r}+\gor{s}) + \Phi(\gor{r}+\gor{t})$
\item $\Phi(\gu,\mu\gv) = \mu\Phi(\gu,\gv)$
\end{itemize}


\paragraph{Ejemplo} en $\real^3$ para facilitar las cuentas.

\[\Phi(\gu,\gv) = \Phi(u_1e_1+u_2e_2+u_3e_3,v_1e_1+v_2e_2+v_3e_3)\]
Aplicando las propiedades anteriores obtenemos:

\begin{gather*}
\overbrace{u_1v_1\Phi(e_1,e_1)}^{\equiv 0} + u_1v_2\Phi(e_1,e_2) + u_1v_3+\Phi(e_1,e_3)+\\
u_2v_1+\Phi(e_2,e_1)+u_2v_2+\Phi(e_2,e_2)+u_2v_3+\Phi(e_2,e_3)+\\
u_3v_1\Phi(e_3,e_1)+u_3v_2+\Phi(e_3,e_2)+u_3v_3+\Phi(e_3,e_3) = \\
\underbrace{(u_1v_2-u_2v_1)}_{\left|\begin{matrix}
u_1&u_2\\v_1&v_2
\end{matrix}\right|}\overbrace{\Phi(e_1,e_2)}^{C_1}+(u_1v_3-u_3v_1)\Phi(e_1,e_3)+(u_2v_3-u_3v_2)\Phi(e_2,e_3)
\end{gather*}

Hemos demostrado que \[\Phi(\gu,\gv) = C_1B_{12}(\gu,\gv) + C_2B_{13}(\gu,\gv) + C_3B_{23}(\gu,\gv)\]

\subparagraph{Notación:} $B_{ij} = dx_i\y dx_j$

\[dx\y dx_j [\gu,\gv] = \det \begin{pmatrix}
u_i&u_j\\v_i&v_j
\end{pmatrix} = \det \begin{pmatrix}
dx_i[\gu]&dx_j[\gu]\\dx_[\gv]&dx_j[\gv]
\end{pmatrix}\]

\begin{defn}[2-forma]
\[\beta = \sum_{i,j=1}^N F_i(\gx) dx_i\y dx_j\]
\begin{itemize}
\item Se evalúan en puntos $x\in\real^N$
\item Actúan sobre pares de vecotres $[\gu,\gv]\in\real^N\x\real^N$.
\end{itemize}

Es decir:

\[\beta(\gx)[\gu,\gv] = \sum F_{ij}(\gx) dx_i\y dx_j[\gu,\gv] = \sum F_{ij} \det \begin{pmatrix}
u_i&v_i\\u_j&v_j
\end{pmatrix}\]

\emph{Ojo} El cambio del orden (en el determiante)es aposta por la segunda propiedad de las 2 formas
\end{defn}


\subsubsection{K-Forma}

Vamos a dar una definición general de una k-forma.

Elementos básicos:
\[dx_{i_1} \y dx_{i_2}\y...\y dx_{i_k}[\gu^1,\gu^2,...,\gu^k] = \det\begin{pmatrix}
u_{i_1}^1 & ... & u_{i_k}^1\\
\vdots & \ddots & \vdots\\
u_{i_1}^k & ... & u_{i_k}^k
\end{pmatrix}\]

\paragraph{K-forma}

\[
\sum_{i_1,...,i_k=1}^N F_{i_1,...,i_k}(\gx)dx_{i_1} \y ... \y dx_{i_k}
\]

\begin{itemize}
\item Se evalúan en puntos $\gx\in\real^N$
\item Actúa sobre grupos de $K$ vectores.
\end{itemize}

\obs $i_j = i_s \implies dx_{i_j}\y dx_{i_s} = 0$

Esto nos dice que en $\real^N$, teniendo $K$-formas (con $K<N$) tenemos $\comb{N}{K}$ combinaciones distintas.

\obs Si $K>N$ y $\omega$ es una $k-forma \implies \omega \equiv 0$


\paragraph{Ejemplo:} En $\real^3$.

\begin{itemize}
\item 0-forma $\leadsto f(x,y,z) = 0$
\item 1-forma $\leadsto f_1(x,y,z)dx + f_2(x,y,z)dy + f_3(x,y,z)dz$
\item 2-forma $\leadsto g_1(x,y,z)\df{y,z} + g_2(x,y,z)dz\y dx + g_3(x,y,z)dx\y dy$
\item 3-formas $\leadsto h(x,y,z)\df{x,y,z}$
\end{itemize}

\index{Orden cíclico}
\emph{Ojo} Al cambio en la 2-forma, que es $dzdx$. Esto es para seguir el \textbf{orden cíclico} (por temas de la orientación). Esto es $x\to y \to z \to x$


\obs Las \textit{funciones escalares } las podemos interpretar como 0-formas y como 3-formas. Los \textit{campos vectoriales} los podemos interpretar como 1-formas y también como 2-formas.

\paragraph{Notación}
Para escribir un conjunto de subíndices $\{i_1,i_2,...,i_k\} \equiv I$

También acortaremos  $dx_{i_1} \y dx_{i_2} \y ... \y dx_{i_k} \equiv dx_I$. 

La definición quedaría $\displaystyle \sum_I F_I(\gx)dx_I$

\subsection{Operaciones}
Siempre se puede multiplicar por 0-formas y sumar (formas del mismo orden). Estas operaciones son triviales porque son operaciones internas.

Vamos a definir las operaciones externas:

\begin{defn}[Producto \IS exterior (de k-formas)]
Sea \[\omega = \sum_I F_I dx_I (k-forma\in\real^N)\]
\[\beta = \sum_J G_j dx_J (s-forma\in\real^N)\]

\[\omega\y\beta = \sum_{I,J} F_IG_J dx_I\y dx_J (k+s-forma)\]
\end{defn}

\obs Si $K+S>N \implies \omega\y\beta=0$


\paragraph{Ejemplo} $\real^3$

Sea \[\omega = f_1(x,y,z)dx + f_2(x,y,z)dy + f_3(x,y,z) dz\]
\[\beta= g_1(x,y,z)dx + g_2 (x,y,z) dy + f_3 (x,y,z) dz\]

Vamos a calcular $\omega\y\beta$

\[
\omega\y\beta  = f_1g_1dx\y dx + f_1g_2dx\y dy + f_1g_3dx\y dz + f_2g_1dy\y dx + f_2g_2dy\y dy + f_2g_3dy\y dz+ f_3g_1dz\y dx+f_3g_2dz\y dy+f_3g_3dz\y dz
\]
Tachamos los que sean 0 ($dx\y dx = 0$) y tenemos cuidado con el orden cíclico.

\[
(f_2g_3-f_3g_2)dy\y dz + (f_3g_1-f_1g_3)dz\y dx + (f_1g_2 - f_2g_1) dx \y dy
\]

Partiendo de 2 campos vectoriales que eran 1-formas hemos llegado a una 2-forma. 

\obs Acabamos de llegar al producto vectorial de $\real^3$ 

\[\overrightarrow{F}\x\overrightarrow{G} = 
\left((f_2g_3-f_3g_2),(f_3g_1-f_1g_3),(f_1g_2 - f_2g_1)\right)\]

\begin{defn}[Diferencial \IS exterior (de k-formas)]

\[d(\sum_I F_i(\gx)dx_I) = \sum_I \underbrace{dF_I}_{1-forma} \overbrace{\y}^{Prod.ext} \underbrace{dx_I}_{k-forma}\]
\end{defn}

\paragraph{Ejemplo} en $\real^3$

Sean $G = (g_1,g_2,g_3)$

$\omega = $

Vamos a calcular $d\omega$

\[d\omega dg_1\y dy\y dz + dg_2 \y dz\y dz + dg_3 \y dx \y dy = COMPLETAR = \dpa{g_1}{x} + \dpa{g_2}{y}+\dpa{g_3}{z}dx\y dy\y dz\] ¡Que es la divergencia!

\paragraph{Ejemplo 2}

\[d(F_1 dx + F_2dx+F_3dx)\]
\[ = dF_1\y dx + dF_2 \y dy + dF_3\y dz\]
\[\left(\dpa{F_1}{x}dx + \dpa{F_1}{x}dy + \dpa{F_1}{x}dz +
\dpa{F_2}{x}dx + \dpa{F_2}{x}dy + \dpa{F_2}{x}dz+
\dpa{F_3}{x}dx + \dpa{F_3}{x}dy + \dpa{F_3}{x}dz\right)\]
\[\left(\dpa{F_3}{y} - \dpa{F_2}{z} \right)dy\y dz + \left(\dpa{F_1}{z} - \dpa{F_3}{dx}\right)dz \y dx + \left(\dpa{F_2}{x} - \dpa{F_1}{y}\right) dx \y dy\]

Nos queda un campo de la forma:

\[\left(\left(\dpa{F_3}{y} - \dpa{F_2}{z} \right) ,\left(\dpa{F_1}{z} - \dpa{F_3}{x}\right),\left(\dpa{F_2}{x} - \dpa{F_1}{y}\right)\right)\]
Que es el rotacional.

\paragraph{Conclusión}
Tenemos un campo en $\real^3$ que podemos interpretar como 1-forma o como 2-forma. 

\begin{itemize}
\item La diferencial exterior de un campo interpretado como 1-forma tendremos la 2-forma asociada a la divergencia.

\item La diferencial exterior de un campo interpretado como 2-forma tendremos la 3-forma asociada al rotacional.

\item El producto exterior de 2 campos interpretados como 2-formas nos da el campo aosciado al producto vectorial (o algo parecido. Revisar)

\item ¿Cómo tengo que interpretar los campos para conseguir un producto escalar?

\end{itemize} 


\paragraph{Propiedades}
\begin{itemize}
\item $d(\omega + \beta) = d\omega + d\beta$
\item $\omega = \sum_I F_i dx_i$ una k-forma

$f\leadsto 0-forma$

$f\omega = \sum_I fF_Idx_I$

$d(f\omega) = \sum_I d(fF_I) \y dx_I$, donde $d(fF_I) = \sum_{j=1}^N \dpa{f}{x_j}dF_Ix_j + \sum_{j=1}^N f\dpa{F_I}{x_j}dx_j$

Es decir, tenemos:

Revisar
\begin{gather*}
\sum_{I,J} \dpa{f}{x_j} F_Idx_j\y dx_i + \sum_{I,J} f\dpa{F_I}{x_j}dx_j\y dx_I\\
 = \sum_{I,J} \underbrace{\dpa{f}{x_j}dx_j}_{\equiv df} \y F_Idx_I + \underbrace{\sum_{I}\underbrace{\left(\sum_J \dpa{F_I}{x_j}dx_j\right)}_{dF_I}) \y dx_I }_{d\omega}\\
 = \sum_I df\y F_I dx_I \y d\omega = \\
 \underbrace{df \y \sum_IF_Idx_I}_{\omega} + d\omega 
\end{gather*}
\paragraph{Conclusión} Hemos llegado a demostrar que el producto de la derivada es primero derivado por ...

\[d(f\omega) = df\y\omega + fd\omega\]
\begin{itemize}
\item df $\to$ 1-forma
\item $\omega \to $ k-forma
\item $f \to$ 0-forma
\item $d\omega$ k+1-forma.
\end{itemize}

\item $\omega  =\sum f_idx_i; \beta = \sum_j  g_jx_j$

\begin{gather*}
\dpa(\omega \y \beta) = d\left(\sum_{i,j=1}^N f_ig_jdx_y\y dx_j\right)\\
=\sum_{i,j=1}^N d(f_ig_j) \y dx_i\y dx_j\\
= \sum_{i,j=1}^N \left(\sum_{k=1}^N \dpa{(f_ig_j)}{x_k} dx_k\right)\y dx_i\y dx_j\\
=\sum_{i,j,k=1}^N \dpa{f_i}{x_k}g_j + f_i\dpa{g_j}{x_k}dx_k\y dx_i\y dx_j\\
= \sum_{i,j,k=1}^N \dpa{f_i}{x_k}g_j dx_k\y dx_i\y dx_j + \sum_{i,j,k=1}^N f_i\dpa{g_j}{x_k} dx_k\y dx_i\y dx_j\\
\text{Vamos a intentar encontrar }\omega,\beta\\
= \sum_{i,j,k=1}^N \dpa{f_i}{x_k}dx_k\y dx_i\y(g_jdx_j) + \sum_{i,j,k=1}^N\dpa{g_j}{x_k}dx_k\y f_idx_i \y dx_j \\
= \sum_{i,j} \dpa{f_j}{x_k}dx_k\y dx_i \y \underbrace{\left(\sum_j g_jdx_j\right)}_{\beta} + \sum_{j,k}  \dpa{g_j}{x_k}dx_k\y \underbrace{\left(\sum_i f_idx_i\right)}_{\omega} \y dx_j\\
= \underbrace{\sum_i \underbrace{\left(\sum_k \dpa{f_i}{x_k}dx_k\right)}_{df_i}\y dx_j\y \beta }_{d\omega\y \beta} - \underbrace{\sum_{j,k} \omega \y \dpa{g_j}{x_k}dx_k \y dx_j}_{d\beta}
\end{gather*}

Si $\omega,\beta$ 1-formas $\implies d(\omega \y \beta) = d\omega \y \beta - \omega \y d\beta$.

\item $\omega$ k-forma, $\beta$ s-forma.
Repitiendo las cuentas hasta cuando llegamos a intercambiar algo, que nos queda \[d\omega \y \beta + (-1)^k \omega \y d\beta\]
\index{Derivada de producto exterior de k-formas}

\item $\omega$ k-forma con coeficientes $C^2$. Entonces $d(d\omega) \equiv 0$.

La prueba es: no la pienso copiar ni de coña.
\end{itemize}


\subsubsection{Pull-back}
Entramos en como transformar k-formas de $\real^N$ a $\real^M$. Partimos de la base de que existe una transformación $T$ que va de $\real^N \to \real^M$, tal que $T(s)=x$ y buscamos una $T^{\ast} \tlq \real^M\to\real^N$.

En caso de 0-formas, el \emph{pull-back} es lo mismo que la composición.

Supongamos que tenemos una $\omega$ k-forma en $\real^N$.
Para construir $T^{\ast}$:

tenemos $\gor{s} \in \real^N, \gv_1,\gv_2,..,\gv_k$ vectores en $\real^N$.

Queremos definir $T^{\ast}\omega$ en términos de $T$ y $\omega$.

\[
(T^{\ast}\omega)(\underbrace{\gor{s}}_{\in\real^N}) [\underbrace{\gv_1}_{\in\real^N},...,\gv_k] = \omega(\underbrace{T(s)}_{\in\real^M}) [\underbrace{DT(s)\gv_j}_{\in\real^M}]
\]

\obs


\[\omega(T(s)) [DT(s)\gv]\equiv \sum f_i(T(s))dx_i[DT(s)\gv]\]

\[DT(s) = \begin{pmatrix}
\dpa{T_1}{s_1}(s) & ... & \dpa{T_1}{s_N}\\
\vdots & \ddots & \vdots \\
\dpa{T_M}{s_1} & \dots & \dpa{T_M}{s_N}
\end{pmatrix} \cdot \begin{pmatrix}
v_1\\
\downarrow\\
v_N
\end{pmatrix}\]

¿Que significa $dx_i[DT(s)\gv]$? Vamos a ver que pasa con el producto de una de las filas de la matriz.

\[dx_i[DT(s)\gv] = \dpa{T_i}{s_1}v_1 + ... + \dpa{T_i}{s_N}v_n\]
Podemos darnos cuenta de que $v_1 = ds_1[\gv]$. Con esto tenemos:
\[\underbrace{\left(\dpa{T_i}{s_1}ds_1 + ... + \dpa{T_i}{s_N}ds_N \right)}_{dT_i}[\gv]\]


\paragraph{Ejemplos}

\subparagraph{Ej 1)} Sea  $f(x)dx_i$ una 1-forma de la que queremos calcular el \emph{pull-back}

\[T^{\ast}(fdx_i)(s)[v] = f(T(s)) dx_1[DT(s)\gv]\]
Supongamos $f\equiv 1$
\[T^{\ast}(fdx_i)(s)[v] = dx_1[DT(s)\gv] = dT_i[\gv]\]
\textbf{Conclusión: } $T^{\ast}dx_i = dT_i$.

\subparagraph{Ej 2)} Sea $\omega = \sum_i f_i dx_i$ una 1-forma de la que queremos calcular el \emph{pull-back}

\[ 
 (T^{\ast} \omega )(s)[\gv]= \sum_i f_i(T(s))dx_i [DT(s)\gv] = \sum_if_i(T(s)) dT_i[\gv] = T^{\ast} (\sum_i f_idx_i) = \sum_i f_i \circ T dT_i
\]

\textbf{Conclusión: } $T^{\ast} (\sum_i f_idx_i) = \sum_i f_i \circ T dT_i$

\subparagraph{Ej 3)} ¿Cómo se comporta con el producto exterior? Vamos a trabajar con $f\equiv 1$.

\[
T^{\ast}(dx_i\y dx_j) [\gu,\gv] = dx_i\y dx_j \left[DT(s)[\gu], DT(s)[\gv]\right] =\]\[ \left|\begin{matrix}
dx_i[DT(s)\gu] & dx_j[DT(s)\gu] \\
dx_i[DT(s)\gv] & dx_j[DT(s)\gv] 
\end{matrix}\right| = \left| \begin{matrix}
dT_i[\gu] & dT_j[\gu]\\
dT_i[\gv] & dT_j[\gv]
\end{matrix}\right| = (1) = dT_i \y dT_j [\gu,\gv]
\]
(1) = Por las propiedades del producto exterior de 1-formas.

\textbf{Conclusión: } $T^{\ast} (dx_i \y dx_j) = dT_i \y dT_j$.

\subparagraph{Ej 4)} ¿Qué pasa cuando tenemos el producto de 2-formas generadas?

\[\omega = \sum f_idx_i\,;\,\beta=\sum g_jdx_j\]
Vamos con el $\y$.

\begin{gather*}
T^{\ast} (\omega \y \beta) (s) [\gu,\gv] = \sum_{i,j} f_i(T(s))g_j(T(s)) \underbrace{dx_i\y dx_j [DT(s)\gu, DT(s),\gv]}_{\text{Calculado justo arriba}}\\
= \sum_{i,j} (f_i\circ T) ... \\
= \left(\sum (f_i\circ T)dT_i\right)\y\left(\sum(g_j \circ T) dT_j\right) = T^{\ast}\omega \y T^{\ast}\beta
\end{gather*}

\textbf{Conclusión: } $T^{\ast}(\omega \y \beta) = T^{\ast}\omega \y T^{\ast}\beta$. 

Esto es válido para multindices $I$, es decir, para $\omega$ k-forma y $\beta$ s-forma.


\paragraph{Pull-back y diferencial exterior}
una vez visto cómo se comporta el \emph{pull-back} respecto del producto exterior vamos a ver como se comporta con respecto de la diferencial exterior.

\begin{gather*}
d(T^{\ast} \omega) = d\left(\sum_i f_i(T(s)) dx_i [DT(s)\gv]\right) = d\left(\sum (f_i \circ T)(s) dT_i[\gv]\right)\\
= \sum_i d(f_i\circ T)\y dT_i
\end{gather*}
\text{Vamos a ver que significa:  $d(f_i\circ T)$}

\begin{gather*}
d(f_i\circ T) =  \sum_k \dpa{f_i\circ T}{s_k} ds_k \\
\dpa{f_i\circ T}{s_k} = \sum_j \dpa{f_i}{x_j}(T(s))\cdot \dpa{x_j}{s_k}; \text{Donde }x_j = T_j(s)\\
\text{Juntando todo tenemos: }
\sum_{i,j,k} \dpa{f_i}{x_j}(t(s)) \dpa{T_j(s)}{s_k} ds_k \y dT_i = \sum_{i,j} \dpa{f_i}{s_j}(T(s)) dT_j\y dT_i = T^{\ast}(d(\sum f_i dx_i))
 \end{gather*}
 
 \textbf{Conclusión: } $d(T^{\ast}\omega) = T^{\ast}(d\omega)$
 
 
 \paragraph{Ejemplo concreto} ¡¡Por fin!! El cambio a coordenadas polares.
 
 $(\rho,\theta)$.
 
 
 \[T(\rho,\theta) =\left( T_1(\rho,\theta),T_2(\rho,\theta)\right) = (\rho cos\theta,\rho sen\theta)\]
 
 Vamos a calcular los pull-backs:
 
 \[T^{\ast}(dx) = ...\]
 
 \[T^{\ast} (dy) = dT_2 = d(\rho sen\theta) = \dpa{\rho sen\theta}{\rho} d\rho + \dpa{\rho sen\theta}{\theta}d\theta =sen\theta d\rho + \rho cos\theta d\theta\]
 
 \[T^{\ast}(dx\y dy) = ... = \rho d\rho \y d\theta\]
Completar.

\textbf{Interpretación:}

No sé dónde va esto.

Sea $ω$ una k-forma, donde \[ ω = \sum_I f_i \, dx_I \] donde $I$ son k-multiíndices. Entonces el pullback de $ω$ es 

\[ T^\ast ω = \sum_I f_i\circ T \, dT_I \]

Supongamos ahora que quiero calcular la diferencial exterior:

\begin{equation} d(T^\ast ω) = \sum_I d(\underbrace{
	(f_i\circ T)}_{\text{0-forma}}
	\underbrace{\,dT_I}_{\text{k-forma}})\label{eqSuputamadre} 
	\end{equation}

Si nos acordamos de la fórmula de Lebiniz \wtf tenemos que

\begin{gather*} 
d(fω) = df\y ω + fdω \\
d(ω\y β) = dω \y β + (-1)^kω\y dβ
\end{gather*}

Usamos la primera fórmula en \ref{eqSuputamadre}

\[  d(T^\ast ω) = \sum_I d(f_i\circ T) \y dT_i + d(f_I\circ T)\underbrace{d(d(T_i))}_{=0} = \sum_{I,K}\dpa{}{s_k} (f_I\circ T) ds_k\y dT_I \]

Usando la regla de la cadena en la derivada parcial

\[ \dpa{}{s_k} (f_I\circ T) = \sum_j\dpa{f}{x_j}\circ T \dpa{T_j}{s_k} \]

Poniendo de nuevo todo junto

\[ d(T^\ast ω) = \sum_{I, K, j} \dpa{f}{x_j}\circ T \dpa{T_j}{s_k} ds_k\y dT_I = \]

Sacando factor común

\[ = \sum_{I, j} \dpa{f}{x_j}\circ T\underbrace{\sum_K \dpa{T_j}{s_k} ds_k}_{dT_j}\y dT_I = \sum_{I,j} \dpa{f_i}{x_j}\circ T dT_j\y dT_I = T^\ast \left(\sum_{I,j} \dpa{f_i}{x_j}dx_j\y d_{x_I} \right) \]

Vemos que lo de dentro es lo mismo que $dω$ y por lo tanto

\[ d(T^\ast ω) T^\ast(dω)\]

Y hay una última propiedad (esto lo ordenas tú) que dice

\[ (T\circ S)^\ast ω = S^\ast(T^\ast ω) \]

\paragraph{Propiedades fundamentales de la operación}
\index{Propiedades! Pull-back}
\begin{enumerate}
\item $T^\ast f = f\circ T$, siendo $f$ una 0-forma.
\item $T^\ast(dω) = d(T^\ast ω)$. En particular $T^\ast (dx_I) = dT_I$.
\item $T^\ast(ω\y β)=(T^\ast ω) \y (T^\ast β)$.
\item $T^\ast(fω) = (f\circ T)(T^\ast ω) = (T^\ast f)(T^\ast ω)$
\item $T^\ast(ω+β)=T^\ast+T^\ast β$.
\end{enumerate}

\begin{example}

Tenemos una aplicación $φ(s,t) = (φ_1(s,t), φ_2(s,t))$. Calculamos su pullback y entonces

\[ φ^\ast(dx) = dφ_1 = \dpa{φ_1}{s}ds + \dpa{φ_1}{t}dt \]

y de la misma forma
\[ φ^\ast(dyx) = dφ_2 = \dpa{φ_2}{s}ds + \dpa{φ_2}{t}dt \]

\begin{gather*}
 φ^\ast(dx\y dy) = dφ_1\y dφ_2 =\left( \dpa{φ_1}{s}ds + \dpa{φ_1}{t}dt \right) \y\left( \dpa{φ_2}{s}ds + \dpa{φ_2}{t}dt \right) = \\
 0 + \dpa{φ_1}{s}\dpa{φ_2}{t}ds\y dt + \dpa{φ_1}{t}\dpa{φ_2}{s} dt\y ds + 0
 \end{gather*}
 
 Los diferenciales están cambiados de orden así que seguimos pagando con un cambio de signo:
 
 \[ \left(\dpa{φ_1}{s}\dpa{φ_2}{t} - \dpa{φ_1}{t}\dpa{φ_2}{s}\right)ds\y dt= \left|\begin{matrix}
 \dpa{φ_1}{s} & \dpa{φ_1}{t} \\
 \dpa{φ_2}{s} & \dpa{φ_2}{t} 
 \end{matrix}\right| =  \det \left(\dpa{φ}{s,t}\right) ds\y dt \]
 \end{example}
 
 \begin{example}
 Tenemos $β$, la 2-dorma asociada a un campo $\vf=(F_1, F_2, F_3)$:
 
 \[ β = F_1dy\y dz + F_2dz\y dx + F_3 dx\y dy \]
 
 Queremos calcular su pullback mediante una aplicación $\appl{Φ}{ℝ^2}{ℝ^3}$. Entonces
 
 \[ Φ^\ast β = g(s,t) ds\y dt \]
 
 ¿Quién es $g$? Calculamos el pullback:
 
 \[ φ^\ast β = F_1\circ Φ\, dΦ_2\y dΦ_3 + F_2\circ Φ\, dΦ_3 \y dΦ-2 + F_3\circ Φ\, dΦ_1\y dΦ_2 \]
 
 Calculamos los distintos diferenciales, que no pienso copiarlos porque no los ha puesto él en la pizarra, y los pinchamos en el morcillo ese. Y operamos. Y yo no voy a operar. Al final sale todo y queda lo siguiente
 
 \[ =\pesc{\vf\circ Φ, \left(\dpa{Φ_1}{s}, \dpa{Φ_2}{s}, \dpa{Φ_3}{s}\right) × \left(\dpa{Φ_1}{t}, \dpa{Φ_2}{t}, \dpa{Φ_3}{t}\right)} ds\y dt \]
 
 Sin embargo, el producto vectorial de esos dos vectores parece el producto vectorial de $T_s×T_x$, el factor que teníamos que poner para integrar un campo en una superficie. 
 \end{example}
 
 \begin{example}
 
 Tenemos una 1-forma asociada a $\vf$:
 
 
 
 \[ ω = \sum_i F_i\,dx_i \]
 
 y una aplicación (curva) $\appl{σ}{ℝ}{ℝ^n}$. Entonces $σ^\ast ω$ es de la forma $g(t)\,dt$. 
 
 Entonces
 
 \begin{gather*} (σ^\ast ω)(t)[λ] = \sum_i F_i(σ(t)) dx_i[Dσ(t)λ] = \sum_i F_i(σ(t)) dx_i[σ_1'(t)λ, \dotsc, σ_N'(t)λ = \\
 = \sum_i F_i(σ(t)) σ_i'(t) dt[λ] = \\
 = \pesc{\vf \circ σ, σ'} dt [λ]
 \end{gather*}
 
 Que, oh, sorpresa de nuevo, es lo que aparece cuando integrábamos un campo sobre una curva. 
 \end{example}
 
 Al final, querremos integrar k-formas en $ℝ^N$ sobre variedades de dimensión $k$. 
 
\section{Integración de formas diferenciales}

Vamos a partir de Ω abierto de $\real^N$. 

Sea $\omega$ n-forma definida en un entorno de Ω, es decir $\omega = f(\gx) \underbrace{\df{x_1,...,x_n}}_{\text{Elemento de volumen}}$

\begin{defn}[Integración\IS n-forma en $\real^N$.]
\[
\int_Ω\omega = \inf f(\gx) dx_1...dx_N
\]
\end{defn}

\obs Supongamos que tenemos una $\appl{\Phi}{\real^K}{\real^N}$, tal que $\Phi(\gor{s}) = \gx$.

Para que lo de la derecha sea una variedad tenemos que $\Phi$ tiene que ser regular, homeomorfismo y rango máximo.

Supongamos $\omega \in \real^N$, con $\omega$ una x-forma.

¿Qué pasaría si queremos integrar $T^{\ast}\omega$?

Si queremos integrar $\pb{\omega}$ en $\real^k, \omega$ tiene que ser una k-forma para poder aplicar la definición.


\begin{example}
Variedad 1-dimensional.

$\appl{\sigma}{\real}{\real^3}$

Sea $\omega = f_1(x,y,z)dx + f_2(x,y,z)dy + f_3(x,y,z)dz$

Tenemos que $\sigma^{\ast}\omega = \pesc{\overrightarrow{F}\circ \sigma,\sigma'}dt$

La integral quedaría:

\[\int_{\sigma(I)} = \int_I  \pesc{\overrightarrow{F}\circ \sigma,\sigma'}dt 
\]

Integrar 1-forma sobre la variedad 1-dimensional es integrar el trabajo del campo $\overrightarrow{F}$ a lo largo de $\sigma(I)$.
\end{example}

\begin{example}
Una variedad de dimensión 2 en $\real^3$.

$\appl{\Phi}{\real^2}{\real^3}$, con $\Phi(s,t) =  (x,y,z)$.

$\beta$ 2-forma. 
\[\beta = G_1(x,y,z) \df{y,z} + G_2(x,y,z) \df{z,x} + G_3(x,y,z) \df{x,y}\]

El pullback (calculado anteriormente es)

\[\Phi^{\ast}\beta = \pesc{\overrightarrow{G}\circ\Phi,\Phi_s\x\Phi_t}\df{s,t}\]

\[\int_{\Phi(D)}ß = \int \int_D \pesc{\overrightarrow{G}\circ\Phi,\Phi_s\x\Phi_t}dsdt\]

La integral sería:


Es decir, integrar una 2-forma sobre $\Phi(D)$ es integrar el \textbf{flujo} del campo $\overrightarrow{G}$ a través de $\Phi(D)$
\end{example}

\paragraph{Caso general}

Sea $M$ una variedad de dimensión k en $\real^N$.

Supongamos que $(D,\Phi)$ una carta local, es decir:
$\appl{\Phi}{D\subset\real^K}{\real^N}, \Phi(D)\subset M; \Phi$ parametrización. ($\Phi(\gor{t}) = \gx$)

Sea \[\omega = \sum_I f_Idx_I; \, \, \, I=\{i_1,i_2,...,i_k\}\]

Por definición:

\[
\int_{\Phi(d) \omega} = \int_D \Phi^{\ast}\omega
\]

El pull-back nos va a dar una k-forma definida en $\real^k$.

\[
\Phi^{\ast}\omega =g(\gor{t})\df{t_1,t_2,...,t_k}
\]

Aplicando esto:
\[
\int_{\Phi(d) \omega} = \int_D g(\gor{t})\df{t_1,t_2,...,t_k}
\]

Vamos a identificar la función $g$ remangándonos y haciendo cuentas:

\[
\Phi^{\ast} \omega = \sum_I f_I\circ\Phi d\Phi_I = 
\]

Vamos a fijarnos en 
$d\Phi_I = \df{\Phi_{i_1},...,\Phi_{i_k}}$, que va a actuar sobre k-vectores, es decir:

\[
d\Phi_I[\gv_1,...,\gv_k] = \df{\Phi_{i_1},...,\Phi_{i_k}} [\gv_1,...,\gv_k] = \det \begin{pmatrix}
d\Phi_{i_1}[\gv_1] &\cdots& d\Phi_{i_1}[\gv_k] \\
\vdots & \ddots & \vdots\\
d\Phi_{i_k}[\gv_1] & \cdots & d\Phi_{i_k}[\gv_k] \\
\end{pmatrix}
\]

Vamos a desarrollar 1 de los elementos de la matriz (escribiendo $\gw$ para generalizar a cualquiera de los vectores sobre los que actúa):
\[
d\Phi_{i_1}[\gw] = \sum_{j=1}^k \left(\dpa{\Phi_{i_1}}{t_j} dt_j\right)[\gw] = \sum_{j=1}^k \dpa{\Phi_{i_1}}{t_j}w_j = \pesc{\grad \Phi_{i_1},\gw}
\]

Aplicando esto:

\[d\Phi_I[\gv_1,...,\gv_k] =  \det \begin{pmatrix}
\pesc{\grad \Phi_{i_1},\gv_1} &\cdots&  \pesc{\grad \Phi_{i_1},\gv_k}\\
\vdots & \ddots & \vdots\\
\pesc{\grad \Phi_{i_k},\gv_1} & \cdots & \pesc{\grad \Phi_{i_k},\gv_k} \\
\end{pmatrix} = \det \left(\begin{pmatrix}
\grad \Phi_{i_1} \rightarrow \\
...\\
\grad \Phi_{i_k} \rightarrow 
\end{pmatrix}
\begin{pmatrix}
\gv_1 & ... & \gv_k\\
\downarrow & ... & \downarrow
\end{pmatrix}\right) =\]
\[ \det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix} \cdot \det 
\begin{pmatrix}
\gv_1 & ... & \gv_k\\
\downarrow & ... & \downarrow
\end{pmatrix} = (1) =
\det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix} \df{t_1,t_2,...,t_k}[\gv_1,\gv_2,...,\gv_k]
\]

(1): Aplicando que $dt_1(\gv)$ es la primera coordenada del vector $\gv$. Un paso intermedio es \[\det \begin{pmatrix}
dt_1(\gv_1) & ... & dt_1(\gv_k)\\
\vdots & \ddots & \vdots\\
dt_k(\gv_1) & ... & dt_k(\gv_k)
\end{pmatrix}\]

\textbf{Conclusión:}
\[
\Phi^{\ast} \omega = \sum_I f_I\circ\Phi d\Phi_I =
\overbrace{\sum_I f_i\circ\Phi 
\det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix}}^{\text{Esta es la g que buscamos}} \df{t_1,...,t_k}
\]

Aplicando a una integral:

\[\int_{\Phi(D)} \omega = \int_D \sum_I f_i\circ\Phi 
\det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix}dt_1dt_2...dt_k\]


\section{Teoremas de Green, divergencia y Stokes en términos de formas diferenciales}
\paragraph{Caso Modelo:}

Vamos a trabajar con el cubo unidad.

\subparagraph{$\real^2$} $\mathcal{Q} = [0,1]\x[0,1]$

\textbf{Notación} $I_{ij}$, donde $i$ es la variable fija (0=x,1=y) y $j$ el valor que toma la variable fija.
\[\begin{array}{cc}
I_{11} =\{(1,y),y\in[0,1]\}&\text{ Orientación: }\, +\\
I_{21} =\{(x,1),x\in[0,1]\}&\text{ Orientación: }\, -\\
I_{10} =\{(0,y),y\in[0,1]\}&\text{ Orientación: }\, -\\
I_{20} =\{(x,0),x\in[0,1]\}&\text{ Orientación: }\, +\\
\end{array}
\]
La orientación está calculada con el sistema de "me coloco en la frontera en la dirección en la que nos movemos. Si la mano izquierda estirada apunta hacia el interior $\implies +$, si apunta hacia fuera $\implies -$.

Sea $\omega = f(x,y)dx + g(x,y)dy$

\[d\omega= \dpa{f}{y}\df{y,x} + \dpa{g}{x}\df{x,y} = \left(\dpa{g}{x} - \dpa{f}{y}\right) \df{x,y}\]

Consideramos $\mathcal{C}^+$ es la frontera de $\mathcal{Q}$ orientada positivamente y vamos a intentar calcular: $\displaystyle \int_{C^+}\omega$.

\[
\int_{C^+} \omega = \int_{I_11}\omega - \int_{I_20}\omega - \int_{I_21}\omega + \int_{I_10}\omega = \int_0^1 g(1,y)dy + \int_0^1 f(x,0)dx - \int_0^1f(x,1)dx - \int_0^1 g(0,y)dy = \int_0^1 g(1,y)-g(0,y)dy - \int_0^1f(x,1)-f(x,0)dx = \int_0^1\int_0^1 \dpa{g}{x}(x,y)dxdy - \int_0^1\int_0^1 \dpa{f}{x}(x,y)dydx = \int\int_Q \dpa{g}{x} - \dpa{f}{y} dxdy = \int\int_Q d\omega
\]

\textbf{Conclusión: } $\displaystyle\int_{C^{+}} \omega = \int\int_Q d\omega$. Esto escrito en términos de cálculo II es: $\displaystyle\int_{C^{+}}(P,Q) = \int\int_Q \dpa{Q}{x} - \dpa{P}{y}$, que es el teorema de Green. \index{ Teorema de integración \IS Green formas diferenciales}

\subparagraph{$\real^3$}
Ahora vamos a hacer algo parecido en $\real^3$, sea $Q=[0,1]\x[0,1]\x[0,1]$. (Trabajando con la normal exterior para las orientaciones)

Vamos a distinguir las caras con la notación anterior:


\[\begin{array}{cc}
I_{10} = \{(0,y,z), y\in[0,1],z\in[0,1]\} & \text{ Orientación: -} \\
I_{11} = \{(1,y,z), y\in[0,1],z\in[0,1]\} & \text{ Orientación: +} \\
I_{20} = \{(x,0,z), x\in[0,1],z\in[0,1]\} & \text{ Orientación: +} \\
I_{21} = \{(x,1,z), x\in[0,1],z\in[0,1]\} & \text{ Orientación: -} \\
I_{30} = \{(x,y,0), x\in[0,1],y\in[0,1]\} & \text{ Orientación: -} \\
I_{31} = \{(x,y,1), x\in[0,1],y\in[0,1]\} & \text{ Orientación: +} \\
\end{array}
\]

Las orientaciones están calculadas (según el primer caso) \[\left.\begin{array}{cc}
T_y = (0,1,0)\\T_z=(0,0,1)\end{array}\right\}\implies T_y\x T_z = (1,0,0)\] Mirando en el dibujo, identificamos que la cara en la que estamos trabajando (en este caso la de detrás) y comprobamos que apunta hacia dentro del cubo (dirección contraria a la normal exterior), y concluimos orientación negativa.

Repitiendo el proceso llegamos a la conclusión de: \[
\begin{array}{cc}
T_y\x T_z &= (0,0,1)\\
T_x\x T_z &= (0,-1,0)\\
T_x\x T_y &= (1,0,0)
\end{array}\]

\textit{Truquillo para orientaciones} Si la suma de los subíndices es par $\implies +$, si por el contrario, es impar $\implies -$. Detrás de esta idea hay un teorema que no vamos a ver. Además hay que tener cuidado con el orden en el que se hacen las cosas y se escriben los vectores.

Sea
\[\omega = F_1(x,y,z)\df{y,z}+F_2(x,y,z)\df{y,x}+F_3(x,y,z)\df{x,y}\]
 
\[d\omega = \dpa{F_1}{x}\df{x,y,z} + \dpa{F_2}{y}\df{y,x,z} + \dpa{F_3}{z}\df{z,x,y} = \left(\dpa{F_1}{x}+\dpa{F_2}{y} + \dpa{F_3}{z}\right) \df{x,y,z}
\]

Vamos a calcular \[\int_{dQ^+} \omega\] siendo $dQ^+$ la frontera del cubo unidad.

\[\int_{Q^+} \omega =\underbrace{ -\int_{I_{10}} \omega +  \int_{I_{11}} \omega}_{(1)} + \int_{I_{20}} \omega -  \int_{I_{21}} \omega  -\int_{I_{30}} \omega +  \int_{I_{31}} \omega\]

\[
(1) = \int_0^1\int_0^1\Phi^{\ast}_{11}\omega - \int_0^1\int_0^1 \Phi^{\ast}_{10} \omega = (2) =\]
\[
 \underbrace{\int_0^1\int_0^1 \pesc{\overrightarrow{F}\circ \Phi_{11},(1,0,0)}dydz}_{F_1(\Phi_{11})(y,z)}
 - \underbrace{ \int_0^1\int_0^1 \pesc{\overrightarrow{F}\circ \Phi_{10},(1,0,0)}dydz}_{F_1(\Phi_{10})(y,z)}
\]
(2) = Aplicando un cáculo realizado anteriormente ¿Cuándo? no se...
\index{Teorema! de la divergencia formas diferenciales}
\[
= \int_0^1\int_0^1 F_1(\Phi_{11})(y,z)-F_1(\Phi_{10})(y,z)dydz = \int\int\int_Q \dpa{F_1}{x}dxdydz
\]

Aplicando las mismas cuentas con las que faltan llegamos al teorema de la divergencia para el cubo.


\paragraph{Conclusión}
\[\int_{dQ^+} \omega = \int_{Q}d\omega\]
Ideas que en $\real^2$ se traduce en el Teorema de Green y que en $\real^3$ se traduce en el Teorema de la divergencia. ¿Dónde queda el Teorema de Stokes? Vamos ahora a encontrarlo.

\section{Teorema de Stokes}

%\index{Teorema! de Stokes formas diferenciales.}

\todo{Completar}

$\appl{\Phi}{Q}{\Phi(Q)\subset\real^n}$. Sea $\omega$ una k-forma en $\real^n$.

Queremos calcular \[
\int_{\Phi(Q)} d\omega = \int_Q \Phi^{\ast}d\omega) = \int_Q d(\Phi^{\ast}\omega) = \int_{dQ^{+}} \Phi^{\ast}\omega
\]

Donde $dQ^{+}$ es la frontera del cubo $Q$ orientada debidamente. El último paso es aplicar el teorma anterior.

Sea $\appl{\sigma}{I}{dQ}$. Entonces, $\appl{\Phi\circ\sigma}{I}{\Gamma}$, siendo $\Gamma$ la frontera de $\Phi(Q)$.

Aplicando esto a la integral que estamos calculando:

\[
\int_{\Phi(dQ^{+})} \omega \equiv \int{\Phi\circ\sigma(I)} = \int_I (\Phi\circ\sigma)^{\ast} \omega = \int_I \sigma^{\ast}\left(\Phi^{\ast}(\omega)\right) = \int_{\sigma(I)} \Phi^{\ast}\omega = \int_{dQ}\Phi^{\ast}\omega
\]
\todo{L850}
\textbf{Conclusión:} $\displaystyle \int_{dQ^+} \Phi^{\ast} \omega = \int_{\Phi(dQ^+)} \omega$

\todo{L852}

A nosotros lo que nos gustaría sería que $\displaystyle\int_{\Phi(dQ^+)} \omega = \int_{d(\Phi^{\ast}(Q))} \omega$, es decir, que la imagen de la frontera sea la frontera de la imagen. Esto no es inmediato.

Esto se ve claramente con el cambio a coordenadas polares.

\[
\int_{\Phi(Q_1)}d\omega + \int_{\Phi(Q_2)}d\omega=
\int_{\Phi(\partial  Q_1)^+}\omega+\int_{\Phi(\partial  Q_2)^+}\omega
\]

\textbf{Conclusión (Teorema de Stokes):} $\displaystyle\int_M d\omega = \int_{\partial  M^+}\omega$, si M se puede descomponer como unión de celdas con interior disjunto.

\obs Es un resultado general, vale también para más dimensiones. Por ejemplo, en el caso $\displaystyle \int_{\Phi(Q)} d\omega = \int_{\partial (\Phi(Q))} \omega$, tenemos
\begin{itemize}
\item $\Phi(Q)$ es una superficie
\item $\omega$ es una 1-forma.
\item $d\omega$ es una 2-forma, que además (como ya hemos visto anteriormente (ref)) $d\omega = \left(\dpa{F_3}{y}-\dpa{F_2}{z}\right) \df{y,z} + 
\left(\dpa{F_1}{z}-\dpa{F_3}{x}\right) \df{z,x} + 
\left(\dpa{F_2}{x}-\dpa{F_1}{y}\right) \df{x,y}$
\item $\partial \Phi(Q)$ es ... 
\end{itemize} 
Con el segundo punto de la lista, podemos escribir la fórmula de la siguiente manera:
\index{Teorema! Stokes en formas diferenciales}
\[
\int_{\Gamma^+} \overrightarrow{F} \equiv \int \int_S rot \overrightarrow{F}
\]

\subsection{Frontera de una superficie}
Vamos a intentar definir en serio la frontera de objetos en $\real^3$, que es algo que necesitamos tener realmente muy claro, por ejemplo:

\todo{L880}

Hace un tiempo, cuando definíamos una subvariedad, demostramos la existencia de un difeomorfismo que "aplanaba un trozo" de subvariedad. 

La frontera de una superficie es el conjunto de los puntos (llamados en clase de Tipo 2) que al aplanar nos quedan en la frontera de un objeto de dimensión 2.

Aquí iría la prueba que haremos la semana que viene. Ahora vamos a ver aplicaciones:


\subsection{Aplicaciones de los teoremas de integración}
\paragraph{Teorema de Green}
Sea $\omega$ 1-forma en $\real^2$, y $D$ un conjunto cerrado con frontera orientable.

\[
\int_{\partial  D^+} Pdx+Qdy = \int \int_D d(Pdx+Qdy) = \int\int_D \dpa{Q}{x} - \dpa{Q}{y}\df{x,y}
\]

\obs Podemos elegir $(P,Q)$ de tal modo que $\displaystyle \dpa{Q}{x}-\dpa{Q}{y} = 1$.

Entonces: Área (D) = $\displaystyle\int_{\partial  D^+} (P,Q)d\sigma$

\subparagraph{Aplicación} Área de la hoja folium de Descartes:

\[x^3+y^3 = mxy\]


\easyimgw{imgs/FoliumDescartes.png}{Folium de Descartes}{lblFolium}{0.3}

Vamos a parametrizarla, siguiendo la indicación: $t = \frac{y}{x}$

Se deja como ejercicio para el lector, llegar a la fórmula:

\[\begin{array}{cc}
x&=\displaystyle\frac{mt}{1+t^3}\\
y= xt &= \displaystyle\frac{mt^2}{1+t^3}
\end{array}\]

Quedando por definir que valores toman los parámetros. En este caso es $(0,\infty)$. Estos valores parametrizan la región cerrada. 

Podríamos plantearnos para qué valores de $t$ que recorren las ramas que se van a infinito. En $t=-1$, se va a infinito, entonces una de las ramas será $t\in(-1,0)$ y la otra será $t\in(-\infty,-1)$.

¿Que orientación nos da esta parametrización?

La idea es ver el vector tangente en el 0. Si es horizontal empezaremos por la rama de abajo. Si es vertical, empezaremos por la rama de arriba

\[\sigma'(t) = \left(\frac{m(1+t^3)-3mt^3}{(1+t^3)^2},\frac{2mt(1+t^3) - 3mt^4}{(1+t^3)^2}\right)\]

Podemos comprobar que $\sigma'(t) \convs[][t\to 0^+] (m,0)$. Además, $\sigma'(t) \convs[][t\to\infty](0,m)$, quedando una orientación positiva.

\[
A(D) = \int_{\partial  D^+} (0,x)d\sigma = \int_0^{+\infty} \pesc{\left(0,\frac{mt}{1+t^3}\right), \left(\ast,\frac{2mt(1+t^3)-3mt^4}{(1+t^3)^2}\right)}dt = ...
\]

Wolfram dice que el resultado de la integral es $\frac{m^2}{6}$ y que el área encerrada por el folium de Descartes es también $\frac{m^2}{6}$ lo que nos hace pensar que está bien planteado y bien resuelto el problema.


\paragraph{Ejercicio para el lector} Hacer lo mismo con la curva $x^4+y^4=4xy$. El área de la hoja contenida en el primer cuadrante.
%\easyimgw{imgs/FoliumALaCuarta.png}{$x^4+y^4=4xy$}{lblFoliumALaCuarta}{0.3}

\paragraph{Teorema de Stokes en $\real^3$}

El teorema decía: \[
\int_{\Gamma^+}\overrightarrow{F}d\sigma = \int \int_{S^+} rot\overrightarrow{F} dS
\]
Siendo $S$ la superficie, $\Gamma$ la ¿frontera?, tomando la orientación positiva con la normal exterior.

\subparagraph{Ejemplo}
\[
\left.\begin{array}{cc}
z=x^2+y^2\\
z=mx \end{array} \right\} \equiv \Gamma
\]

Queremos calcular $\displaystyle \int_{\Gamma}y dz$

Esto es lo mismo que calcular la integral del campo $(0,0,y)$.

El primer paso es \textbf{parametrizar} $\Gamma$

Tenemos que la proyección en el plano xy es \[mx=x^2+y^2 \equiv \left(x-\frac{m}{2}\right)^2 + y^2 = \frac{m^2}{4}\]

Viendo los cuadrados lo lógico es pensar en utilizar polares.

Llamando $x=rsen(\theta),y=rsen(\theta)$ tenemos:
\[\sigma(\theta) = \left(mcos^2(\theta),mcos(\theta)sen(\theta),m^2cos^2(\theta)\right)\,\,\,\theta\in\left(\frac{-\pi}{2},\frac{\pi}{2}\right)\]

Calculamos el vector tangente para ver en que orientación recorre la curva esta parametrización:
\[\sigma'(\theta) = ()\]
\[\sigma'(0) = (0,m,0)\]
Suponemos (porque no me lo dicen, que $m>0$) y nos da la orientación. Como en el enunciado no nos hablan de hacerlo con ninguna orientación, la integral que calculemos será de acuerdo con esta orientación.

Vamos con la integral:

\[\int_{\Gamma}(0,0,y) d\sigma =\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}}\pesc{\underbrace{\left(0,0,mcos(\theta)sen(\theta)\right)}_{\overrightarrow{F}(\sigma(\theta))},\ast} d\theta\]

Como camino alternativo a la fórmula, podemos aplicar el teorema:

\[ = \int\int_{D^+}  rot(0,0,y)dS\] Siendo el vector normal el que tenga la tercera componente positiva (razonando geométricamente).

Calculamos el rotacional del campo:$rot\overrightarrow{F} =\left|\begin{matrix}
i&j&k\\dx&dy&dz\\0&0&y
\end{matrix}\right| = (1,0,0)$.

Utilizamos la parametrización: $S = (x,y,mx), x,y\in C$
\[ = \int\int_D^+ rot(0,0,y)dS = \int \int_C \pesc{(1,0,0) ,T_x\x T_y} dxdy= (1) =\]
\[ \int \int_C \pesc{(1,0,0),(-m,0,1)} = \int\int -m dxdy = -m \cdot \, Area (C) = -m\frac{m^2}{4}\pi \]
$(1): T_x = (1,0,m); T_y = (0,1,0) $. Otra cosa aplicada es que el vector normal $(-m,0,1)$ como la tercera componente es positiva, tenemos que esta parametrización induce la orientación positiva.


\begin{theorem}[Teorema\IS de Stokes general]
Sea $M$ una subvariedad compacta, orientable, con frontera relativa $\partial  M$.

Entonces

\[\int_{\partial  M^+}\omega = \int_M d\omega \]

\end{theorem}

\begin{proof}
Esta visto para celdas.

Vamos a extender la idea:

%\todo{L1004}

\subparagraph{Paso 1: Compaciada}
$\forall P\in M$ eiste una celda $C_p$ tal que $C_p\in P$

Entonces: $M\subset \displaystyle\bigcup_{p\in M} C_p$

Tomamos el interior de las celdas, para trabajar con conjuntos abiertos.

Hemos recubierto con abiertos la subvariedad.

Por compacidad (\ref{compacidad}), entonces existe un subrecubrimiento \textbf{finito}.

A pesar de haber conseguido esto, no lo tenemos ya hecho, porque no podemos garantizar que las celdas son disjuntas. Si lo fueran, aplicaríamos el teorema a cada celda y listo.

\subparagraph{Paso 2: Particiones de la unidad}

Vamos a intentar resolver el problema de los solapamientos.

Sea $\appl{\phi}{\real^N}{\real}$. Además, 
\begin{itemize}
\item $\Phi>0$ en $\mathcal{Q}$
\item $\Phi = 0$ em $\partial  \mathcal{Q}$
\item $\Phi = 0$ en $\real^N-\mathcal{Q}$
\item $\Phi\in C^1$.
\end{itemize}

Consideramos $\Phi_i = \Phi\circ\Psi_{p_i}$, siendo $\Psi_{p_i}$ el difeomorfismo que cubre la celda $p_i$, con $i=1,...,k$.

Además, nos gustaría poder tener definido:

\[\displaystyle \sum \Phi_{i}(\gx) = 1,\forall \gx \in M\]

Aunque parece una cosa imposible, vamos a ver que es algo perfectamente factible:

\[\tilde{\Phi}(\gx) = \frac{\Phi_i (\gx)}{\sum_{i=1}{k}\Phi_i(\gx)}\]

Estas $\tilde{\Phi}_i$ satisfacen todas las propiedades que nos interesan.

\subparagraph{Paso 3: Descomposición del problema}

Lo que nosotros queremos es calcular $\int_M d\omega$.


\textbf{Idea:} Sobre $M, \omega=\sum_i^k\tilde{\Phi}_i(x)\omega$

Entonces \[d\omega = \sum_{i=1}^k d(\tilde{\Phi}_i,\omega) = \sum_{i=1}^k d\tilde{\Phi}_i\y \omega + \tilde{\Phi}_i d\omega = \underbrace{d\left(\sum \tilde{\Phi}_i\right) \y \omega}_{\equiv 0} + \sum \tilde{\Phi}_id\omega\]

Aplicando esto descomponemos:
\[\int_M d\omega = \sum_i = \int_M \tilde{\Phi}d\omega = \sum{i=1}^k \int_{C_{p_i}} \tilde{\Phi}_id\omega=\sum_{i=1}^k \int_{C_{p_i}} d(\tilde{\Phi}_i\omega)
\]

Hemos conseguido definir la integral como una suma finita de integrales sobre celdas en las que sí podemos aplicar el teorema.

Entonces tenemos:

\[\int_M d\omega = \sum_{i=1}^k \int_{\partial  C{P_i}^{+}} \tilde{\Phi}_i\omega\]

%\todo{1062}

Por como hemos definido $\tilde{\Phi}_i$ tenemos que todas las celdas de tipo 1 valen 0. En cambio en las celdas de tipo 2, tenemos una parte sobre la que $\tilde{\Phi}_i \neq 0$.

Esto nos deja: \[\int_M d\omega = \sum_{i=1}^k \int_{\partial  C{P_i}^{+}} \tilde{\Phi}_i\omega = \sum \int_{\partial M^+} \tilde{\Phi}_i \omega = \int_{\partial M^+} \omega\]

\end{proof}

\obs Si $\partial M \neq Ø$ (la esfera por ejemplo) entonces $\int_{M} d\omega =0$



\begin{example}
Sea $M$ una superficie en $\real^3$, un trozo de $x^2+y^2+z^2 = 1$ dentro de $x^2+y^2\leq y$, con $z\geq 0$. Ver figura \ref{imgEsferaCono}. Consideramos la orientación positiva como la de la normal hacia abajo.

\easyimg{imgs/EsferaCilindro.png}{Intersección de la esfera y el cilindro. La línea azul es la frontera de $M$, y las flechas naranjas indican su orientación positiva.}{imgEsferaCono}


Calcular las tres integrales

\[ \int_{\partial M} x \id y;\;\int_{\partial M} y \id z;\; \int_{\partial M} z \id x \]

\textbf{Previos} Nos damos cuenta de que $x^2+y^2\leq y$ es una circunferencia. Si completamos cuadrados tenemos la siguiente ecuación: \[x^2+\left(y-\frac{1}{2}\right)^2 = \frac{1}{4}\]

Es decir, una circunferencia de radio $\frac{1}{2}$ centrada en $(0,\frac{1}{2})$.

\paragraph{Resolución de $\int_{\partial M} x \dif y$}

\[
\int_{\partial M^{+}} x\dif y =\int_{\partial M^{+}} (0,x,0) \dif\sigma \stackrel{\mathrm{Stokes} }{=} \iint\limits_M \rot (0,x,0) \dif S = \iint\limits_M (0,0,1) \dif S
\]

Para calcular esta integral (que es calcular el flujo) aplicamos la fórmula de siempre. Para ello necesitamos parametrizar la superficie $M$:
\begin{equation}\label{eqEjStokes}
\Phi(x,y) = (x,y,\sqrt{1-x^2+y^2}), (x,y)\in D = \{x^2+y^2\leq y\} 
\end{equation}

Calculamos $T_x\x T_y =\displaystyle \left(\frac{x}{\sqrt{\cdot}},\frac{x}{\sqrt{\cdot}},1\right)$

\[
\int \int_M (0,0,1) dS = - \int \int_D \pesc{(0,0,1),(\ast,\ast,1)} \,dx\,dy = -\text{ Area } (D) = -\pi\frac{1}{4}
\]

\[ \int_{∂M^+}y\df{z} = \int_{∂M^+}(0,0,y)\df{σ} \stackrel{Stokes}{=} \int\int_{M^+} \rot (0,0,y)\df{S} \]

En \ref{eqEjStokes} teníamos la parametrización, así que la aplicamos. Teniendo en cuenta la orientación, tenemos que cambiar el signo y nos queda 

\[ 
- \iint_D \pesc{(1,0,0),\left(\frac{x}{\sqrt{\ast}}, \frac{y}{\sqrt{\ast}}, 1}\right) \df{x}\df{y} 
= -\iint_D\frac{x}{\sqrt{1-x^2-y^2}} \d{x,y} 
\]

Viendo que estamos integrando una función impar en una región simétrica, la integral vale 0.

\paragraph{Resolución de $\int_{\partial M} z \dif x$}

Pasamos ahora a calcular la integral de $z$. Tenemos lo mismo que antes:

\begin{gather*}
 \int_{∂M^+}z\df{x} = \int_{∂M^+}(z,0,0)\df{σ} \stackrel{Stokes}{=} \iint_{M^+} \rot (z,0,0)\df{S} = \iint_{M^+}(0,1,0)\id{S}= \\
 - \iint_D \pesc{(0,1,0),\left(\frac{x}{\sqrt{\ast}}, \frac{y}{\sqrt{\ast}}, 1\right)}\id{x,y} 
= -\iint_D\frac{y}{\sqrt{1-x^2-y^2}} \id{x,y}
\end{gather*}

Vemos que esta integral es muy complicada y nos va a ganar, así que pasamos a tratar de integrar como una curva. Deberemos parametrizar la curva, encontrar la orientación correcta y aplicar la fórmula.

Empezamos con la parametrización:

\[ ∂M = \left\{ \begin{matrix}
x^2+y^2+z^2 = 1 \implies y + z^2 = 1 \implies z = \sqrt{1-y} \\
x^2 + y^2 = y \implies x = \pm \sqrt{y-y^2}
\end{matrix}\right. \]

Por lo tanto, podemos parametrizar en dos trozos

\begin{align*}
Γ_1\equiv &(\sqrt{y-y^2}, y, \sqrt{1-y});\;y∈[0,1] \\
Γ_2\equiv &(-\sqrt{y-y^2}, y, \sqrt{1-y});\;y∈[0,1] \\
\end{align*}

\easyimgw{imgs/OrientacionesEsferaCilindro}{Orientaciones de $Γ_1$ y $Γ_2$}{imgOrientacionesEsferaCilindro}{0.7}

Tal y como vemos en la figura \ref{imgOrientacionesEsferaCilindro}, la orientación es negativa en $Γ_1$ y positiva en $Γ_2$. Entonces

\[ \int_{∂M^+}(z,0,0)\df{σ} = \int_{Γ_1^+}(z,0,0)\dif{σ_1} +  \int_{Γ_2^+}(z,0,0)\dif{σ_2} \]

Operando con la primera integral:

\[  \int_{Γ_1^+}(z,0,0)\dif{σ_1} = - \int_0^1\pesc{(\sqrt{1-y},0,0),\left(\frac{1-2y}{2\sqrt{y-y^2}},\ast,\ast\right)}\dif y = 
-\int_0^1\frac{1-2y}{2\sqrt{y}}\dif y = \]

Separamos en dos sumandos

\[ = \int_0^1\sqrt{y}\d y - \int_0^1 \frac{1}{2\sqrt y}\dif y = \dotsb \]

Podríamos tomar otra parametrización alternativa usando coordenadas esféricas. La esfera queda determinada por la parametrización

\[ \left\{\begin{matrix}
x &=& \cos θ \sin φ \\
y &=& \sin θ \sin φ \\ 
z &=& \cos φ
\end{matrix}\right. \]

Añadiendo la restricción de $x^2+y^2=y$, nos queda que $\sin φ = \sin θ$. Entonces podemos seguir parametrizando 

\[\left. \begin{matrix}
x = \cos θ \sin θ \\
y = \sin^2 θ \\
 z = \sqrt{1-\sin^2 θ} = \abs{\cos θ}
\end{matrix} \right\} θ∈[0,π]
\]

y tenemos que

\[ σ_1(θ) = (\cos θ \sin θ, \sin^2 θ, \cos θ) \]

Por otra parte, si pusiésemos los límites de integración en la región D pasaría algo.
\end{example}

\section{Campos conservativos}

\todo[inline]{Revisar la sección/numeración de este capítulo}

\begin{defn}[Campo\IS conservativo] Consideramos $\vf$, un campo en $ℝ^N$. Se dice que $\vf$ es conservativo si y sólo si $∃\appl{V}{ℝ^N}{ℝ}$ tal que $F=\grad V$, donde $V$ es el \textbf{potencial}\index{Potencial} del campo $\vf$.
\end{defn}

\begin{theorem} Sea $\vf$ un campo $C^1$ en $ℝ^3$. Entonces $\vf $ es conservativo si y sólo si $\rot \vf = \vec{0}$.
\end{theorem}

\begin{proof}
\paragraph{Implicación a la derecha} Como $F=\grad V$, y $F∈C^1$, entonces $V∈C^2$. Calculamos ahora el rotacional:

\[ \rot\vf = \left|\begin{matrix}
\vec{i} & \vec{j} & \vec{k} \\
\pd{}{x} & \pd{}{y} & \pd{}{z} \\
F_1 & F_2 & F_3
\end{matrix}\right| = \left(\dpa{F_3}{y}-\dpa{F_2}{z}, \dpa{F_1}{z}-\dpa{F_3}{x}, \dpa{F_2}{x}-\dpa{F_1}{y} \right) \]

Sustituyendo con $F=\grad V$ veríamos que sale todo cero.

\paragraph{Implicación hacia la izquierda}

Supongamos $Γ$ una curva cerrada, que sea la frontera de una superficie $M$. Entonces

\[ \int_{M^+}\vf  \stackrel{Stokes}{=} \iint_M\rot \vf\dif S = 0 \]

Es decir, que la integral de $F$ sobre cualquier curva cerrada va a dar 0. Buscamos ahora un $V$ tal que $\grad V = \vf$.

Supongamos un punto cualquiera $(x,y,z)$: A ese punto podemos llegar a través de varias rectas paralelas a los ejes. Con esas rectas podríamos construir un camino, y entonces tendríamos que 

\todo[inline]{Poner dibujito aquí}

\[ \int_{Γ_1}\vf = \int_{Γ_2} \vf = \int_{Γ_3}\vf \]

ya que cogiendo dos pares $Γ_i$ y cambiando la orientación de uno de ellos construimos una curva cerrada.

Parametrizamos $Γ_1$:

\[ Γ_1 \equiv \{ (t,0,0)\,t∈[0,x]\} 
	\cup \{ (x,s,0)\,s∈[0,y]\}
	\cup \{ (x,y,r)\,r∈[0,z]\} \]
	
Entonces

\begin{align*}
\int_{Γ_1}\vf&= \int_0^x\pesc{(F_1(t,0,0),F_2(t,0,0), F_3(t,0,0),(1,0,0)}\dif t \\
&+\int_0^y\pesc{(F_1(x,s,0),F_2(x,s,0), F_3(x,s,0),(0,1,0)}\dif s \\
&+\int_0^x\pesc{(F_1(x,y,r),F_2(x,y,r), F_3(x,y,r),(1,0,0)}\dif r \\
&=\int_0^x F_1(t,0,0)\dif t + \int_0^y F_2(x,s,0)\dif s+ \int_0^z F_3(x,y,r)\dif r 
\end{align*}

De la misma forma, tendríamos

\begin{gather*}
\int_{Γ_2}\vf = \int_0^y F_2(0,t,0)\dif t + \int_0^z F_3(0,y,s)\dif s+ \int_0^x F_1(x,y,r)\dif r \\ 
\int_{Γ_3}\vf = \int_0^x F_1(t,0,0)\dif t + \int_0^z F_3(x,0,s)\dif s+ \int_0^y F_1(x,r,z)\dif r \\
\end{gather*}

Las tres integrales son iguales, así que podemos derivar con respecto de la que nos venga mejor para construir la función $V$, que es la que buscábamos.

\end{proof}

\subsection{Ejemplos}

\begin{example} Tenemos un cono de base $D$ y altura $h$, y buscamos integrar el campo $\vf(x,y,z)= (x,y,z)$. $Ω$ es el espacio del cono, y $∂Ω$, su superficie, se divide en la superficie lateral y la base.

\todo[inline]{Dibujito L1244}

Si escribimos el teorema de Gauss, tenemos que 

\[ \iiint_Ω \dv \vf \id{x,y,z} = \iint_{∂Ω^+} \vf \dif S \]

En este caso, tenemos la suerte de que $\dv \vf = 3$, y por lo tanto

\[ \iiint_Ω \dv \vf \id{x,y,z} = 3 \cdot \mathrm{Volumen}\,(Ω) \]

Por lo tanto, para hallar el volumen calculamos la integral sobre la superficie y dividimos entre tres.

La idea geométrica es que en la cara lateral, la componente normal de $\vf$ es 0. Lo vemos fácilmente sabiendo que la recta definida por el vector $(x,y,z)$ y que pasa por el origen (el vértice del cono), tiene exactamente la misma dirección de la generatriz. Entones,\textbf{ la integral sobre la cara lateral es 0} y por lo tanto podemos ignorarla. Nos centramos sólo en la integral de la base:

\[ \iint_{\mathrm{Base}}\vf \id S \]

Parametrizar $S$ es sencillo:

\[ S \equiv \{ (x,y,h)\tq (x,y)∈ D \} \]

Calculamos su vector normal:

\[ \left.\begin{matrix}
T_x = (1,0,0) \\
T_y = (0,1,0)
\end{matrix}\right\} T_x × T_y = (0,0,1) \]

y entonces

\[ \iint_{\mathrm{Base}}\vf \id S = \iint_D\pesc{(x,y,h),(0,0,1)} \id{x,y} = \iint_D h \id{x,y} = h \cdot \mathrm{Area}\,(D) \] 

Finalmente 

\[ \mathrm{Volumen}\,(Ω) = \frac{h \cdot \mathrm{Area}\,(D)}{3} \]
\end{example}

\begin{example}[Cálculo del campo eléctrico o gravitatorio]

La expresión del campo eléctrico es 

\[ \vf(\vx) = C \frac{\vx}{\norm{\vx}^3} \]

Calculando las derivadas parciales

\begin{align*}
\dpa{F_1}{x}&=\frac{y^2+z^2-2x^2}{(x^2+y^2+z^2)^{\frac{5}{2}}} \\
\dpa{F_2}{y}&=\frac{x^2+z^2-2y^2}{(x^2+y^2+z^2)^{\frac{5}{2}}} \\
\dpa{F_3}{z}&=\frac{x^2+y^2-2z^2}{(x^2+y^2+z^2)^{\frac{5}{2}}}
\end{align*}

nos queda que

\[ \dv\vf = 0 \]

Consideramos ahora una bola de radio $R$ centrada en el origen, es decir, $B_R(0,0,0)$. Integramos sobre su superficie:

\[ \iint\limits_{∂B_R^+} \vf \id{S} = 
\iint\limits_{x^2+y^2+z^2 = R^2} \left(\frac{x}{R^3},\frac{y}{R^3},\frac{z}{R^3} \right) \id S = \frac{1}{R^3} \iint\limits_{x^2+y^2+z^2 = R^2} (x,y,z)\id S = \]

Dado que integrar el vector es integrar su componente escalar, la integral nos queda

\[ = \frac{1}{R^3} \iint\limits_{x^2+y^2+z^2 = R^2} R\id{x,y} = \frac{1}{R^2}\cdot \mathrm{Area\; esfera} = 4π \]

Ahora bien, si no supiésemos ese argumento geométrico, empezaríamos parametrizando la esfera:

\[ Φ(θ,φ) =(R\cos θ \sin φ, R\sin θ\sin φ, R\cos φ);\, θ∈[0,2π], φ∈[0,π] \]

Calculamos los vectores tangentes y el normal:

\[ \left. \begin{matrix}
T_θ = (-R\sin θ \sin φ, R\cos θ \sin φ, 0) \\
T_φ = (R\cos θ\cos φ, R\sin θ \cos φ, -R\sin φ)
\end{matrix}\right\} T_θ×T_φ = -R\sin φ (R\cos θ, \]

La normal es interior, así que pagamos con un cambio de signo:

\begin{gather*} \iint\limits_{x^2+y^2+z^2 = R^2} \left(\frac{x}{R^3},\frac{y}{R^3},\frac{z}{R^3} \right) \id S = \\
- \int_0^{2π}\int_0^π\pesc{\left(\frac{R\cos θ \sin φ}{R^3},{R\sin θ \sin φ}{R^3},{R\cos φ}{R^3}\right), (,,)} = \dotsb \mathrm{Calculos\; aqui} \end{gather*}

y al final sale lo mismo que antes pero con cuentas mucho más desagrable.

¿Por qué el teorema de Gauss no funciona? La divergencia es 0 y la superficie es cerrada. Sin embargo, $\vf$ no es $C^1$ en el origen (no existe en ese punto) así que no podemos aplicarlo.

Pero, por otra parte, siempre hemos visto que la integral no se ve afectada por lo que ocurra en un punto. ¿Por qué no funciona el teorema de Gauss sólo por lo que pasa en el origen?

En realidad en el origen la divergencia vale $4π$ por un churrumillo muy raro (delta de algo) \todo{Dibujar el churrumillo}

Pero también podemos hacer un apaño. Consideramos un conjunto $Ω$, y una bola $B_ε(0,0,0)$. Entonces

\[ Ω_ε = Ω - B_ε \]

de tal forma que $\vf∈C^1$ en $Ω_ε$. Aplicando el teorema de Gauss:

\[ 0 = \iiint\limits_{Ω_ε}\dv \vf \id{x,y,z} = \iint\limits_{∂Ω_ε^+}\vf \id{S} \]

La frontera se divide en dos partes: $∂Ω_ε = ∂Ω + ∂B_ε^+$. Entonces

\[ 0 = \iint\limits_{∂Ω^+}\vf \id S - \iint\limits_{∂B_ε^+}\vf \id S \]

y por lo tanto, tenemos que 

\[ \iint\limits_{∂Ω^+}\vf \id{S} = 4π\; ∀Ω \tq 0 ∈ Ω \]

\end{example}


