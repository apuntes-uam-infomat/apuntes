\section{Lenguaje de las formas diferenciales}

\paragraph{0-formas}
\index{Formas\IS 0}

Son funciones escalares definidas en un abierto de $\real^n$
\[\appl{f}{\Omega\subset\real^N}{\real}\]

Operaciones habituales:
\begin{itemize}
\item Suma: sí
\item Producto: sí
\item Composiciones: no (porque no cuadran las dimensiones)
\end{itemize}

\paragraph{1-formas}
\index{Formas\IS 1}

Sea $\mathcal{C} = \{e_1,e_2,...,e_n\}$ la base canónica en $\real^N$.

Sea $L$ una aplicación lineal
\[\appl{L}{\real^N}{\real}\]

Que recordamos que cumplen:
\[ L(\gx+\gy) = L(\gx)+L(\gy); L(\lambda\gx) = \lambda L(\gx)\]

Definimos $\gy\in\real^N \leadsto \gy = \displaystyle\sum_1^n y_i e_i$, con lo que \[L(\gy) = \sum y_i L(e_i)\]

Entonces \[\left.\begin{array}{cc}
v_i = L(e_i)\\
y_i = P_i(\gy)
\end{array}\right\} \rightarrow L(\gy) = \sum_i v_iP_i(y)\]

Siendo $P_i$ las proyecciones, una base del espacio dual.

\textbf{Notación:}

$P_i \equiv dx_i$.

$dx_i[\gy] \equiv P_i(\gy) = y_i$

Entonces, dado un $\gv$ podemos construir 
\[L \equiv \sum_i^N v_idx_i\]

\[L[\gy] = \sum_i^N v_idx_i[\gy] = \sum_i^N v_iy_i\]

\begin{defn}[1-forma]
\[\omega(\gx)= \sum_1^N F_i(\gx) dx_i\]

\begin{itemize}
\item Se evalúa en $\gx\in\real$
\item Actúa sobre $\gy\in\real^N$ 
\end{itemize}

Es decir, \[\omega(\gx)[\gy] = \left(\sum F_i(\gx)dx_i\right)[\gy] = \sum F_i(\gx)dx_i[\gy] = \sum F_i(\gx)y_i\]
\end{defn}

Indicaremos con paréntesis el punto en el que estamos evaluando, y con corchetes el punto en el que estamso actuando.

\textbf{Operaciones:}
\begin{itemize}
\item Sumar: sí (lo razonable)
\item Multiplicar: por una función escalar sí está definida.
\end{itemize}


\paragraph{Ejemplo:}

Supongamos $f$ una función escalar (una 0-forma).

\[\grad f(\gx) = \left( \dpa{f}{x_i}(\gx)\right)\, i=1,...,N\]

Nos podemos construir una 1-forma desde el gradiente

\[\dpa{f}{x_i}(\gx)dx_i \]

A esta 1-forma en particular la llamaremos $df(\gx)$.

¿Utilidad? Ya la veremos, pero es una forma de escribir el producto escalar.
\[\pesc{\grad f(\gx),\gy} = df(\gx)[\gy]\]


\paragraph{2-formas}
\index{Formas\IS 2}

Punto de partida: Aplicaciones \textbf{bilineales alternadas}

\[\appl{\Phi}{\real^N\x\real^N}{\real}\]

Que cumplen \begin{itemize}
\item $\Phi([\gu,\gv]) = - \Phi([\gv,\gu]) \implies \Phi(\gu,\gu)=0$
\item $ \Phi([\gu+\gv,\gw]) = \Phi ([\gu,\gw]) + \Phi([u,w])$
\item$\Phi([\lambda \gu,\gv]) = \lambda \Phi([\gu,\gv])$
\end{itemize}

Consecuencias:

\begin{itemize}
\item $\Phi(\gor{r}, \gor{s}+\gor{t}) = \Phi(\gor{r}+\gor{s}) + \Phi(\gor{r}+\gor{t})$
\item $\Phi(\gu,\mu\gv) = \mu\Phi(\gu,\gv)$
\end{itemize}


\paragraph{Ejemplo} en $\real^3$ para facilitar las cuentas.

\[\Phi(\gu,\gv) = \Phi(u_1e_1+u_2e_2+u_3e_3,v_1e_1+v_2e_2+v_3e_3)\]
Aplicando las propiedades anteriores obtenemos:

\begin{gather*}
\overbrace{u_1v_1\Phi(e_1,e_1)}^{\equiv 0} + u_1v_2\Phi(e_1,e_2) + u_1v_3+\Phi(e_1,e_3)+\\
u_2v_1+\Phi(e_2,e_1)+u_2v_2+\Phi(e_2,e_2)+u_2v_3+\Phi(e_2,e_3)+\\
u_3v_1\Phi(e_3,e_1)+u_3v_2+\Phi(e_3,e_2)+u_3v_3+\Phi(e_3,e_3) = \\
\underbrace{(u_1v_2-u_2v_1)}_{\left|\begin{matrix}
u_1&u_2\\v_1&v_2
\end{matrix}\right|}\overbrace{\Phi(e_1,e_2)}^{C_1}+(u_1v_3-u_3v_1)\Phi(e_1,e_3)+(u_2v_3-u_3v_2)\Phi(e_2,e_3)
\end{gather*}

Hemos demostrado que \[\Phi(\gu,\gv) = C_1B_{12}(\gu,\gv) + C_2B_{13}(\gu,\gv) + C_3B_{23}(\gu,\gv)\]

\subparagraph{Notación:} $B_{ij} = dx_i\y dx_j$

\[dx\y dx_j [\gu,\gv] = \det \begin{pmatrix}
u_i&u_j\\v_i&v_j
\end{pmatrix} = \det \begin{pmatrix}
dx_i[\gu]&dx_j[\gu]\\dx_[\gv]&dx_j[\gv]
\end{pmatrix}\]

\begin{defn}[2-forma]
\[\beta = \sum_{i,j=1}^N F_i(\gx) dx_i\y dx_j\]
\begin{itemize}
\item Se evalúan en puntos $x\in\real^N$
\item Actúan sobre pares de vecotres $[\gu,\gv]\in\real^N\x\real^N$.
\end{itemize}

Es decir:

\[\beta(\gx)[\gu,\gv] = \sum F_{ij}(\gx) dx_i\y dx_j[\gu,\gv] = \sum F_{ij} \det \begin{pmatrix}
u_i&v_i\\u_j&v_j
\end{pmatrix}\]

\emph{Ojo} El cambio del orden (en el determiante)es aposta por la segunda propiedad de las 2 formas
\end{defn}


\subsubsection{K-Forma}

Vamos a dar una definición general de una k-forma.

Elementos básicos:
\[dx_{i_1} \y dx_{i_2}\y...\y dx_{i_k}[\gu^1,\gu^2,...,\gu^k] = \det\begin{pmatrix}
u_{i_1}^1 & ... & u_{i_k}^1\\
\vdots & \ddots & \vdots\\
u_{i_1}^k & ... & u_{i_k}^k
\end{pmatrix}\]

\paragraph{K-forma}

\[
\sum_{i_1,...,i_k=1}^N F_{i_1,...,i_k}(\gx)dx_{i_1} \y ... \y dx_{i_k}
\]

\begin{itemize}
\item Se evalúan en puntos $\gx\in\real^N$
\item Actúa sobre grupos de $K$ vectores.
\end{itemize}

\obs $i_j = i_s \implies dx_{i_j}\y dx_{i_s} = 0$

Esto nos dice que en $\real^N$, teniendo $K$-formas (con $K<N$) tenemos $\comb{N}{K}$ combinaciones distintas.

\obs Si $K>N$ y $\omega$ es una $k-forma \implies \omega \equiv 0$


\paragraph{Ejemplo:} En $\real^3$.

\begin{itemize}
\item 0-forma $\leadsto f(x,y,z) = 0$
\item 1-forma $\leadsto f_1(x,y,z)dx + f_2(x,y,z)dy + f_3(x,y,z)dz$
\item 2-forma $\leadsto g_1(x,y,z)\df{y,z} + g_2(x,y,z)dz\y dx + g_3(x,y,z)dx\y dy$
\item 3-formas $\leadsto h(x,y,z)\df{x,y,z}$
\end{itemize}

\index{Orden cíclico}
\emph{Ojo} Al cambio en la 2-forma, que es $dzdx$. Esto es para seguir el \textbf{orden cíclico} (por temas de la orientación). Esto es $x\to y \to z \to x$


\obs Las \textit{funciones escalares } las podemos interpretar como 0-formas y como 3-formas. Los \textit{campos vectoriales} los podemos interpretar como 1-formas y también como 2-formas.

\paragraph{Notación}
Para escribir un conjunto de subíndices $\{i_1,i_2,...,i_k\} \equiv I$

También acortaremos  $dx_{i_1} \y dx_{i_2} \y ... \y dx_{i_k} \equiv dx_I$. 

La definición quedaría $\displaystyle \sum_I F_I(\gx)dx_I$

\subsection{Operaciones}
Siempre se puede multiplicar por 0-formas y sumar (formas del mismo orden). Estas operaciones son triviales porque son operaciones internas.

Vamos a definir las operaciones externas:

\begin{defn}[Producto \IS exterior (de k-formas)]
Sea \[\omega = \sum_I F_I dx_I (k-forma\in\real^N)\]
\[\beta = \sum_J G_j dx_J (s-forma\in\real^N)\]

\[\omega\y\beta = \sum_{I,J} F_IG_J dx_I\y dx_J (k+s-forma)\]
\end{defn}

\obs Si $K+S>N \implies \omega\y\beta=0$


\paragraph{Ejemplo} $\real^3$

Sea \[\omega = f_1(x,y,z)dx + f_2(x,y,z)dy + f_3(x,y,z) dz\]
\[\beta= g_1(x,y,z)dx + g_2 (x,y,z) dy + f_3 (x,y,z) dz\]

Vamos a calcular $\omega\y\beta$

\[
\omega\y\beta  = f_1g_1dx\y dx + f_1g_2dx\y dy + f_1g_3dx\y dz + f_2g_1dy\y dx + f_2g_2dy\y dy + f_2g_3dy\y dz+ f_3g_1dz\y dx+f_3g_2dz\y dy+f_3g_3dz\y dz
\]
Tachamos los que sean 0 ($dx\y dx = 0$) y tenemos cuidado con el orden cíclico.

\[
(f_2g_3-f_3g_2)dy\y dz + (f_3g_1-f_1g_3)dz\y dx + (f_1g_2 - f_2g_1) dx \y dy
\]

Partiendo de 2 campos vectoriales que eran 1-formas hemos llegado a una 2-forma. 

\obs Acabamos de llegar al producto vectorial de $\real^3$ 

\[\overrightarrow{F}\x\overrightarrow{G} = 
\left((f_2g_3-f_3g_2),(f_3g_1-f_1g_3),(f_1g_2 - f_2g_1)\right)\]

\begin{defn}[Diferencial \IS exterior (de k-formas)]

\[d(\sum_I F_i(\gx)dx_I) = \sum_I \underbrace{dF_I}_{1-forma} \overbrace{\y}^{Prod.ext} \underbrace{dx_I}_{k-forma}\]
\end{defn}

\paragraph{Ejemplo} en $\real^3$

Sean $G = (g_1,g_2,g_3)$

$\omega = $

Vamos a calcular $d\omega$

\[d\omega dg_1\y dy\y dz + dg_2 \y dz\y dz + dg_3 \y dx \y dy = COMPLETAR = \dpa{g_1}{x} + \dpa{g_2}{y}+\dpa{g_3}{z}dx\y dy\y dz\] ¡Que es la divergencia!

\paragraph{Ejemplo 2}

\[d(F_1 dx + F_2dx+F_3dx)\]
\[ = dF_1\y dx + dF_2 \y dy + dF_3\y dz\]
\[\left(\dpa{F_1}{x}dx + \dpa{F_1}{x}dy + \dpa{F_1}{x}dz +
\dpa{F_2}{x}dx + \dpa{F_2}{x}dy + \dpa{F_2}{x}dz+
\dpa{F_3}{x}dx + \dpa{F_3}{x}dy + \dpa{F_3}{x}dz\right)\]
\[\left(\dpa{F_3}{y} - \dpa{F_2}{z} \right)dy\y dz + \left(\dpa{F_1}{z} - \dpa{F_3}{dx}\right)dz \y dx + \left(\dpa{F_2}{x} - \dpa{F_1}{y}\right) dx \y dy\]

Nos queda un campo de la forma:

\[\left(\left(\dpa{F_3}{y} - \dpa{F_2}{z} \right) ,\left(\dpa{F_1}{z} - \dpa{F_3}{x}\right),\left(\dpa{F_2}{x} - \dpa{F_1}{y}\right)\right)\]
Que es el rotacional.

\paragraph{Conclusión}
Tenemos un campo en $\real^3$ que podemos interpretar como 1-forma o como 2-forma. 

\begin{itemize}
\item La diferencial exterior de un campo interpretado como 1-forma tendremos la 2-forma asociada a la divergencia.

\item La diferencial exterior de un campo interpretado como 2-forma tendremos la 3-forma asociada al rotacional.

\item El producto exterior de 2 campos interpretados como 2-formas nos da el campo aosciado al producto vectorial (o algo parecido. Revisar)

\item ¿Cómo tengo que interpretar los campos para conseguir un producto escalar?

\end{itemize} 


\paragraph{Propiedades}
\begin{itemize}
\item $d(\omega + \beta) = d\omega + d\beta$
\item $\omega = \sum_I F_i dx_i$ una k-forma

$f\leadsto 0-forma$

$f\omega = \sum_I fF_Idx_I$

$d(f\omega) = \sum_I d(fF_I) \y dx_I$, donde $d(fF_I) = \sum_{j=1}^N \dpa{f}{x_j}dF_Ix_j + \sum_{j=1}^N f\dpa{F_I}{x_j}dx_j$

Es decir, tenemos:

Revisar
\begin{gather*}
\sum_{I,J} \dpa{f}{x_j} F_Idx_j\y dx_i + \sum_{I,J} f\dpa{F_I}{x_j}dx_j\y dx_I\\
 = \sum_{I,J} \underbrace{\dpa{f}{x_j}dx_j}_{\equiv df} \y F_Idx_I + \underbrace{\sum_{I}\underbrace{\left(\sum_J \dpa{F_I}{x_j}dx_j\right)}_{dF_I}) \y dx_I }_{d\omega}\\
 = \sum_I df\y F_I dx_I \y d\omega = \\
 \underbrace{df \y \sum_IF_Idx_I}_{\omega} + d\omega 
\end{gather*}
\paragraph{Conclusión} Hemos llegado a demostrar que el producto de la derivada es primero derivado por ...

\[d(f\omega) = df\y\omega + fd\omega\]
\begin{itemize}
\item df $\to$ 1-forma
\item $\omega \to $ k-forma
\item $f \to$ 0-forma
\item $d\omega$ k+1-forma.
\end{itemize}

\item $\omega  =\sum f_idx_i; \beta = \sum_j  g_jx_j$

\begin{gather*}
\dpa(\omega \y \beta) = d\left(\sum_{i,j=1}^N f_ig_jdx_y\y dx_j\right)\\
=\sum_{i,j=1}^N d(f_ig_j) \y dx_i\y dx_j\\
= \sum_{i,j=1}^N \left(\sum_{k=1}^N \dpa{(f_ig_j)}{x_k} dx_k\right)\y dx_i\y dx_j\\
=\sum_{i,j,k=1}^N \dpa{f_i}{x_k}g_j + f_i\dpa{g_j}{x_k}dx_k\y dx_i\y dx_j\\
= \sum_{i,j,k=1}^N \dpa{f_i}{x_k}g_j dx_k\y dx_i\y dx_j + \sum_{i,j,k=1}^N f_i\dpa{g_j}{x_k} dx_k\y dx_i\y dx_j\\
\text{Vamos a intentar encontrar }\omega,\beta\\
= \sum_{i,j,k=1}^N \dpa{f_i}{x_k}dx_k\y dx_i\y(g_jdx_j) + \sum_{i,j,k=1}^N\dpa{g_j}{x_k}dx_k\y f_idx_i \y dx_j \\
= \sum_{i,j} \dpa{f_j}{x_k}dx_k\y dx_i \y \underbrace{\left(\sum_j g_jdx_j\right)}_{\beta} + \sum_{j,k}  \dpa{g_j}{x_k}dx_k\y \underbrace{\left(\sum_i f_idx_i\right)}_{\omega} \y dx_j\\
= \underbrace{\sum_i \underbrace{\left(\sum_k \dpa{f_i}{x_k}dx_k\right)}_{df_i}\y dx_j\y \beta }_{d\omega\y \beta} - \underbrace{\sum_{j,k} \omega \y \dpa{g_j}{x_k}dx_k \y dx_j}_{d\beta}
\end{gather*}

Si $\omega,\beta$ 1-formas $\implies d(\omega \y \beta) = d\omega \y \beta - \omega \y d\beta$.

\item $\omega$ k-forma, $\beta$ s-forma.
Repitiendo las cuentas hasta cuando llegamos a intercambiar algo, que nos queda \[d\omega \y \beta + (-1)^k \omega \y d\beta\]
\index{Derivada de producto exterior de k-formas}

\item $\omega$ k-forma con coeficientes $C^2$. Entonces $d(d\omega) \equiv 0$.

La prueba es: no la pienso copiar ni de coña.
\end{itemize}


\subsubsection{Pull-back}
Entramos en como transformar k-formas de $\real^N$ a $\real^M$. Partimos de la base de que existe una transformación $T$ que va de $\real^N \to \real^M$, tal que $T(s)=x$ y buscamos una $T^{\ast} \tlq \real^M\to\real^N$.

En caso de 0-formas, el \emph{pull-back} es lo mismo que la composición.

Supongamos que tenemos una $\omega$ k-forma en $\real^N$.
Para construir $T^{\ast}$:

tenemos $\gor{s} \in \real^N, \gv_1,\gv_2,..,\gv_k$ vectores en $\real^N$.

Queremos definir $T^{\ast}\omega$ en términos de $T$ y $\omega$.

\[
(T^{\ast}\omega)(\underbrace{\gor{s}}_{\in\real^N}) [\underbrace{\gv_1}_{\in\real^N},...,\gv_k] = \omega(\underbrace{T(s)}_{\in\real^M}) [\underbrace{DT(s)\gv_j}_{\in\real^M}]
\]

\obs


\[\omega(T(s)) [DT(s)\gv]\equiv \sum f_i(T(s))dx_i[DT(s)\gv]\]

\[DT(s) = \begin{pmatrix}
\dpa{T_1}{s_1}(s) & ... & \dpa{T_1}{s_N}\\
\vdots & \ddots & \vdots \\
\dpa{T_M}{s_1} & \dots & \dpa{T_M}{s_N}
\end{pmatrix} \cdot \begin{pmatrix}
v_1\\
\downarrow\\
v_N
\end{pmatrix}\]

¿Que significa $dx_i[DT(s)\gv]$? Vamos a ver que pasa con el producto de una de las filas de la matriz.

\[dx_i[DT(s)\gv] = \dpa{T_i}{s_1}v_1 + ... + \dpa{T_i}{s_N}v_n\]
Podemos darnos cuenta de que $v_1 = ds_1[\gv]$. Con esto tenemos:
\[\underbrace{\left(\dpa{T_i}{s_1}ds_1 + ... + \dpa{T_i}{s_N}ds_N \right)}_{dT_i}[\gv]\]


\paragraph{Ejemplos}

\subparagraph{Ej 1)} Sea  $f(x)dx_i$ una 1-forma de la que queremos calcular el \emph{pull-back}

\[T^{\ast}(fdx_i)(s)[v] = f(T(s)) dx_1[DT(s)\gv]\]
Supongamos $f\equiv 1$
\[T^{\ast}(fdx_i)(s)[v] = dx_1[DT(s)\gv] = dT_i[\gv]\]
\textbf{Conclusión: } $T^{\ast}dx_i = dT_i$.

\subparagraph{Ej 2)} Sea $\omega = \sum_i f_i dx_i$ una 1-forma de la que queremos calcular el \emph{pull-back}

\[ 
 (T^{\ast} \omega )(s)[\gv]= \sum_i f_i(T(s))dx_i [DT(s)\gv] = \sum_if_i(T(s)) dT_i[\gv] = T^{\ast} (\sum_i f_idx_i) = \sum_i f_i \circ T dT_i
\]

\textbf{Conclusión: } $T^{\ast} (\sum_i f_idx_i) = \sum_i f_i \circ T dT_i$

\subparagraph{Ej 3)} ¿Cómo se comporta con el producto exterior? Vamos a trabajar con $f\equiv 1$.

\[
T^{\ast}(dx_i\y dx_j) [\gu,\gv] = dx_i\y dx_j \left[DT(s)[\gu], DT(s)[\gv]\right] =\]\[ \left|\begin{matrix}
dx_i[DT(s)\gu] & dx_j[DT(s)\gu] \\
dx_i[DT(s)\gv] & dx_j[DT(s)\gv] 
\end{matrix}\right| = \left| \begin{matrix}
dT_i[\gu] & dT_j[\gu]\\
dT_i[\gv] & dT_j[\gv]
\end{matrix}\right| = (1) = dT_i \y dT_j [\gu,\gv]
\]
(1) = Por las propiedades del producto exterior de 1-formas.

\textbf{Conclusión: } $T^{\ast} (dx_i \y dx_j) = dT_i \y dT_j$.

\subparagraph{Ej 4)} ¿Qué pasa cuando tenemos el producto de 2-formas generadas?

\[\omega = \sum f_idx_i\,;\,\beta=\sum g_jdx_j\]
Vamos con el $\y$.

\begin{gather*}
T^{\ast} (\omega \y \beta) (s) [\gu,\gv] = \sum_{i,j} f_i(T(s))g_j(T(s)) \underbrace{dx_i\y dx_j [DT(s)\gu, DT(s),\gv]}_{\text{Calculado justo arriba}}\\
= \sum_{i,j} (f_i\circ T) ... \\
= \left(\sum (f_i\circ T)dT_i\right)\y\left(\sum(g_j \circ T) dT_j\right) = T^{\ast}\omega \y T^{\ast}\beta
\end{gather*}

\textbf{Conclusión: } $T^{\ast}(\omega \y \beta) = T^{\ast}\omega \y T^{\ast}\beta$. 

Esto es válido para multindices $I$, es decir, para $\omega$ k-forma y $\beta$ s-forma.


\paragraph{Pull-back y diferencial exterior}
una vez visto cómo se comporta el \emph{pull-back} respecto del producto exterior vamos a ver como se comporta con respecto de la diferencial exterior.

\begin{gather*}
d(T^{\ast} \omega) = d\left(\sum_i f_i(T(s)) dx_i [DT(s)\gv]\right) = d\left(\sum (f_i \circ T)(s) dT_i[\gv]\right)\\
= \sum_i d(f_i\circ T)\y dT_i
\end{gather*}
\text{Vamos a ver que significa:  $d(f_i\circ T)$}

\begin{gather*}
d(f_i\circ T) =  \sum_k \dpa{f_i\circ T}{s_k} ds_k \\
\dpa{f_i\circ T}{s_k} = \sum_j \dpa{f_i}{x_j}(T(s))\cdot \dpa{x_j}{s_k}; \text{Donde }x_j = T_j(s)\\
\text{Juntando todo tenemos: }
\sum_{i,j,k} \dpa{f_i}{x_j}(t(s)) \dpa{T_j(s)}{s_k} ds_k \y dT_i = \sum_{i,j} \dpa{f_i}{s_j}(T(s)) dT_j\y dT_i = T^{\ast}(d(\sum f_i dx_i))
 \end{gather*}
 
 \textbf{Conclusión: } $d(T^{\ast}\omega) = T^{\ast}(d\omega)$
 
 
 \paragraph{Ejemplo concreto} ¡¡Por fin!! El cambio a coordenadas polares.
 
 $(\rho,\theta)$.
 
 
 \[T(\rho,\theta) =\left( T_1(\rho,\theta),T_2(\rho,\theta)\right) = (\rho cos\theta,\rho sen\theta)\]
 
 Vamos a calcular los pull-backs:
 
 \[T^{\ast}(dx) = ...\]
 
 \[T^{\ast} (dy) = dT_2 = d(\rho sen\theta) = \dpa{\rho sen\theta}{\rho} d\rho + \dpa{\rho sen\theta}{\theta}d\theta =sen\theta d\rho + \rho cos\theta d\theta\]
 
 \[T^{\ast}(dx\y dy) = ... = \rho d\rho \y d\theta\]
Completar.

\textbf{Interpretación:}

No sé dónde va esto.

Sea $ω$ una k-forma, donde \[ ω = \sum_I f_i \, dx_I \] donde $I$ son k-multiíndices. Entonces el pullback de $ω$ es 

\[ T^\ast ω = \sum_I f_i\circ T \, dT_I \]

Supongamos ahora que quiero calcular la diferencial exterior:

\begin{equation} d(T^\ast ω) = \sum_I d(\underbrace{
	(f_i\circ T)}_{\text{0-forma}}
	\underbrace{\,dT_I}_{\text{k-forma}})\label{eqSuputamadre} 
	\end{equation}

Si nos acordamos de la fórmula de Lebiniz \wtf tenemos que

\begin{gather*} 
d(fω) = df\y ω + fdω \\
d(ω\y β) = dω \y β + (-1)^kω\y dβ
\end{gather*}

Usamos la primera fórmula en \ref{eqSuputamadre}

\[  d(T^\ast ω) = \sum_I d(f_i\circ T) \y dT_i + d(f_I\circ T)\underbrace{d(d(T_i))}_{=0} = \sum_{I,K}\dpa{}{s_k} (f_I\circ T) ds_k\y dT_I \]

Usando la regla de la cadena en la derivada parcial

\[ \dpa{}{s_k} (f_I\circ T) = \sum_j\dpa{f}{x_j}\circ T \dpa{T_j}{s_k} \]

Poniendo de nuevo todo junto

\[ d(T^\ast ω) = \sum_{I, K, j} \dpa{f}{x_j}\circ T \dpa{T_j}{s_k} ds_k\y dT_I = \]

Sacando factor común

\[ = \sum_{I, j} \dpa{f}{x_j}\circ T\underbrace{\sum_K \dpa{T_j}{s_k} ds_k}_{dT_j}\y dT_I = \sum_{I,j} \dpa{f_i}{x_j}\circ T dT_j\y dT_I = T^\ast \left(\sum_{I,j} \dpa{f_i}{x_j}dx_j\y d_{x_I} \right) \]

Vemos que lo de dentro es lo mismo que $dω$ y por lo tanto

\[ d(T^\ast ω) T^\ast(dω)\]

Y hay una última propiedad (esto lo ordenas tú) que dice

\[ (T\circ S)^\ast ω = S^\ast(T^\ast ω) \]

\paragraph{Propiedades fundamentales de la operación}
\index{Propiedades! Pull-back}
\begin{enumerate}
\item $T^\ast f = f\circ T$, siendo $f$ una 0-forma.
\item $T^\ast(dω) = d(T^\ast ω)$. En particular $T^\ast (dx_I) = dT_I$.
\item $T^\ast(ω\y β)=(T^\ast ω) \y (T^\ast β)$.
\item $T^\ast(fω) = (f\circ T)(T^\ast ω) = (T^\ast f)(T^\ast ω)$
\item $T^\ast(ω+β)=T^\ast+T^\ast β$.
\end{enumerate}

\begin{example}

Tenemos una aplicación $φ(s,t) = (φ_1(s,t), φ_2(s,t))$. Calculamos su pullback y entonces

\[ φ^\ast(dx) = dφ_1 = \dpa{φ_1}{s}ds + \dpa{φ_1}{t}dt \]

y de la misma forma
\[ φ^\ast(dyx) = dφ_2 = \dpa{φ_2}{s}ds + \dpa{φ_2}{t}dt \]

\begin{gather*}
 φ^\ast(dx\y dy) = dφ_1\y dφ_2 =\left( \dpa{φ_1}{s}ds + \dpa{φ_1}{t}dt \right) \y\left( \dpa{φ_2}{s}ds + \dpa{φ_2}{t}dt \right) = \\
 0 + \dpa{φ_1}{s}\dpa{φ_2}{t}ds\y dt + \dpa{φ_1}{t}\dpa{φ_2}{s} dt\y ds + 0
 \end{gather*}
 
 Los diferenciales están cambiados de orden así que seguimos pagando con un cambio de signo:
 
 \[ \left(\dpa{φ_1}{s}\dpa{φ_2}{t} - \dpa{φ_1}{t}\dpa{φ_2}{s}\right)ds\y dt= \left|\begin{matrix}
 \dpa{φ_1}{s} & \dpa{φ_1}{t} \\
 \dpa{φ_2}{s} & \dpa{φ_2}{t} 
 \end{matrix}\right| =  \det \left(\dpa{φ}{s,t}\right) ds\y dt \]
 \end{example}
 
 \begin{example}
 Tenemos $β$, la 2-dorma asociada a un campo $\vf=(F_1, F_2, F_3)$:
 
 \[ β = F_1dy\y dz + F_2dz\y dx + F_3 dx\y dy \]
 
 Queremos calcular su pullback mediante una aplicación $\appl{Φ}{ℝ^2}{ℝ^3}$. Entonces
 
 \[ Φ^\ast β = g(s,t) ds\y dt \]
 
 ¿Quién es $g$? Calculamos el pullback:
 
 \[ φ^\ast β = F_1\circ Φ\, dΦ_2\y dΦ_3 + F_2\circ Φ\, dΦ_3 \y dΦ-2 + F_3\circ Φ\, dΦ_1\y dΦ_2 \]
 
 Calculamos los distintos diferenciales, que no pienso copiarlos porque no los ha puesto él en la pizarra, y los pinchamos en el morcillo ese. Y operamos. Y yo no voy a operar. Al final sale todo y queda lo siguiente
 
 \[ =\pesc{\vf\circ Φ, \left(\dpa{Φ_1}{s}, \dpa{Φ_2}{s}, \dpa{Φ_3}{s}\right) × \left(\dpa{Φ_1}{t}, \dpa{Φ_2}{t}, \dpa{Φ_3}{t}\right)} ds\y dt \]
 
 Sin embargo, el producto vectorial de esos dos vectores parece el producto vectorial de $T_s×T_x$, el factor que teníamos que poner para integrar un campo en una superficie. 
 \end{example}
 
 \begin{example}
 
 Tenemos una 1-forma asociada a $\vf$:
 
 
 
 \[ ω = \sum_i F_i\,dx_i \]
 
 y una aplicación (curva) $\appl{σ}{ℝ}{ℝ^n}$. Entonces $σ^\ast ω$ es de la forma $g(t)\,dt$. 
 
 Entonces
 
 \begin{gather*} (σ^\ast ω)(t)[λ] = \sum_i F_i(σ(t)) dx_i[Dσ(t)λ] = \sum_i F_i(σ(t)) dx_i[σ_1'(t)λ, \dotsc, σ_N'(t)λ = \\
 = \sum_i F_i(σ(t)) σ_i'(t) dt[λ] = \\
 = \pesc{\vf \circ σ, σ'} dt [λ]
 \end{gather*}
 
 Que, oh, sorpresa de nuevo, es lo que aparece cuando integrábamos un campo sobre una curva. 
 \end{example}
 
 Al final, querremos integrar k-formas en $ℝ^N$ sobre variedades de dimensión $k$. 
 
 \subsection{Integración de formas diferenciales}

Vamos a partir de Ω abierto de $\real^N$. 

Sea $\omega$ n-forma definida en un entorno de Ω, es decir $\omega = f(\gx) \underbrace{\df{x_1,...,x_n}}_{\text{Elemento de volumen}}$

\begin{defn}[Integración\IS n-forma en $\real^N$.]
\[
\int_Ω\omega = \inf f(\gx) dx_1...dx_N
\]
\end{defn}

\obs Supongamos que tenemos una $\appl{\Phi}{\real^K}{\real^N}$, tal que $\Phi(\gor{s}) = \gx$.

Para que lo de la derecha sea una variedad tenemos que $\Phi$ tiene que ser regular, homeomorfismo y rango máximo.

Supongamos $\omega \in \real^N$, con $\omega$ una x-forma.

¿Qué pasaría si queremos integrar $T^{\ast}\omega$?

Si queremos integrar $\pb{\omega}$ en $\real^k, \omega$ tiene que ser una k-forma para poder aplicar la definición.


\begin{example}
Variedad 1-dimensional.

$\appl{\sigma}{\real}{\real^3}$

Sea $\omega = f_1(x,y,z)dx + f_2(x,y,z)dy + f_3(x,y,z)dz$

Tenemos que $\sigma^{\ast}\omega = \pesc{\overrightarrow{F}\circ \sigma,\sigma'}dt$

La integral quedaría:

\[\int_{\sigma(I)} = \int_I  \pesc{\overrightarrow{F}\circ \sigma,\sigma'}dt 
\]

Integrar 1-forma sobre la variedad 1-dimensional es integrar el trabajo del campo $\overrightarrow{F}$ a lo largo de $\sigma(I)$.
\end{example}

\begin{example}
Una variedad de dimensión 2 en $\real^3$.

$\appl{\Phi}{\real^2}{\real^3}$, con $\Phi(s,t) =  (x,y,z)$.

$\beta$ 2-forma. 
\[\beta = G_1(x,y,z) \df{y,z} + G_2(x,y,z) \df{z,x} + G_3(x,y,z) \df{x,y}\]

El pullback (calculado anteriormente es)

\[\Phi^{\ast}\beta = \pesc{\overrightarrow{G}\circ\Phi,\Phi_s\x\Phi_t}\df{s,t}\]

\[\int_{\Phi(D)}ß = \int \int_D \pesc{\overrightarrow{G}\circ\Phi,\Phi_s\x\Phi_t}dsdt\]

La integral sería:


Es decir, integrar una 2-forma sobre $\Phi(D)$ es integrar el \textbf{flujo} del campo $\overrightarrow{G}$ a través de $\Phi(D)$
\end{example}

\paragraph{Caso general}

Sea $M$ una variedad de dimensión k en $\real^N$.

Supongamos que $(D,\Phi)$ una carta local, es decir:
$\appl{\Phi}{D\subset\real^K}{\real^N}, \Phi(D)\subset M; \Phi$ parametrización. ($\Phi(\gor{t}) = \gx$)

Sea \[\omega = \sum_I f_Idx_I; \, \, \, I=\{i_1,i_2,...,i_k\}\]

Por definición:

\[
\int_{\Phi(d) \omega} = \int_D \Phi^{\ast}\omega
\]

El pull-back nos va a dar una k-forma definida en $\real^k$.

\[
\Phi^{\ast}\omega =g(\gor{t})\df{t_1,t_2,...,t_k}
\]

Aplicando esto:
\[
\int_{\Phi(d) \omega} = \int_D g(\gor{t})\df{t_1,t_2,...,t_k}
\]

Vamos a identificar la función $g$ remangándonos y haciendo cuentas:

\[
\Phi^{\ast} \omega = \sum_I f_I\circ\Phi d\Phi_I = 
\]

Vamos a fijarnos en 
$d\Phi_I = \df{\Phi_{i_1},...,\Phi_{i_k}}$, que va a actuar sobre k-vectores, es decir:

\[
d\Phi_I[\gv_1,...,\gv_k] = \df{\Phi_{i_1},...,\Phi_{i_k}} [\gv_1,...,\gv_k] = \det \begin{pmatrix}
d\Phi_{i_1}[\gv_1] &\cdots& d\Phi_{i_1}[\gv_k] \\
\vdots & \ddots & \vdots\\
d\Phi_{i_k}[\gv_1] & \cdots & d\Phi_{i_k}[\gv_k] \\
\end{pmatrix}
\]

Vamos a desarrollar 1 de los elementos de la matriz (escribiendo $\gw$ para generalizar a cualquiera de los vectores sobre los que actúa):
\[
d\Phi_{i_1}[\gw] = \sum_{j=1}^k \left(\dpa{\Phi_{i_1}}{t_j} dt_j\right)[\gw] = \sum_{j=1}^k \dpa{\Phi_{i_1}}{t_j}w_j = \pesc{\grad \Phi_{i_1},\gw}
\]

Aplicando esto:

\[d\Phi_I[\gv_1,...,\gv_k] =  \det \begin{pmatrix}
\pesc{\grad \Phi_{i_1},\gv_1} &\cdots&  \pesc{\grad \Phi_{i_1},\gv_k}\\
\vdots & \ddots & \vdots\\
\pesc{\grad \Phi_{i_k},\gv_1} & \cdots & \pesc{\grad \Phi_{i_k},\gv_k} \\
\end{pmatrix} = \det \left(\begin{pmatrix}
\grad \Phi_{i_1} \rightarrow \\
...\\
\grad \Phi_{i_k} \rightarrow 
\end{pmatrix}
\begin{pmatrix}
\gv_1 & ... & \gv_k\\
\downarrow & ... & \downarrow
\end{pmatrix}\right) =\]
\[ \det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix} \cdot \det 
\begin{pmatrix}
\gv_1 & ... & \gv_k\\
\downarrow & ... & \downarrow
\end{pmatrix} = (1) =
\det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix} \df{t_1,t_2,...,t_k}[\gv_1,\gv_2,...,\gv_k]
\]

(1): Aplicando que $dt_1(\gv)$ es la primera coordenada del vector $\gv$. Un paso intermedio es \[\det \begin{pmatrix}
dt_1(\gv_1) & ... & dt_1(\gv_k)\\
\vdots & \ddots & \vdots\\
dt_k(\gv_1) & ... & dt_k(\gv_k)
\end{pmatrix}\]

\textbf{Conclusión:}
\[
\Phi^{\ast} \omega = \sum_I f_I\circ\Phi d\Phi_I =
\overbrace{\sum_I f_i\circ\Phi 
\det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix}}^{\text{Esta es la g que buscamos}} \df{t_1,...,t_k}
\]

Aplicando a una integral:

\[\int_{\Phi(D)} \omega = \int_D \sum_I f_i\circ\Phi 
\det\begin{pmatrix}
\grad \Phi_{i_1} \longrightarrow \\
...\\
\grad \Phi_{i_k} \longrightarrow 
\end{pmatrix}dt_1dt_2...dt_k\]


\subsubsection{Teoremas de Green, divergencia y Stokes en términos de formas diferenciales}
\paragraph{Caso Modelo:}

Vamos a trabajar con el cubo unidad.

\subparagraph{$\real^2$} $\mathcal{Q} = [0,1]\x[0,1]$

\textbf{Notación} $I_{ij}$, donde $i$ es la variable fija (0=x,1=y) y $j$ el valor que toma la variable fija.
\[\begin{array}{cc}
I_{11} =\{(1,y),y\in[0,1]\}&\text{ Orientación: }\, +\\
I_{21} =\{(x,1),x\in[0,1]\}&\text{ Orientación: }\, -\\
I_{10} =\{(0,y),y\in[0,1]\}&\text{ Orientación: }\, -\\
I_{20} =\{(x,0),x\in[0,1]\}&\text{ Orientación: }\, +\\
\end{array}
\]
La orientación está calculada con el sistema de "me coloco en la frontera en la dirección en la que nos movemos. Si la mano izquierda estirada apunta hacia el interior $\implies +$, si apunta hacia fuera $\implies -$.

Sea $\omega = f(x,y)dx + g(x,y)dy$

\[d\omega= \dpa{f}{y}\df{y,x} + \dpa{g}{x}\df{x,y} = \left(\dpa{g}{x} - \dpa{f}{y}\right) \df{x,y}\]

Consideramos $\mathcal{C}^+$ es la frontera de $\mathcal{Q}$ orientada positivamente y vamos a intentar calcular: $\displaystyle \int_{C^+}\omega$.

\[
\int_{C^+} \omega = \int_{I_11}\omega - \int_{I_20}\omega - \int_{I_21}\omega + \int_{I_10}\omega = \int_0^1 g(1,y)dy + \int_0^1 f(x,0)dx - \int_0^1f(x,1)dx - \int_0^1 g(0,y)dy = \int_0^1 g(1,y)-g(0,y)dy - \int_0^1f(x,1)-f(x,0)dx = \int_0^1\int_0^1 \dpa{g}{x}(x,y)dxdy - \int_0^1\int_0^1 \dpa{f}{x}(x,y)dydx = \int\int_Q \dpa{g}{x} - \dpa{f}{y} dxdy = \int\int_Q d\omega
\]

\textbf{Conclusión: } $\displaystyle\int_{C^{+}} \omega = \int\int_Q d\omega$. Esto escrito en términos de cálculo II es: $\displaystyle\int_{C^{+}}(P,Q) = \int\int_Q \dpa{Q}{x} - \dpa{P}{y}$, que es el teorema de Green. \index{ Teorema de integración \IS Green formas diferenciales}

\subparagraph{$\real^3$}
Ahora vamos a hacer algo parecido en $\real^3$, sea $Q=[0,1]\x[0,1]\x[0,1]$. (Trabajando con la normal exterior para las orientaciones)

Vamos a distinguir las caras con la notación anterior:


\[\begin{array}{cc}
I_{10} = \{(0,y,z), y\in[0,1],z\in[0,1]\} & \text{ Orientación: -} \\
I_{11} = \{(1,y,z), y\in[0,1],z\in[0,1]\} & \text{ Orientación: +} \\
I_{20} = \{(x,0,z), x\in[0,1],z\in[0,1]\} & \text{ Orientación: +} \\
I_{21} = \{(x,1,z), x\in[0,1],z\in[0,1]\} & \text{ Orientación: -} \\
I_{30} = \{(x,y,0), x\in[0,1],y\in[0,1]\} & \text{ Orientación: -} \\
I_{31} = \{(x,y,1), x\in[0,1],y\in[0,1]\} & \text{ Orientación: +} \\
\end{array}
\]

Las orientaciones están calculadas (según el primer caso) \[\left.\begin{array}{cc}
T_y = (0,1,0)\\T_z=(0,0,1)\end{array}\right\}\implies T_y\x T_z = (1,0,0)\] Mirando en el dibujo, identificamos que la cara en la que estamos trabajando (en este caso la de detrás) y comprobamos que apunta hacia dentro del cubo (dirección contraria a la normal exterior), y concluimos orientación negativa.

Repitiendo el proceso llegamos a la conclusión de: \[
\begin{array}{cc}
T_y\x T_z &= (0,0,1)\\
T_x\x T_z &= (0,-1,0)\\
T_x\x T_y &= (1,0,0)
\end{array}\]

\textit{Truquillo para orientaciones} Si la suma de los subíndices es par $\implies +$, si por el contrario, es impar $\implies -$. Detrás de esta idea hay un teorema que no vamos a ver. Además hay que tener cuidado con el orden en el que se hacen las cosas y se escriben los vectores.

Sea
\[\omega = F_1(x,y,z)\df{y,z}+F_2(x,y,z)\df{y,x}+F_3(x,y,z)\df{x,y}\]
 
\[d\omega = \dpa{F_1}{x}\df{x,y,z} + \dpa{F_2}{y}\df{y,x,z} + \dpa{F_3}{z}\df{z,x,y} = \left(\dpa{F_1}{x}+\dpa{F_2}{y} + \dpa{F_3}{z}\right) \df{x,y,z}
\]

Vamos a calcular \[\int_{dQ^+} \omega\] siendo $dQ^+$ la frontera del cubo unidad.

\[\int_{Q^+} \omega =\underbrace{ -\int_{I_{10}} \omega +  \int_{I_{11}} \omega}_{(1)} + \int_{I_{20}} \omega -  \int_{I_{21}} \omega  -\int_{I_{30}} \omega +  \int_{I_{31}} \omega\]

\[
(1) = \int_0^1\int_0^1\Phi^{\ast}_{11}\omega - \int_0^1\int_0^1 \Phi^{\ast}_{10} \omega = (2) =\]
\[
 \underbrace{\int_0^1\int_0^1 \pesc{\overrightarrow{F}\circ \Phi_{11},(1,0,0)}dydz}_{F_1(\Phi_{11})(y,z)}
 - \underbrace{ \int_0^1\int_0^1 \pesc{\overrightarrow{F}\circ \Phi_{10},(1,0,0)}dydz}_{F_1(\Phi_{10})(y,z)}
\]
(2) = Aplicando un cáculo realizado anteriormente ¿Cuándo? no se...
\index{Teorema! de la divergencia formas diferenciales}
\[
= \int_0^1\int_0^1 F_1(\Phi_{11})(y,z)-F_1(\Phi_{10})(y,z)dydz = \int\int\int_Q \dpa{F_1}{x}dxdydz
\]

Aplicando las mismas cuentas con las que faltan llegamos al teorema de la divergencia para el cubo.


\paragraph{Conclusión}
\[\int_{dQ^+} \omega = \int_{Q}d\omega\]
Ideas que en $\real^2$ se traduce en el Teorema de Green y que en $\real^3$ se traduce en el Teorema de la divergencia. ¿Dónde queda el Teorema de Stokes? Vamos ahora a encontrarlo.


%\index{Teorema! de Stokes formas diferenciales.}

\todo{Completar}

$\appl{\Phi}{Q}{\Phi(Q)\subset\real^n}$. Sea $\omega$ una k-forma en $\real^n$.

Queremos calcular \[
\int_{\Phi(Q)} d\omega = \int_Q \Phi^{\ast}d\omega) = \int_Q d(\Phi^{\ast}\omega) = \int_{dQ^{+}} \Phi^{\ast}\omega
\]

Donde $dQ^{+}$ es la frontera del cubo $Q$ orientada debidamente. El último paso es aplicar el teorma anterior.

Sea $\appl{\sigma}{I}{dQ}$. Entonces, $\appl{\Phi\circ\sigma}{I}{\Gamma}$, siendo $\Gamma$ la frontera de $\Phi(Q)$.

Aplicando esto a la integral que estamos calculando:

\[
\int_{\Phi(dQ^{+})} \omega \equiv \int{\Phi\circ\sigma(I)} = \int_I (\Phi\circ\sigma)^{\ast} \omega = \int_I \sigma^{\ast}\left(\Phi^{\ast}(\omega)\right) = \int_{\sigma(I)} \Phi^{\ast}\omega = \int_{dQ}\Phi^{\ast}\omega
\]
\todo{L850}
\textbf{Conclusión:} $\displaystyle \int_{dQ^+} \Phi^{\ast} \omega = \int_{\Phi(dQ^+)} \omega$

\todo{L852}

A nosotros lo que nos gustaría sería que $\displaystyle\int_{\Phi(dQ^+)} \omega = \int_{d(\Phi^{\ast}(Q))} \omega$, es decir, que la imagen de la frontera sea la frontera de la imagen. Esto no es inmediato.

Esto se ve claramente con el cambio a coordenadas polares.

\[
\int_{\Phi(Q_1)}d\omega + \int_{\Phi(Q_2)}d\omega=
\int_{\Phi(\delta Q_1)^+}\omega+\int_{\Phi(\delta Q_2)^+}\omega
\]

\textbf{Conclusión (Teorema de Stokes):} $\displaystyle\int_M d\omega = \int_{\delta M^+}\omega$, si M se puede descomponer como unión de celdas con interior disjunto.

\obs Es un resultado general, vale también para más dimensiones. Por ejemplo, en el caso $\displaystyle \int_{\Phi(Q)} d\omega = \int_{\delta(\Phi(Q))} \omega$, tenemos
\begin{itemize}
\item $\Phi(Q)$ es una superficie
\item $\omega$ es una 1-forma.
\item $d\omega$ es una 2-forma, que además (como ya hemos visto anteriormente (ref)) $d\omega = \left(\dpa{F_3}{y}-\dpa{F_2}{z}\right) \df{y,z} + 
\left(\dpa{F_1}{z}-\dpa{F_3}{x}\right) \df{z,x} + 
\left(\dpa{F_2}{x}-\dpa{F_1}{y}\right) \df{x,y}$
\item $\delta\Phi(Q)$ es ... 
\end{itemize} 
Con el segundo punto de la lista, podemos escribir la fórmula de la siguiente manera:
\index{Teorema! Stokes en formas diferenciales}
\[
\int_{\Gamma^+} \overrightarrow{F} \equiv \int \int_S rot \overrightarrow{F}
\]

\subsubsection{Frontera de una superficie}
Vamos a intentar definir en serio la frontera de objetos en $\real^3$, que es algo que necesitamos tener realmente muy claro, por ejemplo:

\todo{L880}

Hace un tiempo, cuando definíamos una subvariedad, demostramos la existencia de un difeomorfismo que "aplanaba un trozo" de subvariedad. 

La frontera de una superficie es el conjunto de los puntos (llamados en clase de Tipo 2) que al aplanar nos quedan en la frontera de un objeto de dimensión 2.

Aquí iría la prueba que haremos la semana que viene. Ahora vamos a ver aplicaciones:


\subsubsection{Aplicaciones de los teoremas de integración}
\paragraph{Teorema de Green}
Sea $\omega$ 1-forma en $\real^2$, y $D$ un conjunto cerrado con frontera orientable.

\[
\int_{\delta D^+} Pdx+Qdy = \int \int_D d(Pdx+Qdy) = \int\int_D \dpa{Q}{x} - \dpa{Q}{y}\df{x,y}
\]

\obs Podemos elegir $(P,Q)$ de tal modo que $\displaystyle \dpa{Q}{x}-\dpa{Q}{y} = 1$.

Entonces: Área (D) = $\displaystyle_{\delta D^+} (P,Q)d\sigma$

\subparagraph{Aplicación} Área de la hoja folium de Descartes:

\[x^3+y^3 = mxy\]


\easyimgw{imgs/FoliumDescartes.png}{Folium de Descartes}{lblFolium}{0.3}

Vamos a parametrizarla, siguiendo la indicación: $t = \frac{y}{x}$

Se deja como ejercicio para el lector, llegar a la fórmula:

\[\begin{array}{cc}
x&=\displaystyle\frac{mt}{1+t^3}\\
y= xt &= \displaystyle\frac{mt^2}{1+t^3}
\end{array}\]

Quedando por definir que valores toman los parámetros. En este caso es $(0,\infty)$. Estos valores parametrizan la región cerrada. 

Podríamos plantearnos para qué valores de $t$ que recorren las ramas que se van a infinito. En $t=-1$, se va a infinito, entonces una de las ramas será $t\in(-1,0)$ y la otra será $t\in(-\infty,-1)$.

¿Que orientación nos da esta parametrización?

La idea es ver el vector tangente en el 0. Si es horizontal empezaremos por la rama de abajo. Si es vertical, empezaremos por la rama de arriba

\[\sigma'(t) = \left(\frac{m(1+t^3)-3mt^3}{(1+t^3)^2},\frac{2mt(1+t^3) - 3mt^4}{(1+t^3)^2}\right)\]

Podemos comprobar que $\sigma'(t) \convs[][t\to 0^+] (m,0)$. Además, $\sigma'(t) \convs[][t\to\infty](0,m)$, quedando una orientación positiva.

Vamos a calcular el área (se deja como ejercicio para el lector la comprobación porque Azorero no está muy convencido de esto)

\[
A(D) = \int_{\delta D^+} (0,x)d\sigma = \int_0^{+\infty} \pesc{\left(0,\frac{mt}{1+t^3}\right), \left(\ast,\frac{2mt(1+t^3)-3mt^4}{(1+t^3)^2}\right)}dt = ...
\]

Wolfram dice que el resultado es $\frac{m^2}{12}$


\paragraph{Ejercicio para el lector} Hacer lo mismo con la curva $x^4+y^4=4xy$. El área de la hoja contenida en el primer cuadrante.
%\easyimgw{imgs/FoliumALaCuarta.png}{$x^4+y^4=4xy$}{lblFoliumALaCuarta}{0.3}

\paragraph{Teorema de Stokes en $\real^3$}

El teorema decía: \[
\int_{\Gamma^+}\overrightarrow{F}d\sigma = \int \int_{S^+} rot\overrightarrow{F} dS
\]
Siendo $S$ la superficie, $\Gamma$ la ¿frontera?, tomando la orientación positiva con la normal exterior.

\subparagraph{Ejemplo}
\[
\left.\begin{array}{cc}
z=x^2+y^2\\
z=mx \end{array} \right\} \equiv \Gamma
\]

Queremos calcular $\displaystyle \int_{\Gamma}y dz$

Esto es lo mismo que calcular la integral del campo $(0,0,y)$.

El primer paso es \textbf{parametrizar} $\Gamma$

Tenemos que la proyección en el plano xy es \[mx=x^2+y^2 \equiv \left(x-\frac{m}{2}\right)^2 + y^2 = \frac{m^2}{4}\]

Viendo los cuadrados lo lógico es pensar en utilizar polares.

Llamando $x=rsen(\theta),y=rsen(\theta)$ tenemos:
\[\sigma(\theta) = \left(mcos^2(\theta),mcos(\theta)sen(\theta),m^2cos^2(\theta)\right)\,\,\,\theta\in\left(\frac{-\pi}{2},\frac{\pi}{2}\right)\]

Calculamos el vector tangente para ver en que orientación recorre la curva esta parametrización:
\[\sigma'(\theta) = ()\]
\[\sigma'(0) = (0,m,0)\]
Suponemos (porque no me lo dicen, que $m>0$) y nos da la orientación. Como en el enunciado no nos hablan de hacerlo con ninguna orientación, la integral que calculemos será de acuerdo con esta orientación.

Vamos con la integral:

\[\int_{\Gamma}(0,0,y) d\sigma =\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}}\pesc{\underbrace{\left(0,0,mcos(\theta)sen(\theta)\right)}_{\overrightarrow{F}(\sigma(\theta))},\ast} d\theta\]

Como camino alternativo a la fórmula, podemos aplicar el teorema:

\[ = \int\int_{D^+}  rot(0,0,y)dS\] Siendo el vector normal el que tenga la tercera componente positiva (razonando geométricamente).

Para calcular el rotacional necesitamos:$ =\left|\begin{matrix}
i&j&k\\dx&dy&dz\\0&0&y
\end{matrix}\right| = (1,0,0)$.

Utilizamos la parametrización: $S = (x,y,mx), x,y\in C$
\[ = \int\int_D^+ rot(0,0,y)dS = \int \int_C \pesc{(1,0,0) ,T_x\x T_y} dxdy= (1) =\]
\[ \int \int_C \pesc{(1,0,0),(-m,0,1)} = \int\int -m dxdy = -m \cdot \, Area (C) = -m\frac{m^2}{4}\pi \]
$(1): T_x = (1,0,m); T_y = (0,1,0) $. Otra cosa aplicada es que el vector normal $(-m,0,1)$ como la tercera componente es positiva, tenemos que esta parametrización induce la orientación positiva.